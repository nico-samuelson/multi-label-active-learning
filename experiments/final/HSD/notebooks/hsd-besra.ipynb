{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449d8668",
   "metadata": {
    "papermill": {
     "duration": 0.011397,
     "end_time": "2025-06-08T08:00:06.241883",
     "exception": false,
     "start_time": "2025-06-08T08:00:06.230486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b5bc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:06.263374Z",
     "iopub.status.busy": "2025-06-08T08:00:06.263170Z",
     "iopub.status.idle": "2025-06-08T08:00:29.075942Z",
     "shell.execute_reply": "2025-06-08T08:00:29.075028Z"
    },
    "papermill": {
     "duration": 22.82519,
     "end_time": "2025-06-08T08:00:29.077597",
     "exception": false,
     "start_time": "2025-06-08T08:00:06.252407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abf16f",
   "metadata": {
    "papermill": {
     "duration": 0.010224,
     "end_time": "2025-06-08T08:00:29.098903",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.088679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348feb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.121174Z",
     "iopub.status.busy": "2025-06-08T08:00:29.120660Z",
     "iopub.status.idle": "2025-06-08T08:00:29.123898Z",
     "shell.execute_reply": "2025-06-08T08:00:29.123261Z"
    },
    "papermill": {
     "duration": 0.015826,
     "end_time": "2025-06-08T08:00:29.125027",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.109201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdec8b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.146404Z",
     "iopub.status.busy": "2025-06-08T08:00:29.146195Z",
     "iopub.status.idle": "2025-06-08T08:00:29.149763Z",
     "shell.execute_reply": "2025-06-08T08:00:29.148978Z"
    },
    "papermill": {
     "duration": 0.015686,
     "end_time": "2025-06-08T08:00:29.151013",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.135327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167884cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.172654Z",
     "iopub.status.busy": "2025-06-08T08:00:29.172453Z",
     "iopub.status.idle": "2025-06-08T08:00:29.180808Z",
     "shell.execute_reply": "2025-06-08T08:00:29.180216Z"
    },
    "papermill": {
     "duration": 0.020452,
     "end_time": "2025-06-08T08:00:29.182010",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.161558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319e1f2",
   "metadata": {
    "papermill": {
     "duration": 0.013522,
     "end_time": "2025-06-08T08:00:29.206313",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.192791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae79d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.227653Z",
     "iopub.status.busy": "2025-06-08T08:00:29.227451Z",
     "iopub.status.idle": "2025-06-08T08:00:29.281187Z",
     "shell.execute_reply": "2025-06-08T08:00:29.279815Z"
    },
    "papermill": {
     "duration": 0.066286,
     "end_time": "2025-06-08T08:00:29.282883",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.216597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-besra'\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "sequence_length = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d5a1b",
   "metadata": {
    "papermill": {
     "duration": 0.010474,
     "end_time": "2025-06-08T08:00:29.303850",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.293376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af479d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.325807Z",
     "iopub.status.busy": "2025-06-08T08:00:29.325520Z",
     "iopub.status.idle": "2025-06-08T08:00:29.471053Z",
     "shell.execute_reply": "2025-06-08T08:00:29.469815Z"
    },
    "papermill": {
     "duration": 0.158451,
     "end_time": "2025-06-08T08:00:29.472682",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.314231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcfcbd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.504420Z",
     "iopub.status.busy": "2025-06-08T08:00:29.504167Z",
     "iopub.status.idle": "2025-06-08T08:00:29.516050Z",
     "shell.execute_reply": "2025-06-08T08:00:29.515193Z"
    },
    "papermill": {
     "duration": 0.029237,
     "end_time": "2025-06-08T08:00:29.517425",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.488188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d833d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.542240Z",
     "iopub.status.busy": "2025-06-08T08:00:29.542019Z",
     "iopub.status.idle": "2025-06-08T08:00:29.554332Z",
     "shell.execute_reply": "2025-06-08T08:00:29.553481Z"
    },
    "papermill": {
     "duration": 0.025086,
     "end_time": "2025-06-08T08:00:29.555546",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.530460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3428d610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.578914Z",
     "iopub.status.busy": "2025-06-08T08:00:29.578655Z",
     "iopub.status.idle": "2025-06-08T08:00:29.583919Z",
     "shell.execute_reply": "2025-06-08T08:00:29.583196Z"
    },
    "papermill": {
     "duration": 0.018552,
     "end_time": "2025-06-08T08:00:29.585187",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.566635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabe35fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.614348Z",
     "iopub.status.busy": "2025-06-08T08:00:29.613986Z",
     "iopub.status.idle": "2025-06-08T08:00:29.629464Z",
     "shell.execute_reply": "2025-06-08T08:00:29.628339Z"
    },
    "papermill": {
     "duration": 0.035912,
     "end_time": "2025-06-08T08:00:29.632205",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.596293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (7309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd8bdc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.666450Z",
     "iopub.status.busy": "2025-06-08T08:00:29.666080Z",
     "iopub.status.idle": "2025-06-08T08:00:29.677933Z",
     "shell.execute_reply": "2025-06-08T08:00:29.677102Z"
    },
    "papermill": {
     "duration": 0.03066,
     "end_time": "2025-06-08T08:00:29.679877",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.649217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c18ad801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.705506Z",
     "iopub.status.busy": "2025-06-08T08:00:29.705272Z",
     "iopub.status.idle": "2025-06-08T08:00:29.717988Z",
     "shell.execute_reply": "2025-06-08T08:00:29.717322Z"
    },
    "papermill": {
     "duration": 0.026628,
     "end_time": "2025-06-08T08:00:29.719213",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.692585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5411d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.742993Z",
     "iopub.status.busy": "2025-06-08T08:00:29.742786Z",
     "iopub.status.idle": "2025-06-08T08:00:29.745992Z",
     "shell.execute_reply": "2025-06-08T08:00:29.745359Z"
    },
    "papermill": {
     "duration": 0.016353,
     "end_time": "2025-06-08T08:00:29.747124",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.730771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851931c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:29.770956Z",
     "iopub.status.busy": "2025-06-08T08:00:29.770741Z",
     "iopub.status.idle": "2025-06-08T08:00:30.129327Z",
     "shell.execute_reply": "2025-06-08T08:00:30.128659Z"
    },
    "papermill": {
     "duration": 0.371949,
     "end_time": "2025-06-08T08:00:30.130571",
     "exception": false,
     "start_time": "2025-06-08T08:00:29.758622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535,) (10535, 12)\n",
      "(2634,) (2634, 12)\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "# Define the labels columns for multi-label classification\n",
    "label_columns = data.columns[1:]  # Assuming label columns start from the third column\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[label_columns].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[label_columns].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbc72c",
   "metadata": {
    "papermill": {
     "duration": 0.011595,
     "end_time": "2025-06-08T08:00:30.153948",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.142353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42358e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:30.177811Z",
     "iopub.status.busy": "2025-06-08T08:00:30.177556Z",
     "iopub.status.idle": "2025-06-08T08:00:30.840600Z",
     "shell.execute_reply": "2025-06-08T08:00:30.839751Z"
    },
    "papermill": {
     "duration": 0.67661,
     "end_time": "2025-06-08T08:00:30.842095",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.165485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36325a7c734d47d4a52ce838a035c1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ac16458df44dcd8ee587df0912e7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdfcf27346348fbad31ab832c7e5024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc8da33648d4005ab054a27d6880b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431e5933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:30.867822Z",
     "iopub.status.busy": "2025-06-08T08:00:30.867553Z",
     "iopub.status.idle": "2025-06-08T08:00:30.871929Z",
     "shell.execute_reply": "2025-06-08T08:00:30.871128Z"
    },
    "papermill": {
     "duration": 0.018237,
     "end_time": "2025-06-08T08:00:30.873089",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.854852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=64, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bec13",
   "metadata": {
    "papermill": {
     "duration": 0.011958,
     "end_time": "2025-06-08T08:00:30.897164",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.885206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7fb08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:30.922413Z",
     "iopub.status.busy": "2025-06-08T08:00:30.922184Z",
     "iopub.status.idle": "2025-06-08T08:00:30.926104Z",
     "shell.execute_reply": "2025-06-08T08:00:30.925122Z"
    },
    "papermill": {
     "duration": 0.017992,
     "end_time": "2025-06-08T08:00:30.927407",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.909415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e625662d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:30.952583Z",
     "iopub.status.busy": "2025-06-08T08:00:30.952342Z",
     "iopub.status.idle": "2025-06-08T08:00:30.957089Z",
     "shell.execute_reply": "2025-06-08T08:00:30.956298Z"
    },
    "papermill": {
     "duration": 0.018646,
     "end_time": "2025-06-08T08:00:30.958288",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.939642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d861df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:30.983310Z",
     "iopub.status.busy": "2025-06-08T08:00:30.983094Z",
     "iopub.status.idle": "2025-06-08T08:00:30.995052Z",
     "shell.execute_reply": "2025-06-08T08:00:30.994255Z"
    },
    "papermill": {
     "duration": 0.025865,
     "end_time": "2025-06-08T08:00:30.996276",
     "exception": false,
     "start_time": "2025-06-08T08:00:30.970411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "\n",
    "            nearest_cp = current_train_size\n",
    "            if nearest_cp not in checkpoints:\n",
    "                for cp in checkpoints:\n",
    "                    if cp > current_train_size:\n",
    "                        nearest_cp = cp\n",
    "                        break\n",
    "            percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdf6da",
   "metadata": {
    "papermill": {
     "duration": 0.011956,
     "end_time": "2025-06-08T08:00:31.020609",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.008653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7fe3b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:31.046084Z",
     "iopub.status.busy": "2025-06-08T08:00:31.045875Z",
     "iopub.status.idle": "2025-06-08T08:00:31.050901Z",
     "shell.execute_reply": "2025-06-08T08:00:31.050272Z"
    },
    "papermill": {
     "duration": 0.019076,
     "end_time": "2025-06-08T08:00:31.051994",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.032918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6eb41f",
   "metadata": {
    "papermill": {
     "duration": 0.012212,
     "end_time": "2025-06-08T08:00:31.076760",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.064548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f131d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_score(predicted_prob_one, true_outcome, alpha_param=0.1, beta_param=3.0):\n",
    "    epsilon = 1e-9\n",
    "    # Clip probabilities to avoid log(0) or other numerical errors\n",
    "    safe_prob = np.clip(predicted_prob_one, epsilon, 1.0 - epsilon)\n",
    "\n",
    "    if true_outcome == 1:\n",
    "        # Arguments for the betaln function when the true outcome is 1\n",
    "        arg1 = alpha_param\n",
    "        arg2 = beta_param + 1\n",
    "        arg3 = alpha_param + safe_prob\n",
    "        arg4 = beta_param + 1 - safe_prob\n",
    "        if not (arg1 > 0 and arg2 > 0 and arg3 > 0 and arg4 > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1, arg2) + betaln(arg3, arg4)\n",
    "    elif true_outcome == 0:\n",
    "        # Arguments for the betaln function when the true outcome is 0\n",
    "        arg1 = alpha_param + 1\n",
    "        arg2 = beta_param\n",
    "        arg3 = alpha_param + 1 - safe_prob\n",
    "        arg4 = beta_param + safe_prob\n",
    "        if not (arg1 > 0 and arg2 > 0 and arg3 > 0 and arg4 > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1, arg2) + betaln(arg3, arg4)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: true_outcome must be 0 or 1.\")\n",
    "\n",
    "def calculate_expected_beta_score(predicted_prob_one, alpha_param, beta_param):\n",
    "    score_if_one = beta_score(predicted_prob_one, 1, alpha_param, beta_param)\n",
    "    score_if_zero = beta_score(predicted_prob_one, 0, alpha_param, beta_param)\n",
    "    \n",
    "    # The expected score is the average of scores for each possible outcome, weighted by the predicted probability of that outcome.\n",
    "    expected_score = predicted_prob_one * score_if_one + (1.0 - predicted_prob_one) * score_if_zero\n",
    "    return expected_score\n",
    "\n",
    "def get_ensemble_predictions_for_batch(ensemble_models, input_ids, attention_mask, device):\n",
    "    individual_model_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for model_instance in ensemble_models:\n",
    "            model_instance.eval()\n",
    "            model_instance.to(device)\n",
    "            outputs = model_instance(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            individual_model_probs_list.append(probs)\n",
    "\n",
    "        if not individual_model_probs_list:\n",
    "            return torch.empty(0, device=device), []\n",
    "\n",
    "        individual_probs_tensor = torch.stack(individual_model_probs_list)\n",
    "        average_probs = torch.mean(individual_probs_tensor, dim=0)\n",
    "\n",
    "    return average_probs, individual_model_probs_list\n",
    "\n",
    "def besra_sampling(\n",
    "    ensemble_models,\n",
    "    unlabeled_data,\n",
    "    train_indices,\n",
    "    remaining_indices,\n",
    "    estimation_data,\n",
    "    tokenizer,\n",
    "    num_classes,\n",
    "    sampling_dur,\n",
    "    new_samples,\n",
    "    trials,\n",
    "    alpha_param=0.1,\n",
    "    beta_param=3.0,\n",
    "    n_clusters=min_increment,\n",
    "):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    unlabeled_dataset = HateSpeechDataset(unlabeled_data,\n",
    "                                        np.zeros((len(unlabeled_data), num_classes)),\n",
    "                                        tokenizer,\n",
    "                                        max_length=sequence_length) \n",
    "    unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=batch_size, num_workers=4, pin_memory=True) \n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "\n",
    "    estimation_dataset = HateSpeechDataset(estimation_data,\n",
    "                                        np.zeros((len(estimation_data), num_classes)),\n",
    "                                        tokenizer,\n",
    "                                        max_length=sequence_length) # sequence_length\n",
    "    estimation_dataloader = DataLoader(estimation_dataset, batch_size=len(estimation_dataset), num_workers=4, pin_memory=True)\n",
    "    estimation_dataloader, unlabeled_dataloader = accelerator.prepare(estimation_dataloader, unlabeled_dataloader)\n",
    "\n",
    "    for model in ensemble_models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    candidate_scores = []\n",
    "    all_estimation_avg_probs = []\n",
    "    all_estimation_individual_probs_by_model = []\n",
    "    all_candidate_avg_probs = []\n",
    "    all_candidate_individual_probs_by_model = []\n",
    "\n",
    "    # --- Pre-computation: Get predictions for all estimation and candidate samples ---\n",
    "    for est_batch in estimation_dataloader:\n",
    "        est_input_ids = est_batch['input_ids']\n",
    "        est_attention_mask = est_batch['attention_mask']\n",
    "        est_avg_probs_batch, est_indiv_probs_batch = get_ensemble_predictions_for_batch(ensemble_models, est_input_ids, est_attention_mask, device)\n",
    "        all_estimation_avg_probs.append(accelerator.gather_for_metrics(est_avg_probs_batch))\n",
    "\n",
    "        if not all_estimation_individual_probs_by_model:\n",
    "            all_estimation_individual_probs_by_model = [[] for _ in est_indiv_probs_batch]\n",
    "        for model_idx, probs in enumerate(est_indiv_probs_batch):\n",
    "            all_estimation_individual_probs_by_model[model_idx].append(accelerator.gather_for_metrics(probs))\n",
    "\n",
    "    for cand_batch in unlabeled_dataloader:\n",
    "        cand_input_ids = cand_batch['input_ids']\n",
    "        cand_attention_mask = cand_batch['attention_mask']\n",
    "        cand_avg_probs_batch, cand_indiv_probs_batch = get_ensemble_predictions_for_batch(ensemble_models, cand_input_ids, cand_attention_mask, device)\n",
    "        all_candidate_avg_probs.append(accelerator.gather_for_metrics(cand_avg_probs_batch))\n",
    "\n",
    "        if not all_candidate_individual_probs_by_model:\n",
    "             all_candidate_individual_probs_by_model = [[] for _ in cand_indiv_probs_batch]\n",
    "        for model_idx, probs in enumerate(cand_indiv_probs_batch):\n",
    "            all_candidate_individual_probs_by_model[model_idx].append(accelerator.gather_for_metrics(probs))\n",
    "\n",
    "    # Concatenate all batch predictions into single tensors\n",
    "    all_estimation_avg_probs = torch.cat(all_estimation_avg_probs, dim=0)\n",
    "    all_estimation_individual_probs = [torch.cat(model_probs, dim=0) for model_probs in all_estimation_individual_probs_by_model]\n",
    "    all_candidate_avg_probs = torch.cat(all_candidate_avg_probs, dim=0)\n",
    "    all_candidate_individual_probs = [torch.cat(model_probs, dim=0) for model_probs in all_candidate_individual_probs_by_model]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Equation 2: Change in Quality Score ΔQ(x|L)\n",
    "    # This section calculates the core acquisition score for each candidate sample 'x'.\n",
    "    # It estimates the expected improvement to the model by looping through all\n",
    "    # estimation samples 'x_prime' and all possible labels 'y' for the candidate.\n",
    "    for candidate_index in range(len(all_candidate_avg_probs)):\n",
    "        candidate_avg_probs_for_sample = all_candidate_avg_probs[candidate_index]\n",
    "        candidate_individual_probs_for_sample = [m_probs[candidate_index] for m_probs in all_candidate_individual_probs]\n",
    "        candidate_delta_q_score = 0.0\n",
    "\n",
    "        for estimation_index in range(len(all_estimation_avg_probs)):\n",
    "            estimation_avg_probs_for_sample = all_estimation_avg_probs[estimation_index]\n",
    "            estimation_individual_probs_for_sample = [m_probs[estimation_index] for m_probs in all_estimation_individual_probs]\n",
    "\n",
    "            # Calculate the initial expected score on the estimation sample before any update\n",
    "            initial_expected_score_on_estimation_sample = 0.0\n",
    "            for estimation_class_index in range(num_classes):\n",
    "                prob_one_avg_for_est_class = estimation_avg_probs_for_sample[estimation_class_index].item()\n",
    "                initial_expected_score_on_estimation_sample += calculate_expected_beta_score(prob_one_avg_for_est_class, alpha_param, beta_param)\n",
    "\n",
    "            delta_q_for_one_estimation_sample = 0.0\n",
    "            for candidate_class_index in range(num_classes):\n",
    "                expected_score_after_update = 0.0\n",
    "                for hypothetical_candidate_label in [0, 1]:\n",
    "                    prob_one_avg_for_candidate_class = candidate_avg_probs_for_sample[candidate_class_index].item()\n",
    "                    prob_of_hypothetical_label_avg = prob_one_avg_for_candidate_class if hypothetical_candidate_label == 1 else (1.0 - prob_one_avg_for_candidate_class)\n",
    "\n",
    "                    # Equations 4 & 3: Bayesian Update and New Prediction\n",
    "                    # This inner block performs the Bayesian update. It re-weights the ensemble models based on \n",
    "                    # how well they predict the hypothetical label for the candidate sample (Eq. 4) and then \n",
    "                    # computes a new, updated prediction or the estimation sample using these weights (Eq. 3).\n",
    "                    model_weights = []\n",
    "                    for model_index in range(len(ensemble_models)):\n",
    "                        prob_one_individual_for_candidate_class = candidate_individual_probs_for_sample[model_index][candidate_class_index].item()\n",
    "                        prob_of_hypothetical_label_individual = prob_one_individual_for_candidate_class if hypothetical_candidate_label == 1 else (1.0 - prob_one_individual_for_candidate_class)\n",
    "                        model_weights.append(prob_of_hypothetical_label_individual)\n",
    "\n",
    "                    sum_model_weights = sum(model_weights)\n",
    "                    if sum_model_weights < 1e-9:\n",
    "                        normalized_model_weights = [1.0 / len(ensemble_models)] * len(ensemble_models)\n",
    "                    else:\n",
    "                        normalized_model_weights = [w / sum_model_weights for w in model_weights]\n",
    "\n",
    "                    reweighted_prob_one_for_est_class = 0.0\n",
    "                    for model_index in range(len(ensemble_models)):\n",
    "                        prob_one_individual_for_est_class = estimation_individual_probs_for_sample[model_index][candidate_class_index].item()\n",
    "                        reweighted_prob_one_for_est_class += normalized_model_weights[model_index] * prob_one_individual_for_est_class\n",
    "\n",
    "                    updated_expected_score_on_estimation_sample = calculate_expected_beta_score(reweighted_prob_one_for_est_class, alpha_param, beta_param)\n",
    "                    expected_score_after_update += prob_of_hypothetical_label_avg * updated_expected_score_on_estimation_sample\n",
    "\n",
    "                delta_q_for_one_estimation_sample += (expected_score_after_update - initial_expected_score_on_estimation_sample)\n",
    "            candidate_delta_q_score += delta_q_for_one_estimation_sample\n",
    "        candidate_scores.append(candidate_delta_q_score)\n",
    "                \n",
    "    # K-Means Clustering and Selection\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        candidate_scores = np.array(candidate_scores)\n",
    "        candidate_scores = candidate_scores.reshape(-1, 1)\n",
    "    \n",
    "        accelerator.print(f\"BESRA Uncertainty Score Threshold {np.percentile(candidate_scores, 90)}\")\n",
    "    \n",
    "        target_samples = math.ceil(0.1 * len(unlabeled_data)) \n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "    \n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "    \n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'HS': [y_train[i][0] for i in temp],\n",
    "                'Abusive': [y_train[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                'HS_Group': [y_train[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                'HS_Race': [y_train[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                'HS_Other': [y_train[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train[i][11] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "    \n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(candidate_scores)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(candidate_scores[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "    \n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'HS': [y_train[i][0] for i in temp],\n",
    "                    'Abusive': [y_train[i][1] for i in temp],\n",
    "                    'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                    'HS_Group': [y_train[i][3] for i in temp],\n",
    "                    'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                    'HS_Race': [y_train[i][5] for i in temp],\n",
    "                    'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                    'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                    'HS_Other': [y_train[i][8] for i in temp],\n",
    "                    'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                    'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                    'HS_Strong': [y_train[i][11] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "    \n",
    "            # threshold_data = pd.DataFrame({\n",
    "            #     'Threshold': thresholds\n",
    "            # })\n",
    "            # threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa8714",
   "metadata": {
    "papermill": {
     "duration": 0.012702,
     "end_time": "2025-06-08T08:00:31.162898",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.150196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c49db661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:31.189152Z",
     "iopub.status.busy": "2025-06-08T08:00:31.188919Z",
     "iopub.status.idle": "2025-06-08T08:00:31.200093Z",
     "shell.execute_reply": "2025-06-08T08:00:31.199484Z"
    },
    "papermill": {
     "duration": 0.025144,
     "end_time": "2025-06-08T08:00:31.201266",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.176122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "\n",
    "        nearest_cp = current_train_size\n",
    "        if nearest_cp not in checkpoints:\n",
    "            for cp in checkpoints:\n",
    "                if cp > current_train_size:\n",
    "                    nearest_cp = cp\n",
    "                    break\n",
    "        percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            # model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=12)\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}-{percentage}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        estimation_pool_indices = np.random.choice(remaining_indices, size=min(16, int(0.1 * len(remaining_indices))), replace=False).tolist()\n",
    "        estimation_pool_data = [X_train[i] for i in estimation_pool_indices]\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            models, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            estimation_pool_data,\n",
    "            tokenizer, \n",
    "            12,\n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645588cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:31.226291Z",
     "iopub.status.busy": "2025-06-08T08:00:31.226087Z",
     "iopub.status.idle": "2025-06-08T08:00:31.229341Z",
     "shell.execute_reply": "2025-06-08T08:00:31.228720Z"
    },
    "papermill": {
     "duration": 0.016863,
     "end_time": "2025-06-08T08:00:31.230399",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.213536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593f1a1",
   "metadata": {
    "papermill": {
     "duration": 0.012184,
     "end_time": "2025-06-08T08:00:31.254909",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.242725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6072, Accuracy: 0.8196, F1 Micro: 0.158, F1 Macro: 0.056\n",
      "Epoch 2/10, Train Loss: 0.4725, Accuracy: 0.8281, F1 Micro: 0.0079, F1 Macro: 0.0031\n",
      "Epoch 3/10, Train Loss: 0.4011, Accuracy: 0.8324, F1 Micro: 0.0621, F1 Macro: 0.0249\n",
      "Epoch 4/10, Train Loss: 0.4044, Accuracy: 0.833, F1 Micro: 0.0698, F1 Macro: 0.0275\n",
      "Epoch 5/10, Train Loss: 0.39, Accuracy: 0.835, F1 Micro: 0.0933, F1 Macro: 0.0364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3774, Accuracy: 0.8482, F1 Micro: 0.2492, F1 Macro: 0.0877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3646, Accuracy: 0.8569, F1 Micro: 0.3507, F1 Macro: 0.1183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3408, Accuracy: 0.871, F1 Micro: 0.473, F1 Macro: 0.208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3045, Accuracy: 0.8761, F1 Micro: 0.5665, F1 Macro: 0.2665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.272, Accuracy: 0.8786, F1 Micro: 0.5727, F1 Macro: 0.2739\n",
      "Model 1 - Iteration 658: Accuracy: 0.8786, F1 Micro: 0.5727, F1 Macro: 0.2739\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.70      0.75      1134\n",
      "      Abusive       0.81      0.74      0.77       992\n",
      "HS_Individual       0.65      0.48      0.55       732\n",
      "     HS_Group       0.49      0.05      0.10       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.55      0.59       762\n",
      "      HS_Weak       0.63      0.44      0.52       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.47      0.57      5556\n",
      "    macro avg       0.34      0.25      0.27      5556\n",
      " weighted avg       0.60      0.47      0.52      5556\n",
      "  samples avg       0.36      0.27      0.28      5556\n",
      "\n",
      "Training completed in 58.74248719215393 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6268, Accuracy: 0.8269, F1 Micro: 0.1933, F1 Macro: 0.0535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4844, Accuracy: 0.8431, F1 Micro: 0.2905, F1 Macro: 0.0901\n",
      "Epoch 3/10, Train Loss: 0.4084, Accuracy: 0.8367, F1 Micro: 0.1258, F1 Macro: 0.0434\n",
      "Epoch 4/10, Train Loss: 0.4073, Accuracy: 0.8361, F1 Micro: 0.1149, F1 Macro: 0.0411\n",
      "Epoch 5/10, Train Loss: 0.3883, Accuracy: 0.8442, F1 Micro: 0.2237, F1 Macro: 0.0783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3744, Accuracy: 0.8537, F1 Micro: 0.3359, F1 Macro: 0.1086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.361, Accuracy: 0.8638, F1 Micro: 0.4298, F1 Macro: 0.1602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3381, Accuracy: 0.872, F1 Micro: 0.4844, F1 Macro: 0.2089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3027, Accuracy: 0.8754, F1 Micro: 0.5488, F1 Macro: 0.2523\n",
      "Epoch 10/10, Train Loss: 0.2788, Accuracy: 0.877, F1 Micro: 0.5471, F1 Macro: 0.2563\n",
      "Model 2 - Iteration 658: Accuracy: 0.8754, F1 Micro: 0.5488, F1 Macro: 0.2523\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.72      0.76      1134\n",
      "      Abusive       0.80      0.69      0.74       992\n",
      "HS_Individual       0.64      0.44      0.52       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.51      0.57       762\n",
      "      HS_Weak       0.64      0.34      0.44       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.44      0.55      5556\n",
      "    macro avg       0.29      0.22      0.25      5556\n",
      " weighted avg       0.56      0.44      0.49      5556\n",
      "  samples avg       0.35      0.25      0.27      5556\n",
      "\n",
      "Training completed in 57.95445370674133 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6425, Accuracy: 0.7948, F1 Micro: 0.3968, F1 Macro: 0.1198\n",
      "Epoch 2/10, Train Loss: 0.4902, Accuracy: 0.8356, F1 Micro: 0.1886, F1 Macro: 0.0538\n",
      "Epoch 3/10, Train Loss: 0.4047, Accuracy: 0.8326, F1 Micro: 0.0804, F1 Macro: 0.0306\n",
      "Epoch 4/10, Train Loss: 0.4031, Accuracy: 0.8344, F1 Micro: 0.1031, F1 Macro: 0.0375\n",
      "Epoch 5/10, Train Loss: 0.3878, Accuracy: 0.8367, F1 Micro: 0.1276, F1 Macro: 0.047\n",
      "Epoch 6/10, Train Loss: 0.3748, Accuracy: 0.8493, F1 Micro: 0.2727, F1 Macro: 0.0935\n",
      "Epoch 7/10, Train Loss: 0.3632, Accuracy: 0.8552, F1 Micro: 0.3382, F1 Macro: 0.1118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3437, Accuracy: 0.8634, F1 Micro: 0.4097, F1 Macro: 0.1654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3109, Accuracy: 0.8763, F1 Micro: 0.5497, F1 Macro: 0.2516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2794, Accuracy: 0.8776, F1 Micro: 0.5601, F1 Macro: 0.2635\n",
      "Model 3 - Iteration 658: Accuracy: 0.8776, F1 Micro: 0.5601, F1 Macro: 0.2635\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.69      0.75      1134\n",
      "      Abusive       0.82      0.70      0.76       992\n",
      "HS_Individual       0.67      0.46      0.54       732\n",
      "     HS_Group       0.44      0.02      0.03       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.53      0.58       762\n",
      "      HS_Weak       0.62      0.42      0.50       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.74      0.45      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.60      0.45      0.50      5556\n",
      "  samples avg       0.38      0.27      0.29      5556\n",
      "\n",
      "Training completed in 53.98909783363342 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8772, F1 Micro: 0.5606, F1 Macro: 0.2632\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 380.7623026990762\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 651.6236529350281 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.532, Accuracy: 0.8305, F1 Micro: 0.0618, F1 Macro: 0.0215\n",
      "Epoch 2/10, Train Loss: 0.4024, Accuracy: 0.8325, F1 Micro: 0.0582, F1 Macro: 0.0244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.385, Accuracy: 0.8572, F1 Micro: 0.3965, F1 Macro: 0.1208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3542, Accuracy: 0.8768, F1 Micro: 0.5495, F1 Macro: 0.2543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3147, Accuracy: 0.8823, F1 Micro: 0.6066, F1 Macro: 0.2957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2789, Accuracy: 0.8859, F1 Micro: 0.6218, F1 Macro: 0.3098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2497, Accuracy: 0.8898, F1 Micro: 0.6433, F1 Macro: 0.378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.223, Accuracy: 0.8886, F1 Micro: 0.676, F1 Macro: 0.4507\n",
      "Epoch 9/10, Train Loss: 0.1979, Accuracy: 0.8939, F1 Micro: 0.6694, F1 Macro: 0.446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1855, Accuracy: 0.8942, F1 Micro: 0.6803, F1 Macro: 0.4702\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8942, F1 Micro: 0.6803, F1 Macro: 0.4702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.83      0.81      1134\n",
      "      Abusive       0.83      0.83      0.83       992\n",
      "HS_Individual       0.65      0.66      0.65       732\n",
      "     HS_Group       0.65      0.47      0.54       402\n",
      "  HS_Religion       0.57      0.27      0.37       157\n",
      "      HS_Race       0.81      0.37      0.51       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.71      0.69       762\n",
      "      HS_Weak       0.62      0.64      0.63       689\n",
      "  HS_Moderate       0.50      0.34      0.40       331\n",
      "    HS_Strong       0.88      0.12      0.22       114\n",
      "\n",
      "    micro avg       0.71      0.65      0.68      5556\n",
      "    macro avg       0.58      0.44      0.47      5556\n",
      " weighted avg       0.69      0.65      0.66      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 79.38143610954285 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5492, Accuracy: 0.8442, F1 Micro: 0.3474, F1 Macro: 0.1014\n",
      "Epoch 2/10, Train Loss: 0.4038, Accuracy: 0.8405, F1 Micro: 0.1655, F1 Macro: 0.0623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3801, Accuracy: 0.8634, F1 Micro: 0.4593, F1 Macro: 0.1663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3478, Accuracy: 0.877, F1 Micro: 0.5339, F1 Macro: 0.2445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3148, Accuracy: 0.8826, F1 Micro: 0.5957, F1 Macro: 0.2779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2817, Accuracy: 0.8855, F1 Micro: 0.6171, F1 Macro: 0.2981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2531, Accuracy: 0.888, F1 Micro: 0.6457, F1 Macro: 0.3805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2274, Accuracy: 0.8842, F1 Micro: 0.6743, F1 Macro: 0.4732\n",
      "Epoch 9/10, Train Loss: 0.2052, Accuracy: 0.8921, F1 Micro: 0.6695, F1 Macro: 0.4675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1912, Accuracy: 0.8956, F1 Micro: 0.6774, F1 Macro: 0.4688\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8956, F1 Micro: 0.6774, F1 Macro: 0.4688\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.81      0.81      1134\n",
      "      Abusive       0.85      0.82      0.83       992\n",
      "HS_Individual       0.66      0.62      0.64       732\n",
      "     HS_Group       0.65      0.48      0.55       402\n",
      "  HS_Religion       0.65      0.25      0.37       157\n",
      "      HS_Race       0.84      0.42      0.56       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.68      0.69       762\n",
      "      HS_Weak       0.62      0.60      0.61       689\n",
      "  HS_Moderate       0.50      0.37      0.42       331\n",
      "    HS_Strong       0.90      0.08      0.15       114\n",
      "\n",
      "    micro avg       0.72      0.64      0.68      5556\n",
      "    macro avg       0.60      0.43      0.47      5556\n",
      " weighted avg       0.71      0.64      0.66      5556\n",
      "  samples avg       0.39      0.35      0.35      5556\n",
      "\n",
      "Training completed in 79.00186228752136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5572, Accuracy: 0.8348, F1 Micro: 0.1475, F1 Macro: 0.0484\n",
      "Epoch 2/10, Train Loss: 0.4036, Accuracy: 0.8334, F1 Micro: 0.0747, F1 Macro: 0.0301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3827, Accuracy: 0.8562, F1 Micro: 0.3693, F1 Macro: 0.1152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3554, Accuracy: 0.8737, F1 Micro: 0.5062, F1 Macro: 0.2232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3187, Accuracy: 0.8807, F1 Micro: 0.5967, F1 Macro: 0.2807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2818, Accuracy: 0.8846, F1 Micro: 0.6189, F1 Macro: 0.3047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2544, Accuracy: 0.8898, F1 Micro: 0.6449, F1 Macro: 0.3717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.229, Accuracy: 0.8883, F1 Micro: 0.678, F1 Macro: 0.4715\n",
      "Epoch 9/10, Train Loss: 0.2065, Accuracy: 0.8944, F1 Micro: 0.6648, F1 Macro: 0.4585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1925, Accuracy: 0.8948, F1 Micro: 0.6893, F1 Macro: 0.5028\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8948, F1 Micro: 0.6893, F1 Macro: 0.5028\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.84      0.81      1134\n",
      "      Abusive       0.82      0.85      0.83       992\n",
      "HS_Individual       0.65      0.66      0.66       732\n",
      "     HS_Group       0.59      0.55      0.57       402\n",
      "  HS_Religion       0.57      0.35      0.43       157\n",
      "      HS_Race       0.74      0.52      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.72      0.70       762\n",
      "      HS_Weak       0.63      0.62      0.62       689\n",
      "  HS_Moderate       0.48      0.45      0.47       331\n",
      "    HS_Strong       0.96      0.20      0.33       114\n",
      "\n",
      "    micro avg       0.70      0.68      0.69      5556\n",
      "    macro avg       0.58      0.48      0.50      5556\n",
      " weighted avg       0.69      0.68      0.67      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 80.12825393676758 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8949, F1 Micro: 0.6823, F1 Macro: 0.4806\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 620.1904960974515\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 583.8714985847473 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4943, Accuracy: 0.8321, F1 Micro: 0.0584, F1 Macro: 0.0241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3861, Accuracy: 0.8559, F1 Micro: 0.3718, F1 Macro: 0.1204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3428, Accuracy: 0.8774, F1 Micro: 0.5361, F1 Macro: 0.25\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2929, Accuracy: 0.8851, F1 Micro: 0.5883, F1 Macro: 0.292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2544, Accuracy: 0.8912, F1 Micro: 0.6613, F1 Macro: 0.3964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2307, Accuracy: 0.8964, F1 Micro: 0.6811, F1 Macro: 0.449\n",
      "Epoch 7/10, Train Loss: 0.2047, Accuracy: 0.8978, F1 Micro: 0.6677, F1 Macro: 0.4734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1722, Accuracy: 0.9008, F1 Micro: 0.6822, F1 Macro: 0.4466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1588, Accuracy: 0.9009, F1 Micro: 0.7075, F1 Macro: 0.5131\n",
      "Epoch 10/10, Train Loss: 0.1359, Accuracy: 0.9031, F1 Micro: 0.7074, F1 Macro: 0.5318\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9009, F1 Micro: 0.7075, F1 Macro: 0.5131\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.85      0.82      1134\n",
      "      Abusive       0.83      0.88      0.85       992\n",
      "HS_Individual       0.65      0.71      0.68       732\n",
      "     HS_Group       0.70      0.53      0.60       402\n",
      "  HS_Religion       0.78      0.36      0.50       157\n",
      "      HS_Race       0.84      0.47      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.75      0.71       762\n",
      "      HS_Weak       0.62      0.69      0.65       689\n",
      "  HS_Moderate       0.53      0.37      0.43       331\n",
      "    HS_Strong       0.91      0.18      0.31       114\n",
      "\n",
      "    micro avg       0.72      0.70      0.71      5556\n",
      "    macro avg       0.61      0.48      0.51      5556\n",
      " weighted avg       0.71      0.70      0.69      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 96.37584042549133 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5066, Accuracy: 0.8353, F1 Micro: 0.1054, F1 Macro: 0.0387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.384, Accuracy: 0.8575, F1 Micro: 0.3707, F1 Macro: 0.1275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3402, Accuracy: 0.877, F1 Micro: 0.5274, F1 Macro: 0.2438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.294, Accuracy: 0.8843, F1 Micro: 0.6104, F1 Macro: 0.2869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2591, Accuracy: 0.8905, F1 Micro: 0.6474, F1 Macro: 0.3569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2368, Accuracy: 0.8951, F1 Micro: 0.6877, F1 Macro: 0.4701\n",
      "Epoch 7/10, Train Loss: 0.2113, Accuracy: 0.8972, F1 Micro: 0.6621, F1 Macro: 0.4563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1792, Accuracy: 0.8988, F1 Micro: 0.6888, F1 Macro: 0.4382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1607, Accuracy: 0.9009, F1 Micro: 0.7041, F1 Macro: 0.5069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1468, Accuracy: 0.9035, F1 Micro: 0.7042, F1 Macro: 0.5263\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9035, F1 Micro: 0.7042, F1 Macro: 0.5263\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.80      0.82      1134\n",
      "      Abusive       0.87      0.84      0.85       992\n",
      "HS_Individual       0.64      0.72      0.68       732\n",
      "     HS_Group       0.79      0.42      0.55       402\n",
      "  HS_Religion       0.80      0.28      0.42       157\n",
      "      HS_Race       0.85      0.47      0.60       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.70      0.71       762\n",
      "      HS_Weak       0.61      0.69      0.65       689\n",
      "  HS_Moderate       0.61      0.29      0.40       331\n",
      "    HS_Strong       0.93      0.46      0.62       114\n",
      "\n",
      "    micro avg       0.75      0.67      0.70      5556\n",
      "    macro avg       0.72      0.48      0.53      5556\n",
      " weighted avg       0.75      0.67      0.69      5556\n",
      "  samples avg       0.40      0.37      0.37      5556\n",
      "\n",
      "Training completed in 97.8353328704834 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5107, Accuracy: 0.8387, F1 Micro: 0.1753, F1 Macro: 0.0642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3847, Accuracy: 0.8556, F1 Micro: 0.3645, F1 Macro: 0.1132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3477, Accuracy: 0.8779, F1 Micro: 0.55, F1 Macro: 0.2548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2987, Accuracy: 0.8836, F1 Micro: 0.5876, F1 Macro: 0.2855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2618, Accuracy: 0.8901, F1 Micro: 0.6579, F1 Macro: 0.3754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.237, Accuracy: 0.8932, F1 Micro: 0.6866, F1 Macro: 0.4743\n",
      "Epoch 7/10, Train Loss: 0.2127, Accuracy: 0.8995, F1 Micro: 0.6698, F1 Macro: 0.4694\n",
      "Epoch 8/10, Train Loss: 0.1818, Accuracy: 0.9005, F1 Micro: 0.6819, F1 Macro: 0.4371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1641, Accuracy: 0.9012, F1 Micro: 0.7053, F1 Macro: 0.5194\n",
      "Epoch 10/10, Train Loss: 0.1478, Accuracy: 0.9045, F1 Micro: 0.7049, F1 Macro: 0.5352\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9012, F1 Micro: 0.7053, F1 Macro: 0.5194\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1134\n",
      "      Abusive       0.85      0.85      0.85       992\n",
      "HS_Individual       0.64      0.72      0.68       732\n",
      "     HS_Group       0.71      0.47      0.56       402\n",
      "  HS_Religion       0.80      0.36      0.50       157\n",
      "      HS_Race       0.81      0.49      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.75      0.71       762\n",
      "      HS_Weak       0.62      0.68      0.65       689\n",
      "  HS_Moderate       0.54      0.34      0.42       331\n",
      "    HS_Strong       0.91      0.28      0.43       114\n",
      "\n",
      "    micro avg       0.73      0.69      0.71      5556\n",
      "    macro avg       0.61      0.48      0.52      5556\n",
      " weighted avg       0.71      0.69      0.69      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 93.47825741767883 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9019, F1 Micro: 0.7056, F1 Macro: 0.5196\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 478.5533897244275\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 533.465847492218 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.467, Accuracy: 0.8348, F1 Micro: 0.093, F1 Macro: 0.0357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3622, Accuracy: 0.8755, F1 Micro: 0.5177, F1 Macro: 0.2382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3078, Accuracy: 0.8844, F1 Micro: 0.5767, F1 Macro: 0.2812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2631, Accuracy: 0.8885, F1 Micro: 0.599, F1 Macro: 0.3493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2293, Accuracy: 0.8989, F1 Micro: 0.671, F1 Macro: 0.4274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2014, Accuracy: 0.8985, F1 Micro: 0.707, F1 Macro: 0.5193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1709, Accuracy: 0.9045, F1 Micro: 0.7102, F1 Macro: 0.5096\n",
      "Epoch 8/10, Train Loss: 0.1483, Accuracy: 0.9029, F1 Micro: 0.71, F1 Macro: 0.5442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1292, Accuracy: 0.9061, F1 Micro: 0.7196, F1 Macro: 0.5534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1133, Accuracy: 0.9069, F1 Micro: 0.7198, F1 Macro: 0.5405\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9069, F1 Micro: 0.7198, F1 Macro: 0.5405\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1134\n",
      "      Abusive       0.85      0.88      0.86       992\n",
      "HS_Individual       0.67      0.71      0.69       732\n",
      "     HS_Group       0.73      0.50      0.59       402\n",
      "  HS_Religion       0.82      0.38      0.52       157\n",
      "      HS_Race       0.92      0.46      0.61       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.75      0.73       762\n",
      "      HS_Weak       0.64      0.70      0.67       689\n",
      "  HS_Moderate       0.61      0.40      0.49       331\n",
      "    HS_Strong       0.90      0.31      0.46       114\n",
      "\n",
      "    micro avg       0.75      0.69      0.72      5556\n",
      "    macro avg       0.69      0.49      0.54      5556\n",
      " weighted avg       0.74      0.69      0.70      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 114.12178325653076 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4769, Accuracy: 0.8461, F1 Micro: 0.2617, F1 Macro: 0.089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3609, Accuracy: 0.873, F1 Micro: 0.484, F1 Macro: 0.2163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3141, Accuracy: 0.8844, F1 Micro: 0.5903, F1 Macro: 0.2777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2702, Accuracy: 0.8915, F1 Micro: 0.6594, F1 Macro: 0.4342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2324, Accuracy: 0.8963, F1 Micro: 0.6758, F1 Macro: 0.4226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2093, Accuracy: 0.8966, F1 Micro: 0.7006, F1 Macro: 0.4976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1753, Accuracy: 0.9035, F1 Micro: 0.7076, F1 Macro: 0.5192\n",
      "Epoch 8/10, Train Loss: 0.1558, Accuracy: 0.9023, F1 Micro: 0.7036, F1 Macro: 0.54\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1361, Accuracy: 0.9057, F1 Micro: 0.7196, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1178, Accuracy: 0.9063, F1 Micro: 0.7223, F1 Macro: 0.5413\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9063, F1 Micro: 0.7223, F1 Macro: 0.5413\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.84      0.89      0.87       992\n",
      "HS_Individual       0.65      0.72      0.68       732\n",
      "     HS_Group       0.75      0.51      0.61       402\n",
      "  HS_Religion       0.84      0.40      0.54       157\n",
      "      HS_Race       0.89      0.39      0.54       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.78      0.73       762\n",
      "      HS_Weak       0.62      0.70      0.66       689\n",
      "  HS_Moderate       0.65      0.42      0.51       331\n",
      "    HS_Strong       0.93      0.37      0.53       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.72      5556\n",
      "    macro avg       0.64      0.50      0.54      5556\n",
      " weighted avg       0.73      0.71      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 113.6932578086853 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4778, Accuracy: 0.8352, F1 Micro: 0.1225, F1 Macro: 0.0425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3626, Accuracy: 0.8689, F1 Micro: 0.4586, F1 Macro: 0.1979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3156, Accuracy: 0.8826, F1 Micro: 0.5694, F1 Macro: 0.279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2689, Accuracy: 0.8888, F1 Micro: 0.5962, F1 Macro: 0.3638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2366, Accuracy: 0.8963, F1 Micro: 0.6653, F1 Macro: 0.4343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2085, Accuracy: 0.8991, F1 Micro: 0.702, F1 Macro: 0.5108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1758, Accuracy: 0.9015, F1 Micro: 0.714, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 0.9041, F1 Micro: 0.7177, F1 Macro: 0.5565\n",
      "Epoch 9/10, Train Loss: 0.1346, Accuracy: 0.9047, F1 Micro: 0.7172, F1 Macro: 0.5473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.119, Accuracy: 0.9091, F1 Micro: 0.7197, F1 Macro: 0.5446\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9091, F1 Micro: 0.7197, F1 Macro: 0.5446\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.79      0.82      1134\n",
      "      Abusive       0.86      0.87      0.86       992\n",
      "HS_Individual       0.68      0.70      0.69       732\n",
      "     HS_Group       0.76      0.49      0.60       402\n",
      "  HS_Religion       0.91      0.43      0.58       157\n",
      "      HS_Race       0.87      0.52      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.72      0.73       762\n",
      "      HS_Weak       0.65      0.67      0.66       689\n",
      "  HS_Moderate       0.63      0.38      0.48       331\n",
      "    HS_Strong       0.95      0.32      0.47       114\n",
      "\n",
      "    micro avg       0.77      0.68      0.72      5556\n",
      "    macro avg       0.66      0.49      0.54      5556\n",
      " weighted avg       0.75      0.68      0.71      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 113.21744346618652 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9074, F1 Micro: 0.7206, F1 Macro: 0.5421\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 483.6858762073885\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 474.0297386646271 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4564, Accuracy: 0.8407, F1 Micro: 0.1842, F1 Macro: 0.0625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.352, Accuracy: 0.8817, F1 Micro: 0.5757, F1 Macro: 0.2667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2866, Accuracy: 0.8908, F1 Micro: 0.6407, F1 Macro: 0.3726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2533, Accuracy: 0.8957, F1 Micro: 0.6618, F1 Macro: 0.3895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2144, Accuracy: 0.9018, F1 Micro: 0.7079, F1 Macro: 0.5116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1882, Accuracy: 0.906, F1 Micro: 0.7139, F1 Macro: 0.5368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1626, Accuracy: 0.9078, F1 Micro: 0.7335, F1 Macro: 0.5617\n",
      "Epoch 8/10, Train Loss: 0.1392, Accuracy: 0.9081, F1 Micro: 0.7202, F1 Macro: 0.5634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.119, Accuracy: 0.9086, F1 Micro: 0.7348, F1 Macro: 0.5811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1005, Accuracy: 0.9128, F1 Micro: 0.7353, F1 Macro: 0.5941\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9128, F1 Micro: 0.7353, F1 Macro: 0.5941\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.82      1134\n",
      "      Abusive       0.87      0.86      0.87       992\n",
      "HS_Individual       0.71      0.68      0.69       732\n",
      "     HS_Group       0.70      0.58      0.64       402\n",
      "  HS_Religion       0.83      0.45      0.58       157\n",
      "      HS_Race       0.88      0.55      0.68       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.69      0.65      0.67       689\n",
      "  HS_Moderate       0.61      0.45      0.52       331\n",
      "    HS_Strong       0.88      0.65      0.75       114\n",
      "\n",
      "    micro avg       0.77      0.70      0.74      5556\n",
      "    macro avg       0.80      0.54      0.59      5556\n",
      " weighted avg       0.77      0.70      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 127.8384370803833 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4626, Accuracy: 0.8482, F1 Micro: 0.2918, F1 Macro: 0.0954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3489, Accuracy: 0.8795, F1 Micro: 0.5474, F1 Macro: 0.2492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2895, Accuracy: 0.8898, F1 Micro: 0.6336, F1 Macro: 0.3558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2546, Accuracy: 0.8971, F1 Micro: 0.6673, F1 Macro: 0.4182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2163, Accuracy: 0.9005, F1 Micro: 0.6978, F1 Macro: 0.4869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1925, Accuracy: 0.9037, F1 Micro: 0.7005, F1 Macro: 0.4991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1657, Accuracy: 0.9088, F1 Micro: 0.7279, F1 Macro: 0.5573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1466, Accuracy: 0.907, F1 Micro: 0.7283, F1 Macro: 0.5822\n",
      "Epoch 9/10, Train Loss: 0.1253, Accuracy: 0.9015, F1 Micro: 0.7267, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1027, Accuracy: 0.9125, F1 Micro: 0.7314, F1 Macro: 0.6061\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9125, F1 Micro: 0.7314, F1 Macro: 0.6061\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.80      0.82      1134\n",
      "      Abusive       0.89      0.83      0.86       992\n",
      "HS_Individual       0.72      0.65      0.68       732\n",
      "     HS_Group       0.69      0.60      0.64       402\n",
      "  HS_Religion       0.87      0.48      0.62       157\n",
      "      HS_Race       0.86      0.57      0.68       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.83      0.10      0.18        51\n",
      "     HS_Other       0.74      0.74      0.74       762\n",
      "      HS_Weak       0.70      0.62      0.66       689\n",
      "  HS_Moderate       0.59      0.49      0.54       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.69      0.73      5556\n",
      "    macro avg       0.80      0.55      0.61      5556\n",
      " weighted avg       0.78      0.69      0.72      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 126.75982880592346 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4643, Accuracy: 0.8432, F1 Micro: 0.2327, F1 Macro: 0.0748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3518, Accuracy: 0.8793, F1 Micro: 0.5568, F1 Macro: 0.2545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2907, Accuracy: 0.8898, F1 Micro: 0.6449, F1 Macro: 0.3801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2561, Accuracy: 0.8966, F1 Micro: 0.6691, F1 Macro: 0.4366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2177, Accuracy: 0.8988, F1 Micro: 0.702, F1 Macro: 0.4916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1911, Accuracy: 0.9045, F1 Micro: 0.7179, F1 Macro: 0.5433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1668, Accuracy: 0.9055, F1 Micro: 0.7316, F1 Macro: 0.5689\n",
      "Epoch 8/10, Train Loss: 0.1463, Accuracy: 0.9099, F1 Micro: 0.7239, F1 Macro: 0.5804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1267, Accuracy: 0.908, F1 Micro: 0.733, F1 Macro: 0.5874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1053, Accuracy: 0.9113, F1 Micro: 0.7338, F1 Macro: 0.5869\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9113, F1 Micro: 0.7338, F1 Macro: 0.5869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.88      0.85      0.87       992\n",
      "HS_Individual       0.67      0.70      0.69       732\n",
      "     HS_Group       0.71      0.54      0.62       402\n",
      "  HS_Religion       0.80      0.50      0.62       157\n",
      "      HS_Race       0.85      0.59      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.72      0.77      0.75       762\n",
      "      HS_Weak       0.66      0.66      0.66       689\n",
      "  HS_Moderate       0.63      0.41      0.50       331\n",
      "    HS_Strong       0.86      0.72      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.71      0.73      5556\n",
      "    macro avg       0.72      0.55      0.59      5556\n",
      " weighted avg       0.75      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 126.93603444099426 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9122, F1 Micro: 0.7335, F1 Macro: 0.5957\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 577.0367743449543\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 428.11184525489807 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4477, Accuracy: 0.8529, F1 Micro: 0.3422, F1 Macro: 0.1094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3358, Accuracy: 0.8846, F1 Micro: 0.6, F1 Macro: 0.298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2733, Accuracy: 0.8968, F1 Micro: 0.6668, F1 Macro: 0.4133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2342, Accuracy: 0.9047, F1 Micro: 0.6912, F1 Macro: 0.4973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2091, Accuracy: 0.9054, F1 Micro: 0.7225, F1 Macro: 0.5444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1779, Accuracy: 0.9086, F1 Micro: 0.7229, F1 Macro: 0.5259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1457, Accuracy: 0.9132, F1 Micro: 0.7335, F1 Macro: 0.5711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1282, Accuracy: 0.9125, F1 Micro: 0.7463, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1072, Accuracy: 0.9155, F1 Micro: 0.7468, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0958, Accuracy: 0.9141, F1 Micro: 0.7516, F1 Macro: 0.6208\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9141, F1 Micro: 0.7516, F1 Macro: 0.6208\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.84      1134\n",
      "      Abusive       0.85      0.90      0.87       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.64      0.67      0.65       402\n",
      "  HS_Religion       0.69      0.59      0.64       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.62      0.07      0.12        72\n",
      "    HS_Gender       0.44      0.08      0.13        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.55      0.58      0.56       331\n",
      "    HS_Strong       0.88      0.63      0.73       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.70      0.61      0.62      5556\n",
      " weighted avg       0.74      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 139.86129570007324 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4539, Accuracy: 0.8527, F1 Micro: 0.3331, F1 Macro: 0.1087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3365, Accuracy: 0.8829, F1 Micro: 0.5783, F1 Macro: 0.2743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.274, Accuracy: 0.8955, F1 Micro: 0.6729, F1 Macro: 0.4149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2372, Accuracy: 0.9024, F1 Micro: 0.6843, F1 Macro: 0.4978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2104, Accuracy: 0.9052, F1 Micro: 0.713, F1 Macro: 0.5256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9101, F1 Micro: 0.7237, F1 Macro: 0.521\n",
      "Epoch 7/10, Train Loss: 0.151, Accuracy: 0.9098, F1 Micro: 0.7081, F1 Macro: 0.5359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1336, Accuracy: 0.9124, F1 Micro: 0.7491, F1 Macro: 0.6118\n",
      "Epoch 9/10, Train Loss: 0.1112, Accuracy: 0.9125, F1 Micro: 0.7383, F1 Macro: 0.6103\n",
      "Epoch 10/10, Train Loss: 0.0969, Accuracy: 0.916, F1 Micro: 0.7449, F1 Macro: 0.6085\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9124, F1 Micro: 0.7491, F1 Macro: 0.6118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.87      0.84      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.70      0.61      0.65       402\n",
      "  HS_Religion       0.71      0.54      0.61       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.72      0.80      0.76       762\n",
      "      HS_Weak       0.64      0.76      0.70       689\n",
      "  HS_Moderate       0.60      0.52      0.56       331\n",
      "    HS_Strong       0.89      0.56      0.69       114\n",
      "\n",
      "    micro avg       0.74      0.76      0.75      5556\n",
      "    macro avg       0.76      0.59      0.61      5556\n",
      " weighted avg       0.74      0.76      0.74      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 133.4902539253235 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4553, Accuracy: 0.8524, F1 Micro: 0.3264, F1 Macro: 0.1055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3383, Accuracy: 0.8818, F1 Micro: 0.5623, F1 Macro: 0.2683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.8949, F1 Micro: 0.6831, F1 Macro: 0.4414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2377, Accuracy: 0.9037, F1 Micro: 0.6847, F1 Macro: 0.5167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2136, Accuracy: 0.9062, F1 Micro: 0.7165, F1 Macro: 0.52\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1826, Accuracy: 0.9089, F1 Micro: 0.7338, F1 Macro: 0.5508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1518, Accuracy: 0.9136, F1 Micro: 0.7347, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1327, Accuracy: 0.9122, F1 Micro: 0.7453, F1 Macro: 0.5969\n",
      "Epoch 9/10, Train Loss: 0.1105, Accuracy: 0.9163, F1 Micro: 0.7378, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1, Accuracy: 0.9172, F1 Micro: 0.7538, F1 Macro: 0.6172\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9172, F1 Micro: 0.7538, F1 Macro: 0.6172\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.86      0.89      0.87       992\n",
      "HS_Individual       0.71      0.72      0.72       732\n",
      "     HS_Group       0.73      0.61      0.67       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.84      0.66      0.74       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.75      0.77      0.76       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.90      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.75      0.59      0.62      5556\n",
      " weighted avg       0.77      0.74      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 136.8601975440979 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9146, F1 Micro: 0.7515, F1 Macro: 0.6166\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 859.1899839221701\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 388.158718585968 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4394, Accuracy: 0.8594, F1 Micro: 0.4116, F1 Macro: 0.1354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.32, Accuracy: 0.8888, F1 Micro: 0.6422, F1 Macro: 0.3364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2646, Accuracy: 0.898, F1 Micro: 0.65, F1 Macro: 0.4068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2255, Accuracy: 0.9054, F1 Micro: 0.6832, F1 Macro: 0.459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1988, Accuracy: 0.9108, F1 Micro: 0.7244, F1 Macro: 0.5458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.161, Accuracy: 0.9099, F1 Micro: 0.7355, F1 Macro: 0.5697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1407, Accuracy: 0.912, F1 Micro: 0.7481, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1158, Accuracy: 0.918, F1 Micro: 0.7549, F1 Macro: 0.6155\n",
      "Epoch 9/10, Train Loss: 0.0991, Accuracy: 0.9175, F1 Micro: 0.7546, F1 Macro: 0.6241\n",
      "Epoch 10/10, Train Loss: 0.0904, Accuracy: 0.917, F1 Micro: 0.7365, F1 Macro: 0.6167\n",
      "Model 1 - Iteration 5287: Accuracy: 0.918, F1 Micro: 0.7549, F1 Macro: 0.6155\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.69      0.76      0.73       732\n",
      "     HS_Group       0.80      0.54      0.64       402\n",
      "  HS_Religion       0.75      0.50      0.60       157\n",
      "      HS_Race       0.89      0.49      0.63       120\n",
      "  HS_Physical       0.62      0.11      0.19        72\n",
      "    HS_Gender       0.42      0.10      0.16        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.67      0.44      0.54       331\n",
      "    HS_Strong       0.89      0.58      0.70       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.74      0.56      0.62      5556\n",
      " weighted avg       0.78      0.73      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 148.2703800201416 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4445, Accuracy: 0.8617, F1 Micro: 0.4455, F1 Macro: 0.1549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.32, Accuracy: 0.8876, F1 Micro: 0.6308, F1 Macro: 0.3221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2674, Accuracy: 0.8985, F1 Micro: 0.6575, F1 Macro: 0.4118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2304, Accuracy: 0.906, F1 Micro: 0.6896, F1 Macro: 0.4792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2008, Accuracy: 0.9102, F1 Micro: 0.7229, F1 Macro: 0.5632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1647, Accuracy: 0.9104, F1 Micro: 0.7379, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1448, Accuracy: 0.9126, F1 Micro: 0.7479, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1204, Accuracy: 0.9174, F1 Micro: 0.7528, F1 Macro: 0.6299\n",
      "Epoch 9/10, Train Loss: 0.1007, Accuracy: 0.9161, F1 Micro: 0.7423, F1 Macro: 0.6213\n",
      "Epoch 10/10, Train Loss: 0.0934, Accuracy: 0.9145, F1 Micro: 0.7414, F1 Macro: 0.6121\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9174, F1 Micro: 0.7528, F1 Macro: 0.6299\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.80      0.54      0.64       402\n",
      "  HS_Religion       0.77      0.50      0.60       157\n",
      "      HS_Race       0.83      0.54      0.66       120\n",
      "  HS_Physical       0.60      0.08      0.15        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.79      0.75      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.69      0.44      0.54       331\n",
      "    HS_Strong       0.89      0.62      0.73       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.75      0.58      0.63      5556\n",
      " weighted avg       0.78      0.73      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 147.7787263393402 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4464, Accuracy: 0.8567, F1 Micro: 0.3774, F1 Macro: 0.1181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.324, Accuracy: 0.8875, F1 Micro: 0.6277, F1 Macro: 0.3183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2683, Accuracy: 0.8981, F1 Micro: 0.6563, F1 Macro: 0.4308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2311, Accuracy: 0.9057, F1 Micro: 0.6865, F1 Macro: 0.482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.9095, F1 Micro: 0.7252, F1 Macro: 0.5751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1654, Accuracy: 0.9107, F1 Micro: 0.7403, F1 Macro: 0.5802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.145, Accuracy: 0.9145, F1 Micro: 0.7496, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 0.9188, F1 Micro: 0.75, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.9193, F1 Micro: 0.758, F1 Macro: 0.6319\n",
      "Epoch 10/10, Train Loss: 0.0931, Accuracy: 0.9195, F1 Micro: 0.7544, F1 Macro: 0.6269\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9193, F1 Micro: 0.758, F1 Macro: 0.6319\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.74      0.68      0.71       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.74      0.62      0.68       157\n",
      "      HS_Race       0.86      0.67      0.75       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.75      0.06      0.11        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.73      0.65      0.69       689\n",
      "  HS_Moderate       0.57      0.58      0.58       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.79      0.60      0.63      5556\n",
      " weighted avg       0.79      0.73      0.75      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 150.00975108146667 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9182, F1 Micro: 0.7552, F1 Macro: 0.6258\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 412.34816220444316\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 346.63984966278076 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4332, Accuracy: 0.8626, F1 Micro: 0.5003, F1 Macro: 0.1971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3235, Accuracy: 0.889, F1 Micro: 0.627, F1 Macro: 0.3382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2586, Accuracy: 0.9008, F1 Micro: 0.6983, F1 Macro: 0.4531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2227, Accuracy: 0.9081, F1 Micro: 0.7056, F1 Macro: 0.5215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1873, Accuracy: 0.9107, F1 Micro: 0.7304, F1 Macro: 0.5763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9164, F1 Micro: 0.7498, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1328, Accuracy: 0.9171, F1 Micro: 0.7522, F1 Macro: 0.6014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.11, Accuracy: 0.9158, F1 Micro: 0.757, F1 Macro: 0.6244\n",
      "Epoch 9/10, Train Loss: 0.1028, Accuracy: 0.9182, F1 Micro: 0.7523, F1 Macro: 0.6049\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9191, F1 Micro: 0.7568, F1 Macro: 0.6333\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9158, F1 Micro: 0.757, F1 Macro: 0.6244\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.68      0.77      0.72       732\n",
      "     HS_Group       0.69      0.59      0.64       402\n",
      "  HS_Religion       0.76      0.51      0.61       157\n",
      "      HS_Race       0.81      0.62      0.70       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.59      0.50      0.54       331\n",
      "    HS_Strong       0.89      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.76      5556\n",
      "    macro avg       0.74      0.60      0.62      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 157.37844395637512 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4376, Accuracy: 0.8661, F1 Micro: 0.4953, F1 Macro: 0.1899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.325, Accuracy: 0.8887, F1 Micro: 0.6453, F1 Macro: 0.3469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2615, Accuracy: 0.8993, F1 Micro: 0.6927, F1 Macro: 0.4569\n",
      "Epoch 4/10, Train Loss: 0.2256, Accuracy: 0.9061, F1 Micro: 0.6873, F1 Macro: 0.4666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1924, Accuracy: 0.9117, F1 Micro: 0.7189, F1 Macro: 0.5533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1548, Accuracy: 0.9154, F1 Micro: 0.7399, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1407, Accuracy: 0.9182, F1 Micro: 0.7502, F1 Macro: 0.6242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1145, Accuracy: 0.9166, F1 Micro: 0.7528, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1075, Accuracy: 0.9169, F1 Micro: 0.7583, F1 Macro: 0.6377\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.9204, F1 Micro: 0.754, F1 Macro: 0.6344\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9169, F1 Micro: 0.7583, F1 Macro: 0.6377\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.72      0.58      0.64       402\n",
      "  HS_Religion       0.78      0.50      0.61       157\n",
      "      HS_Race       0.81      0.62      0.70       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.54      0.14      0.22        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.74      0.70       689\n",
      "  HS_Moderate       0.63      0.48      0.55       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.75      0.60      0.64      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 156.97176241874695 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4387, Accuracy: 0.8594, F1 Micro: 0.4389, F1 Macro: 0.1494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3278, Accuracy: 0.889, F1 Micro: 0.6364, F1 Macro: 0.3336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.264, Accuracy: 0.9012, F1 Micro: 0.696, F1 Macro: 0.465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2255, Accuracy: 0.9075, F1 Micro: 0.7031, F1 Macro: 0.5102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1913, Accuracy: 0.9137, F1 Micro: 0.7377, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1561, Accuracy: 0.9155, F1 Micro: 0.7504, F1 Macro: 0.6015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1376, Accuracy: 0.9174, F1 Micro: 0.7543, F1 Macro: 0.6037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1126, Accuracy: 0.9165, F1 Micro: 0.7609, F1 Macro: 0.6227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1053, Accuracy: 0.9187, F1 Micro: 0.7634, F1 Macro: 0.6251\n",
      "Epoch 10/10, Train Loss: 0.0837, Accuracy: 0.9195, F1 Micro: 0.763, F1 Macro: 0.6447\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9187, F1 Micro: 0.7634, F1 Macro: 0.6251\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.76      0.56      0.64       157\n",
      "      HS_Race       0.76      0.62      0.68       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.50      0.06      0.11        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.89      0.67      0.76       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.60      0.63      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 158.44554090499878 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9171, F1 Micro: 0.7596, F1 Macro: 0.6291\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 849.4764891326126\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 312.59334993362427 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4274, Accuracy: 0.869, F1 Micro: 0.4938, F1 Macro: 0.2018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.896, F1 Micro: 0.6761, F1 Macro: 0.426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2478, Accuracy: 0.9023, F1 Micro: 0.702, F1 Macro: 0.4606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9115, F1 Micro: 0.732, F1 Macro: 0.5484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1793, Accuracy: 0.9158, F1 Micro: 0.7422, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1551, Accuracy: 0.9171, F1 Micro: 0.7469, F1 Macro: 0.5949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1282, Accuracy: 0.9183, F1 Micro: 0.7556, F1 Macro: 0.6247\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.9186, F1 Micro: 0.7551, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9169, F1 Micro: 0.7601, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0792, Accuracy: 0.9214, F1 Micro: 0.7606, F1 Macro: 0.6514\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9214, F1 Micro: 0.7606, F1 Macro: 0.6514\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1134\n",
      "      Abusive       0.90      0.85      0.88       992\n",
      "HS_Individual       0.76      0.66      0.71       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.78      0.68      0.72       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.75      0.63      0.68       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.73      0.76      5556\n",
      "    macro avg       0.77      0.61      0.65      5556\n",
      " weighted avg       0.80      0.73      0.75      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 168.30185961723328 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.431, Accuracy: 0.8707, F1 Micro: 0.5157, F1 Macro: 0.211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3088, Accuracy: 0.8935, F1 Micro: 0.6688, F1 Macro: 0.4307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2513, Accuracy: 0.9024, F1 Micro: 0.6996, F1 Macro: 0.4809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2153, Accuracy: 0.9107, F1 Micro: 0.7167, F1 Macro: 0.5422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1832, Accuracy: 0.916, F1 Micro: 0.7441, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1603, Accuracy: 0.9181, F1 Micro: 0.7525, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1318, Accuracy: 0.9191, F1 Micro: 0.7531, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1105, Accuracy: 0.9191, F1 Micro: 0.7593, F1 Macro: 0.641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9197, F1 Micro: 0.7609, F1 Macro: 0.6472\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.9202, F1 Micro: 0.7455, F1 Macro: 0.6284\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9197, F1 Micro: 0.7609, F1 Macro: 0.6472\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.79      0.73      0.76       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.64      0.14      0.23        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.67      0.70       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.62      0.65      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 168.67713737487793 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4321, Accuracy: 0.8659, F1 Micro: 0.4605, F1 Macro: 0.178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3083, Accuracy: 0.8927, F1 Micro: 0.675, F1 Macro: 0.432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2509, Accuracy: 0.9036, F1 Micro: 0.7042, F1 Macro: 0.4877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2145, Accuracy: 0.9106, F1 Micro: 0.7176, F1 Macro: 0.544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1833, Accuracy: 0.9156, F1 Micro: 0.7399, F1 Macro: 0.571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1591, Accuracy: 0.9163, F1 Micro: 0.7487, F1 Macro: 0.6031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1309, Accuracy: 0.92, F1 Micro: 0.7585, F1 Macro: 0.6264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1096, Accuracy: 0.9202, F1 Micro: 0.7636, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0925, Accuracy: 0.9192, F1 Micro: 0.7647, F1 Macro: 0.627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0815, Accuracy: 0.9199, F1 Micro: 0.7662, F1 Macro: 0.6391\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9199, F1 Micro: 0.7662, F1 Macro: 0.6391\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.43      0.06      0.10        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.68      0.69       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.74      0.62      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.02217984199524 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9203, F1 Micro: 0.7626, F1 Macro: 0.6459\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1109.1558228226984\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 283.3067557811737 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4228, Accuracy: 0.8731, F1 Micro: 0.5015, F1 Macro: 0.2223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3026, Accuracy: 0.8889, F1 Micro: 0.5896, F1 Macro: 0.3455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2491, Accuracy: 0.9065, F1 Micro: 0.7157, F1 Macro: 0.5284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2099, Accuracy: 0.9108, F1 Micro: 0.719, F1 Macro: 0.5141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1759, Accuracy: 0.9161, F1 Micro: 0.7472, F1 Macro: 0.5901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9182, F1 Micro: 0.7604, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.9177, F1 Micro: 0.7607, F1 Macro: 0.6254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1029, Accuracy: 0.9182, F1 Micro: 0.7613, F1 Macro: 0.6339\n",
      "Epoch 9/10, Train Loss: 0.089, Accuracy: 0.9172, F1 Micro: 0.7583, F1 Macro: 0.648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.9172, F1 Micro: 0.7621, F1 Macro: 0.6539\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9172, F1 Micro: 0.7621, F1 Macro: 0.6539\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.68      0.77      0.72       732\n",
      "     HS_Group       0.69      0.59      0.64       402\n",
      "  HS_Religion       0.72      0.58      0.64       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       0.67      0.17      0.27        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.61      0.49      0.54       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.73      0.63      0.65      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.9581925868988 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4269, Accuracy: 0.8752, F1 Micro: 0.5286, F1 Macro: 0.2347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3045, Accuracy: 0.8907, F1 Micro: 0.6033, F1 Macro: 0.3554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2498, Accuracy: 0.9054, F1 Micro: 0.7021, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2153, Accuracy: 0.9109, F1 Micro: 0.7234, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1794, Accuracy: 0.9161, F1 Micro: 0.7473, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1526, Accuracy: 0.9163, F1 Micro: 0.7552, F1 Macro: 0.6203\n",
      "Epoch 7/10, Train Loss: 0.1247, Accuracy: 0.9144, F1 Micro: 0.7523, F1 Macro: 0.6319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1064, Accuracy: 0.9199, F1 Micro: 0.7574, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0928, Accuracy: 0.9199, F1 Micro: 0.7586, F1 Macro: 0.6679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0784, Accuracy: 0.9204, F1 Micro: 0.7607, F1 Macro: 0.6588\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9204, F1 Micro: 0.7607, F1 Macro: 0.6588\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.74      0.55      0.63       402\n",
      "  HS_Religion       0.77      0.53      0.63       157\n",
      "      HS_Race       0.90      0.57      0.70       120\n",
      "  HS_Physical       0.69      0.15      0.25        72\n",
      "    HS_Gender       0.62      0.25      0.36        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.68      0.49      0.57       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.73      0.76      5556\n",
      "    macro avg       0.76      0.61      0.66      5556\n",
      " weighted avg       0.79      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 173.01324844360352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4284, Accuracy: 0.868, F1 Micro: 0.4611, F1 Macro: 0.1906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3075, Accuracy: 0.8887, F1 Micro: 0.5938, F1 Macro: 0.3328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2524, Accuracy: 0.9047, F1 Micro: 0.7095, F1 Macro: 0.5216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2152, Accuracy: 0.9111, F1 Micro: 0.7231, F1 Macro: 0.5274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1804, Accuracy: 0.9149, F1 Micro: 0.7442, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.152, Accuracy: 0.9163, F1 Micro: 0.7581, F1 Macro: 0.6054\n",
      "Epoch 7/10, Train Loss: 0.1289, Accuracy: 0.9176, F1 Micro: 0.7571, F1 Macro: 0.63\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1055, Accuracy: 0.9212, F1 Micro: 0.767, F1 Macro: 0.6454\n",
      "Epoch 9/10, Train Loss: 0.0928, Accuracy: 0.9178, F1 Micro: 0.7639, F1 Macro: 0.6506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9217, F1 Micro: 0.7676, F1 Macro: 0.662\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9217, F1 Micro: 0.7676, F1 Macro: 0.662\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.76      0.64      0.70       157\n",
      "      HS_Race       0.82      0.69      0.75       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.47      0.18      0.26        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 170.76370215415955 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9198, F1 Micro: 0.7635, F1 Macro: 0.6582\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 787.9184300767691\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 261.7790250778198 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4169, Accuracy: 0.878, F1 Micro: 0.5548, F1 Macro: 0.2565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.8972, F1 Micro: 0.6557, F1 Macro: 0.434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2454, Accuracy: 0.9072, F1 Micro: 0.7084, F1 Macro: 0.5004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2031, Accuracy: 0.9119, F1 Micro: 0.7252, F1 Macro: 0.5439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.9171, F1 Micro: 0.7395, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1443, Accuracy: 0.9164, F1 Micro: 0.7438, F1 Macro: 0.6196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9208, F1 Micro: 0.7571, F1 Macro: 0.6189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.105, Accuracy: 0.9152, F1 Micro: 0.7603, F1 Macro: 0.642\n",
      "Epoch 9/10, Train Loss: 0.0901, Accuracy: 0.9144, F1 Micro: 0.7584, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.9171, F1 Micro: 0.7566, F1 Macro: 0.6378\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9152, F1 Micro: 0.7603, F1 Macro: 0.642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.66      0.78      0.72       732\n",
      "     HS_Group       0.70      0.61      0.65       402\n",
      "  HS_Religion       0.65      0.61      0.63       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.55      0.15      0.24        72\n",
      "    HS_Gender       0.53      0.16      0.24        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.64      0.77      0.70       689\n",
      "  HS_Moderate       0.61      0.54      0.57       331\n",
      "    HS_Strong       0.91      0.59      0.71       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.70      0.63      0.64      5556\n",
      " weighted avg       0.74      0.78      0.75      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 179.24668741226196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4197, Accuracy: 0.8754, F1 Micro: 0.522, F1 Macro: 0.239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2949, Accuracy: 0.8974, F1 Micro: 0.6644, F1 Macro: 0.4525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2489, Accuracy: 0.9063, F1 Micro: 0.6966, F1 Macro: 0.4739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2062, Accuracy: 0.9101, F1 Micro: 0.718, F1 Macro: 0.5245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9181, F1 Micro: 0.7456, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9186, F1 Micro: 0.7508, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.9207, F1 Micro: 0.7566, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.9195, F1 Micro: 0.7597, F1 Macro: 0.6305\n",
      "Epoch 9/10, Train Loss: 0.0918, Accuracy: 0.9141, F1 Micro: 0.7577, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.9204, F1 Micro: 0.761, F1 Macro: 0.6602\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9204, F1 Micro: 0.761, F1 Macro: 0.6602\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.77      0.72      0.75       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.72      0.66      0.69       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5556\n",
      "    macro avg       0.76      0.62      0.66      5556\n",
      " weighted avg       0.78      0.74      0.76      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 180.4106559753418 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4211, Accuracy: 0.8741, F1 Micro: 0.5115, F1 Macro: 0.2306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2958, Accuracy: 0.8975, F1 Micro: 0.6681, F1 Macro: 0.4598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2474, Accuracy: 0.9067, F1 Micro: 0.7091, F1 Macro: 0.5036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2062, Accuracy: 0.9107, F1 Micro: 0.712, F1 Macro: 0.5318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.918, F1 Micro: 0.7468, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1483, Accuracy: 0.9172, F1 Micro: 0.7519, F1 Macro: 0.6013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1243, Accuracy: 0.9209, F1 Micro: 0.7582, F1 Macro: 0.6224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1091, Accuracy: 0.9174, F1 Micro: 0.7603, F1 Macro: 0.6353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9201, F1 Micro: 0.7673, F1 Macro: 0.6642\n",
      "Epoch 10/10, Train Loss: 0.0815, Accuracy: 0.9208, F1 Micro: 0.7669, F1 Macro: 0.6432\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9201, F1 Micro: 0.7673, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.69      0.70      0.70       402\n",
      "  HS_Religion       0.74      0.61      0.66       157\n",
      "      HS_Race       0.77      0.64      0.70       120\n",
      "  HS_Physical       0.65      0.15      0.25        72\n",
      "    HS_Gender       0.53      0.20      0.29        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.61       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.64      0.66      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 180.66220569610596 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9186, F1 Micro: 0.7629, F1 Macro: 0.6555\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1196.2988685047885\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 235.67663621902466 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4144, Accuracy: 0.8785, F1 Micro: 0.5805, F1 Macro: 0.2677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2907, Accuracy: 0.8977, F1 Micro: 0.6898, F1 Macro: 0.4585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.9084, F1 Micro: 0.7232, F1 Macro: 0.5268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2028, Accuracy: 0.9144, F1 Micro: 0.7377, F1 Macro: 0.5716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1717, Accuracy: 0.9177, F1 Micro: 0.7498, F1 Macro: 0.6099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9198, F1 Micro: 0.7579, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.9191, F1 Micro: 0.7617, F1 Macro: 0.6188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9219, F1 Micro: 0.7641, F1 Macro: 0.6636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.9206, F1 Micro: 0.768, F1 Macro: 0.6546\n",
      "Epoch 10/10, Train Loss: 0.0769, Accuracy: 0.9172, F1 Micro: 0.7617, F1 Macro: 0.6606\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9206, F1 Micro: 0.768, F1 Macro: 0.6546\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.50      0.14      0.22        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.73      0.63      0.65      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 188.0872027873993 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4172, Accuracy: 0.8741, F1 Micro: 0.6038, F1 Macro: 0.2765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2931, Accuracy: 0.8971, F1 Micro: 0.6884, F1 Macro: 0.4724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.242, Accuracy: 0.9089, F1 Micro: 0.7133, F1 Macro: 0.5117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.208, Accuracy: 0.9158, F1 Micro: 0.7408, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.9175, F1 Micro: 0.7413, F1 Macro: 0.5915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9216, F1 Micro: 0.7603, F1 Macro: 0.6264\n",
      "Epoch 7/10, Train Loss: 0.1235, Accuracy: 0.9167, F1 Micro: 0.7594, F1 Macro: 0.6208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9233, F1 Micro: 0.7685, F1 Macro: 0.6551\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9213, F1 Micro: 0.7646, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0771, Accuracy: 0.9222, F1 Micro: 0.7702, F1 Macro: 0.6659\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9222, F1 Micro: 0.7702, F1 Macro: 0.6659\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.81      0.55      0.66       157\n",
      "      HS_Race       0.79      0.62      0.69       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.65      0.22      0.32        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 185.9486219882965 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4196, Accuracy: 0.876, F1 Micro: 0.5847, F1 Macro: 0.2676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.8964, F1 Micro: 0.6814, F1 Macro: 0.4596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.243, Accuracy: 0.9086, F1 Micro: 0.7267, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.206, Accuracy: 0.9129, F1 Micro: 0.7418, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9173, F1 Micro: 0.7508, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9195, F1 Micro: 0.7578, F1 Macro: 0.6161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1229, Accuracy: 0.9164, F1 Micro: 0.7582, F1 Macro: 0.6103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.9195, F1 Micro: 0.7607, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9205, F1 Micro: 0.7665, F1 Macro: 0.6556\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.92, F1 Micro: 0.7647, F1 Macro: 0.6493\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9205, F1 Micro: 0.7665, F1 Macro: 0.6556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.76      0.69      0.72       732\n",
      "     HS_Group       0.64      0.71      0.68       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.54      0.14      0.22        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.75      0.65      0.70       689\n",
      "  HS_Moderate       0.56      0.64      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.64      0.66      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 187.88037824630737 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9211, F1 Micro: 0.7682, F1 Macro: 0.6587\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 755.6877539786962\n",
      "Nearest checkpoint: 7901\n",
      "320Acquired samples: \n",
      "Sampling duration: 213.1418650150299 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4085, Accuracy: 0.8813, F1 Micro: 0.5786, F1 Macro: 0.276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.286, Accuracy: 0.899, F1 Micro: 0.691, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9067, F1 Micro: 0.7288, F1 Macro: 0.5572\n",
      "Epoch 4/10, Train Loss: 0.1972, Accuracy: 0.9141, F1 Micro: 0.7288, F1 Macro: 0.56\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1699, Accuracy: 0.9187, F1 Micro: 0.7521, F1 Macro: 0.6053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1396, Accuracy: 0.9142, F1 Micro: 0.7581, F1 Macro: 0.6311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.9231, F1 Micro: 0.7674, F1 Macro: 0.6345\n",
      "Epoch 8/10, Train Loss: 0.097, Accuracy: 0.9217, F1 Micro: 0.7579, F1 Macro: 0.6403\n",
      "Epoch 9/10, Train Loss: 0.0871, Accuracy: 0.924, F1 Micro: 0.7657, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.9205, F1 Micro: 0.7691, F1 Macro: 0.6498\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9205, F1 Micro: 0.7691, F1 Macro: 0.6498\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.67      0.60      0.63       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.84      0.88      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.63      0.65      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.5913052558899 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4125, Accuracy: 0.88, F1 Micro: 0.5827, F1 Macro: 0.2659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2903, Accuracy: 0.8998, F1 Micro: 0.6833, F1 Macro: 0.445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2403, Accuracy: 0.9094, F1 Micro: 0.7263, F1 Macro: 0.549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2013, Accuracy: 0.915, F1 Micro: 0.7398, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1736, Accuracy: 0.9189, F1 Micro: 0.75, F1 Macro: 0.5906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1439, Accuracy: 0.9109, F1 Micro: 0.7524, F1 Macro: 0.6247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1212, Accuracy: 0.9231, F1 Micro: 0.7654, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1001, Accuracy: 0.9233, F1 Micro: 0.7667, F1 Macro: 0.6514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9211, F1 Micro: 0.7672, F1 Macro: 0.671\n",
      "Epoch 10/10, Train Loss: 0.0768, Accuracy: 0.9211, F1 Micro: 0.7664, F1 Macro: 0.669\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9211, F1 Micro: 0.7672, F1 Macro: 0.671\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.84      0.68      0.75       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.78      0.77      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 192.67104578018188 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4133, Accuracy: 0.8786, F1 Micro: 0.541, F1 Macro: 0.2464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2895, Accuracy: 0.8977, F1 Micro: 0.6862, F1 Macro: 0.4431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2408, Accuracy: 0.9066, F1 Micro: 0.724, F1 Macro: 0.5565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2033, Accuracy: 0.9153, F1 Micro: 0.7422, F1 Macro: 0.5832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1732, Accuracy: 0.9176, F1 Micro: 0.7528, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1417, Accuracy: 0.9184, F1 Micro: 0.7632, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.9223, F1 Micro: 0.7664, F1 Macro: 0.6307\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9228, F1 Micro: 0.7617, F1 Macro: 0.6339\n",
      "Epoch 9/10, Train Loss: 0.0858, Accuracy: 0.9213, F1 Micro: 0.7609, F1 Macro: 0.6639\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.921, F1 Micro: 0.7647, F1 Macro: 0.6703\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9223, F1 Micro: 0.7664, F1 Macro: 0.6307\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.80      0.55      0.65       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5556\n",
      "    macro avg       0.76      0.59      0.63      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 188.99735116958618 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9213, F1 Micro: 0.7676, F1 Macro: 0.6505\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 900.0183021044206\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 192.21046543121338 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4095, Accuracy: 0.8774, F1 Micro: 0.5192, F1 Macro: 0.2464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2825, Accuracy: 0.9009, F1 Micro: 0.6708, F1 Macro: 0.4417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2332, Accuracy: 0.9104, F1 Micro: 0.7217, F1 Macro: 0.5367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1982, Accuracy: 0.9167, F1 Micro: 0.7469, F1 Macro: 0.571\n",
      "Epoch 5/10, Train Loss: 0.1624, Accuracy: 0.917, F1 Micro: 0.7416, F1 Macro: 0.598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1405, Accuracy: 0.9208, F1 Micro: 0.7648, F1 Macro: 0.6353\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9199, F1 Micro: 0.7643, F1 Macro: 0.6289\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9196, F1 Micro: 0.7633, F1 Macro: 0.652\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9217, F1 Micro: 0.763, F1 Macro: 0.6535\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9219, F1 Micro: 0.7625, F1 Macro: 0.6564\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9208, F1 Micro: 0.7648, F1 Macro: 0.6353\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.60      0.65       402\n",
      "  HS_Religion       0.76      0.53      0.62       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.57      0.11      0.19        72\n",
      "    HS_Gender       0.40      0.04      0.07        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.72      0.60      0.64      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 191.1651577949524 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4133, Accuracy: 0.876, F1 Micro: 0.5134, F1 Macro: 0.2336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2857, Accuracy: 0.9005, F1 Micro: 0.6702, F1 Macro: 0.4535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9106, F1 Micro: 0.7282, F1 Macro: 0.5399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2033, Accuracy: 0.9152, F1 Micro: 0.7483, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9181, F1 Micro: 0.7551, F1 Macro: 0.6057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1482, Accuracy: 0.918, F1 Micro: 0.7654, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.923, F1 Micro: 0.7656, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.9207, F1 Micro: 0.7695, F1 Macro: 0.6637\n",
      "Epoch 9/10, Train Loss: 0.0887, Accuracy: 0.9225, F1 Micro: 0.7694, F1 Macro: 0.6646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.9225, F1 Micro: 0.7698, F1 Macro: 0.6701\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9225, F1 Micro: 0.7698, F1 Macro: 0.6701\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.77      0.62      0.69       402\n",
      "  HS_Religion       0.78      0.55      0.65       157\n",
      "      HS_Race       0.84      0.64      0.73       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.69      0.53      0.60       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 197.81777477264404 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4139, Accuracy: 0.8792, F1 Micro: 0.5417, F1 Macro: 0.2539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2854, Accuracy: 0.9, F1 Micro: 0.6642, F1 Macro: 0.4589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.9104, F1 Micro: 0.7294, F1 Macro: 0.5564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2009, Accuracy: 0.9163, F1 Micro: 0.7399, F1 Macro: 0.5633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9187, F1 Micro: 0.749, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1438, Accuracy: 0.9211, F1 Micro: 0.7645, F1 Macro: 0.6238\n",
      "Epoch 7/10, Train Loss: 0.125, Accuracy: 0.9213, F1 Micro: 0.7639, F1 Macro: 0.6174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.103, Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6557\n",
      "Epoch 9/10, Train Loss: 0.0904, Accuracy: 0.9186, F1 Micro: 0.7672, F1 Macro: 0.6515\n",
      "Epoch 10/10, Train Loss: 0.0774, Accuracy: 0.9235, F1 Micro: 0.7666, F1 Macro: 0.6607\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6557\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.84      0.68      0.75       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.36      0.08      0.13        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.60      0.61       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.5313081741333 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9218, F1 Micro: 0.7691, F1 Macro: 0.6537\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 455.5602443543833\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 175.27673029899597 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4067, Accuracy: 0.8817, F1 Micro: 0.6092, F1 Macro: 0.2919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.286, Accuracy: 0.9009, F1 Micro: 0.6643, F1 Macro: 0.4509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2304, Accuracy: 0.9082, F1 Micro: 0.7315, F1 Macro: 0.5465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.191, Accuracy: 0.916, F1 Micro: 0.7493, F1 Macro: 0.581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9199, F1 Micro: 0.7607, F1 Macro: 0.6201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.922, F1 Micro: 0.7669, F1 Macro: 0.6396\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9168, F1 Micro: 0.7607, F1 Macro: 0.6509\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.9214, F1 Micro: 0.7663, F1 Macro: 0.6704\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9217, F1 Micro: 0.7648, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9227, F1 Micro: 0.771, F1 Macro: 0.6784\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9227, F1 Micro: 0.771, F1 Macro: 0.6784\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.72      0.56      0.63       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.83      0.68      0.74       120\n",
      "  HS_Physical       0.71      0.24      0.35        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.49      0.56       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.75      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 199.75385904312134 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4107, Accuracy: 0.8783, F1 Micro: 0.5949, F1 Macro: 0.2744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2885, Accuracy: 0.9019, F1 Micro: 0.6789, F1 Macro: 0.4614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9094, F1 Micro: 0.7291, F1 Macro: 0.5588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.916, F1 Micro: 0.7452, F1 Macro: 0.584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9209, F1 Micro: 0.7616, F1 Macro: 0.6268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1435, Accuracy: 0.9224, F1 Micro: 0.7698, F1 Macro: 0.6572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1163, Accuracy: 0.9215, F1 Micro: 0.7713, F1 Macro: 0.6607\n",
      "Epoch 8/10, Train Loss: 0.0994, Accuracy: 0.9202, F1 Micro: 0.7617, F1 Macro: 0.667\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9225, F1 Micro: 0.7653, F1 Macro: 0.6837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0707, Accuracy: 0.9213, F1 Micro: 0.7715, F1 Macro: 0.6848\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9213, F1 Micro: 0.7715, F1 Macro: 0.6848\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.68      0.78      0.73       732\n",
      "     HS_Group       0.75      0.57      0.65       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.85      0.68      0.75       120\n",
      "  HS_Physical       0.67      0.22      0.33        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.66      0.76      0.71       689\n",
      "  HS_Moderate       0.70      0.49      0.57       331\n",
      "    HS_Strong       0.90      0.84      0.87       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 201.30980849266052 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4113, Accuracy: 0.8801, F1 Micro: 0.6006, F1 Macro: 0.2816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.9016, F1 Micro: 0.6819, F1 Macro: 0.4543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9059, F1 Micro: 0.7304, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.195, Accuracy: 0.9143, F1 Micro: 0.7426, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1646, Accuracy: 0.9194, F1 Micro: 0.7605, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9228, F1 Micro: 0.7637, F1 Macro: 0.6256\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9195, F1 Micro: 0.7616, F1 Macro: 0.6299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.919, F1 Micro: 0.7674, F1 Macro: 0.6619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0832, Accuracy: 0.9227, F1 Micro: 0.7688, F1 Macro: 0.6732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.9227, F1 Micro: 0.7722, F1 Macro: 0.6802\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9227, F1 Micro: 0.7722, F1 Macro: 0.6802\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.82      0.68      0.74       120\n",
      "  HS_Physical       0.78      0.19      0.31        72\n",
      "    HS_Gender       0.53      0.31      0.40        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.68      0.49      0.57       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 203.3475821018219 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9222, F1 Micro: 0.7716, F1 Macro: 0.6811\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1445.422941076396\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 159.5438597202301 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3993, Accuracy: 0.8825, F1 Micro: 0.5854, F1 Macro: 0.2806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.9016, F1 Micro: 0.6911, F1 Macro: 0.4428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.9129, F1 Micro: 0.7335, F1 Macro: 0.5494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9164, F1 Micro: 0.7512, F1 Macro: 0.5874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1617, Accuracy: 0.9169, F1 Micro: 0.7538, F1 Macro: 0.6184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9227, F1 Micro: 0.7674, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9244, F1 Micro: 0.7706, F1 Macro: 0.6556\n",
      "Epoch 8/10, Train Loss: 0.0959, Accuracy: 0.921, F1 Micro: 0.7671, F1 Macro: 0.6518\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9242, F1 Micro: 0.7706, F1 Macro: 0.6692\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9221, F1 Micro: 0.7701, F1 Macro: 0.6715\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9244, F1 Micro: 0.7706, F1 Macro: 0.6556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.91      0.88      0.89       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.74      0.62      0.68       402\n",
      "  HS_Religion       0.79      0.55      0.65       157\n",
      "      HS_Race       0.84      0.67      0.74       120\n",
      "  HS_Physical       0.57      0.11      0.19        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.68      0.54      0.60       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.76      0.61      0.66      5556\n",
      " weighted avg       0.80      0.74      0.76      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 205.03764295578003 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.403, Accuracy: 0.8814, F1 Micro: 0.5813, F1 Macro: 0.2703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.9005, F1 Micro: 0.6909, F1 Macro: 0.4483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9122, F1 Micro: 0.7224, F1 Macro: 0.5311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9159, F1 Micro: 0.7531, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9193, F1 Micro: 0.7581, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.9228, F1 Micro: 0.7717, F1 Macro: 0.6457\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9228, F1 Micro: 0.7679, F1 Macro: 0.6578\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9222, F1 Micro: 0.7662, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6776\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9219, F1 Micro: 0.7655, F1 Macro: 0.6731\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.70      0.66      0.68       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.60      0.12      0.21        72\n",
      "    HS_Gender       0.76      0.25      0.38        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.1815047264099 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.405, Accuracy: 0.882, F1 Micro: 0.5889, F1 Macro: 0.2768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.9023, F1 Micro: 0.7001, F1 Macro: 0.467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9127, F1 Micro: 0.7384, F1 Macro: 0.5546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.9164, F1 Micro: 0.7521, F1 Macro: 0.5842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9178, F1 Micro: 0.7622, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1334, Accuracy: 0.9217, F1 Micro: 0.7667, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.922, F1 Micro: 0.7683, F1 Macro: 0.658\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.918, F1 Micro: 0.7635, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9241, F1 Micro: 0.7722, F1 Macro: 0.6773\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9219, F1 Micro: 0.7718, F1 Macro: 0.6778\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9241, F1 Micro: 0.7722, F1 Macro: 0.6773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.76      0.69      0.72       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.79      0.73      0.76       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.72      0.25      0.38        51\n",
      "     HS_Other       0.81      0.76      0.78       762\n",
      "      HS_Weak       0.74      0.66      0.70       689\n",
      "  HS_Moderate       0.61      0.56      0.58       331\n",
      "    HS_Strong       0.83      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 205.96589469909668 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9236, F1 Micro: 0.7721, F1 Macro: 0.6702\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 559.2332404792206\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 143.37875723838806 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.8801, F1 Micro: 0.6339, F1 Macro: 0.3185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2764, Accuracy: 0.901, F1 Micro: 0.6732, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2227, Accuracy: 0.9117, F1 Micro: 0.7431, F1 Macro: 0.5725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9197, F1 Micro: 0.7524, F1 Macro: 0.5899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9206, F1 Micro: 0.7565, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.131, Accuracy: 0.9233, F1 Micro: 0.7696, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9203, F1 Micro: 0.7714, F1 Macro: 0.6677\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9209, F1 Micro: 0.7699, F1 Macro: 0.6763\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9223, F1 Micro: 0.7676, F1 Macro: 0.6716\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9178, F1 Micro: 0.7625, F1 Macro: 0.6618\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9203, F1 Micro: 0.7714, F1 Macro: 0.6677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       0.59      0.14      0.22        72\n",
      "    HS_Gender       0.57      0.24      0.33        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.72      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 209.23371720314026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4007, Accuracy: 0.8792, F1 Micro: 0.6224, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2772, Accuracy: 0.9028, F1 Micro: 0.6826, F1 Macro: 0.4717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9082, F1 Micro: 0.7414, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.9194, F1 Micro: 0.7535, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9211, F1 Micro: 0.7627, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1347, Accuracy: 0.9211, F1 Micro: 0.765, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.9206, F1 Micro: 0.7654, F1 Macro: 0.6532\n",
      "Epoch 8/10, Train Loss: 0.1001, Accuracy: 0.9231, F1 Micro: 0.765, F1 Macro: 0.6633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9206, F1 Micro: 0.7712, F1 Macro: 0.6793\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9169, F1 Micro: 0.7618, F1 Macro: 0.6755\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9206, F1 Micro: 0.7712, F1 Macro: 0.6793\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.79      0.62      0.70       120\n",
      "  HS_Physical       0.76      0.18      0.29        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.91      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 209.72305727005005 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4034, Accuracy: 0.8795, F1 Micro: 0.6286, F1 Macro: 0.3021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.9039, F1 Micro: 0.696, F1 Macro: 0.5005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9083, F1 Micro: 0.7401, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1979, Accuracy: 0.9205, F1 Micro: 0.7549, F1 Macro: 0.6\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9189, F1 Micro: 0.7623, F1 Macro: 0.6245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9219, F1 Micro: 0.7663, F1 Macro: 0.635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1139, Accuracy: 0.9201, F1 Micro: 0.7675, F1 Macro: 0.6502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.6636\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9222, F1 Micro: 0.7686, F1 Macro: 0.6596\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9216, F1 Micro: 0.7657, F1 Macro: 0.678\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.6636\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.76      0.62      0.68       402\n",
      "  HS_Religion       0.82      0.48      0.61       157\n",
      "      HS_Race       0.84      0.62      0.72       120\n",
      "  HS_Physical       0.71      0.17      0.27        72\n",
      "    HS_Gender       0.53      0.18      0.26        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.8853464126587 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9211, F1 Micro: 0.772, F1 Macro: 0.6702\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 718.8671726850602\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 128.6244945526123 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3963, Accuracy: 0.8812, F1 Micro: 0.6353, F1 Macro: 0.3111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2758, Accuracy: 0.9025, F1 Micro: 0.7107, F1 Macro: 0.5312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2269, Accuracy: 0.9107, F1 Micro: 0.737, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1896, Accuracy: 0.9176, F1 Micro: 0.7546, F1 Macro: 0.593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.9189, F1 Micro: 0.7659, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.131, Accuracy: 0.9236, F1 Micro: 0.7698, F1 Macro: 0.6346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9204, F1 Micro: 0.7729, F1 Macro: 0.6691\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.9189, F1 Micro: 0.765, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.9213, F1 Micro: 0.773, F1 Macro: 0.6821\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9195, F1 Micro: 0.7647, F1 Macro: 0.6774\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9213, F1 Micro: 0.773, F1 Macro: 0.6821\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.71      0.59      0.64       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.71      0.21      0.32        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.77      0.82      0.80       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.56      0.57       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 214.06871223449707 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3987, Accuracy: 0.8807, F1 Micro: 0.6268, F1 Macro: 0.293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.9022, F1 Micro: 0.7017, F1 Macro: 0.522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.9105, F1 Micro: 0.7359, F1 Macro: 0.5837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9178, F1 Micro: 0.756, F1 Macro: 0.5941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1614, Accuracy: 0.919, F1 Micro: 0.7643, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9194, F1 Micro: 0.767, F1 Macro: 0.6366\n",
      "Epoch 7/10, Train Loss: 0.1106, Accuracy: 0.9171, F1 Micro: 0.7655, F1 Macro: 0.6696\n",
      "Epoch 8/10, Train Loss: 0.0968, Accuracy: 0.9156, F1 Micro: 0.7582, F1 Macro: 0.6711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9207, F1 Micro: 0.7688, F1 Macro: 0.6801\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.919, F1 Micro: 0.7639, F1 Macro: 0.6879\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9207, F1 Micro: 0.7688, F1 Macro: 0.6801\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.62      0.35      0.45        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.8791241645813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8813, F1 Micro: 0.6296, F1 Macro: 0.3026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2777, Accuracy: 0.9012, F1 Micro: 0.7082, F1 Macro: 0.5188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.9097, F1 Micro: 0.737, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9169, F1 Micro: 0.754, F1 Macro: 0.5949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.9215, F1 Micro: 0.7681, F1 Macro: 0.6248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9219, F1 Micro: 0.7725, F1 Macro: 0.6367\n",
      "Epoch 7/10, Train Loss: 0.1121, Accuracy: 0.9195, F1 Micro: 0.7643, F1 Macro: 0.6503\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9167, F1 Micro: 0.7595, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.9221, F1 Micro: 0.7769, F1 Macro: 0.6821\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9203, F1 Micro: 0.7739, F1 Macro: 0.6834\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9221, F1 Micro: 0.7769, F1 Macro: 0.6821\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.84      0.61      0.71       120\n",
      "  HS_Physical       0.68      0.18      0.29        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 210.40593099594116 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9214, F1 Micro: 0.7729, F1 Macro: 0.6814\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 719.5676823331196\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 115.34246110916138 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8849, F1 Micro: 0.6115, F1 Macro: 0.2989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2735, Accuracy: 0.9058, F1 Micro: 0.7134, F1 Macro: 0.5116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9151, F1 Micro: 0.7247, F1 Macro: 0.541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9194, F1 Micro: 0.7528, F1 Macro: 0.6058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.9226, F1 Micro: 0.759, F1 Macro: 0.6288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1326, Accuracy: 0.9201, F1 Micro: 0.7631, F1 Macro: 0.6368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.916, F1 Micro: 0.7643, F1 Macro: 0.6594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0924, Accuracy: 0.9217, F1 Micro: 0.774, F1 Macro: 0.6692\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.921, F1 Micro: 0.7684, F1 Macro: 0.6689\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9236, F1 Micro: 0.7633, F1 Macro: 0.6725\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9217, F1 Micro: 0.774, F1 Macro: 0.6692\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.67      0.71      0.69       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.20      0.29        51\n",
      "     HS_Other       0.74      0.84      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.58      0.63      0.60       331\n",
      "    HS_Strong       0.86      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 218.4932210445404 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4006, Accuracy: 0.882, F1 Micro: 0.6162, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9044, F1 Micro: 0.7042, F1 Macro: 0.5164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.9149, F1 Micro: 0.735, F1 Macro: 0.5517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9196, F1 Micro: 0.7502, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9219, F1 Micro: 0.7665, F1 Macro: 0.6411\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9215, F1 Micro: 0.7589, F1 Macro: 0.63\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1127, Accuracy: 0.9211, F1 Micro: 0.7702, F1 Macro: 0.677\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.9219, F1 Micro: 0.7642, F1 Macro: 0.6642\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.9221, F1 Micro: 0.769, F1 Macro: 0.6786\n",
      "Epoch 10/10, Train Loss: 0.0692, Accuracy: 0.9234, F1 Micro: 0.7668, F1 Macro: 0.6871\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9211, F1 Micro: 0.7702, F1 Macro: 0.677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.72      0.57      0.64       157\n",
      "      HS_Race       0.77      0.63      0.69       120\n",
      "  HS_Physical       0.52      0.15      0.24        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.89      0.87      0.88       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 213.62934684753418 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4027, Accuracy: 0.8836, F1 Micro: 0.6147, F1 Macro: 0.3002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.9033, F1 Micro: 0.7013, F1 Macro: 0.5162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2268, Accuracy: 0.9134, F1 Micro: 0.7186, F1 Macro: 0.5274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1907, Accuracy: 0.9174, F1 Micro: 0.7337, F1 Macro: 0.593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.9203, F1 Micro: 0.7592, F1 Macro: 0.62\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9202, F1 Micro: 0.7679, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9182, F1 Micro: 0.7692, F1 Macro: 0.6738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6672\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9222, F1 Micro: 0.7728, F1 Macro: 0.6725\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9242, F1 Micro: 0.7744, F1 Macro: 0.6809\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6672\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.81      0.69      0.74       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 217.39591455459595 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9223, F1 Micro: 0.7734, F1 Macro: 0.6711\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 999.7522890702544\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 100.68485713005066 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3943, Accuracy: 0.8833, F1 Micro: 0.5874, F1 Macro: 0.2832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9058, F1 Micro: 0.7139, F1 Macro: 0.5211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2199, Accuracy: 0.9131, F1 Micro: 0.7235, F1 Macro: 0.5677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9187, F1 Micro: 0.7545, F1 Macro: 0.6047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9203, F1 Micro: 0.7651, F1 Macro: 0.611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9216, F1 Micro: 0.7658, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9231, F1 Micro: 0.7695, F1 Macro: 0.6686\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9225, F1 Micro: 0.7607, F1 Macro: 0.6698\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9239, F1 Micro: 0.7689, F1 Macro: 0.6893\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.924, F1 Micro: 0.7692, F1 Macro: 0.6867\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9231, F1 Micro: 0.7695, F1 Macro: 0.6686\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.76      0.68      0.72       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.80      0.69      0.74       120\n",
      "  HS_Physical       0.55      0.15      0.24        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.75      0.64      0.69       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.74      0.63      0.67      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 220.35821390151978 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3976, Accuracy: 0.8815, F1 Micro: 0.5734, F1 Macro: 0.2675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.9037, F1 Micro: 0.712, F1 Macro: 0.5122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2236, Accuracy: 0.9123, F1 Micro: 0.729, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1886, Accuracy: 0.9203, F1 Micro: 0.761, F1 Macro: 0.6094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9216, F1 Micro: 0.7676, F1 Macro: 0.6255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9218, F1 Micro: 0.7687, F1 Macro: 0.6503\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9188, F1 Micro: 0.7681, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0994, Accuracy: 0.9225, F1 Micro: 0.771, F1 Macro: 0.6846\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9217, F1 Micro: 0.7624, F1 Macro: 0.6687\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9219, F1 Micro: 0.7697, F1 Macro: 0.6885\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9225, F1 Micro: 0.771, F1 Macro: 0.6846\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.70      0.72       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.85      0.51      0.64       157\n",
      "      HS_Race       0.85      0.61      0.71       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.68      0.70       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.92      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 218.92652487754822 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3986, Accuracy: 0.8833, F1 Micro: 0.6035, F1 Macro: 0.2899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.903, F1 Micro: 0.7136, F1 Macro: 0.531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9133, F1 Micro: 0.724, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9192, F1 Micro: 0.7566, F1 Macro: 0.6055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9217, F1 Micro: 0.7644, F1 Macro: 0.6161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.133, Accuracy: 0.9229, F1 Micro: 0.7662, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9205, F1 Micro: 0.7711, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.9215, F1 Micro: 0.7724, F1 Macro: 0.6742\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9243, F1 Micro: 0.7722, F1 Macro: 0.6785\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9182, F1 Micro: 0.7704, F1 Macro: 0.6726\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9215, F1 Micro: 0.7724, F1 Macro: 0.6742\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.84      0.67      0.74       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.52      0.24      0.32        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.60      0.64      0.62       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.65      0.67      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 221.88272190093994 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9224, F1 Micro: 0.7709, F1 Macro: 0.6758\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 933.3210113507564\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 89.1565318107605 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.392, Accuracy: 0.8846, F1 Micro: 0.5845, F1 Macro: 0.2836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2671, Accuracy: 0.904, F1 Micro: 0.7106, F1 Macro: 0.4907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2187, Accuracy: 0.9113, F1 Micro: 0.7459, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9154, F1 Micro: 0.7628, F1 Macro: 0.635\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9205, F1 Micro: 0.7502, F1 Macro: 0.6124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1248, Accuracy: 0.921, F1 Micro: 0.7677, F1 Macro: 0.6626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9214, F1 Micro: 0.7687, F1 Macro: 0.6677\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9188, F1 Micro: 0.7685, F1 Macro: 0.669\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9222, F1 Micro: 0.7656, F1 Macro: 0.6729\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9175, F1 Micro: 0.7681, F1 Macro: 0.6802\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9214, F1 Micro: 0.7687, F1 Macro: 0.6677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.70      0.65      0.68       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.47      0.11      0.18        72\n",
      "    HS_Gender       0.72      0.25      0.38        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.63      0.54      0.58       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 219.14413571357727 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3947, Accuracy: 0.882, F1 Micro: 0.5811, F1 Macro: 0.2757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2694, Accuracy: 0.9047, F1 Micro: 0.7094, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2223, Accuracy: 0.9108, F1 Micro: 0.7467, F1 Macro: 0.5852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9138, F1 Micro: 0.7571, F1 Macro: 0.6388\n",
      "Epoch 5/10, Train Loss: 0.157, Accuracy: 0.92, F1 Micro: 0.7516, F1 Macro: 0.6243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9212, F1 Micro: 0.7675, F1 Macro: 0.6602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9234, F1 Micro: 0.7736, F1 Macro: 0.6783\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9194, F1 Micro: 0.7656, F1 Macro: 0.6743\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9214, F1 Micro: 0.769, F1 Macro: 0.6927\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.916, F1 Micro: 0.7631, F1 Macro: 0.6917\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9234, F1 Micro: 0.7736, F1 Macro: 0.6783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.81      0.54      0.64       157\n",
      "      HS_Race       0.81      0.68      0.74       120\n",
      "  HS_Physical       0.67      0.14      0.23        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 219.05873322486877 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8829, F1 Micro: 0.5842, F1 Macro: 0.287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2704, Accuracy: 0.9041, F1 Micro: 0.7148, F1 Macro: 0.5015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2227, Accuracy: 0.9105, F1 Micro: 0.7474, F1 Macro: 0.5869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1835, Accuracy: 0.9119, F1 Micro: 0.7558, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9209, F1 Micro: 0.757, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.923, F1 Micro: 0.7671, F1 Macro: 0.6508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1116, Accuracy: 0.9229, F1 Micro: 0.7716, F1 Macro: 0.6687\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9195, F1 Micro: 0.7693, F1 Macro: 0.6692\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9215, F1 Micro: 0.7661, F1 Macro: 0.6762\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9201, F1 Micro: 0.7652, F1 Macro: 0.673\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9229, F1 Micro: 0.7716, F1 Macro: 0.6687\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.76      0.64      0.70       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.55      0.22      0.31        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.84      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 220.5075581073761 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9225, F1 Micro: 0.7713, F1 Macro: 0.6716\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 332.7276635919119\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 89.69013524055481 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3913, Accuracy: 0.8853, F1 Micro: 0.6065, F1 Macro: 0.3006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.9059, F1 Micro: 0.6963, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2193, Accuracy: 0.9136, F1 Micro: 0.735, F1 Macro: 0.5447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9117, F1 Micro: 0.7509, F1 Macro: 0.6142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9193, F1 Micro: 0.7661, F1 Macro: 0.6316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1252, Accuracy: 0.9205, F1 Micro: 0.7664, F1 Macro: 0.6392\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9221, F1 Micro: 0.7639, F1 Macro: 0.6552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9222, F1 Micro: 0.7713, F1 Macro: 0.6821\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.9201, F1 Micro: 0.7659, F1 Macro: 0.679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9207, F1 Micro: 0.7727, F1 Macro: 0.6971\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9207, F1 Micro: 0.7727, F1 Macro: 0.6971\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.93      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.68      0.59      0.63       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.55      0.33      0.41        72\n",
      "    HS_Gender       0.56      0.49      0.52        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.58      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.71      0.69      0.70      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 225.60500931739807 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3923, Accuracy: 0.8847, F1 Micro: 0.5956, F1 Macro: 0.2984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.268, Accuracy: 0.9057, F1 Micro: 0.6939, F1 Macro: 0.4982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2225, Accuracy: 0.9135, F1 Micro: 0.7358, F1 Macro: 0.5472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9116, F1 Micro: 0.7532, F1 Macro: 0.6197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.9229, F1 Micro: 0.7693, F1 Macro: 0.6394\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9209, F1 Micro: 0.7672, F1 Macro: 0.6492\n",
      "Epoch 7/10, Train Loss: 0.1086, Accuracy: 0.9227, F1 Micro: 0.7672, F1 Macro: 0.6571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9187, F1 Micro: 0.7722, F1 Macro: 0.6924\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9235, F1 Micro: 0.7692, F1 Macro: 0.6912\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9196, F1 Micro: 0.7671, F1 Macro: 0.7037\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9187, F1 Micro: 0.7722, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.84      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.66      0.67      0.66       402\n",
      "  HS_Religion       0.72      0.63      0.67       157\n",
      "      HS_Race       0.71      0.73      0.72       120\n",
      "  HS_Physical       0.76      0.22      0.34        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.73      0.85      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.85      0.89      0.87       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.77      5556\n",
      "    macro avg       0.73      0.69      0.69      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 222.4242513179779 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3958, Accuracy: 0.8839, F1 Micro: 0.6022, F1 Macro: 0.299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.9055, F1 Micro: 0.6859, F1 Macro: 0.5043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9122, F1 Micro: 0.7254, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1821, Accuracy: 0.9117, F1 Micro: 0.7525, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9231, F1 Micro: 0.7692, F1 Macro: 0.6306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9225, F1 Micro: 0.7717, F1 Macro: 0.6399\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9214, F1 Micro: 0.7705, F1 Macro: 0.669\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.916, F1 Micro: 0.7674, F1 Macro: 0.6689\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9219, F1 Micro: 0.7712, F1 Macro: 0.6898\n",
      "Epoch 10/10, Train Loss: 0.0663, Accuracy: 0.9223, F1 Micro: 0.77, F1 Macro: 0.6847\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9225, F1 Micro: 0.7717, F1 Macro: 0.6399\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.80      0.58      0.67       157\n",
      "      HS_Race       0.85      0.60      0.70       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.91      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.78      0.61      0.64      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 221.26280164718628 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9206, F1 Micro: 0.7722, F1 Macro: 0.6765\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 629.4400595749835\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 75.86860513687134 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3897, Accuracy: 0.8866, F1 Micro: 0.6052, F1 Macro: 0.292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.9046, F1 Micro: 0.7152, F1 Macro: 0.5297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9127, F1 Micro: 0.7474, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1769, Accuracy: 0.9193, F1 Micro: 0.7569, F1 Macro: 0.6069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1525, Accuracy: 0.9187, F1 Micro: 0.7661, F1 Macro: 0.6437\n",
      "Epoch 6/10, Train Loss: 0.1284, Accuracy: 0.92, F1 Micro: 0.7649, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9214, F1 Micro: 0.7671, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9235, F1 Micro: 0.7761, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9211, F1 Micro: 0.7707, F1 Macro: 0.6858\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9221, F1 Micro: 0.7705, F1 Macro: 0.6955\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9235, F1 Micro: 0.7761, F1 Macro: 0.6936\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.81      0.69      0.75       120\n",
      "  HS_Physical       0.56      0.31      0.40        72\n",
      "    HS_Gender       0.55      0.33      0.41        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.90      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.74      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 228.251939535141 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3923, Accuracy: 0.8856, F1 Micro: 0.6133, F1 Macro: 0.2945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.9027, F1 Micro: 0.7092, F1 Macro: 0.5303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.218, Accuracy: 0.911, F1 Micro: 0.7474, F1 Macro: 0.5976\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.919, F1 Micro: 0.7455, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1541, Accuracy: 0.9224, F1 Micro: 0.7676, F1 Macro: 0.6363\n",
      "Epoch 6/10, Train Loss: 0.1303, Accuracy: 0.921, F1 Micro: 0.7614, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9218, F1 Micro: 0.7716, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9251, F1 Micro: 0.7755, F1 Macro: 0.6912\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9187, F1 Micro: 0.7673, F1 Macro: 0.6716\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.922, F1 Micro: 0.7709, F1 Macro: 0.7007\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9251, F1 Micro: 0.7755, F1 Macro: 0.6912\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.76      0.60      0.67       402\n",
      "  HS_Religion       0.83      0.52      0.64       157\n",
      "      HS_Race       0.85      0.65      0.74       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.65      0.52      0.58       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.81      0.64      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 226.7647521495819 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3947, Accuracy: 0.8848, F1 Micro: 0.6111, F1 Macro: 0.2962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2659, Accuracy: 0.9019, F1 Micro: 0.7112, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2181, Accuracy: 0.9113, F1 Micro: 0.7465, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9198, F1 Micro: 0.7545, F1 Macro: 0.5955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1553, Accuracy: 0.9178, F1 Micro: 0.7679, F1 Macro: 0.6301\n",
      "Epoch 6/10, Train Loss: 0.1312, Accuracy: 0.9202, F1 Micro: 0.7643, F1 Macro: 0.6473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9251, F1 Micro: 0.7743, F1 Macro: 0.6642\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9219, F1 Micro: 0.7706, F1 Macro: 0.6751\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9208, F1 Micro: 0.77, F1 Macro: 0.6754\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9228, F1 Micro: 0.77, F1 Macro: 0.6943\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9251, F1 Micro: 0.7743, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.75      0.63      0.69       402\n",
      "  HS_Religion       0.81      0.61      0.69       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.62      0.25      0.36        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.77      5556\n",
      "    macro avg       0.77      0.62      0.66      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 225.92040419578552 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9246, F1 Micro: 0.7753, F1 Macro: 0.683\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 349.114197696029\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 63.47227454185486 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3876, Accuracy: 0.8889, F1 Micro: 0.6393, F1 Macro: 0.3183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.9063, F1 Micro: 0.7172, F1 Macro: 0.5224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.9166, F1 Micro: 0.7323, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.9191, F1 Micro: 0.7473, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.9205, F1 Micro: 0.7665, F1 Macro: 0.6556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1273, Accuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9231, F1 Micro: 0.7766, F1 Macro: 0.6727\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9235, F1 Micro: 0.7722, F1 Macro: 0.688\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.9243, F1 Micro: 0.7703, F1 Macro: 0.6987\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9215, F1 Micro: 0.7751, F1 Macro: 0.6913\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9231, F1 Micro: 0.7766, F1 Macro: 0.6727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.74      0.66      0.70       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.74      0.27      0.40        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.89      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 230.6309220790863 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3896, Accuracy: 0.8866, F1 Micro: 0.6323, F1 Macro: 0.3063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9049, F1 Micro: 0.7085, F1 Macro: 0.5169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.917, F1 Micro: 0.7351, F1 Macro: 0.5755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9184, F1 Micro: 0.7452, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9201, F1 Micro: 0.7669, F1 Macro: 0.6709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1279, Accuracy: 0.9204, F1 Micro: 0.7686, F1 Macro: 0.649\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9191, F1 Micro: 0.7685, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9232, F1 Micro: 0.7701, F1 Macro: 0.6786\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.9256, F1 Micro: 0.7687, F1 Macro: 0.7021\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9206, F1 Micro: 0.768, F1 Macro: 0.6977\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9232, F1 Micro: 0.7701, F1 Macro: 0.6786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.73      0.58      0.65       402\n",
      "  HS_Religion       0.83      0.52      0.64       157\n",
      "      HS_Race       0.85      0.60      0.70       120\n",
      "  HS_Physical       0.70      0.22      0.34        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.49      0.56       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.63      0.68      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 229.78606414794922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8863, F1 Micro: 0.6165, F1 Macro: 0.3063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.9053, F1 Micro: 0.7176, F1 Macro: 0.5294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.9173, F1 Micro: 0.7361, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9197, F1 Micro: 0.7514, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.9212, F1 Micro: 0.7694, F1 Macro: 0.6435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9237, F1 Micro: 0.7731, F1 Macro: 0.6564\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9212, F1 Micro: 0.7694, F1 Macro: 0.663\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9221, F1 Micro: 0.7726, F1 Macro: 0.6824\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9224, F1 Micro: 0.7693, F1 Macro: 0.6836\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9216, F1 Micro: 0.7677, F1 Macro: 0.6818\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9237, F1 Micro: 0.7731, F1 Macro: 0.6564\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.91      0.85      0.88       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.78      0.62      0.69       402\n",
      "  HS_Religion       0.79      0.50      0.61       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.63      0.24      0.34        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.70      0.51      0.59       331\n",
      "    HS_Strong       0.86      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.79      0.62      0.66      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 228.21328926086426 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9233, F1 Micro: 0.7733, F1 Macro: 0.6692\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 543.2551085876357\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 49.10603666305542 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.385, Accuracy: 0.8865, F1 Micro: 0.6027, F1 Macro: 0.2927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.261, Accuracy: 0.9057, F1 Micro: 0.7224, F1 Macro: 0.5414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2121, Accuracy: 0.9171, F1 Micro: 0.7514, F1 Macro: 0.5875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.9181, F1 Micro: 0.7669, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9227, F1 Micro: 0.7713, F1 Macro: 0.6508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9234, F1 Micro: 0.7741, F1 Macro: 0.6597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9224, F1 Micro: 0.776, F1 Macro: 0.6844\n",
      "Epoch 8/10, Train Loss: 0.0844, Accuracy: 0.9255, F1 Micro: 0.7742, F1 Macro: 0.6903\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9235, F1 Micro: 0.7745, F1 Macro: 0.6854\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.924, F1 Micro: 0.7748, F1 Macro: 0.6953\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9224, F1 Micro: 0.776, F1 Macro: 0.6844\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.68      0.60      0.64       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.68      0.21      0.32        72\n",
      "    HS_Gender       0.79      0.29      0.43        51\n",
      "     HS_Other       0.77      0.83      0.80       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 236.17071104049683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.886, F1 Micro: 0.607, F1 Macro: 0.302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.9052, F1 Micro: 0.7061, F1 Macro: 0.5294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.9172, F1 Micro: 0.7502, F1 Macro: 0.59\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9203, F1 Micro: 0.7649, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9224, F1 Micro: 0.7695, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1273, Accuracy: 0.9232, F1 Micro: 0.7737, F1 Macro: 0.6707\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9233, F1 Micro: 0.7702, F1 Macro: 0.6805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9252, F1 Micro: 0.775, F1 Macro: 0.6918\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9251, F1 Micro: 0.7746, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9256, F1 Micro: 0.7808, F1 Macro: 0.7097\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9256, F1 Micro: 0.7808, F1 Macro: 0.7097\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.74      0.64      0.69       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.90      0.26      0.41        72\n",
      "    HS_Gender       0.58      0.51      0.54        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.72       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 236.75007820129395 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3899, Accuracy: 0.8836, F1 Micro: 0.5737, F1 Macro: 0.2761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9064, F1 Micro: 0.7155, F1 Macro: 0.5388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9158, F1 Micro: 0.7442, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1799, Accuracy: 0.9187, F1 Micro: 0.7612, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9225, F1 Micro: 0.7713, F1 Macro: 0.6508\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.9239, F1 Micro: 0.7691, F1 Macro: 0.6521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9238, F1 Micro: 0.7716, F1 Macro: 0.6654\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9241, F1 Micro: 0.7674, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9223, F1 Micro: 0.7727, F1 Macro: 0.6725\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.923, F1 Micro: 0.7705, F1 Macro: 0.6971\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9223, F1 Micro: 0.7727, F1 Macro: 0.6725\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.74      0.64      0.68       157\n",
      "      HS_Race       0.84      0.62      0.71       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 235.03631711006165 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9234, F1 Micro: 0.7765, F1 Macro: 0.6889\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 487.76263209037586\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 36.26385283470154 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3839, Accuracy: 0.8889, F1 Micro: 0.6405, F1 Macro: 0.3352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2591, Accuracy: 0.9055, F1 Micro: 0.7181, F1 Macro: 0.5216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9149, F1 Micro: 0.75, F1 Macro: 0.5952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9211, F1 Micro: 0.7534, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1444, Accuracy: 0.9211, F1 Micro: 0.7653, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.119, Accuracy: 0.9223, F1 Micro: 0.7733, F1 Macro: 0.6694\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.9225, F1 Micro: 0.7697, F1 Macro: 0.6733\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9212, F1 Micro: 0.7698, F1 Macro: 0.6614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9185, F1 Micro: 0.7734, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9209, F1 Micro: 0.7746, F1 Macro: 0.7015\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9209, F1 Micro: 0.7746, F1 Macro: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.86      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.66      0.68      0.67       402\n",
      "  HS_Religion       0.67      0.62      0.64       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.76      0.43      0.55        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.57      0.58      0.58       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 241.17267680168152 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3849, Accuracy: 0.8867, F1 Micro: 0.6147, F1 Macro: 0.3198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2607, Accuracy: 0.9026, F1 Micro: 0.7174, F1 Macro: 0.518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9135, F1 Micro: 0.7526, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1757, Accuracy: 0.9215, F1 Micro: 0.7633, F1 Macro: 0.619\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9225, F1 Micro: 0.7615, F1 Macro: 0.6566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.9214, F1 Micro: 0.7727, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.925, F1 Micro: 0.7756, F1 Macro: 0.6957\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9238, F1 Micro: 0.7713, F1 Macro: 0.6701\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9238, F1 Micro: 0.774, F1 Macro: 0.7013\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9239, F1 Micro: 0.7722, F1 Macro: 0.6945\n",
      "Model 2 - Iteration 10218: Accuracy: 0.925, F1 Micro: 0.7756, F1 Macro: 0.6957\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.78      0.67      0.72       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.78      0.64      0.70       157\n",
      "      HS_Race       0.78      0.74      0.76       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.76      0.66      0.71       689\n",
      "  HS_Moderate       0.60      0.63      0.62       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.77      0.66      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 235.65814566612244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3883, Accuracy: 0.8866, F1 Micro: 0.6217, F1 Macro: 0.32\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2627, Accuracy: 0.9033, F1 Micro: 0.7139, F1 Macro: 0.5123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2143, Accuracy: 0.9125, F1 Micro: 0.7471, F1 Macro: 0.5941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9195, F1 Micro: 0.7581, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1463, Accuracy: 0.9219, F1 Micro: 0.7658, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.923, F1 Micro: 0.7741, F1 Macro: 0.6587\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9242, F1 Micro: 0.77, F1 Macro: 0.6715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.925, F1 Micro: 0.7777, F1 Macro: 0.682\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9205, F1 Micro: 0.7727, F1 Macro: 0.6887\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9247, F1 Micro: 0.7775, F1 Macro: 0.7\n",
      "Model 3 - Iteration 10218: Accuracy: 0.925, F1 Micro: 0.7777, F1 Macro: 0.682\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 237.33199429512024 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9236, F1 Micro: 0.776, F1 Macro: 0.6931\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 11.563536290018968\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.824583530426025 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3773, Accuracy: 0.8857, F1 Micro: 0.5941, F1 Macro: 0.3033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.9066, F1 Micro: 0.6943, F1 Macro: 0.5064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9176, F1 Micro: 0.746, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9211, F1 Micro: 0.7585, F1 Macro: 0.6151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9229, F1 Micro: 0.7684, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1176, Accuracy: 0.9245, F1 Micro: 0.7752, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9222, F1 Micro: 0.7764, F1 Macro: 0.682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9246, F1 Micro: 0.7794, F1 Macro: 0.6975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9219, F1 Micro: 0.7801, F1 Macro: 0.6988\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9235, F1 Micro: 0.7741, F1 Macro: 0.7014\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9219, F1 Micro: 0.7801, F1 Macro: 0.6988\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.66      0.71      0.68       402\n",
      "  HS_Religion       0.66      0.68      0.67       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.73      0.26      0.39        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.59      0.63      0.61       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.72      0.70      0.70      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.46      0.45      0.44      5556\n",
      "\n",
      "Training completed in 246.6491413116455 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8829, F1 Micro: 0.5704, F1 Macro: 0.2765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.905, F1 Micro: 0.6798, F1 Macro: 0.4698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9156, F1 Micro: 0.7425, F1 Macro: 0.5599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.9189, F1 Micro: 0.7608, F1 Macro: 0.6173\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9204, F1 Micro: 0.7598, F1 Macro: 0.6246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1193, Accuracy: 0.9222, F1 Micro: 0.7725, F1 Macro: 0.6592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9201, F1 Micro: 0.7737, F1 Macro: 0.6901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.923, F1 Micro: 0.7764, F1 Macro: 0.6934\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9212, F1 Micro: 0.7722, F1 Macro: 0.6974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9231, F1 Micro: 0.7768, F1 Macro: 0.7019\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9231, F1 Micro: 0.7768, F1 Macro: 0.7019\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.77      0.54      0.64       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.81      0.29      0.43        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.66      0.55      0.60       331\n",
      "    HS_Strong       0.91      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 243.30103707313538 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.8863, F1 Micro: 0.6095, F1 Macro: 0.3052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2559, Accuracy: 0.9064, F1 Micro: 0.6879, F1 Macro: 0.4951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9166, F1 Micro: 0.7433, F1 Macro: 0.572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9198, F1 Micro: 0.7607, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1433, Accuracy: 0.9214, F1 Micro: 0.7673, F1 Macro: 0.6161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9233, F1 Micro: 0.7718, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9196, F1 Micro: 0.7738, F1 Macro: 0.6761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9248, F1 Micro: 0.7746, F1 Macro: 0.6887\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9234, F1 Micro: 0.7712, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9239, F1 Micro: 0.77, F1 Macro: 0.687\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9248, F1 Micro: 0.7746, F1 Macro: 0.6887\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.78      0.68      0.73       732\n",
      "     HS_Group       0.70      0.69      0.70       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.65      0.21      0.32        72\n",
      "    HS_Gender       0.54      0.39      0.45        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.76      0.66      0.71       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.75      0.65      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 243.77527356147766 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9232, F1 Micro: 0.7772, F1 Macro: 0.6965\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -12.430251758684985\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 7.83891749382019 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8872, F1 Micro: 0.5935, F1 Macro: 0.2957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.905, F1 Micro: 0.7241, F1 Macro: 0.5437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9181, F1 Micro: 0.7532, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9199, F1 Micro: 0.7626, F1 Macro: 0.6245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9233, F1 Micro: 0.7695, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9226, F1 Micro: 0.7746, F1 Macro: 0.6776\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9228, F1 Micro: 0.7688, F1 Macro: 0.669\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9245, F1 Micro: 0.7726, F1 Macro: 0.6815\n",
      "Epoch 9/10, Train Loss: 0.0698, Accuracy: 0.9221, F1 Micro: 0.7688, F1 Macro: 0.6835\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9195, F1 Micro: 0.7718, F1 Macro: 0.6945\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9226, F1 Micro: 0.7746, F1 Macro: 0.6776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.70      0.69      0.69       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.75      0.63      0.69       120\n",
      "  HS_Physical       0.55      0.15      0.24        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.73      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 244.8725037574768 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3796, Accuracy: 0.8837, F1 Micro: 0.5647, F1 Macro: 0.2726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.9058, F1 Micro: 0.7228, F1 Macro: 0.5387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9169, F1 Micro: 0.7489, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1726, Accuracy: 0.9217, F1 Micro: 0.7584, F1 Macro: 0.62\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.924, F1 Micro: 0.7727, F1 Macro: 0.6591\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9227, F1 Micro: 0.7719, F1 Macro: 0.672\n",
      "Epoch 7/10, Train Loss: 0.0993, Accuracy: 0.9236, F1 Micro: 0.7578, F1 Macro: 0.6639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0859, Accuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6871\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9239, F1 Micro: 0.7721, F1 Macro: 0.6971\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9211, F1 Micro: 0.7742, F1 Macro: 0.7079\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6871\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.80      0.58      0.67       157\n",
      "      HS_Race       0.84      0.68      0.75       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.66      0.56      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.80      0.64      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 242.84138798713684 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3818, Accuracy: 0.8869, F1 Micro: 0.6198, F1 Macro: 0.318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9046, F1 Micro: 0.7141, F1 Macro: 0.54\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9167, F1 Micro: 0.745, F1 Macro: 0.5837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1739, Accuracy: 0.9217, F1 Micro: 0.7554, F1 Macro: 0.6101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9227, F1 Micro: 0.7666, F1 Macro: 0.6226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1183, Accuracy: 0.9196, F1 Micro: 0.7695, F1 Macro: 0.6608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.9226, F1 Micro: 0.7732, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0868, Accuracy: 0.9229, F1 Micro: 0.7734, F1 Macro: 0.6738\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9231, F1 Micro: 0.7728, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9233, F1 Micro: 0.7758, F1 Macro: 0.7026\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9233, F1 Micro: 0.7758, F1 Macro: 0.7026\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.69      0.66      0.68       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.84      0.29      0.43        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.83      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 247.88821077346802 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9235, F1 Micro: 0.775, F1 Macro: 0.6891\n",
      "Total sampling time: 6161.68 seconds\n",
      "Total runtime: 22104.333236932755 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c189d5",
   "metadata": {
    "papermill": {
     "duration": 0.012103,
     "end_time": "2025-06-08T08:00:31.308680",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.296577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5809, Accuracy: 0.8245, F1 Micro: 0.3956, F1 Macro: 0.1034\n",
      "Epoch 2/10, Train Loss: 0.4658, Accuracy: 0.8348, F1 Micro: 0.2636, F1 Macro: 0.0592\n",
      "Epoch 3/10, Train Loss: 0.397, Accuracy: 0.8338, F1 Micro: 0.0955, F1 Macro: 0.0378\n",
      "Epoch 4/10, Train Loss: 0.3979, Accuracy: 0.8362, F1 Micro: 0.14, F1 Macro: 0.0465\n",
      "Epoch 5/10, Train Loss: 0.383, Accuracy: 0.8437, F1 Micro: 0.2102, F1 Macro: 0.0769\n",
      "Epoch 6/10, Train Loss: 0.3654, Accuracy: 0.8522, F1 Micro: 0.3188, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3505, Accuracy: 0.8651, F1 Micro: 0.4503, F1 Macro: 0.1947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3265, Accuracy: 0.8738, F1 Micro: 0.5242, F1 Macro: 0.2445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2968, Accuracy: 0.8777, F1 Micro: 0.5603, F1 Macro: 0.2621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2615, Accuracy: 0.8778, F1 Micro: 0.5617, F1 Macro: 0.2627\n",
      "Model 1 - Iteration 658: Accuracy: 0.8778, F1 Micro: 0.5617, F1 Macro: 0.2627\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.70      0.75      1134\n",
      "      Abusive       0.84      0.69      0.75       992\n",
      "HS_Individual       0.65      0.47      0.55       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.52      0.59       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.74      0.45      0.56      5556\n",
      "    macro avg       0.30      0.24      0.26      5556\n",
      " weighted avg       0.57      0.45      0.50      5556\n",
      "  samples avg       0.36      0.26      0.28      5556\n",
      "\n",
      "Training completed in 47.42198967933655 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6069, Accuracy: 0.818, F1 Micro: 0.3417, F1 Macro: 0.0941\n",
      "Epoch 2/10, Train Loss: 0.4689, Accuracy: 0.8331, F1 Micro: 0.122, F1 Macro: 0.0467\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8344, F1 Micro: 0.0998, F1 Macro: 0.0364\n",
      "Epoch 4/10, Train Loss: 0.3974, Accuracy: 0.8374, F1 Micro: 0.1422, F1 Macro: 0.047\n",
      "Epoch 5/10, Train Loss: 0.3818, Accuracy: 0.8464, F1 Micro: 0.2441, F1 Macro: 0.0839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3661, Accuracy: 0.8562, F1 Micro: 0.35, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3507, Accuracy: 0.865, F1 Micro: 0.4447, F1 Macro: 0.1653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.324, Accuracy: 0.8743, F1 Micro: 0.5154, F1 Macro: 0.2311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2909, Accuracy: 0.8765, F1 Micro: 0.5649, F1 Macro: 0.2614\n",
      "Epoch 10/10, Train Loss: 0.2604, Accuracy: 0.8767, F1 Micro: 0.557, F1 Macro: 0.2676\n",
      "Model 2 - Iteration 658: Accuracy: 0.8765, F1 Micro: 0.5649, F1 Macro: 0.2614\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.72      0.76      1134\n",
      "      Abusive       0.81      0.73      0.77       992\n",
      "HS_Individual       0.64      0.45      0.53       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.62      0.55      0.58       762\n",
      "      HS_Weak       0.60      0.42      0.50       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.47      0.56      5556\n",
      "    macro avg       0.29      0.24      0.26      5556\n",
      " weighted avg       0.55      0.47      0.50      5556\n",
      "  samples avg       0.37      0.27      0.29      5556\n",
      "\n",
      "Training completed in 46.874878883361816 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6221, Accuracy: 0.801, F1 Micro: 0.2723, F1 Macro: 0.0864\n",
      "Epoch 2/10, Train Loss: 0.4752, Accuracy: 0.8292, F1 Micro: 0.0255, F1 Macro: 0.0111\n",
      "Epoch 3/10, Train Loss: 0.3966, Accuracy: 0.83, F1 Micro: 0.0308, F1 Macro: 0.0133\n",
      "Epoch 4/10, Train Loss: 0.4013, Accuracy: 0.8341, F1 Micro: 0.1023, F1 Macro: 0.0364\n",
      "Epoch 5/10, Train Loss: 0.3872, Accuracy: 0.8389, F1 Micro: 0.1576, F1 Macro: 0.0571\n",
      "Epoch 6/10, Train Loss: 0.3748, Accuracy: 0.8414, F1 Micro: 0.1864, F1 Macro: 0.0657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3672, Accuracy: 0.8496, F1 Micro: 0.2928, F1 Macro: 0.0947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3502, Accuracy: 0.8627, F1 Micro: 0.4235, F1 Macro: 0.1614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3159, Accuracy: 0.8761, F1 Micro: 0.5531, F1 Macro: 0.2496\n",
      "Epoch 10/10, Train Loss: 0.2849, Accuracy: 0.8769, F1 Micro: 0.5458, F1 Macro: 0.2515\n",
      "Model 3 - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5531, F1 Macro: 0.2496\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.73      0.75      1134\n",
      "      Abusive       0.77      0.76      0.77       992\n",
      "HS_Individual       0.69      0.37      0.48       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.55      0.59       762\n",
      "      HS_Weak       0.70      0.29      0.41       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.44      0.55      5556\n",
      "    macro avg       0.30      0.22      0.25      5556\n",
      " weighted avg       0.56      0.44      0.48      5556\n",
      "  samples avg       0.39      0.27      0.29      5556\n",
      "\n",
      "Training completed in 44.76917886734009 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8768, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 759.5093331483013\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 623.2086145877838 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5192, Accuracy: 0.8348, F1 Micro: 0.2139, F1 Macro: 0.0531\n",
      "Epoch 2/10, Train Loss: 0.4011, Accuracy: 0.8356, F1 Micro: 0.1145, F1 Macro: 0.0437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3731, Accuracy: 0.855, F1 Micro: 0.3571, F1 Macro: 0.1314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3425, Accuracy: 0.8779, F1 Micro: 0.5606, F1 Macro: 0.2621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2947, Accuracy: 0.8828, F1 Micro: 0.5844, F1 Macro: 0.2821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2745, Accuracy: 0.8884, F1 Micro: 0.6498, F1 Macro: 0.3579\n",
      "Epoch 7/10, Train Loss: 0.2518, Accuracy: 0.8906, F1 Micro: 0.6253, F1 Macro: 0.349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2226, Accuracy: 0.893, F1 Micro: 0.6606, F1 Macro: 0.4214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1918, Accuracy: 0.8957, F1 Micro: 0.6686, F1 Macro: 0.458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1773, Accuracy: 0.8959, F1 Micro: 0.6859, F1 Macro: 0.4916\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8959, F1 Micro: 0.6859, F1 Macro: 0.4916\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.83      0.81      1134\n",
      "      Abusive       0.87      0.80      0.84       992\n",
      "HS_Individual       0.66      0.65      0.66       732\n",
      "     HS_Group       0.62      0.55      0.58       402\n",
      "  HS_Religion       0.62      0.36      0.45       157\n",
      "      HS_Race       0.70      0.59      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.70      0.69       762\n",
      "      HS_Weak       0.63      0.62      0.62       689\n",
      "  HS_Moderate       0.47      0.41      0.44       331\n",
      "    HS_Strong       0.92      0.10      0.17       114\n",
      "\n",
      "    micro avg       0.71      0.66      0.69      5556\n",
      "    macro avg       0.58      0.47      0.49      5556\n",
      " weighted avg       0.70      0.66      0.67      5556\n",
      "  samples avg       0.40      0.37      0.36      5556\n",
      "\n",
      "Training completed in 66.17478203773499 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5295, Accuracy: 0.8335, F1 Micro: 0.1176, F1 Macro: 0.0455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3997, Accuracy: 0.8384, F1 Micro: 0.1466, F1 Macro: 0.0527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3707, Accuracy: 0.8584, F1 Micro: 0.3971, F1 Macro: 0.124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3371, Accuracy: 0.8753, F1 Micro: 0.5166, F1 Macro: 0.2335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.291, Accuracy: 0.8797, F1 Micro: 0.614, F1 Macro: 0.3001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.273, Accuracy: 0.8859, F1 Micro: 0.6209, F1 Macro: 0.3233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2497, Accuracy: 0.8911, F1 Micro: 0.6504, F1 Macro: 0.369\n",
      "Epoch 8/10, Train Loss: 0.2224, Accuracy: 0.8922, F1 Micro: 0.6473, F1 Macro: 0.398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1926, Accuracy: 0.8947, F1 Micro: 0.6638, F1 Macro: 0.4584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1802, Accuracy: 0.8944, F1 Micro: 0.6686, F1 Macro: 0.4792\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8944, F1 Micro: 0.6686, F1 Macro: 0.4792\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.78      0.80      1134\n",
      "      Abusive       0.89      0.76      0.82       992\n",
      "HS_Individual       0.66      0.62      0.64       732\n",
      "     HS_Group       0.66      0.49      0.56       402\n",
      "  HS_Religion       0.56      0.32      0.41       157\n",
      "      HS_Race       0.82      0.49      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.65      0.67       762\n",
      "      HS_Weak       0.63      0.58      0.60       689\n",
      "  HS_Moderate       0.47      0.36      0.41       331\n",
      "    HS_Strong       0.79      0.13      0.23       114\n",
      "\n",
      "    micro avg       0.73      0.62      0.67      5556\n",
      "    macro avg       0.58      0.43      0.48      5556\n",
      " weighted avg       0.71      0.62      0.65      5556\n",
      "  samples avg       0.38      0.34      0.34      5556\n",
      "\n",
      "Training completed in 67.9199812412262 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5415, Accuracy: 0.8288, F1 Micro: 0.0174, F1 Macro: 0.0077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4042, Accuracy: 0.8329, F1 Micro: 0.0732, F1 Macro: 0.0283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3804, Accuracy: 0.843, F1 Micro: 0.2115, F1 Macro: 0.0705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3575, Accuracy: 0.8668, F1 Micro: 0.4376, F1 Macro: 0.1784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3076, Accuracy: 0.8804, F1 Micro: 0.5769, F1 Macro: 0.2725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2862, Accuracy: 0.8843, F1 Micro: 0.6376, F1 Macro: 0.3628\n",
      "Epoch 7/10, Train Loss: 0.2562, Accuracy: 0.89, F1 Micro: 0.6241, F1 Macro: 0.3711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2262, Accuracy: 0.8931, F1 Micro: 0.6551, F1 Macro: 0.4221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1977, Accuracy: 0.8964, F1 Micro: 0.6811, F1 Macro: 0.4762\n",
      "Epoch 10/10, Train Loss: 0.1811, Accuracy: 0.8968, F1 Micro: 0.6791, F1 Macro: 0.4789\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8964, F1 Micro: 0.6811, F1 Macro: 0.4762\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.82      0.81      1134\n",
      "      Abusive       0.87      0.80      0.83       992\n",
      "HS_Individual       0.67      0.60      0.64       732\n",
      "     HS_Group       0.64      0.57      0.60       402\n",
      "  HS_Religion       0.61      0.24      0.35       157\n",
      "      HS_Race       0.86      0.49      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.70      0.70       762\n",
      "      HS_Weak       0.63      0.56      0.59       689\n",
      "  HS_Moderate       0.48      0.46      0.47       331\n",
      "    HS_Strong       1.00      0.05      0.10       114\n",
      "\n",
      "    micro avg       0.72      0.64      0.68      5556\n",
      "    macro avg       0.60      0.44      0.48      5556\n",
      " weighted avg       0.71      0.64      0.66      5556\n",
      "  samples avg       0.39      0.36      0.35      5556\n",
      "\n",
      "Training completed in 67.00707650184631 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8956, F1 Micro: 0.6785, F1 Macro: 0.4824\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 422.4392967529854\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 563.2527391910553 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4775, Accuracy: 0.8342, F1 Micro: 0.1027, F1 Macro: 0.0406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3785, Accuracy: 0.8554, F1 Micro: 0.3574, F1 Macro: 0.1341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3315, Accuracy: 0.8834, F1 Micro: 0.5923, F1 Macro: 0.2787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2904, Accuracy: 0.89, F1 Micro: 0.6442, F1 Macro: 0.3425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2498, Accuracy: 0.8938, F1 Micro: 0.6584, F1 Macro: 0.393\n",
      "Epoch 6/10, Train Loss: 0.2266, Accuracy: 0.8962, F1 Micro: 0.6427, F1 Macro: 0.4024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1971, Accuracy: 0.8981, F1 Micro: 0.6801, F1 Macro: 0.4719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1836, Accuracy: 0.9027, F1 Micro: 0.7027, F1 Macro: 0.4969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1583, Accuracy: 0.9044, F1 Micro: 0.7077, F1 Macro: 0.5123\n",
      "Epoch 10/10, Train Loss: 0.1363, Accuracy: 0.9003, F1 Micro: 0.6841, F1 Macro: 0.5159\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9044, F1 Micro: 0.7077, F1 Macro: 0.5123\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.83      0.88      0.86       992\n",
      "HS_Individual       0.71      0.62      0.66       732\n",
      "     HS_Group       0.67      0.60      0.63       402\n",
      "  HS_Religion       0.71      0.48      0.57       157\n",
      "      HS_Race       0.85      0.52      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.67      0.69       762\n",
      "      HS_Weak       0.67      0.62      0.65       689\n",
      "  HS_Moderate       0.52      0.49      0.50       331\n",
      "    HS_Strong       1.00      0.06      0.12       114\n",
      "\n",
      "    micro avg       0.75      0.67      0.71      5556\n",
      "    macro avg       0.63      0.48      0.51      5556\n",
      " weighted avg       0.73      0.67      0.69      5556\n",
      "  samples avg       0.42      0.38      0.38      5556\n",
      "\n",
      "Training completed in 80.28725290298462 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4874, Accuracy: 0.8366, F1 Micro: 0.1199, F1 Macro: 0.0465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3791, Accuracy: 0.8542, F1 Micro: 0.3349, F1 Macro: 0.1096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3307, Accuracy: 0.8795, F1 Micro: 0.5862, F1 Macro: 0.2739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2888, Accuracy: 0.8875, F1 Micro: 0.6249, F1 Macro: 0.319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2517, Accuracy: 0.894, F1 Micro: 0.6431, F1 Macro: 0.3689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2274, Accuracy: 0.8969, F1 Micro: 0.6607, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.201, Accuracy: 0.8985, F1 Micro: 0.6949, F1 Macro: 0.4929\n",
      "Epoch 8/10, Train Loss: 0.1822, Accuracy: 0.9021, F1 Micro: 0.6935, F1 Macro: 0.5104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1649, Accuracy: 0.904, F1 Micro: 0.7107, F1 Macro: 0.5244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1425, Accuracy: 0.9015, F1 Micro: 0.7167, F1 Macro: 0.5442\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9015, F1 Micro: 0.7167, F1 Macro: 0.5442\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.87      0.82      1134\n",
      "      Abusive       0.84      0.88      0.86       992\n",
      "HS_Individual       0.67      0.68      0.68       732\n",
      "     HS_Group       0.59      0.66      0.63       402\n",
      "  HS_Religion       0.66      0.62      0.64       157\n",
      "      HS_Race       0.78      0.61      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.75      0.72       762\n",
      "      HS_Weak       0.65      0.64      0.64       689\n",
      "  HS_Moderate       0.47      0.56      0.51       331\n",
      "    HS_Strong       0.92      0.21      0.34       114\n",
      "\n",
      "    micro avg       0.71      0.72      0.72      5556\n",
      "    macro avg       0.59      0.54      0.54      5556\n",
      " weighted avg       0.70      0.72      0.71      5556\n",
      "  samples avg       0.41      0.40      0.38      5556\n",
      "\n",
      "Training completed in 81.47290921211243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4936, Accuracy: 0.8296, F1 Micro: 0.0241, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3847, Accuracy: 0.8488, F1 Micro: 0.2829, F1 Macro: 0.0928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3455, Accuracy: 0.8788, F1 Micro: 0.5712, F1 Macro: 0.2622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3004, Accuracy: 0.8869, F1 Micro: 0.6235, F1 Macro: 0.3404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2564, Accuracy: 0.8947, F1 Micro: 0.6589, F1 Macro: 0.4192\n",
      "Epoch 6/10, Train Loss: 0.2311, Accuracy: 0.8965, F1 Micro: 0.6459, F1 Macro: 0.4171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2021, Accuracy: 0.8993, F1 Micro: 0.6935, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1809, Accuracy: 0.903, F1 Micro: 0.711, F1 Macro: 0.5171\n",
      "Epoch 9/10, Train Loss: 0.161, Accuracy: 0.9048, F1 Micro: 0.7074, F1 Macro: 0.5154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1355, Accuracy: 0.9055, F1 Micro: 0.7167, F1 Macro: 0.5434\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9055, F1 Micro: 0.7167, F1 Macro: 0.5434\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.82      1134\n",
      "      Abusive       0.86      0.85      0.86       992\n",
      "HS_Individual       0.68      0.66      0.67       732\n",
      "     HS_Group       0.69      0.60      0.64       402\n",
      "  HS_Religion       0.71      0.48      0.58       157\n",
      "      HS_Race       0.86      0.59      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.73      0.71       762\n",
      "      HS_Weak       0.65      0.64      0.65       689\n",
      "  HS_Moderate       0.55      0.53      0.54       331\n",
      "    HS_Strong       0.93      0.22      0.35       114\n",
      "\n",
      "    micro avg       0.74      0.69      0.72      5556\n",
      "    macro avg       0.62      0.51      0.54      5556\n",
      " weighted avg       0.73      0.69      0.70      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 80.42642045021057 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9038, F1 Micro: 0.7137, F1 Macro: 0.5333\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 669.5378968465175\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 507.21204710006714 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4588, Accuracy: 0.8439, F1 Micro: 0.2248, F1 Macro: 0.0794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3556, Accuracy: 0.8772, F1 Micro: 0.53, F1 Macro: 0.2443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2972, Accuracy: 0.8917, F1 Micro: 0.6397, F1 Macro: 0.3477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2592, Accuracy: 0.8988, F1 Micro: 0.6651, F1 Macro: 0.4222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.222, Accuracy: 0.9017, F1 Micro: 0.6986, F1 Macro: 0.4808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1929, Accuracy: 0.9067, F1 Micro: 0.7075, F1 Macro: 0.5358\n",
      "Epoch 7/10, Train Loss: 0.1617, Accuracy: 0.907, F1 Micro: 0.7064, F1 Macro: 0.5273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1422, Accuracy: 0.9079, F1 Micro: 0.7196, F1 Macro: 0.5515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1197, Accuracy: 0.9105, F1 Micro: 0.724, F1 Macro: 0.5482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1033, Accuracy: 0.9111, F1 Micro: 0.732, F1 Macro: 0.583\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9111, F1 Micro: 0.732, F1 Macro: 0.583\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.83      1134\n",
      "      Abusive       0.85      0.88      0.87       992\n",
      "HS_Individual       0.75      0.61      0.67       732\n",
      "     HS_Group       0.64      0.70      0.67       402\n",
      "  HS_Religion       0.67      0.61      0.64       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.77      0.71      0.74       762\n",
      "      HS_Weak       0.71      0.57      0.63       689\n",
      "  HS_Moderate       0.53      0.61      0.57       331\n",
      "    HS_Strong       0.89      0.55      0.68       114\n",
      "\n",
      "    micro avg       0.76      0.70      0.73      5556\n",
      "    macro avg       0.62      0.56      0.58      5556\n",
      " weighted avg       0.75      0.70      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 94.5212242603302 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4654, Accuracy: 0.8464, F1 Micro: 0.248, F1 Macro: 0.0862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3533, Accuracy: 0.8799, F1 Micro: 0.5682, F1 Macro: 0.2621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3005, Accuracy: 0.8885, F1 Micro: 0.6336, F1 Macro: 0.3164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2607, Accuracy: 0.8946, F1 Micro: 0.6513, F1 Macro: 0.3956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2273, Accuracy: 0.8988, F1 Micro: 0.6954, F1 Macro: 0.4758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1993, Accuracy: 0.9046, F1 Micro: 0.7018, F1 Macro: 0.5225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1697, Accuracy: 0.9076, F1 Micro: 0.7122, F1 Macro: 0.5293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1477, Accuracy: 0.9079, F1 Micro: 0.7171, F1 Macro: 0.5591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1273, Accuracy: 0.9102, F1 Micro: 0.7202, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1088, Accuracy: 0.9071, F1 Micro: 0.7242, F1 Macro: 0.5798\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9071, F1 Micro: 0.7242, F1 Macro: 0.5798\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.83      1134\n",
      "      Abusive       0.83      0.91      0.87       992\n",
      "HS_Individual       0.73      0.58      0.64       732\n",
      "     HS_Group       0.60      0.74      0.67       402\n",
      "  HS_Religion       0.62      0.64      0.63       157\n",
      "      HS_Race       0.75      0.62      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.25      0.02      0.04        51\n",
      "     HS_Other       0.77      0.72      0.74       762\n",
      "      HS_Weak       0.70      0.54      0.61       689\n",
      "  HS_Moderate       0.51      0.64      0.57       331\n",
      "    HS_Strong       0.89      0.57      0.70       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.72      5556\n",
      "    macro avg       0.62      0.57      0.58      5556\n",
      " weighted avg       0.73      0.71      0.71      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 96.28380870819092 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4709, Accuracy: 0.8404, F1 Micro: 0.1721, F1 Macro: 0.0639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3656, Accuracy: 0.8672, F1 Micro: 0.4334, F1 Macro: 0.1796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3071, Accuracy: 0.8882, F1 Micro: 0.6393, F1 Macro: 0.3532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.262, Accuracy: 0.8973, F1 Micro: 0.6615, F1 Macro: 0.4302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.225, Accuracy: 0.9027, F1 Micro: 0.6985, F1 Macro: 0.4897\n",
      "Epoch 6/10, Train Loss: 0.1967, Accuracy: 0.9046, F1 Micro: 0.6915, F1 Macro: 0.5238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1656, Accuracy: 0.9072, F1 Micro: 0.7171, F1 Macro: 0.5251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1407, Accuracy: 0.9098, F1 Micro: 0.7304, F1 Macro: 0.575\n",
      "Epoch 9/10, Train Loss: 0.1219, Accuracy: 0.9097, F1 Micro: 0.713, F1 Macro: 0.5645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1042, Accuracy: 0.9114, F1 Micro: 0.737, F1 Macro: 0.5994\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9114, F1 Micro: 0.737, F1 Macro: 0.5994\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.86      0.87      0.87       992\n",
      "HS_Individual       0.71      0.64      0.68       732\n",
      "     HS_Group       0.65      0.67      0.66       402\n",
      "  HS_Religion       0.62      0.64      0.63       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.25      0.02      0.04        51\n",
      "     HS_Other       0.74      0.74      0.74       762\n",
      "      HS_Weak       0.69      0.62      0.65       689\n",
      "  HS_Moderate       0.55      0.59      0.57       331\n",
      "    HS_Strong       0.87      0.64      0.74       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.69      0.58      0.60      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 93.26369881629944 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9099, F1 Micro: 0.7311, F1 Macro: 0.5874\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 802.7709509025816\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 453.75374603271484 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4492, Accuracy: 0.8486, F1 Micro: 0.2983, F1 Macro: 0.0983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3476, Accuracy: 0.8837, F1 Micro: 0.5889, F1 Macro: 0.2857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2798, Accuracy: 0.8941, F1 Micro: 0.644, F1 Macro: 0.3677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2389, Accuracy: 0.8999, F1 Micro: 0.6717, F1 Macro: 0.4262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2106, Accuracy: 0.9015, F1 Micro: 0.6893, F1 Macro: 0.4614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1893, Accuracy: 0.9072, F1 Micro: 0.7274, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.161, Accuracy: 0.9089, F1 Micro: 0.7313, F1 Macro: 0.5632\n",
      "Epoch 8/10, Train Loss: 0.1361, Accuracy: 0.9106, F1 Micro: 0.7285, F1 Macro: 0.5793\n",
      "Epoch 9/10, Train Loss: 0.1175, Accuracy: 0.91, F1 Micro: 0.7297, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1012, Accuracy: 0.912, F1 Micro: 0.7369, F1 Macro: 0.5945\n",
      "Model 1 - Iteration 4055: Accuracy: 0.912, F1 Micro: 0.7369, F1 Macro: 0.5945\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.71      0.65      0.68       732\n",
      "     HS_Group       0.65      0.64      0.65       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.74      0.74       762\n",
      "      HS_Weak       0.69      0.62      0.65       689\n",
      "  HS_Moderate       0.56      0.54      0.55       331\n",
      "    HS_Strong       0.88      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.62      0.57      0.59      5556\n",
      " weighted avg       0.74      0.72      0.73      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 101.17935061454773 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4547, Accuracy: 0.852, F1 Micro: 0.3219, F1 Macro: 0.1044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3439, Accuracy: 0.8825, F1 Micro: 0.5951, F1 Macro: 0.2819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.8934, F1 Micro: 0.6386, F1 Macro: 0.3537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2394, Accuracy: 0.8981, F1 Micro: 0.657, F1 Macro: 0.4101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2153, Accuracy: 0.9035, F1 Micro: 0.7016, F1 Macro: 0.5109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1914, Accuracy: 0.9077, F1 Micro: 0.7254, F1 Macro: 0.5373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1645, Accuracy: 0.9095, F1 Micro: 0.7348, F1 Macro: 0.5575\n",
      "Epoch 8/10, Train Loss: 0.1397, Accuracy: 0.9116, F1 Micro: 0.7298, F1 Macro: 0.5849\n",
      "Epoch 9/10, Train Loss: 0.1212, Accuracy: 0.91, F1 Micro: 0.7289, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1019, Accuracy: 0.9111, F1 Micro: 0.7366, F1 Macro: 0.5993\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9111, F1 Micro: 0.7366, F1 Macro: 0.5993\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.73      0.64      0.68       732\n",
      "     HS_Group       0.63      0.68      0.65       402\n",
      "  HS_Religion       0.70      0.59      0.64       157\n",
      "      HS_Race       0.76      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.75      0.06      0.11        51\n",
      "     HS_Other       0.73      0.75      0.74       762\n",
      "      HS_Weak       0.70      0.60      0.65       689\n",
      "  HS_Moderate       0.53      0.57      0.55       331\n",
      "    HS_Strong       0.86      0.64      0.73       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.76      0.58      0.60      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 102.70263290405273 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4592, Accuracy: 0.8483, F1 Micro: 0.2951, F1 Macro: 0.0973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3576, Accuracy: 0.8812, F1 Micro: 0.5652, F1 Macro: 0.263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2862, Accuracy: 0.8935, F1 Micro: 0.6362, F1 Macro: 0.3873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2405, Accuracy: 0.8992, F1 Micro: 0.6722, F1 Macro: 0.428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2121, Accuracy: 0.9033, F1 Micro: 0.6907, F1 Macro: 0.4803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1893, Accuracy: 0.9095, F1 Micro: 0.7285, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.164, Accuracy: 0.9114, F1 Micro: 0.7414, F1 Macro: 0.582\n",
      "Epoch 8/10, Train Loss: 0.1383, Accuracy: 0.9133, F1 Micro: 0.7256, F1 Macro: 0.5801\n",
      "Epoch 9/10, Train Loss: 0.1183, Accuracy: 0.913, F1 Micro: 0.7339, F1 Macro: 0.592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0994, Accuracy: 0.9141, F1 Micro: 0.742, F1 Macro: 0.6101\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9141, F1 Micro: 0.742, F1 Macro: 0.6101\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.72      0.67      0.69       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.71      0.59      0.65       157\n",
      "      HS_Race       0.80      0.60      0.69       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.73      0.75      0.74       762\n",
      "      HS_Weak       0.69      0.64      0.67       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.85      0.72      0.78       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5556\n",
      "    macro avg       0.75      0.58      0.61      5556\n",
      " weighted avg       0.77      0.72      0.73      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 101.86850762367249 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9124, F1 Micro: 0.7385, F1 Macro: 0.6013\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 860.4392303383311\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 407.13749289512634 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4347, Accuracy: 0.8539, F1 Micro: 0.3748, F1 Macro: 0.1192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.331, Accuracy: 0.8872, F1 Micro: 0.6238, F1 Macro: 0.3137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2729, Accuracy: 0.8983, F1 Micro: 0.6815, F1 Macro: 0.4178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.233, Accuracy: 0.905, F1 Micro: 0.7099, F1 Macro: 0.5314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2009, Accuracy: 0.906, F1 Micro: 0.7303, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1742, Accuracy: 0.9069, F1 Micro: 0.7336, F1 Macro: 0.5583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1401, Accuracy: 0.9121, F1 Micro: 0.7451, F1 Macro: 0.5848\n",
      "Epoch 8/10, Train Loss: 0.1237, Accuracy: 0.9138, F1 Micro: 0.731, F1 Macro: 0.5786\n",
      "Epoch 9/10, Train Loss: 0.1049, Accuracy: 0.912, F1 Micro: 0.7399, F1 Macro: 0.605\n",
      "Epoch 10/10, Train Loss: 0.0948, Accuracy: 0.9146, F1 Micro: 0.7349, F1 Macro: 0.5923\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9121, F1 Micro: 0.7451, F1 Macro: 0.5848\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.69      0.62      0.66       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.76      0.74       762\n",
      "      HS_Weak       0.65      0.71      0.68       689\n",
      "  HS_Moderate       0.56      0.53      0.55       331\n",
      "    HS_Strong       0.92      0.43      0.59       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.75      5556\n",
      "    macro avg       0.61      0.57      0.58      5556\n",
      " weighted avg       0.73      0.75      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 109.4246814250946 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.8575, F1 Micro: 0.4029, F1 Macro: 0.1239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3283, Accuracy: 0.8841, F1 Micro: 0.6043, F1 Macro: 0.3029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2731, Accuracy: 0.8964, F1 Micro: 0.6789, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2345, Accuracy: 0.9042, F1 Micro: 0.6991, F1 Macro: 0.521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2047, Accuracy: 0.9016, F1 Micro: 0.7268, F1 Macro: 0.5539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1793, Accuracy: 0.91, F1 Micro: 0.7334, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1437, Accuracy: 0.9118, F1 Micro: 0.735, F1 Macro: 0.5839\n",
      "Epoch 8/10, Train Loss: 0.123, Accuracy: 0.9129, F1 Micro: 0.7203, F1 Macro: 0.5709\n",
      "Epoch 9/10, Train Loss: 0.1097, Accuracy: 0.912, F1 Micro: 0.7305, F1 Macro: 0.5949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0984, Accuracy: 0.915, F1 Micro: 0.7454, F1 Macro: 0.6161\n",
      "Model 2 - Iteration 4703: Accuracy: 0.915, F1 Micro: 0.7454, F1 Macro: 0.6161\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.68      0.75      0.71       732\n",
      "     HS_Group       0.78      0.54      0.64       402\n",
      "  HS_Religion       0.77      0.51      0.61       157\n",
      "      HS_Race       0.85      0.53      0.65       120\n",
      "  HS_Physical       0.43      0.08      0.14        72\n",
      "    HS_Gender       0.54      0.14      0.22        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.69      0.46      0.55       331\n",
      "    HS_Strong       0.85      0.63      0.72       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.75      5556\n",
      "    macro avg       0.73      0.57      0.62      5556\n",
      " weighted avg       0.77      0.72      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 111.06621241569519 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4425, Accuracy: 0.8525, F1 Micro: 0.3552, F1 Macro: 0.1106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3396, Accuracy: 0.8833, F1 Micro: 0.6148, F1 Macro: 0.3143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2752, Accuracy: 0.8984, F1 Micro: 0.6795, F1 Macro: 0.4239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.233, Accuracy: 0.9055, F1 Micro: 0.7066, F1 Macro: 0.5319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2015, Accuracy: 0.9085, F1 Micro: 0.7383, F1 Macro: 0.58\n",
      "Epoch 6/10, Train Loss: 0.1747, Accuracy: 0.9098, F1 Micro: 0.7306, F1 Macro: 0.5578\n",
      "Epoch 7/10, Train Loss: 0.1384, Accuracy: 0.9111, F1 Micro: 0.738, F1 Macro: 0.5915\n",
      "Epoch 8/10, Train Loss: 0.1201, Accuracy: 0.912, F1 Micro: 0.721, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1048, Accuracy: 0.9104, F1 Micro: 0.7404, F1 Macro: 0.6192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0903, Accuracy: 0.914, F1 Micro: 0.7477, F1 Macro: 0.6139\n",
      "Model 3 - Iteration 4703: Accuracy: 0.914, F1 Micro: 0.7477, F1 Macro: 0.6139\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.68      0.73      0.71       732\n",
      "     HS_Group       0.71      0.59      0.65       402\n",
      "  HS_Religion       0.75      0.52      0.62       157\n",
      "      HS_Race       0.84      0.51      0.63       120\n",
      "  HS_Physical       0.67      0.08      0.15        72\n",
      "    HS_Gender       0.33      0.06      0.10        51\n",
      "     HS_Other       0.71      0.79      0.75       762\n",
      "      HS_Weak       0.67      0.71      0.69       689\n",
      "  HS_Moderate       0.62      0.52      0.57       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.71      0.58      0.61      5556\n",
      " weighted avg       0.75      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 110.67491459846497 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9137, F1 Micro: 0.7461, F1 Macro: 0.605\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 735.7534061611665\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 368.7799172401428 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4302, Accuracy: 0.865, F1 Micro: 0.4914, F1 Macro: 0.2058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3177, Accuracy: 0.8914, F1 Micro: 0.6564, F1 Macro: 0.3802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2621, Accuracy: 0.9017, F1 Micro: 0.6964, F1 Macro: 0.482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2223, Accuracy: 0.9062, F1 Micro: 0.7216, F1 Macro: 0.5424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9112, F1 Micro: 0.7283, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1633, Accuracy: 0.9131, F1 Micro: 0.7432, F1 Macro: 0.5942\n",
      "Epoch 7/10, Train Loss: 0.136, Accuracy: 0.9157, F1 Micro: 0.7394, F1 Macro: 0.5897\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.9134, F1 Micro: 0.7293, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0994, Accuracy: 0.9151, F1 Micro: 0.7454, F1 Macro: 0.6032\n",
      "Epoch 10/10, Train Loss: 0.0829, Accuracy: 0.9136, F1 Micro: 0.7321, F1 Macro: 0.6001\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9151, F1 Micro: 0.7454, F1 Macro: 0.6032\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.73      0.65      0.69       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.77      0.59      0.66       157\n",
      "      HS_Race       0.81      0.56      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.33      0.02      0.04        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.71      0.62      0.66       689\n",
      "  HS_Moderate       0.59      0.58      0.58       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.75      5556\n",
      "    macro avg       0.66      0.58      0.60      5556\n",
      " weighted avg       0.76      0.72      0.74      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 120.0100302696228 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4317, Accuracy: 0.8656, F1 Micro: 0.4803, F1 Macro: 0.1854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3151, Accuracy: 0.8893, F1 Micro: 0.6545, F1 Macro: 0.3616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2633, Accuracy: 0.8998, F1 Micro: 0.6952, F1 Macro: 0.4713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2227, Accuracy: 0.9042, F1 Micro: 0.72, F1 Macro: 0.5517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9085, F1 Micro: 0.7279, F1 Macro: 0.5817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1669, Accuracy: 0.9131, F1 Micro: 0.7367, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1379, Accuracy: 0.9156, F1 Micro: 0.737, F1 Macro: 0.5971\n",
      "Epoch 8/10, Train Loss: 0.1134, Accuracy: 0.9134, F1 Micro: 0.7344, F1 Macro: 0.5896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.914, F1 Micro: 0.7483, F1 Macro: 0.6194\n",
      "Epoch 10/10, Train Loss: 0.0832, Accuracy: 0.914, F1 Micro: 0.7419, F1 Macro: 0.6112\n",
      "Model 2 - Iteration 5287: Accuracy: 0.914, F1 Micro: 0.7483, F1 Macro: 0.6194\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1134\n",
      "      Abusive       0.86      0.89      0.88       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.65      0.66      0.66       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.78      0.57      0.66       120\n",
      "  HS_Physical       0.38      0.07      0.12        72\n",
      "    HS_Gender       0.36      0.10      0.15        51\n",
      "     HS_Other       0.73      0.78      0.75       762\n",
      "      HS_Weak       0.69      0.67      0.68       689\n",
      "  HS_Moderate       0.56      0.59      0.58       331\n",
      "    HS_Strong       0.85      0.67      0.75       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5556\n",
      "    macro avg       0.68      0.59      0.62      5556\n",
      " weighted avg       0.75      0.74      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 121.713876247406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4391, Accuracy: 0.8549, F1 Micro: 0.3881, F1 Macro: 0.12\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3245, Accuracy: 0.8878, F1 Micro: 0.6576, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2631, Accuracy: 0.9014, F1 Micro: 0.6846, F1 Macro: 0.4647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2237, Accuracy: 0.9075, F1 Micro: 0.7216, F1 Macro: 0.5391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1847, Accuracy: 0.9107, F1 Micro: 0.7234, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1639, Accuracy: 0.9125, F1 Micro: 0.736, F1 Macro: 0.588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1312, Accuracy: 0.9154, F1 Micro: 0.7444, F1 Macro: 0.6053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9151, F1 Micro: 0.7486, F1 Macro: 0.6057\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9163, F1 Micro: 0.742, F1 Macro: 0.6106\n",
      "Epoch 10/10, Train Loss: 0.0824, Accuracy: 0.9152, F1 Micro: 0.7461, F1 Macro: 0.6215\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9151, F1 Micro: 0.7486, F1 Macro: 0.6057\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.69      0.73      0.71       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.87      0.60      0.71       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.67      0.72      0.69       689\n",
      "  HS_Moderate       0.61      0.49      0.54       331\n",
      "    HS_Strong       0.92      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5556\n",
      "    macro avg       0.76      0.57      0.61      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 121.79284477233887 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9148, F1 Micro: 0.7474, F1 Macro: 0.6094\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 594.819565210556\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 331.1326241493225 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4238, Accuracy: 0.8711, F1 Micro: 0.5662, F1 Macro: 0.2615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3106, Accuracy: 0.8937, F1 Micro: 0.6454, F1 Macro: 0.3896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2528, Accuracy: 0.9037, F1 Micro: 0.6882, F1 Macro: 0.4918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2186, Accuracy: 0.9095, F1 Micro: 0.7248, F1 Macro: 0.5467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1826, Accuracy: 0.9116, F1 Micro: 0.7369, F1 Macro: 0.573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1544, Accuracy: 0.9081, F1 Micro: 0.7474, F1 Macro: 0.5869\n",
      "Epoch 7/10, Train Loss: 0.1331, Accuracy: 0.9147, F1 Micro: 0.7463, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1089, Accuracy: 0.9161, F1 Micro: 0.7532, F1 Macro: 0.6067\n",
      "Epoch 9/10, Train Loss: 0.0969, Accuracy: 0.9176, F1 Micro: 0.7485, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0818, Accuracy: 0.9188, F1 Micro: 0.7567, F1 Macro: 0.6347\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9188, F1 Micro: 0.7567, F1 Macro: 0.6347\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.83      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.74      0.68      0.71       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.57      0.06      0.10        72\n",
      "    HS_Gender       0.40      0.08      0.13        51\n",
      "     HS_Other       0.76      0.76      0.76       762\n",
      "      HS_Weak       0.72      0.65      0.68       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.72      0.61      0.63      5556\n",
      " weighted avg       0.77      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 129.33994317054749 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.426, Accuracy: 0.8715, F1 Micro: 0.5416, F1 Macro: 0.2322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3078, Accuracy: 0.8915, F1 Micro: 0.6277, F1 Macro: 0.3609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2532, Accuracy: 0.9023, F1 Micro: 0.6775, F1 Macro: 0.4618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.221, Accuracy: 0.9072, F1 Micro: 0.7051, F1 Macro: 0.532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.907, F1 Micro: 0.7347, F1 Macro: 0.5832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1567, Accuracy: 0.9083, F1 Micro: 0.7428, F1 Macro: 0.5747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.138, Accuracy: 0.9134, F1 Micro: 0.7443, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.112, Accuracy: 0.9124, F1 Micro: 0.7484, F1 Macro: 0.6037\n",
      "Epoch 9/10, Train Loss: 0.096, Accuracy: 0.9156, F1 Micro: 0.7365, F1 Macro: 0.6101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0846, Accuracy: 0.9162, F1 Micro: 0.7494, F1 Macro: 0.6255\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9162, F1 Micro: 0.7494, F1 Macro: 0.6255\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.74      0.67      0.70       732\n",
      "     HS_Group       0.67      0.66      0.66       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.74      0.61      0.67       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.75      0.76      0.76       762\n",
      "      HS_Weak       0.71      0.64      0.67       689\n",
      "  HS_Moderate       0.58      0.56      0.57       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.72      0.60      0.63      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 131.37202763557434 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4326, Accuracy: 0.8584, F1 Micro: 0.4254, F1 Macro: 0.1607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3189, Accuracy: 0.8912, F1 Micro: 0.6512, F1 Macro: 0.414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2535, Accuracy: 0.9019, F1 Micro: 0.671, F1 Macro: 0.48\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2184, Accuracy: 0.91, F1 Micro: 0.7239, F1 Macro: 0.5586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1847, Accuracy: 0.9119, F1 Micro: 0.7418, F1 Macro: 0.5915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1533, Accuracy: 0.9118, F1 Micro: 0.7493, F1 Macro: 0.5965\n",
      "Epoch 7/10, Train Loss: 0.1321, Accuracy: 0.9155, F1 Micro: 0.7486, F1 Macro: 0.6014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1109, Accuracy: 0.9176, F1 Micro: 0.7504, F1 Macro: 0.6053\n",
      "Epoch 9/10, Train Loss: 0.0962, Accuracy: 0.9181, F1 Micro: 0.7419, F1 Macro: 0.6181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0784, Accuracy: 0.9175, F1 Micro: 0.7567, F1 Macro: 0.6487\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9175, F1 Micro: 0.7567, F1 Macro: 0.6487\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.74      0.67      0.70       732\n",
      "     HS_Group       0.64      0.72      0.68       402\n",
      "  HS_Religion       0.68      0.61      0.64       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.50      0.18      0.26        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.73      0.63      0.68       689\n",
      "  HS_Moderate       0.57      0.63      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.73      0.63      0.65      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 129.22140049934387 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9175, F1 Micro: 0.7543, F1 Macro: 0.6363\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 781.6267231845387\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 299.37474608421326 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4167, Accuracy: 0.8743, F1 Micro: 0.5298, F1 Macro: 0.2472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.8904, F1 Micro: 0.6792, F1 Macro: 0.4214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2459, Accuracy: 0.9057, F1 Micro: 0.6927, F1 Macro: 0.4988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2101, Accuracy: 0.9088, F1 Micro: 0.734, F1 Macro: 0.5638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1786, Accuracy: 0.9144, F1 Micro: 0.7372, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1537, Accuracy: 0.9159, F1 Micro: 0.7568, F1 Macro: 0.6056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1256, Accuracy: 0.9156, F1 Micro: 0.7577, F1 Macro: 0.6167\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.9168, F1 Micro: 0.7555, F1 Macro: 0.6297\n",
      "Epoch 9/10, Train Loss: 0.0932, Accuracy: 0.9179, F1 Micro: 0.7547, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0771, Accuracy: 0.9165, F1 Micro: 0.7613, F1 Macro: 0.6444\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9165, F1 Micro: 0.7613, F1 Macro: 0.6444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.67      0.64      0.66       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.78      0.65      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.42      0.10      0.16        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.67      0.74      0.71       689\n",
      "  HS_Moderate       0.57      0.55      0.56       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.72      0.64      0.64      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 136.83879733085632 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4192, Accuracy: 0.8765, F1 Micro: 0.5464, F1 Macro: 0.2418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.307, Accuracy: 0.8896, F1 Micro: 0.6806, F1 Macro: 0.4077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.9058, F1 Micro: 0.6984, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2127, Accuracy: 0.9088, F1 Micro: 0.733, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1786, Accuracy: 0.9133, F1 Micro: 0.7365, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1567, Accuracy: 0.9159, F1 Micro: 0.747, F1 Macro: 0.5906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1315, Accuracy: 0.9159, F1 Micro: 0.7542, F1 Macro: 0.6222\n",
      "Epoch 8/10, Train Loss: 0.1104, Accuracy: 0.9153, F1 Micro: 0.7478, F1 Macro: 0.6238\n",
      "Epoch 9/10, Train Loss: 0.0971, Accuracy: 0.9175, F1 Micro: 0.7492, F1 Macro: 0.6353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.078, Accuracy: 0.915, F1 Micro: 0.7579, F1 Macro: 0.6432\n",
      "Model 2 - Iteration 6285: Accuracy: 0.915, F1 Micro: 0.7579, F1 Macro: 0.6432\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.63      0.68      0.66       402\n",
      "  HS_Religion       0.69      0.63      0.66       157\n",
      "      HS_Race       0.86      0.59      0.70       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.50      0.14      0.22        51\n",
      "     HS_Other       0.71      0.83      0.77       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.56      0.60      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.74      0.77      0.76      5556\n",
      "    macro avg       0.72      0.63      0.64      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 137.01549458503723 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4276, Accuracy: 0.8612, F1 Micro: 0.4014, F1 Macro: 0.1632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3094, Accuracy: 0.8933, F1 Micro: 0.6681, F1 Macro: 0.4132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2464, Accuracy: 0.9068, F1 Micro: 0.7102, F1 Macro: 0.5206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.9122, F1 Micro: 0.7422, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.9147, F1 Micro: 0.7535, F1 Macro: 0.6048\n",
      "Epoch 6/10, Train Loss: 0.1536, Accuracy: 0.916, F1 Micro: 0.7434, F1 Macro: 0.5929\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.9156, F1 Micro: 0.7489, F1 Macro: 0.6189\n",
      "Epoch 8/10, Train Loss: 0.1036, Accuracy: 0.9161, F1 Micro: 0.7532, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.9203, F1 Micro: 0.7634, F1 Macro: 0.6504\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9163, F1 Micro: 0.7632, F1 Macro: 0.6601\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9203, F1 Micro: 0.7634, F1 Macro: 0.6504\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.74      0.59      0.66       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.47      0.14      0.21        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.63      0.53      0.58       331\n",
      "    HS_Strong       0.91      0.69      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.75      0.61      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 133.5981888771057 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9173, F1 Micro: 0.7609, F1 Macro: 0.646\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1053.2155178586904\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 268.1243977546692 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4172, Accuracy: 0.8728, F1 Micro: 0.5071, F1 Macro: 0.2322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2982, Accuracy: 0.8961, F1 Micro: 0.6657, F1 Macro: 0.3805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2447, Accuracy: 0.9054, F1 Micro: 0.7219, F1 Macro: 0.526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2101, Accuracy: 0.9116, F1 Micro: 0.732, F1 Macro: 0.5522\n",
      "Epoch 5/10, Train Loss: 0.1749, Accuracy: 0.9133, F1 Micro: 0.7211, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1497, Accuracy: 0.9187, F1 Micro: 0.7543, F1 Macro: 0.6069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1251, Accuracy: 0.9209, F1 Micro: 0.7636, F1 Macro: 0.6272\n",
      "Epoch 8/10, Train Loss: 0.1088, Accuracy: 0.9216, F1 Micro: 0.7538, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.09, Accuracy: 0.9196, F1 Micro: 0.7647, F1 Macro: 0.6402\n",
      "Epoch 10/10, Train Loss: 0.0799, Accuracy: 0.9196, F1 Micro: 0.762, F1 Macro: 0.6437\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9196, F1 Micro: 0.7647, F1 Macro: 0.6402\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.76      0.63      0.69       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.36      0.08      0.13        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.62      0.51      0.56       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.74      0.61      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 138.6466839313507 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4167, Accuracy: 0.8766, F1 Micro: 0.5554, F1 Macro: 0.2503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2975, Accuracy: 0.8942, F1 Micro: 0.6637, F1 Macro: 0.3714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2453, Accuracy: 0.9045, F1 Micro: 0.7142, F1 Macro: 0.5233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2075, Accuracy: 0.9105, F1 Micro: 0.7282, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.174, Accuracy: 0.9145, F1 Micro: 0.7353, F1 Macro: 0.5761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1521, Accuracy: 0.9138, F1 Micro: 0.7522, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.9191, F1 Micro: 0.7558, F1 Macro: 0.6318\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.9184, F1 Micro: 0.7505, F1 Macro: 0.6278\n",
      "Epoch 9/10, Train Loss: 0.0901, Accuracy: 0.9166, F1 Micro: 0.7506, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0814, Accuracy: 0.9178, F1 Micro: 0.7581, F1 Macro: 0.6481\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9178, F1 Micro: 0.7581, F1 Macro: 0.6481\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.72      0.71      0.71       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.81      0.54      0.64       157\n",
      "      HS_Race       0.78      0.62      0.69       120\n",
      "  HS_Physical       0.50      0.10      0.16        72\n",
      "    HS_Gender       0.52      0.24      0.32        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.59      0.57      0.58       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.72      0.62      0.65      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 140.4514398574829 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4258, Accuracy: 0.8661, F1 Micro: 0.4563, F1 Macro: 0.1888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3046, Accuracy: 0.8926, F1 Micro: 0.6534, F1 Macro: 0.3501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2476, Accuracy: 0.9062, F1 Micro: 0.7159, F1 Macro: 0.5083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.9104, F1 Micro: 0.7226, F1 Macro: 0.5447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1752, Accuracy: 0.9142, F1 Micro: 0.73, F1 Macro: 0.5898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1485, Accuracy: 0.9138, F1 Micro: 0.7564, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.122, Accuracy: 0.9186, F1 Micro: 0.7632, F1 Macro: 0.6451\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9197, F1 Micro: 0.7525, F1 Macro: 0.6362\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9173, F1 Micro: 0.7595, F1 Macro: 0.6476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.9203, F1 Micro: 0.7637, F1 Macro: 0.6657\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9203, F1 Micro: 0.7637, F1 Macro: 0.6657\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.76      0.59      0.67       402\n",
      "  HS_Religion       0.81      0.54      0.65       157\n",
      "      HS_Race       0.83      0.64      0.72       120\n",
      "  HS_Physical       0.61      0.15      0.24        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.90      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.75      0.62      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 140.63954091072083 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9193, F1 Micro: 0.7622, F1 Macro: 0.6513\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 809.6104219124625\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 249.7521936893463 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4105, Accuracy: 0.8778, F1 Micro: 0.6105, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2882, Accuracy: 0.8995, F1 Micro: 0.6865, F1 Macro: 0.4431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.9072, F1 Micro: 0.7195, F1 Macro: 0.4985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.9133, F1 Micro: 0.7225, F1 Macro: 0.5701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.9158, F1 Micro: 0.7495, F1 Macro: 0.5774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1406, Accuracy: 0.9182, F1 Micro: 0.7531, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1218, Accuracy: 0.9191, F1 Micro: 0.7629, F1 Macro: 0.6192\n",
      "Epoch 8/10, Train Loss: 0.1032, Accuracy: 0.9209, F1 Micro: 0.7625, F1 Macro: 0.6397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9195, F1 Micro: 0.7659, F1 Macro: 0.6463\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9175, F1 Micro: 0.7598, F1 Macro: 0.6531\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9195, F1 Micro: 0.7659, F1 Macro: 0.6463\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.70      0.75      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.83      0.60      0.70       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.58      0.14      0.22        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.77      0.61      0.65      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 147.45410466194153 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4128, Accuracy: 0.8791, F1 Micro: 0.5924, F1 Macro: 0.2698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2891, Accuracy: 0.8987, F1 Micro: 0.6843, F1 Macro: 0.4358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2417, Accuracy: 0.9041, F1 Micro: 0.7159, F1 Macro: 0.4929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2041, Accuracy: 0.912, F1 Micro: 0.7217, F1 Macro: 0.5717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1776, Accuracy: 0.9161, F1 Micro: 0.7563, F1 Macro: 0.5948\n",
      "Epoch 6/10, Train Loss: 0.1441, Accuracy: 0.9158, F1 Micro: 0.751, F1 Macro: 0.6091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1245, Accuracy: 0.9181, F1 Micro: 0.7578, F1 Macro: 0.6236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1032, Accuracy: 0.9206, F1 Micro: 0.7605, F1 Macro: 0.6504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0924, Accuracy: 0.9197, F1 Micro: 0.7607, F1 Macro: 0.6476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9199, F1 Micro: 0.7649, F1 Macro: 0.6727\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9199, F1 Micro: 0.7649, F1 Macro: 0.6727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.74      0.62      0.68       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.52      0.21      0.30        72\n",
      "    HS_Gender       0.52      0.29      0.38        51\n",
      "     HS_Other       0.78      0.77      0.77       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.60      0.55      0.58       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.72      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 148.60798025131226 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4225, Accuracy: 0.8741, F1 Micro: 0.5453, F1 Macro: 0.2398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2943, Accuracy: 0.8996, F1 Micro: 0.6917, F1 Macro: 0.4734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2439, Accuracy: 0.9049, F1 Micro: 0.7152, F1 Macro: 0.4837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2054, Accuracy: 0.9145, F1 Micro: 0.7273, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1761, Accuracy: 0.9172, F1 Micro: 0.7556, F1 Macro: 0.6006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1419, Accuracy: 0.9174, F1 Micro: 0.7616, F1 Macro: 0.6291\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.9158, F1 Micro: 0.7573, F1 Macro: 0.6186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1015, Accuracy: 0.9207, F1 Micro: 0.7641, F1 Macro: 0.6519\n",
      "Epoch 9/10, Train Loss: 0.0859, Accuracy: 0.9176, F1 Micro: 0.7638, F1 Macro: 0.6523\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.9202, F1 Micro: 0.7619, F1 Macro: 0.662\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9207, F1 Micro: 0.7641, F1 Macro: 0.6519\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.91      0.86      0.88       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.77      0.59      0.67       402\n",
      "  HS_Religion       0.77      0.57      0.66       157\n",
      "      HS_Race       0.81      0.63      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.71      0.47      0.56       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.78      0.61      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 145.52970004081726 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.92, F1 Micro: 0.765, F1 Macro: 0.657\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1048.7318135046232\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 223.02768969535828 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4058, Accuracy: 0.8813, F1 Micro: 0.5794, F1 Macro: 0.2679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2897, Accuracy: 0.8996, F1 Micro: 0.6845, F1 Macro: 0.448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2369, Accuracy: 0.909, F1 Micro: 0.7022, F1 Macro: 0.5197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1979, Accuracy: 0.9135, F1 Micro: 0.7434, F1 Macro: 0.5763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1685, Accuracy: 0.9186, F1 Micro: 0.7487, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.144, Accuracy: 0.9199, F1 Micro: 0.7611, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1201, Accuracy: 0.9212, F1 Micro: 0.7632, F1 Macro: 0.6264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.9184, F1 Micro: 0.7676, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.9212, F1 Micro: 0.769, F1 Macro: 0.6559\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9199, F1 Micro: 0.7573, F1 Macro: 0.638\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9212, F1 Micro: 0.769, F1 Macro: 0.6559\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.78      0.62      0.69       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.44      0.16      0.23        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 153.81597423553467 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.879, F1 Micro: 0.5397, F1 Macro: 0.2423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.9005, F1 Micro: 0.6864, F1 Macro: 0.4652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2409, Accuracy: 0.9081, F1 Micro: 0.6916, F1 Macro: 0.5069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2018, Accuracy: 0.9115, F1 Micro: 0.7391, F1 Macro: 0.566\n",
      "Epoch 5/10, Train Loss: 0.171, Accuracy: 0.9153, F1 Micro: 0.7385, F1 Macro: 0.5869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1462, Accuracy: 0.9182, F1 Micro: 0.7537, F1 Macro: 0.6126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1228, Accuracy: 0.9181, F1 Micro: 0.7552, F1 Macro: 0.6151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9189, F1 Micro: 0.7567, F1 Macro: 0.6378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.921, F1 Micro: 0.7604, F1 Macro: 0.6381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0799, Accuracy: 0.9213, F1 Micro: 0.7629, F1 Macro: 0.6537\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9213, F1 Micro: 0.7629, F1 Macro: 0.6537\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.81      0.84      1134\n",
      "      Abusive       0.91      0.88      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.77      0.59      0.66       402\n",
      "  HS_Religion       0.82      0.60      0.69       157\n",
      "      HS_Race       0.80      0.62      0.70       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.62      0.16      0.25        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.73      0.76      5556\n",
      "    macro avg       0.77      0.61      0.65      5556\n",
      " weighted avg       0.79      0.73      0.76      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 153.3536412715912 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4157, Accuracy: 0.8752, F1 Micro: 0.5222, F1 Macro: 0.2267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2938, Accuracy: 0.8995, F1 Micro: 0.6994, F1 Macro: 0.4938\n",
      "Epoch 3/10, Train Loss: 0.2409, Accuracy: 0.9092, F1 Micro: 0.6975, F1 Macro: 0.5226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1988, Accuracy: 0.9159, F1 Micro: 0.7528, F1 Macro: 0.5969\n",
      "Epoch 5/10, Train Loss: 0.1681, Accuracy: 0.9189, F1 Micro: 0.7471, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9205, F1 Micro: 0.7629, F1 Macro: 0.623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.9201, F1 Micro: 0.767, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1037, Accuracy: 0.9188, F1 Micro: 0.7688, F1 Macro: 0.6594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0825, Accuracy: 0.92, F1 Micro: 0.7698, F1 Macro: 0.6648\n",
      "Epoch 10/10, Train Loss: 0.0759, Accuracy: 0.9213, F1 Micro: 0.7697, F1 Macro: 0.6666\n",
      "Model 3 - Iteration 7336: Accuracy: 0.92, F1 Micro: 0.7698, F1 Macro: 0.6648\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.70      0.65      0.68       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.50      0.18      0.26        51\n",
      "     HS_Other       0.74      0.82      0.77       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.89      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.65      0.66      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 150.0907917022705 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9208, F1 Micro: 0.7672, F1 Macro: 0.6581\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 939.167894608476\n",
      "Nearest checkpoint: 7901\n",
      " Acquired samples:320\n",
      "Sampling duration: 201.992746591568 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4018, Accuracy: 0.8813, F1 Micro: 0.6067, F1 Macro: 0.2832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2843, Accuracy: 0.9019, F1 Micro: 0.6983, F1 Macro: 0.4809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2352, Accuracy: 0.9108, F1 Micro: 0.7174, F1 Macro: 0.524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1954, Accuracy: 0.9139, F1 Micro: 0.7303, F1 Macro: 0.5711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9171, F1 Micro: 0.7543, F1 Macro: 0.6023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.9177, F1 Micro: 0.765, F1 Macro: 0.6494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9219, F1 Micro: 0.7681, F1 Macro: 0.6503\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.9203, F1 Micro: 0.7514, F1 Macro: 0.6074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.9209, F1 Micro: 0.7717, F1 Macro: 0.6711\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9225, F1 Micro: 0.7688, F1 Macro: 0.6648\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9209, F1 Micro: 0.7717, F1 Macro: 0.6711\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.65      0.74      0.69       402\n",
      "  HS_Religion       0.72      0.67      0.70       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.56      0.67      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 155.66269302368164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4034, Accuracy: 0.8792, F1 Micro: 0.5976, F1 Macro: 0.276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2861, Accuracy: 0.9004, F1 Micro: 0.6947, F1 Macro: 0.4806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2374, Accuracy: 0.9097, F1 Micro: 0.7081, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.916, F1 Micro: 0.7417, F1 Macro: 0.5775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9166, F1 Micro: 0.7587, F1 Macro: 0.6189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.9197, F1 Micro: 0.7647, F1 Macro: 0.6407\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9195, F1 Micro: 0.7646, F1 Macro: 0.6645\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.916, F1 Micro: 0.7593, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0946, Accuracy: 0.9179, F1 Micro: 0.766, F1 Macro: 0.6585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9213, F1 Micro: 0.7697, F1 Macro: 0.6802\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9213, F1 Micro: 0.7697, F1 Macro: 0.6802\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.76      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.77      0.61      0.68       120\n",
      "  HS_Physical       0.53      0.25      0.34        72\n",
      "    HS_Gender       0.49      0.39      0.43        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.72      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.72      0.65      0.68      5556\n",
      " weighted avg       0.77      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 156.0133707523346 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4118, Accuracy: 0.8773, F1 Micro: 0.5479, F1 Macro: 0.2482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2887, Accuracy: 0.9011, F1 Micro: 0.7019, F1 Macro: 0.4936\n",
      "Epoch 3/10, Train Loss: 0.2352, Accuracy: 0.9084, F1 Micro: 0.6896, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1961, Accuracy: 0.9161, F1 Micro: 0.7423, F1 Macro: 0.591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9179, F1 Micro: 0.7609, F1 Macro: 0.6179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9215, F1 Micro: 0.768, F1 Macro: 0.654\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9213, F1 Micro: 0.7621, F1 Macro: 0.6572\n",
      "Epoch 8/10, Train Loss: 0.1012, Accuracy: 0.9203, F1 Micro: 0.768, F1 Macro: 0.6569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0846, Accuracy: 0.9206, F1 Micro: 0.7699, F1 Macro: 0.6685\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9205, F1 Micro: 0.7691, F1 Macro: 0.6876\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9206, F1 Micro: 0.7699, F1 Macro: 0.6685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.68      0.71      0.69       402\n",
      "  HS_Religion       0.69      0.67      0.68       157\n",
      "      HS_Race       0.84      0.62      0.71       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.55      0.22      0.31        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.68      0.69       689\n",
      "  HS_Moderate       0.60      0.63      0.61       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 153.10781002044678 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9209, F1 Micro: 0.7704, F1 Macro: 0.6733\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 924.842249509265\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 181.75034880638123 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.399, Accuracy: 0.8834, F1 Micro: 0.5904, F1 Macro: 0.2876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2802, Accuracy: 0.9019, F1 Micro: 0.6708, F1 Macro: 0.4709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9055, F1 Micro: 0.732, F1 Macro: 0.5458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1989, Accuracy: 0.914, F1 Micro: 0.751, F1 Macro: 0.5856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1704, Accuracy: 0.919, F1 Micro: 0.7573, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9201, F1 Micro: 0.7614, F1 Macro: 0.6205\n",
      "Epoch 7/10, Train Loss: 0.1173, Accuracy: 0.9212, F1 Micro: 0.7536, F1 Macro: 0.6263\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9203, F1 Micro: 0.7607, F1 Macro: 0.643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0869, Accuracy: 0.9214, F1 Micro: 0.7689, F1 Macro: 0.6527\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.92, F1 Micro: 0.767, F1 Macro: 0.6711\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9214, F1 Micro: 0.7689, F1 Macro: 0.6527\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.76      0.55      0.64       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.53      0.18      0.26        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.69      0.47      0.56       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.62      0.65      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 158.03697037696838 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3997, Accuracy: 0.8823, F1 Micro: 0.6017, F1 Macro: 0.2896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2775, Accuracy: 0.9015, F1 Micro: 0.6765, F1 Macro: 0.4731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.9069, F1 Micro: 0.7325, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9116, F1 Micro: 0.7515, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1707, Accuracy: 0.92, F1 Micro: 0.7646, F1 Macro: 0.6177\n",
      "Epoch 6/10, Train Loss: 0.1433, Accuracy: 0.9222, F1 Micro: 0.7603, F1 Macro: 0.6392\n",
      "Epoch 7/10, Train Loss: 0.1179, Accuracy: 0.9203, F1 Micro: 0.7567, F1 Macro: 0.6297\n",
      "Epoch 8/10, Train Loss: 0.0994, Accuracy: 0.9216, F1 Micro: 0.7596, F1 Macro: 0.6434\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.9213, F1 Micro: 0.7633, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.9188, F1 Micro: 0.7651, F1 Macro: 0.6667\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9188, F1 Micro: 0.7651, F1 Macro: 0.6667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.76      0.58      0.66       120\n",
      "  HS_Physical       0.55      0.22      0.32        72\n",
      "    HS_Gender       0.54      0.27      0.36        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.62      0.51      0.56       331\n",
      "    HS_Strong       0.86      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.72      0.64      0.67      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 156.29965949058533 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4095, Accuracy: 0.8792, F1 Micro: 0.5553, F1 Macro: 0.2512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2831, Accuracy: 0.8998, F1 Micro: 0.6725, F1 Macro: 0.4861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.9083, F1 Micro: 0.7336, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1984, Accuracy: 0.9159, F1 Micro: 0.757, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1693, Accuracy: 0.9211, F1 Micro: 0.762, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9207, F1 Micro: 0.7691, F1 Macro: 0.6276\n",
      "Epoch 7/10, Train Loss: 0.1126, Accuracy: 0.9205, F1 Micro: 0.7627, F1 Macro: 0.6526\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9221, F1 Micro: 0.7652, F1 Macro: 0.6533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.9213, F1 Micro: 0.7711, F1 Macro: 0.6667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0728, Accuracy: 0.9216, F1 Micro: 0.7758, F1 Macro: 0.6883\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9216, F1 Micro: 0.7758, F1 Macro: 0.6883\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.71      0.64      0.68       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.81      0.60      0.69       120\n",
      "  HS_Physical       0.76      0.22      0.34        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 160.05860090255737 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9206, F1 Micro: 0.7699, F1 Macro: 0.6692\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 823.3714696703391\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 165.51269340515137 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4021, Accuracy: 0.8832, F1 Micro: 0.6026, F1 Macro: 0.2861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2799, Accuracy: 0.9036, F1 Micro: 0.6911, F1 Macro: 0.4498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2323, Accuracy: 0.9083, F1 Micro: 0.7283, F1 Macro: 0.5518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.195, Accuracy: 0.9156, F1 Micro: 0.75, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1645, Accuracy: 0.9213, F1 Micro: 0.7607, F1 Macro: 0.5996\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.9221, F1 Micro: 0.7562, F1 Macro: 0.615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9219, F1 Micro: 0.7626, F1 Macro: 0.6464\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9227, F1 Micro: 0.7626, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0859, Accuracy: 0.9223, F1 Micro: 0.772, F1 Macro: 0.6655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9219, F1 Micro: 0.7762, F1 Macro: 0.6853\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9219, F1 Micro: 0.7762, F1 Macro: 0.6853\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.68      0.71      0.69       402\n",
      "  HS_Religion       0.69      0.69      0.69       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.72      0.18      0.29        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.59      0.63      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.67      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 164.4904010295868 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4014, Accuracy: 0.8821, F1 Micro: 0.6039, F1 Macro: 0.2865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.904, F1 Micro: 0.6967, F1 Macro: 0.468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.9079, F1 Micro: 0.721, F1 Macro: 0.5553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1968, Accuracy: 0.9151, F1 Micro: 0.7532, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9194, F1 Micro: 0.7577, F1 Macro: 0.6225\n",
      "Epoch 6/10, Train Loss: 0.1361, Accuracy: 0.9188, F1 Micro: 0.7499, F1 Macro: 0.6049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1195, Accuracy: 0.9218, F1 Micro: 0.7683, F1 Macro: 0.6556\n",
      "Epoch 8/10, Train Loss: 0.0968, Accuracy: 0.9205, F1 Micro: 0.7658, F1 Macro: 0.6431\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9197, F1 Micro: 0.7677, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.921, F1 Micro: 0.7705, F1 Macro: 0.6758\n",
      "Model 2 - Iteration 8165: Accuracy: 0.921, F1 Micro: 0.7705, F1 Macro: 0.6758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.67      0.66      0.66       402\n",
      "  HS_Religion       0.66      0.66      0.66       157\n",
      "      HS_Race       0.72      0.71      0.71       120\n",
      "  HS_Physical       0.68      0.18      0.29        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.24608182907104 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4103, Accuracy: 0.8816, F1 Micro: 0.6011, F1 Macro: 0.2823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.284, Accuracy: 0.9037, F1 Micro: 0.6893, F1 Macro: 0.4814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.9104, F1 Micro: 0.7196, F1 Macro: 0.5601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1961, Accuracy: 0.9167, F1 Micro: 0.7626, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1646, Accuracy: 0.9215, F1 Micro: 0.7652, F1 Macro: 0.6097\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9214, F1 Micro: 0.7604, F1 Macro: 0.6196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9228, F1 Micro: 0.7716, F1 Macro: 0.6619\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9231, F1 Micro: 0.7714, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.9221, F1 Micro: 0.7718, F1 Macro: 0.6784\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9231, F1 Micro: 0.7696, F1 Macro: 0.6784\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9221, F1 Micro: 0.7718, F1 Macro: 0.6784\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.76      0.61      0.67       157\n",
      "      HS_Race       0.79      0.72      0.75       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.54      0.29      0.38        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 161.74716591835022 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9216, F1 Micro: 0.7728, F1 Macro: 0.6798\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 694.3839407361113\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 147.41908025741577 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8813, F1 Micro: 0.6315, F1 Macro: 0.3106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2802, Accuracy: 0.9037, F1 Micro: 0.6896, F1 Macro: 0.4825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.23, Accuracy: 0.9107, F1 Micro: 0.7369, F1 Macro: 0.5793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1975, Accuracy: 0.9167, F1 Micro: 0.7539, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.9199, F1 Micro: 0.7571, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.921, F1 Micro: 0.7688, F1 Macro: 0.6174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1171, Accuracy: 0.9226, F1 Micro: 0.7735, F1 Macro: 0.6619\n",
      "Epoch 8/10, Train Loss: 0.1007, Accuracy: 0.9215, F1 Micro: 0.7701, F1 Macro: 0.6526\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9147, F1 Micro: 0.7641, F1 Macro: 0.6576\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9222, F1 Micro: 0.7641, F1 Macro: 0.6583\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9226, F1 Micro: 0.7735, F1 Macro: 0.6619\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.89      0.62      0.73       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.56      0.20      0.29        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.62      0.66      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 165.8225817680359 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.397, Accuracy: 0.8804, F1 Micro: 0.6183, F1 Macro: 0.2987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2808, Accuracy: 0.9035, F1 Micro: 0.6925, F1 Macro: 0.4861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9097, F1 Micro: 0.7359, F1 Macro: 0.5771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9168, F1 Micro: 0.7588, F1 Macro: 0.6038\n",
      "Epoch 5/10, Train Loss: 0.1634, Accuracy: 0.9192, F1 Micro: 0.7536, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9183, F1 Micro: 0.7646, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1154, Accuracy: 0.9189, F1 Micro: 0.7654, F1 Macro: 0.6584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1006, Accuracy: 0.923, F1 Micro: 0.7659, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.9196, F1 Micro: 0.7687, F1 Macro: 0.6547\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9222, F1 Micro: 0.767, F1 Macro: 0.6494\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9196, F1 Micro: 0.7687, F1 Macro: 0.6547\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.66      0.64      0.65       157\n",
      "      HS_Race       0.71      0.68      0.69       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.64      0.18      0.28        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.58      0.58      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.64      0.65      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 168.20017194747925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.881, F1 Micro: 0.5958, F1 Macro: 0.2853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.282, Accuracy: 0.9011, F1 Micro: 0.664, F1 Macro: 0.4755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2288, Accuracy: 0.9113, F1 Micro: 0.7423, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1946, Accuracy: 0.9195, F1 Micro: 0.7642, F1 Macro: 0.6088\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.9211, F1 Micro: 0.7603, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1367, Accuracy: 0.9213, F1 Micro: 0.7682, F1 Macro: 0.6281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1121, Accuracy: 0.9231, F1 Micro: 0.7736, F1 Macro: 0.6659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9247, F1 Micro: 0.7786, F1 Macro: 0.673\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.922, F1 Micro: 0.7736, F1 Macro: 0.6768\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9256, F1 Micro: 0.7707, F1 Macro: 0.681\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9247, F1 Micro: 0.7786, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.71      0.59      0.64       157\n",
      "      HS_Race       0.76      0.71      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 166.1192765235901 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9223, F1 Micro: 0.7736, F1 Macro: 0.6632\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 833.0837292895011\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 135.60308456420898 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3905, Accuracy: 0.8851, F1 Micro: 0.6015, F1 Macro: 0.302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.9043, F1 Micro: 0.7103, F1 Macro: 0.5032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.221, Accuracy: 0.9121, F1 Micro: 0.7376, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.19, Accuracy: 0.9196, F1 Micro: 0.7548, F1 Macro: 0.5956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.921, F1 Micro: 0.7662, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.9187, F1 Micro: 0.7697, F1 Macro: 0.6466\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9246, F1 Micro: 0.7692, F1 Macro: 0.643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9245, F1 Micro: 0.7753, F1 Macro: 0.6589\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9208, F1 Micro: 0.7735, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9251, F1 Micro: 0.7812, F1 Macro: 0.6735\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9251, F1 Micro: 0.7812, F1 Macro: 0.6735\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.61      0.22      0.32        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.73      0.73      0.73       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 172.3249475955963 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.8834, F1 Micro: 0.6011, F1 Macro: 0.2897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2736, Accuracy: 0.9039, F1 Micro: 0.7081, F1 Macro: 0.509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9121, F1 Micro: 0.7286, F1 Macro: 0.5666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1912, Accuracy: 0.919, F1 Micro: 0.7517, F1 Macro: 0.5993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.9207, F1 Micro: 0.7626, F1 Macro: 0.632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1384, Accuracy: 0.9206, F1 Micro: 0.7704, F1 Macro: 0.6562\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9221, F1 Micro: 0.769, F1 Macro: 0.6488\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9208, F1 Micro: 0.7672, F1 Macro: 0.6605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0804, Accuracy: 0.9213, F1 Micro: 0.7705, F1 Macro: 0.6812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9238, F1 Micro: 0.7713, F1 Macro: 0.6748\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9238, F1 Micro: 0.7713, F1 Macro: 0.6748\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.79      0.59      0.67       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.63      0.67      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 172.3243374824524 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.8819, F1 Micro: 0.5742, F1 Macro: 0.2784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2745, Accuracy: 0.9051, F1 Micro: 0.7044, F1 Macro: 0.5017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.222, Accuracy: 0.9134, F1 Micro: 0.7342, F1 Macro: 0.5758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.9213, F1 Micro: 0.7637, F1 Macro: 0.6122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1545, Accuracy: 0.9238, F1 Micro: 0.7752, F1 Macro: 0.6359\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9193, F1 Micro: 0.7717, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1105, Accuracy: 0.9259, F1 Micro: 0.7755, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6686\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9214, F1 Micro: 0.7681, F1 Macro: 0.6763\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9247, F1 Micro: 0.7734, F1 Macro: 0.6777\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6686\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.76      0.60      0.67       402\n",
      "  HS_Religion       0.76      0.63      0.69       157\n",
      "      HS_Race       0.74      0.68      0.70       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.71      0.20      0.31        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.90      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.1856620311737 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9244, F1 Micro: 0.7761, F1 Macro: 0.6723\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 567.5698596803046\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 121.55132818222046 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3894, Accuracy: 0.8838, F1 Micro: 0.5804, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.276, Accuracy: 0.9075, F1 Micro: 0.7102, F1 Macro: 0.5001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2213, Accuracy: 0.9139, F1 Micro: 0.7359, F1 Macro: 0.5469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1893, Accuracy: 0.9178, F1 Micro: 0.7584, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1569, Accuracy: 0.9226, F1 Micro: 0.7684, F1 Macro: 0.6185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9239, F1 Micro: 0.7708, F1 Macro: 0.6429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.108, Accuracy: 0.9226, F1 Micro: 0.7745, F1 Macro: 0.6702\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.924, F1 Micro: 0.7712, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9248, F1 Micro: 0.7781, F1 Macro: 0.6915\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.9215, F1 Micro: 0.7695, F1 Macro: 0.6632\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9248, F1 Micro: 0.7781, F1 Macro: 0.6915\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.82      0.56      0.67       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.62      0.21      0.31        72\n",
      "    HS_Gender       0.53      0.39      0.45        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.73      0.70      0.72       689\n",
      "  HS_Moderate       0.66      0.58      0.62       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.7813847064972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3903, Accuracy: 0.883, F1 Micro: 0.5826, F1 Macro: 0.2785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2769, Accuracy: 0.9058, F1 Micro: 0.6996, F1 Macro: 0.4896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.913, F1 Micro: 0.7408, F1 Macro: 0.5596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.9181, F1 Micro: 0.7537, F1 Macro: 0.6091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9225, F1 Micro: 0.7617, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.9229, F1 Micro: 0.7681, F1 Macro: 0.6275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9203, F1 Micro: 0.7706, F1 Macro: 0.6738\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9197, F1 Micro: 0.7682, F1 Macro: 0.6626\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9212, F1 Micro: 0.7701, F1 Macro: 0.6797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9226, F1 Micro: 0.7721, F1 Macro: 0.6781\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9226, F1 Micro: 0.7721, F1 Macro: 0.6781\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.70      0.63      0.67       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.7965075969696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.881, F1 Micro: 0.556, F1 Macro: 0.2624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2787, Accuracy: 0.9059, F1 Micro: 0.7077, F1 Macro: 0.5104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2227, Accuracy: 0.916, F1 Micro: 0.7467, F1 Macro: 0.5592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9187, F1 Micro: 0.7593, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.157, Accuracy: 0.9233, F1 Micro: 0.7686, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1311, Accuracy: 0.9221, F1 Micro: 0.773, F1 Macro: 0.6446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1022, Accuracy: 0.9247, F1 Micro: 0.7828, F1 Macro: 0.6924\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9251, F1 Micro: 0.7814, F1 Macro: 0.6853\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9254, F1 Micro: 0.7787, F1 Macro: 0.7017\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9239, F1 Micro: 0.7798, F1 Macro: 0.6931\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9247, F1 Micro: 0.7828, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.73      0.66      0.70       402\n",
      "  HS_Religion       0.79      0.64      0.70       157\n",
      "      HS_Race       0.82      0.70      0.75       120\n",
      "  HS_Physical       0.57      0.22      0.32        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 171.92650032043457 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.924, F1 Micro: 0.7776, F1 Macro: 0.6873\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 743.3805103272485\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 108.67555165290833 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.8852, F1 Micro: 0.6316, F1 Macro: 0.3181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.9042, F1 Micro: 0.6808, F1 Macro: 0.4862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.9137, F1 Micro: 0.7427, F1 Macro: 0.5681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9198, F1 Micro: 0.7512, F1 Macro: 0.6036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1562, Accuracy: 0.9231, F1 Micro: 0.7698, F1 Macro: 0.6345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9238, F1 Micro: 0.7757, F1 Macro: 0.6476\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9225, F1 Micro: 0.7725, F1 Macro: 0.6617\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9233, F1 Micro: 0.768, F1 Macro: 0.6518\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9231, F1 Micro: 0.7749, F1 Macro: 0.672\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9236, F1 Micro: 0.7732, F1 Macro: 0.6757\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9238, F1 Micro: 0.7757, F1 Macro: 0.6476\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.80      0.58      0.67       157\n",
      "      HS_Race       0.84      0.64      0.73       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.86      0.12      0.21        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.85      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.79      0.62      0.65      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 174.0194275379181 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8837, F1 Micro: 0.625, F1 Macro: 0.3156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.9049, F1 Micro: 0.6952, F1 Macro: 0.487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9146, F1 Micro: 0.7433, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1821, Accuracy: 0.9186, F1 Micro: 0.7458, F1 Macro: 0.5965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.9207, F1 Micro: 0.7528, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9239, F1 Micro: 0.7732, F1 Macro: 0.6588\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9239, F1 Micro: 0.7656, F1 Macro: 0.6499\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9217, F1 Micro: 0.7715, F1 Macro: 0.6643\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9234, F1 Micro: 0.7652, F1 Macro: 0.6802\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9244, F1 Micro: 0.773, F1 Macro: 0.6946\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9239, F1 Micro: 0.7732, F1 Macro: 0.6588\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.80      0.57      0.66       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.64      0.12      0.21        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.70      0.45      0.55       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.76      0.61      0.66      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 173.34099864959717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.8822, F1 Micro: 0.6009, F1 Macro: 0.2943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.274, Accuracy: 0.9051, F1 Micro: 0.6984, F1 Macro: 0.4946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2196, Accuracy: 0.9157, F1 Micro: 0.7452, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9198, F1 Micro: 0.7597, F1 Macro: 0.6158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9222, F1 Micro: 0.7669, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9237, F1 Micro: 0.7736, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.9244, F1 Micro: 0.7759, F1 Macro: 0.674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9214, F1 Micro: 0.7768, F1 Macro: 0.6802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9235, F1 Micro: 0.7793, F1 Macro: 0.6849\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9252, F1 Micro: 0.774, F1 Macro: 0.6984\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9235, F1 Micro: 0.7793, F1 Macro: 0.6849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.56      0.27      0.37        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 179.2048361301422 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9237, F1 Micro: 0.7761, F1 Macro: 0.6638\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 777.6787957179145\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 96.82422709465027 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8845, F1 Micro: 0.5835, F1 Macro: 0.2926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.9073, F1 Micro: 0.7111, F1 Macro: 0.5229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9152, F1 Micro: 0.7457, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9106, F1 Micro: 0.753, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9225, F1 Micro: 0.7709, F1 Macro: 0.615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1308, Accuracy: 0.9227, F1 Micro: 0.7752, F1 Macro: 0.636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9249, F1 Micro: 0.7795, F1 Macro: 0.6687\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.9213, F1 Micro: 0.775, F1 Macro: 0.6631\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9238, F1 Micro: 0.7746, F1 Macro: 0.6617\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9217, F1 Micro: 0.7726, F1 Macro: 0.6717\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9249, F1 Micro: 0.7795, F1 Macro: 0.6687\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.80      0.69      0.74       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.68      0.55      0.60       331\n",
      "    HS_Strong       0.84      0.88      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 177.48246312141418 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8793, F1 Micro: 0.5369, F1 Macro: 0.2574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2671, Accuracy: 0.9078, F1 Micro: 0.7107, F1 Macro: 0.5168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9138, F1 Micro: 0.7474, F1 Macro: 0.5932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9141, F1 Micro: 0.7581, F1 Macro: 0.6036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.154, Accuracy: 0.924, F1 Micro: 0.7687, F1 Macro: 0.6445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9212, F1 Micro: 0.7701, F1 Macro: 0.6455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.9224, F1 Micro: 0.7759, F1 Macro: 0.6621\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9209, F1 Micro: 0.7709, F1 Macro: 0.6666\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9223, F1 Micro: 0.7682, F1 Macro: 0.6734\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9192, F1 Micro: 0.7687, F1 Macro: 0.6646\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9224, F1 Micro: 0.7759, F1 Macro: 0.6621\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.81      0.55      0.66       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.59      0.20      0.29        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.61      0.55      0.58       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.64      0.66      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 177.37923645973206 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.398, Accuracy: 0.8788, F1 Micro: 0.5361, F1 Macro: 0.2578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9079, F1 Micro: 0.7111, F1 Macro: 0.505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9163, F1 Micro: 0.7554, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1855, Accuracy: 0.913, F1 Micro: 0.758, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.921, F1 Micro: 0.7745, F1 Macro: 0.6351\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9185, F1 Micro: 0.7718, F1 Macro: 0.6452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.108, Accuracy: 0.9245, F1 Micro: 0.779, F1 Macro: 0.6761\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9238, F1 Micro: 0.7776, F1 Macro: 0.6834\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9235, F1 Micro: 0.7768, F1 Macro: 0.6831\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.9213, F1 Micro: 0.7759, F1 Macro: 0.6921\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9245, F1 Micro: 0.779, F1 Macro: 0.6761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.83      0.69      0.75       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.82      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 175.86328768730164 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9239, F1 Micro: 0.7781, F1 Macro: 0.669\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 552.7649475596327\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 83.74406170845032 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.8868, F1 Micro: 0.6368, F1 Macro: 0.327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.9073, F1 Micro: 0.7132, F1 Macro: 0.5029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9161, F1 Micro: 0.7385, F1 Macro: 0.5533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9201, F1 Micro: 0.7588, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9204, F1 Micro: 0.7663, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9247, F1 Micro: 0.7755, F1 Macro: 0.6634\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9192, F1 Micro: 0.7727, F1 Macro: 0.6584\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9219, F1 Micro: 0.7712, F1 Macro: 0.6654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9247, F1 Micro: 0.7764, F1 Macro: 0.67\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.9235, F1 Micro: 0.7724, F1 Macro: 0.6876\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9247, F1 Micro: 0.7764, F1 Macro: 0.67\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.75      0.72      0.74       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 180.04234743118286 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8842, F1 Micro: 0.5884, F1 Macro: 0.2843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2669, Accuracy: 0.9073, F1 Micro: 0.7092, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9158, F1 Micro: 0.7479, F1 Macro: 0.5831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1799, Accuracy: 0.9209, F1 Micro: 0.7548, F1 Macro: 0.5957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1545, Accuracy: 0.9184, F1 Micro: 0.7652, F1 Macro: 0.6213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9241, F1 Micro: 0.7693, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.92, F1 Micro: 0.7735, F1 Macro: 0.6538\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9208, F1 Micro: 0.7722, F1 Macro: 0.6772\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9227, F1 Micro: 0.7687, F1 Macro: 0.6704\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9223, F1 Micro: 0.7704, F1 Macro: 0.692\n",
      "Model 2 - Iteration 9218: Accuracy: 0.92, F1 Micro: 0.7735, F1 Macro: 0.6538\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.69      0.79      0.74       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.74      0.62      0.67       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.73      0.85      0.78       762\n",
      "      HS_Weak       0.67      0.77      0.72       689\n",
      "  HS_Moderate       0.62      0.59      0.60       331\n",
      "    HS_Strong       0.87      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.73      0.65      0.65      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 179.63944292068481 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8828, F1 Micro: 0.6196, F1 Macro: 0.3259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9066, F1 Micro: 0.7093, F1 Macro: 0.5085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.918, F1 Micro: 0.7502, F1 Macro: 0.5866\n",
      "Epoch 4/10, Train Loss: 0.1798, Accuracy: 0.9204, F1 Micro: 0.7443, F1 Macro: 0.5889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9228, F1 Micro: 0.7718, F1 Macro: 0.626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1262, Accuracy: 0.9262, F1 Micro: 0.7767, F1 Macro: 0.6848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9231, F1 Micro: 0.7796, F1 Macro: 0.6764\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9253, F1 Micro: 0.778, F1 Macro: 0.6779\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9259, F1 Micro: 0.7765, F1 Macro: 0.6839\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.7024\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9231, F1 Micro: 0.7796, F1 Macro: 0.6764\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.62      0.59      0.60       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 177.47929668426514 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9226, F1 Micro: 0.7765, F1 Macro: 0.6667\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 473.8008778214616\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 83.47501373291016 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.886, F1 Micro: 0.6113, F1 Macro: 0.3121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2611, Accuracy: 0.9057, F1 Micro: 0.7177, F1 Macro: 0.5134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9139, F1 Micro: 0.7484, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9185, F1 Micro: 0.7608, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9161, F1 Micro: 0.7634, F1 Macro: 0.6167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9231, F1 Micro: 0.7711, F1 Macro: 0.6485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9249, F1 Micro: 0.7778, F1 Macro: 0.6693\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9242, F1 Micro: 0.7773, F1 Macro: 0.6733\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9224, F1 Micro: 0.7759, F1 Macro: 0.6776\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9229, F1 Micro: 0.7757, F1 Macro: 0.6911\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9249, F1 Micro: 0.7778, F1 Macro: 0.6693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.63      0.24      0.34        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.66      0.55      0.60       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 181.5125732421875 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3871, Accuracy: 0.8843, F1 Micro: 0.6046, F1 Macro: 0.2975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2636, Accuracy: 0.9045, F1 Micro: 0.7137, F1 Macro: 0.4893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2213, Accuracy: 0.9139, F1 Micro: 0.7478, F1 Macro: 0.5771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.919, F1 Micro: 0.7628, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.9226, F1 Micro: 0.7731, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9244, F1 Micro: 0.776, F1 Macro: 0.6555\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9213, F1 Micro: 0.7758, F1 Macro: 0.6813\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9223, F1 Micro: 0.7729, F1 Macro: 0.6801\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9235, F1 Micro: 0.7751, F1 Macro: 0.6895\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9216, F1 Micro: 0.7708, F1 Macro: 0.6819\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9244, F1 Micro: 0.776, F1 Macro: 0.6555\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.91      0.84      0.88       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.77      0.63      0.69       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.68      0.54      0.61       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 179.50807285308838 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8825, F1 Micro: 0.5871, F1 Macro: 0.2951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2667, Accuracy: 0.9051, F1 Micro: 0.718, F1 Macro: 0.501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2221, Accuracy: 0.916, F1 Micro: 0.7525, F1 Macro: 0.5927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1831, Accuracy: 0.9213, F1 Micro: 0.7667, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9237, F1 Micro: 0.777, F1 Macro: 0.6399\n",
      "Epoch 6/10, Train Loss: 0.1262, Accuracy: 0.9255, F1 Micro: 0.773, F1 Macro: 0.6586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1046, Accuracy: 0.9219, F1 Micro: 0.7776, F1 Macro: 0.6826\n",
      "Epoch 8/10, Train Loss: 0.0878, Accuracy: 0.9215, F1 Micro: 0.7745, F1 Macro: 0.6847\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9236, F1 Micro: 0.7771, F1 Macro: 0.6829\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9248, F1 Micro: 0.7746, F1 Macro: 0.7037\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9219, F1 Micro: 0.7776, F1 Macro: 0.6826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.67      0.72      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.73      0.71      0.72       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.59      0.65      0.62       331\n",
      "    HS_Strong       0.86      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.68      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 180.24777793884277 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9237, F1 Micro: 0.7771, F1 Macro: 0.6691\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 710.4186628292479\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 71.76494240760803 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3791, Accuracy: 0.8876, F1 Micro: 0.6122, F1 Macro: 0.3075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.9078, F1 Micro: 0.7087, F1 Macro: 0.5031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9129, F1 Micro: 0.7433, F1 Macro: 0.5561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9191, F1 Micro: 0.7612, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9236, F1 Micro: 0.7694, F1 Macro: 0.6329\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9235, F1 Micro: 0.7643, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9231, F1 Micro: 0.7726, F1 Macro: 0.6456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0878, Accuracy: 0.9231, F1 Micro: 0.7757, F1 Macro: 0.6801\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9201, F1 Micro: 0.7717, F1 Macro: 0.6861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9247, F1 Micro: 0.78, F1 Macro: 0.6975\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9247, F1 Micro: 0.78, F1 Macro: 0.6975\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.56      0.39      0.46        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 188.7361695766449 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3826, Accuracy: 0.8868, F1 Micro: 0.6156, F1 Macro: 0.2974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.26, Accuracy: 0.9073, F1 Micro: 0.7164, F1 Macro: 0.5198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9146, F1 Micro: 0.7455, F1 Macro: 0.5728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1812, Accuracy: 0.92, F1 Micro: 0.7645, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9251, F1 Micro: 0.7707, F1 Macro: 0.6557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.923, F1 Micro: 0.7731, F1 Macro: 0.658\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9227, F1 Micro: 0.7714, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.924, F1 Micro: 0.7778, F1 Macro: 0.6785\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9252, F1 Micro: 0.7743, F1 Macro: 0.6799\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.922, F1 Micro: 0.7758, F1 Macro: 0.7013\n",
      "Model 2 - Iteration 9618: Accuracy: 0.924, F1 Micro: 0.7778, F1 Macro: 0.6785\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.76      0.59      0.66       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.62      0.29      0.40        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.59      0.60      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 185.47566032409668 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3901, Accuracy: 0.8839, F1 Micro: 0.5871, F1 Macro: 0.2801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2636, Accuracy: 0.9085, F1 Micro: 0.7131, F1 Macro: 0.5053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.915, F1 Micro: 0.7448, F1 Macro: 0.5697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9202, F1 Micro: 0.7684, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.9253, F1 Micro: 0.774, F1 Macro: 0.6488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.9259, F1 Micro: 0.7827, F1 Macro: 0.6787\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9242, F1 Micro: 0.7784, F1 Macro: 0.6661\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9226, F1 Micro: 0.7777, F1 Macro: 0.6833\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.9262, F1 Micro: 0.7806, F1 Macro: 0.6948\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9258, F1 Micro: 0.7751, F1 Macro: 0.6993\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9259, F1 Micro: 0.7827, F1 Macro: 0.6787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.72      0.79      0.75       732\n",
      "     HS_Group       0.78      0.62      0.69       402\n",
      "  HS_Religion       0.85      0.52      0.64       157\n",
      "      HS_Race       0.81      0.69      0.74       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.58      0.27      0.37        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.77      0.73       689\n",
      "  HS_Moderate       0.68      0.53      0.59       331\n",
      "    HS_Strong       0.86      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 183.4293177127838 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9249, F1 Micro: 0.7802, F1 Macro: 0.6849\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 296.1137622839787\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 58.779879331588745 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3831, Accuracy: 0.8878, F1 Micro: 0.5968, F1 Macro: 0.293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2552, Accuracy: 0.9069, F1 Micro: 0.7121, F1 Macro: 0.5035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9126, F1 Micro: 0.7435, F1 Macro: 0.5662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9212, F1 Micro: 0.7643, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9185, F1 Micro: 0.7652, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1239, Accuracy: 0.9233, F1 Micro: 0.7782, F1 Macro: 0.6685\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9207, F1 Micro: 0.7696, F1 Macro: 0.6598\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9243, F1 Micro: 0.774, F1 Macro: 0.6686\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9236, F1 Micro: 0.7771, F1 Macro: 0.6875\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9238, F1 Micro: 0.7761, F1 Macro: 0.683\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9233, F1 Micro: 0.7782, F1 Macro: 0.6685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.71      0.66      0.69       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 186.21750712394714 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3835, Accuracy: 0.8851, F1 Micro: 0.5865, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2573, Accuracy: 0.9077, F1 Micro: 0.7074, F1 Macro: 0.5039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2159, Accuracy: 0.9121, F1 Micro: 0.746, F1 Macro: 0.5739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9206, F1 Micro: 0.7636, F1 Macro: 0.6088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1541, Accuracy: 0.9223, F1 Micro: 0.7703, F1 Macro: 0.6375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.9248, F1 Micro: 0.7729, F1 Macro: 0.6506\n",
      "Epoch 7/10, Train Loss: 0.1099, Accuracy: 0.923, F1 Micro: 0.7686, F1 Macro: 0.659\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.9218, F1 Micro: 0.7701, F1 Macro: 0.674\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9215, F1 Micro: 0.772, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9219, F1 Micro: 0.7744, F1 Macro: 0.6891\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9219, F1 Micro: 0.7744, F1 Macro: 0.6891\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.71      0.61      0.66       402\n",
      "  HS_Religion       0.70      0.62      0.66       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 187.57960629463196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3919, Accuracy: 0.8841, F1 Micro: 0.5766, F1 Macro: 0.2801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.259, Accuracy: 0.9084, F1 Micro: 0.7217, F1 Macro: 0.5269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2141, Accuracy: 0.9166, F1 Micro: 0.7463, F1 Macro: 0.5842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9227, F1 Micro: 0.7713, F1 Macro: 0.6266\n",
      "Epoch 5/10, Train Loss: 0.1481, Accuracy: 0.9216, F1 Micro: 0.7712, F1 Macro: 0.6455\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9249, F1 Micro: 0.7709, F1 Macro: 0.6592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9247, F1 Micro: 0.7793, F1 Macro: 0.6821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9231, F1 Micro: 0.7798, F1 Macro: 0.6911\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9219, F1 Micro: 0.7774, F1 Macro: 0.6966\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9222, F1 Micro: 0.7759, F1 Macro: 0.6872\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9231, F1 Micro: 0.7798, F1 Macro: 0.6911\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.75      0.63      0.69       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.55      0.33      0.41        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.67      0.56      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 185.81699323654175 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9228, F1 Micro: 0.7775, F1 Macro: 0.6829\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 417.95986510315805\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 46.35561180114746 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3797, Accuracy: 0.8844, F1 Micro: 0.573, F1 Macro: 0.2905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.906, F1 Micro: 0.7207, F1 Macro: 0.5217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9172, F1 Micro: 0.7481, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1743, Accuracy: 0.9174, F1 Micro: 0.764, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.9209, F1 Micro: 0.7728, F1 Macro: 0.6592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.124, Accuracy: 0.9252, F1 Micro: 0.7751, F1 Macro: 0.6526\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9209, F1 Micro: 0.7731, F1 Macro: 0.671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9232, F1 Micro: 0.7771, F1 Macro: 0.6834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9258, F1 Micro: 0.7794, F1 Macro: 0.6921\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9255, F1 Micro: 0.7767, F1 Macro: 0.694\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9258, F1 Micro: 0.7794, F1 Macro: 0.6921\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.81      0.62      0.70       157\n",
      "      HS_Race       0.80      0.63      0.71       120\n",
      "  HS_Physical       1.00      0.18      0.31        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.54399919509888 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3801, Accuracy: 0.885, F1 Micro: 0.5866, F1 Macro: 0.2947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2583, Accuracy: 0.9046, F1 Micro: 0.7216, F1 Macro: 0.5288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.9175, F1 Micro: 0.7498, F1 Macro: 0.5815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9166, F1 Micro: 0.7632, F1 Macro: 0.6235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9209, F1 Micro: 0.7741, F1 Macro: 0.6573\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.9244, F1 Micro: 0.7682, F1 Macro: 0.641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9223, F1 Micro: 0.7763, F1 Macro: 0.6805\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.9243, F1 Micro: 0.7757, F1 Macro: 0.6855\n",
      "Epoch 9/10, Train Loss: 0.0811, Accuracy: 0.9241, F1 Micro: 0.7739, F1 Macro: 0.6917\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9232, F1 Micro: 0.7733, F1 Macro: 0.7007\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9223, F1 Micro: 0.7763, F1 Macro: 0.6805\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.68      0.80      0.74       732\n",
      "     HS_Group       0.75      0.58      0.66       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.65      0.15      0.25        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.67      0.79      0.73       689\n",
      "  HS_Moderate       0.67      0.48      0.56       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 189.8452844619751 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3876, Accuracy: 0.8837, F1 Micro: 0.5745, F1 Macro: 0.2911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.9093, F1 Micro: 0.7267, F1 Macro: 0.5435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.919, F1 Micro: 0.7628, F1 Macro: 0.6095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.9174, F1 Micro: 0.7649, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1468, Accuracy: 0.9227, F1 Micro: 0.7771, F1 Macro: 0.6538\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.926, F1 Micro: 0.7762, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.9242, F1 Micro: 0.781, F1 Macro: 0.6915\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9265, F1 Micro: 0.777, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9237, F1 Micro: 0.7742, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9232, F1 Micro: 0.7778, F1 Macro: 0.7011\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9242, F1 Micro: 0.781, F1 Macro: 0.6915\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.69      0.70      0.69       157\n",
      "      HS_Race       0.80      0.72      0.76       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.58      0.37      0.45        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.54      0.59       331\n",
      "    HS_Strong       0.86      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 189.4199013710022 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9241, F1 Micro: 0.7789, F1 Macro: 0.688\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 311.4874834289458\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 34.517805099487305 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3755, Accuracy: 0.8849, F1 Micro: 0.5695, F1 Macro: 0.2886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.9084, F1 Micro: 0.7272, F1 Macro: 0.5433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9173, F1 Micro: 0.7461, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9213, F1 Micro: 0.7583, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.148, Accuracy: 0.9243, F1 Micro: 0.7745, F1 Macro: 0.6388\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9234, F1 Micro: 0.7718, F1 Macro: 0.6646\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9236, F1 Micro: 0.7733, F1 Macro: 0.6736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9239, F1 Micro: 0.7768, F1 Macro: 0.6827\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9199, F1 Micro: 0.7747, F1 Macro: 0.6842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9244, F1 Micro: 0.778, F1 Macro: 0.702\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9244, F1 Micro: 0.778, F1 Macro: 0.702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.59      0.62       331\n",
      "    HS_Strong       0.83      0.88      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.9018325805664 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3747, Accuracy: 0.8833, F1 Micro: 0.5569, F1 Macro: 0.2766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2567, Accuracy: 0.9079, F1 Micro: 0.7176, F1 Macro: 0.5254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2078, Accuracy: 0.9145, F1 Micro: 0.7282, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1812, Accuracy: 0.9207, F1 Micro: 0.7637, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9223, F1 Micro: 0.7709, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9239, F1 Micro: 0.7736, F1 Macro: 0.669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9228, F1 Micro: 0.7748, F1 Macro: 0.684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9216, F1 Micro: 0.7761, F1 Macro: 0.6775\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9229, F1 Micro: 0.7758, F1 Macro: 0.6906\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9246, F1 Micro: 0.774, F1 Macro: 0.6936\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9216, F1 Micro: 0.7761, F1 Macro: 0.6775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.70      0.58      0.63       157\n",
      "      HS_Race       0.80      0.62      0.70       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.64      0.35      0.46        51\n",
      "     HS_Other       0.75      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 195.4458839893341 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3839, Accuracy: 0.8793, F1 Micro: 0.5251, F1 Macro: 0.266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9084, F1 Micro: 0.723, F1 Macro: 0.5476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2078, Accuracy: 0.9174, F1 Micro: 0.7394, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9221, F1 Micro: 0.7673, F1 Macro: 0.6214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9221, F1 Micro: 0.7734, F1 Macro: 0.6483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1225, Accuracy: 0.9221, F1 Micro: 0.7794, F1 Macro: 0.6786\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9258, F1 Micro: 0.7777, F1 Macro: 0.6931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9239, F1 Micro: 0.7821, F1 Macro: 0.7018\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9249, F1 Micro: 0.7781, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9271, F1 Micro: 0.7834, F1 Macro: 0.7141\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9271, F1 Micro: 0.7834, F1 Macro: 0.7141\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.76      0.63      0.69       157\n",
      "      HS_Race       0.81      0.67      0.73       120\n",
      "  HS_Physical       0.92      0.31      0.46        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.74      0.69      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.83      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.78      0.68      0.71      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.86437702178955 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9244, F1 Micro: 0.7792, F1 Macro: 0.6979\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -17.043777178508417\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.020992040634155 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3696, Accuracy: 0.8886, F1 Micro: 0.6049, F1 Macro: 0.3077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.9073, F1 Micro: 0.7274, F1 Macro: 0.5414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1996, Accuracy: 0.9144, F1 Micro: 0.7461, F1 Macro: 0.5852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9221, F1 Micro: 0.7588, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.6622\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.9224, F1 Micro: 0.7693, F1 Macro: 0.6661\n",
      "Epoch 7/10, Train Loss: 0.1046, Accuracy: 0.9232, F1 Micro: 0.7705, F1 Macro: 0.6698\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9219, F1 Micro: 0.772, F1 Macro: 0.6829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.071, Accuracy: 0.9248, F1 Micro: 0.7738, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9258, F1 Micro: 0.779, F1 Macro: 0.6964\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9258, F1 Micro: 0.779, F1 Macro: 0.6964\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.82      0.59      0.68       157\n",
      "      HS_Race       0.82      0.64      0.72       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.62       331\n",
      "    HS_Strong       0.87      0.87      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 198.26093530654907 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3709, Accuracy: 0.8843, F1 Micro: 0.5698, F1 Macro: 0.2815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9032, F1 Micro: 0.7183, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2038, Accuracy: 0.9134, F1 Micro: 0.7461, F1 Macro: 0.588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9221, F1 Micro: 0.763, F1 Macro: 0.6148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9237, F1 Micro: 0.7675, F1 Macro: 0.6579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.9234, F1 Micro: 0.7741, F1 Macro: 0.6736\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.9222, F1 Micro: 0.7673, F1 Macro: 0.6577\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9228, F1 Micro: 0.7651, F1 Macro: 0.6685\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9218, F1 Micro: 0.7695, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9225, F1 Micro: 0.7767, F1 Macro: 0.6952\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9225, F1 Micro: 0.7767, F1 Macro: 0.6952\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.67      0.64      0.66       402\n",
      "  HS_Religion       0.68      0.62      0.65       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.26      0.42        72\n",
      "    HS_Gender       0.64      0.35      0.46        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.57      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.97300457954407 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.884, F1 Micro: 0.566, F1 Macro: 0.2927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9076, F1 Micro: 0.7273, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2002, Accuracy: 0.9164, F1 Micro: 0.7481, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9229, F1 Micro: 0.7671, F1 Macro: 0.6254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1465, Accuracy: 0.9258, F1 Micro: 0.7739, F1 Macro: 0.6679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.118, Accuracy: 0.9227, F1 Micro: 0.7765, F1 Macro: 0.6769\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9246, F1 Micro: 0.7719, F1 Macro: 0.6862\n",
      "Epoch 8/10, Train Loss: 0.0838, Accuracy: 0.923, F1 Micro: 0.7716, F1 Macro: 0.6947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9265, F1 Micro: 0.7832, F1 Macro: 0.7125\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9226, F1 Micro: 0.7784, F1 Macro: 0.6969\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9265, F1 Micro: 0.7832, F1 Macro: 0.7125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.74      0.66      0.69       157\n",
      "      HS_Race       0.83      0.68      0.75       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.55      0.51      0.53        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 199.69661784172058 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9249, F1 Micro: 0.7796, F1 Macro: 0.7014\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -10.24939993290143\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 6.973012208938599 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3671, Accuracy: 0.8903, F1 Micro: 0.6328, F1 Macro: 0.3282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9067, F1 Micro: 0.7233, F1 Macro: 0.5535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9168, F1 Micro: 0.7512, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9191, F1 Micro: 0.7636, F1 Macro: 0.6186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9266, F1 Micro: 0.7764, F1 Macro: 0.6486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9243, F1 Micro: 0.7796, F1 Macro: 0.664\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9229, F1 Micro: 0.7788, F1 Macro: 0.6715\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9243, F1 Micro: 0.7756, F1 Macro: 0.6837\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.9218, F1 Micro: 0.7771, F1 Macro: 0.7055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9212, F1 Micro: 0.7797, F1 Macro: 0.7016\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9212, F1 Micro: 0.7797, F1 Macro: 0.7016\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.70      0.79      0.74       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.68      0.63      0.66       157\n",
      "      HS_Race       0.67      0.73      0.70       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.73      0.86      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.85      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.75      0.81      0.78      5556\n",
      "    macro avg       0.74      0.70      0.70      5556\n",
      " weighted avg       0.75      0.81      0.78      5556\n",
      "  samples avg       0.45      0.45      0.44      5556\n",
      "\n",
      "Training completed in 201.4235758781433 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.8885, F1 Micro: 0.6307, F1 Macro: 0.3224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9074, F1 Micro: 0.7182, F1 Macro: 0.5471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.9169, F1 Micro: 0.7401, F1 Macro: 0.5914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9204, F1 Micro: 0.77, F1 Macro: 0.6459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1462, Accuracy: 0.9225, F1 Micro: 0.7745, F1 Macro: 0.6585\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9233, F1 Micro: 0.7726, F1 Macro: 0.6481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9228, F1 Micro: 0.7749, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.9253, F1 Micro: 0.7767, F1 Macro: 0.6863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9244, F1 Micro: 0.7773, F1 Macro: 0.7041\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.921, F1 Micro: 0.7746, F1 Macro: 0.6977\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9244, F1 Micro: 0.7773, F1 Macro: 0.7041\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.68      0.71      0.69       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.67      0.28      0.39        72\n",
      "    HS_Gender       0.52      0.51      0.51        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.03702640533447 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3771, Accuracy: 0.8863, F1 Micro: 0.6123, F1 Macro: 0.3374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9087, F1 Micro: 0.7245, F1 Macro: 0.56\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2036, Accuracy: 0.9186, F1 Micro: 0.753, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.9223, F1 Micro: 0.7692, F1 Macro: 0.6248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9252, F1 Micro: 0.7794, F1 Macro: 0.6808\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9218, F1 Micro: 0.7773, F1 Macro: 0.6706\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.9219, F1 Micro: 0.7766, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0824, Accuracy: 0.9223, F1 Micro: 0.7805, F1 Macro: 0.6979\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9248, F1 Micro: 0.7774, F1 Macro: 0.7112\n",
      "Epoch 10/10, Train Loss: 0.061, Accuracy: 0.9246, F1 Micro: 0.7799, F1 Macro: 0.7155\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9223, F1 Micro: 0.7805, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.71      0.79      0.75       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.74      0.59      0.66       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.73      0.84      0.78       762\n",
      "      HS_Weak       0.70      0.77      0.73       689\n",
      "  HS_Moderate       0.61      0.58      0.60       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 199.50797247886658 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9226, F1 Micro: 0.7792, F1 Macro: 0.7012\n",
      "Total sampling time: 5861.72 seconds\n",
      "Total runtime: 18972.44310092926 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf779b",
   "metadata": {
    "papermill": {
     "duration": 0.012123,
     "end_time": "2025-06-08T08:00:31.398003",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.385880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.8279, F1 Micro: 0.0872, F1 Macro: 0.0282\n",
      "Epoch 2/10, Train Loss: 0.4622, Accuracy: 0.8279, F1 Micro: 0.0229, F1 Macro: 0.0086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3927, Accuracy: 0.8335, F1 Micro: 0.1068, F1 Macro: 0.0416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3964, Accuracy: 0.8362, F1 Micro: 0.1315, F1 Macro: 0.046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3802, Accuracy: 0.8472, F1 Micro: 0.2582, F1 Macro: 0.0889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3611, Accuracy: 0.8539, F1 Micro: 0.347, F1 Macro: 0.1151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3431, Accuracy: 0.8712, F1 Micro: 0.5052, F1 Macro: 0.2252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3176, Accuracy: 0.8724, F1 Micro: 0.5088, F1 Macro: 0.2394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2825, Accuracy: 0.8742, F1 Micro: 0.5594, F1 Macro: 0.2676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2533, Accuracy: 0.8761, F1 Micro: 0.5629, F1 Macro: 0.2795\n",
      "Model 1 - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5629, F1 Macro: 0.2795\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.73      0.76      1134\n",
      "      Abusive       0.84      0.67      0.74       992\n",
      "HS_Individual       0.64      0.46      0.54       732\n",
      "     HS_Group       0.57      0.10      0.17       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.53      0.58       762\n",
      "      HS_Weak       0.59      0.41      0.49       689\n",
      "  HS_Moderate       0.45      0.05      0.08       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.46      0.56      5556\n",
      "    macro avg       0.38      0.25      0.28      5556\n",
      " weighted avg       0.62      0.46      0.52      5556\n",
      "  samples avg       0.35      0.26      0.27      5556\n",
      "\n",
      "Training completed in 61.794012784957886 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6617, Accuracy: 0.8259, F1 Micro: 0.1583, F1 Macro: 0.0585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4885, Accuracy: 0.8383, F1 Micro: 0.2329, F1 Macro: 0.0784\n",
      "Epoch 3/10, Train Loss: 0.4062, Accuracy: 0.8355, F1 Micro: 0.1232, F1 Macro: 0.047\n",
      "Epoch 4/10, Train Loss: 0.4042, Accuracy: 0.8358, F1 Micro: 0.1132, F1 Macro: 0.0443\n",
      "Epoch 5/10, Train Loss: 0.3882, Accuracy: 0.8426, F1 Micro: 0.1993, F1 Macro: 0.0725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3714, Accuracy: 0.8533, F1 Micro: 0.3255, F1 Macro: 0.106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3558, Accuracy: 0.8608, F1 Micro: 0.4029, F1 Macro: 0.1366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.335, Accuracy: 0.8717, F1 Micro: 0.4978, F1 Macro: 0.2183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3044, Accuracy: 0.8757, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "Epoch 10/10, Train Loss: 0.2723, Accuracy: 0.876, F1 Micro: 0.5502, F1 Macro: 0.2581\n",
      "Model 2 - Iteration 658: Accuracy: 0.8757, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.76      0.76      1134\n",
      "      Abusive       0.77      0.71      0.74       992\n",
      "HS_Individual       0.64      0.45      0.53       732\n",
      "     HS_Group       0.50      0.00      0.01       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.51      0.58       762\n",
      "      HS_Weak       0.63      0.38      0.47       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.46      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.58      0.46      0.50      5556\n",
      "  samples avg       0.37      0.26      0.28      5556\n",
      "\n",
      "Training completed in 54.62780022621155 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6172, Accuracy: 0.8198, F1 Micro: 0.3684, F1 Macro: 0.1118\n",
      "Epoch 2/10, Train Loss: 0.4799, Accuracy: 0.8282, F1 Micro: 0.0142, F1 Macro: 0.0063\n",
      "Epoch 3/10, Train Loss: 0.4012, Accuracy: 0.833, F1 Micro: 0.0831, F1 Macro: 0.031\n",
      "Epoch 4/10, Train Loss: 0.4015, Accuracy: 0.8381, F1 Micro: 0.1679, F1 Macro: 0.0518\n",
      "Epoch 5/10, Train Loss: 0.3869, Accuracy: 0.8431, F1 Micro: 0.215, F1 Macro: 0.0724\n",
      "Epoch 6/10, Train Loss: 0.3725, Accuracy: 0.8499, F1 Micro: 0.2925, F1 Macro: 0.0972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3585, Accuracy: 0.8575, F1 Micro: 0.3854, F1 Macro: 0.1216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3352, Accuracy: 0.8693, F1 Micro: 0.4738, F1 Macro: 0.2005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2977, Accuracy: 0.8755, F1 Micro: 0.5714, F1 Macro: 0.2756\n",
      "Epoch 10/10, Train Loss: 0.2676, Accuracy: 0.8765, F1 Micro: 0.561, F1 Macro: 0.2835\n",
      "Model 3 - Iteration 658: Accuracy: 0.8755, F1 Micro: 0.5714, F1 Macro: 0.2756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.73      0.75      1134\n",
      "      Abusive       0.80      0.74      0.77       992\n",
      "HS_Individual       0.63      0.45      0.53       732\n",
      "     HS_Group       0.45      0.04      0.08       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.58      0.60       762\n",
      "      HS_Weak       0.60      0.45      0.52       689\n",
      "  HS_Moderate       0.40      0.04      0.07       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.48      0.57      5556\n",
      "    macro avg       0.36      0.25      0.28      5556\n",
      " weighted avg       0.60      0.48      0.52      5556\n",
      "  samples avg       0.38      0.29      0.30      5556\n",
      "\n",
      "Training completed in 50.04361939430237 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8757, F1 Micro: 0.5647, F1 Macro: 0.271\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 450.4693972687419\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 701.8661472797394 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5199, Accuracy: 0.8291, F1 Micro: 0.0536, F1 Macro: 0.019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3979, Accuracy: 0.8369, F1 Micro: 0.1416, F1 Macro: 0.0522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3682, Accuracy: 0.8548, F1 Micro: 0.3592, F1 Macro: 0.1209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3289, Accuracy: 0.8775, F1 Micro: 0.5452, F1 Macro: 0.2569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2881, Accuracy: 0.8823, F1 Micro: 0.6257, F1 Macro: 0.33\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2567, Accuracy: 0.885, F1 Micro: 0.6616, F1 Macro: 0.3985\n",
      "Epoch 7/10, Train Loss: 0.2372, Accuracy: 0.8906, F1 Micro: 0.6219, F1 Macro: 0.3663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2162, Accuracy: 0.8895, F1 Micro: 0.6774, F1 Macro: 0.4657\n",
      "Epoch 9/10, Train Loss: 0.195, Accuracy: 0.8946, F1 Micro: 0.6531, F1 Macro: 0.4505\n",
      "Epoch 10/10, Train Loss: 0.1789, Accuracy: 0.8941, F1 Micro: 0.6739, F1 Macro: 0.4787\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8895, F1 Micro: 0.6774, F1 Macro: 0.4657\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.87      0.81      1134\n",
      "      Abusive       0.83      0.81      0.82       992\n",
      "HS_Individual       0.64      0.67      0.65       732\n",
      "     HS_Group       0.57      0.57      0.57       402\n",
      "  HS_Religion       0.51      0.24      0.32       157\n",
      "      HS_Race       0.81      0.40      0.54       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.75      0.70       762\n",
      "      HS_Weak       0.59      0.64      0.61       689\n",
      "  HS_Moderate       0.44      0.40      0.42       331\n",
      "    HS_Strong       1.00      0.08      0.15       114\n",
      "\n",
      "    micro avg       0.68      0.67      0.68      5556\n",
      "    macro avg       0.57      0.45      0.47      5556\n",
      " weighted avg       0.67      0.67      0.66      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 73.21042704582214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5651, Accuracy: 0.8391, F1 Micro: 0.2774, F1 Macro: 0.0878\n",
      "Epoch 2/10, Train Loss: 0.4062, Accuracy: 0.8434, F1 Micro: 0.2338, F1 Macro: 0.0782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3753, Accuracy: 0.8534, F1 Micro: 0.3334, F1 Macro: 0.1085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3422, Accuracy: 0.873, F1 Micro: 0.4993, F1 Macro: 0.2198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.304, Accuracy: 0.8815, F1 Micro: 0.5916, F1 Macro: 0.2878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2723, Accuracy: 0.8814, F1 Micro: 0.6615, F1 Macro: 0.3633\n",
      "Epoch 7/10, Train Loss: 0.252, Accuracy: 0.8864, F1 Micro: 0.5916, F1 Macro: 0.3146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2301, Accuracy: 0.8887, F1 Micro: 0.6667, F1 Macro: 0.3893\n",
      "Epoch 9/10, Train Loss: 0.2065, Accuracy: 0.8927, F1 Micro: 0.6461, F1 Macro: 0.3928\n",
      "Epoch 10/10, Train Loss: 0.1954, Accuracy: 0.893, F1 Micro: 0.6622, F1 Macro: 0.44\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8887, F1 Micro: 0.6667, F1 Macro: 0.3893\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.87      0.81      1134\n",
      "      Abusive       0.82      0.81      0.82       992\n",
      "HS_Individual       0.63      0.66      0.65       732\n",
      "     HS_Group       0.60      0.50      0.55       402\n",
      "  HS_Religion       0.60      0.08      0.14       157\n",
      "      HS_Race       1.00      0.03      0.05       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.74      0.69       762\n",
      "      HS_Weak       0.59      0.64      0.61       689\n",
      "  HS_Moderate       0.46      0.29      0.36       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.65      0.67      5556\n",
      "    macro avg       0.51      0.38      0.39      5556\n",
      " weighted avg       0.66      0.65      0.63      5556\n",
      "  samples avg       0.39      0.36      0.35      5556\n",
      "\n",
      "Training completed in 71.8843469619751 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5383, Accuracy: 0.8284, F1 Micro: 0.0142, F1 Macro: 0.0064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4023, Accuracy: 0.8417, F1 Micro: 0.2072, F1 Macro: 0.0709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.375, Accuracy: 0.8528, F1 Micro: 0.3296, F1 Macro: 0.1065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3419, Accuracy: 0.8753, F1 Micro: 0.5212, F1 Macro: 0.2361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2967, Accuracy: 0.8826, F1 Micro: 0.6096, F1 Macro: 0.3141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2649, Accuracy: 0.8811, F1 Micro: 0.6629, F1 Macro: 0.3862\n",
      "Epoch 7/10, Train Loss: 0.2443, Accuracy: 0.8895, F1 Micro: 0.6131, F1 Macro: 0.3553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2228, Accuracy: 0.8897, F1 Micro: 0.674, F1 Macro: 0.4181\n",
      "Epoch 9/10, Train Loss: 0.2009, Accuracy: 0.8954, F1 Micro: 0.6584, F1 Macro: 0.4248\n",
      "Epoch 10/10, Train Loss: 0.1882, Accuracy: 0.893, F1 Micro: 0.671, F1 Macro: 0.4599\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8897, F1 Micro: 0.674, F1 Macro: 0.4181\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.86      0.80      1134\n",
      "      Abusive       0.84      0.81      0.83       992\n",
      "HS_Individual       0.65      0.66      0.66       732\n",
      "     HS_Group       0.57      0.52      0.55       402\n",
      "  HS_Religion       0.54      0.24      0.33       157\n",
      "      HS_Race       1.00      0.06      0.11       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.77      0.70       762\n",
      "      HS_Weak       0.62      0.64      0.63       689\n",
      "  HS_Moderate       0.44      0.39      0.41       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.66      0.67      5556\n",
      "    macro avg       0.51      0.41      0.42      5556\n",
      " weighted avg       0.66      0.66      0.65      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 74.89425945281982 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8893, F1 Micro: 0.6727, F1 Macro: 0.4244\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 515.0016328337047\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 636.3063356876373 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4814, Accuracy: 0.8316, F1 Micro: 0.0634, F1 Macro: 0.0262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3704, Accuracy: 0.8553, F1 Micro: 0.3383, F1 Macro: 0.1275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3213, Accuracy: 0.8811, F1 Micro: 0.6036, F1 Macro: 0.3026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.279, Accuracy: 0.8909, F1 Micro: 0.6574, F1 Macro: 0.405\n",
      "Epoch 5/10, Train Loss: 0.2413, Accuracy: 0.896, F1 Micro: 0.6559, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2143, Accuracy: 0.8987, F1 Micro: 0.6845, F1 Macro: 0.4655\n",
      "Epoch 7/10, Train Loss: 0.1897, Accuracy: 0.8993, F1 Micro: 0.6669, F1 Macro: 0.4837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1677, Accuracy: 0.9036, F1 Micro: 0.7055, F1 Macro: 0.5081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1536, Accuracy: 0.9014, F1 Micro: 0.7056, F1 Macro: 0.5272\n",
      "Epoch 10/10, Train Loss: 0.1303, Accuracy: 0.904, F1 Micro: 0.7002, F1 Macro: 0.5246\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9014, F1 Micro: 0.7056, F1 Macro: 0.5272\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.82      0.81      1134\n",
      "      Abusive       0.83      0.87      0.85       992\n",
      "HS_Individual       0.67      0.67      0.67       732\n",
      "     HS_Group       0.64      0.55      0.59       402\n",
      "  HS_Religion       0.70      0.45      0.55       157\n",
      "      HS_Race       0.74      0.57      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.72      0.73       762\n",
      "      HS_Weak       0.63      0.64      0.64       689\n",
      "  HS_Moderate       0.49      0.43      0.46       331\n",
      "    HS_Strong       0.93      0.25      0.39       114\n",
      "\n",
      "    micro avg       0.73      0.69      0.71      5556\n",
      "    macro avg       0.60      0.50      0.53      5556\n",
      " weighted avg       0.71      0.69      0.69      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 89.73448133468628 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5138, Accuracy: 0.8335, F1 Micro: 0.0827, F1 Macro: 0.0332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3786, Accuracy: 0.8544, F1 Micro: 0.3213, F1 Macro: 0.11\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3332, Accuracy: 0.8796, F1 Micro: 0.5675, F1 Macro: 0.2698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2918, Accuracy: 0.8863, F1 Micro: 0.6417, F1 Macro: 0.3344\n",
      "Epoch 5/10, Train Loss: 0.2557, Accuracy: 0.8912, F1 Micro: 0.6268, F1 Macro: 0.3625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2268, Accuracy: 0.8966, F1 Micro: 0.6738, F1 Macro: 0.4145\n",
      "Epoch 7/10, Train Loss: 0.198, Accuracy: 0.8955, F1 Micro: 0.6495, F1 Macro: 0.4433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1794, Accuracy: 0.8996, F1 Micro: 0.6961, F1 Macro: 0.4901\n",
      "Epoch 9/10, Train Loss: 0.1654, Accuracy: 0.9016, F1 Micro: 0.6902, F1 Macro: 0.4919\n",
      "Epoch 10/10, Train Loss: 0.1426, Accuracy: 0.9028, F1 Micro: 0.6924, F1 Macro: 0.5134\n",
      "Model 2 - Iteration 2535: Accuracy: 0.8996, F1 Micro: 0.6961, F1 Macro: 0.4901\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.82      0.82      1134\n",
      "      Abusive       0.86      0.83      0.84       992\n",
      "HS_Individual       0.65      0.68      0.67       732\n",
      "     HS_Group       0.69      0.50      0.58       402\n",
      "  HS_Religion       0.67      0.27      0.38       157\n",
      "      HS_Race       0.85      0.43      0.57       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.74      0.72       762\n",
      "      HS_Weak       0.61      0.67      0.64       689\n",
      "  HS_Moderate       0.49      0.33      0.40       331\n",
      "    HS_Strong       0.94      0.15      0.26       114\n",
      "\n",
      "    micro avg       0.73      0.67      0.70      5556\n",
      "    macro avg       0.61      0.45      0.49      5556\n",
      " weighted avg       0.71      0.67      0.68      5556\n",
      "  samples avg       0.40      0.37      0.36      5556\n",
      "\n",
      "Training completed in 88.035964012146 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4955, Accuracy: 0.8306, F1 Micro: 0.0451, F1 Macro: 0.0186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3773, Accuracy: 0.8514, F1 Micro: 0.3144, F1 Macro: 0.1045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3337, Accuracy: 0.8805, F1 Micro: 0.5957, F1 Macro: 0.3013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2889, Accuracy: 0.8896, F1 Micro: 0.6493, F1 Macro: 0.3766\n",
      "Epoch 5/10, Train Loss: 0.2479, Accuracy: 0.8927, F1 Micro: 0.6443, F1 Macro: 0.3969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.222, Accuracy: 0.8969, F1 Micro: 0.685, F1 Macro: 0.4518\n",
      "Epoch 7/10, Train Loss: 0.1927, Accuracy: 0.8977, F1 Micro: 0.658, F1 Macro: 0.4626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1762, Accuracy: 0.8964, F1 Micro: 0.7056, F1 Macro: 0.5246\n",
      "Epoch 9/10, Train Loss: 0.1622, Accuracy: 0.9033, F1 Micro: 0.6973, F1 Macro: 0.5166\n",
      "Epoch 10/10, Train Loss: 0.137, Accuracy: 0.9024, F1 Micro: 0.6947, F1 Macro: 0.5296\n",
      "Model 3 - Iteration 2535: Accuracy: 0.8964, F1 Micro: 0.7056, F1 Macro: 0.5246\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.86      0.81      1134\n",
      "      Abusive       0.84      0.86      0.85       992\n",
      "HS_Individual       0.64      0.72      0.68       732\n",
      "     HS_Group       0.61      0.59      0.60       402\n",
      "  HS_Religion       0.62      0.42      0.50       157\n",
      "      HS_Race       0.69      0.53      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.79      0.72       762\n",
      "      HS_Weak       0.60      0.72      0.66       689\n",
      "  HS_Moderate       0.47      0.47      0.47       331\n",
      "    HS_Strong       0.84      0.27      0.41       114\n",
      "\n",
      "    micro avg       0.69      0.72      0.71      5556\n",
      "    macro avg       0.56      0.52      0.52      5556\n",
      " weighted avg       0.68      0.72      0.69      5556\n",
      "  samples avg       0.41      0.40      0.38      5556\n",
      "\n",
      "Training completed in 86.51797986030579 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8991, F1 Micro: 0.7024, F1 Macro: 0.514\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 705.0115057328148\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 566.0765573978424 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4575, Accuracy: 0.8368, F1 Micro: 0.1387, F1 Macro: 0.0511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3592, Accuracy: 0.8758, F1 Micro: 0.6019, F1 Macro: 0.305\n",
      "Epoch 3/10, Train Loss: 0.2979, Accuracy: 0.8877, F1 Micro: 0.5889, F1 Macro: 0.3085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2608, Accuracy: 0.8959, F1 Micro: 0.681, F1 Macro: 0.4546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2316, Accuracy: 0.901, F1 Micro: 0.6963, F1 Macro: 0.4816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.198, Accuracy: 0.9022, F1 Micro: 0.7191, F1 Macro: 0.5354\n",
      "Epoch 7/10, Train Loss: 0.167, Accuracy: 0.905, F1 Micro: 0.7166, F1 Macro: 0.5404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1464, Accuracy: 0.9071, F1 Micro: 0.7198, F1 Macro: 0.5511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1237, Accuracy: 0.9063, F1 Micro: 0.7291, F1 Macro: 0.5547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1066, Accuracy: 0.9084, F1 Micro: 0.7346, F1 Macro: 0.5762\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9084, F1 Micro: 0.7346, F1 Macro: 0.5762\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.83      0.88      0.86       992\n",
      "HS_Individual       0.67      0.74      0.70       732\n",
      "     HS_Group       0.70      0.62      0.66       402\n",
      "  HS_Religion       0.72      0.44      0.55       157\n",
      "      HS_Race       0.72      0.62      0.67       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.71      0.79      0.75       762\n",
      "      HS_Weak       0.63      0.71      0.67       689\n",
      "  HS_Moderate       0.59      0.47      0.52       331\n",
      "    HS_Strong       0.93      0.50      0.65       114\n",
      "\n",
      "    micro avg       0.73      0.74      0.73      5556\n",
      "    macro avg       0.73      0.56      0.58      5556\n",
      " weighted avg       0.73      0.74      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 104.44349002838135 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4841, Accuracy: 0.837, F1 Micro: 0.1242, F1 Macro: 0.0485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3681, Accuracy: 0.8728, F1 Micro: 0.5346, F1 Macro: 0.2389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3069, Accuracy: 0.8868, F1 Micro: 0.6171, F1 Macro: 0.3146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2659, Accuracy: 0.8951, F1 Micro: 0.6742, F1 Macro: 0.3972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2364, Accuracy: 0.8983, F1 Micro: 0.6752, F1 Macro: 0.444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2037, Accuracy: 0.9, F1 Micro: 0.7006, F1 Macro: 0.5031\n",
      "Epoch 7/10, Train Loss: 0.1733, Accuracy: 0.9017, F1 Micro: 0.677, F1 Macro: 0.4922\n",
      "Epoch 8/10, Train Loss: 0.1537, Accuracy: 0.9046, F1 Micro: 0.7002, F1 Macro: 0.531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1333, Accuracy: 0.9052, F1 Micro: 0.7186, F1 Macro: 0.536\n",
      "Epoch 10/10, Train Loss: 0.1124, Accuracy: 0.905, F1 Micro: 0.718, F1 Macro: 0.5468\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9052, F1 Micro: 0.7186, F1 Macro: 0.536\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.83      1134\n",
      "      Abusive       0.82      0.88      0.85       992\n",
      "HS_Individual       0.67      0.68      0.68       732\n",
      "     HS_Group       0.71      0.60      0.65       402\n",
      "  HS_Religion       0.68      0.44      0.53       157\n",
      "      HS_Race       0.75      0.62      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.74      0.74       762\n",
      "      HS_Weak       0.64      0.67      0.65       689\n",
      "  HS_Moderate       0.53      0.44      0.48       331\n",
      "    HS_Strong       0.92      0.21      0.34       114\n",
      "\n",
      "    micro avg       0.73      0.70      0.72      5556\n",
      "    macro avg       0.61      0.51      0.54      5556\n",
      " weighted avg       0.72      0.70      0.70      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 101.2411117553711 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4689, Accuracy: 0.8397, F1 Micro: 0.1843, F1 Macro: 0.0608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3654, Accuracy: 0.8767, F1 Micro: 0.5717, F1 Macro: 0.276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3038, Accuracy: 0.8879, F1 Micro: 0.5906, F1 Macro: 0.3138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.263, Accuracy: 0.8963, F1 Micro: 0.6743, F1 Macro: 0.4426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2345, Accuracy: 0.9008, F1 Micro: 0.6945, F1 Macro: 0.4786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2023, Accuracy: 0.9021, F1 Micro: 0.7143, F1 Macro: 0.517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.166, Accuracy: 0.9051, F1 Micro: 0.7196, F1 Macro: 0.535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1487, Accuracy: 0.9088, F1 Micro: 0.73, F1 Macro: 0.5572\n",
      "Epoch 9/10, Train Loss: 0.1262, Accuracy: 0.9045, F1 Micro: 0.7292, F1 Macro: 0.5548\n",
      "Epoch 10/10, Train Loss: 0.1124, Accuracy: 0.9065, F1 Micro: 0.7298, F1 Macro: 0.5657\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9088, F1 Micro: 0.73, F1 Macro: 0.5572\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.88      0.84      0.86       992\n",
      "HS_Individual       0.69      0.71      0.70       732\n",
      "     HS_Group       0.68      0.61      0.65       402\n",
      "  HS_Religion       0.68      0.46      0.55       157\n",
      "      HS_Race       0.72      0.62      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.77      0.74       762\n",
      "      HS_Weak       0.66      0.70      0.68       689\n",
      "  HS_Moderate       0.55      0.48      0.51       331\n",
      "    HS_Strong       0.91      0.35      0.51       114\n",
      "\n",
      "    micro avg       0.74      0.72      0.73      5556\n",
      "    macro avg       0.61      0.53      0.56      5556\n",
      " weighted avg       0.73      0.72      0.72      5556\n",
      "  samples avg       0.41      0.39      0.39      5556\n",
      "\n",
      "Training completed in 104.0886607170105 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9074, F1 Micro: 0.7277, F1 Macro: 0.5565\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 377.6839763132565\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 512.2773306369781 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4513, Accuracy: 0.852, F1 Micro: 0.3556, F1 Macro: 0.1096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.336, Accuracy: 0.8808, F1 Micro: 0.5649, F1 Macro: 0.2757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2771, Accuracy: 0.8966, F1 Micro: 0.6784, F1 Macro: 0.4176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2415, Accuracy: 0.9023, F1 Micro: 0.6924, F1 Macro: 0.4651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2111, Accuracy: 0.904, F1 Micro: 0.7056, F1 Macro: 0.5059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9071, F1 Micro: 0.7177, F1 Macro: 0.5376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1614, Accuracy: 0.9101, F1 Micro: 0.7272, F1 Macro: 0.5491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1317, Accuracy: 0.9102, F1 Micro: 0.7299, F1 Macro: 0.5658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1149, Accuracy: 0.9083, F1 Micro: 0.7349, F1 Macro: 0.5942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0976, Accuracy: 0.9097, F1 Micro: 0.7354, F1 Macro: 0.5911\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9097, F1 Micro: 0.7354, F1 Macro: 0.5911\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.87      0.88      0.87       992\n",
      "HS_Individual       0.70      0.67      0.69       732\n",
      "     HS_Group       0.63      0.65      0.64       402\n",
      "  HS_Religion       0.71      0.49      0.58       157\n",
      "      HS_Race       0.68      0.66      0.67       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.72      0.79      0.75       762\n",
      "      HS_Weak       0.68      0.65      0.66       689\n",
      "  HS_Moderate       0.53      0.56      0.54       331\n",
      "    HS_Strong       0.91      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.74      0.73      0.74      5556\n",
      "    macro avg       0.73      0.57      0.59      5556\n",
      " weighted avg       0.74      0.73      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 117.69141364097595 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4744, Accuracy: 0.8514, F1 Micro: 0.348, F1 Macro: 0.1083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3466, Accuracy: 0.8795, F1 Micro: 0.5609, F1 Macro: 0.2616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.289, Accuracy: 0.891, F1 Micro: 0.6676, F1 Macro: 0.3624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2512, Accuracy: 0.8994, F1 Micro: 0.6792, F1 Macro: 0.4263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2181, Accuracy: 0.903, F1 Micro: 0.6891, F1 Macro: 0.4654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1893, Accuracy: 0.9053, F1 Micro: 0.7067, F1 Macro: 0.5257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1636, Accuracy: 0.9066, F1 Micro: 0.7273, F1 Macro: 0.5471\n",
      "Epoch 8/10, Train Loss: 0.1391, Accuracy: 0.9077, F1 Micro: 0.7086, F1 Macro: 0.5352\n",
      "Epoch 9/10, Train Loss: 0.1232, Accuracy: 0.9087, F1 Micro: 0.7231, F1 Macro: 0.5506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1047, Accuracy: 0.9111, F1 Micro: 0.7317, F1 Macro: 0.5811\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9111, F1 Micro: 0.7317, F1 Macro: 0.5811\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.87      0.86      0.87       992\n",
      "HS_Individual       0.72      0.66      0.69       732\n",
      "     HS_Group       0.66      0.62      0.64       402\n",
      "  HS_Religion       0.68      0.45      0.54       157\n",
      "      HS_Race       0.75      0.67      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.76      0.75      0.75       762\n",
      "      HS_Weak       0.68      0.64      0.66       689\n",
      "  HS_Moderate       0.54      0.52      0.53       331\n",
      "    HS_Strong       0.94      0.54      0.69       114\n",
      "\n",
      "    micro avg       0.76      0.70      0.73      5556\n",
      "    macro avg       0.66      0.55      0.58      5556\n",
      " weighted avg       0.75      0.70      0.72      5556\n",
      "  samples avg       0.41      0.39      0.39      5556\n",
      "\n",
      "Training completed in 114.25476026535034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4616, Accuracy: 0.8495, F1 Micro: 0.3048, F1 Macro: 0.0994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3453, Accuracy: 0.8823, F1 Micro: 0.5742, F1 Macro: 0.2796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2837, Accuracy: 0.8913, F1 Micro: 0.677, F1 Macro: 0.3948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2475, Accuracy: 0.9012, F1 Micro: 0.7012, F1 Macro: 0.461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2173, Accuracy: 0.9032, F1 Micro: 0.7165, F1 Macro: 0.5427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1855, Accuracy: 0.9084, F1 Micro: 0.7256, F1 Macro: 0.5418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1589, Accuracy: 0.9102, F1 Micro: 0.7257, F1 Macro: 0.5572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1349, Accuracy: 0.9118, F1 Micro: 0.7287, F1 Macro: 0.5493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1167, Accuracy: 0.9103, F1 Micro: 0.7378, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1003, Accuracy: 0.912, F1 Micro: 0.7424, F1 Macro: 0.5956\n",
      "Model 3 - Iteration 4055: Accuracy: 0.912, F1 Micro: 0.7424, F1 Macro: 0.5956\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.84      1134\n",
      "      Abusive       0.88      0.84      0.86       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.68      0.60      0.64       402\n",
      "  HS_Religion       0.72      0.53      0.61       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.60      0.06      0.11        51\n",
      "     HS_Other       0.71      0.81      0.76       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.56      0.50      0.53       331\n",
      "    HS_Strong       0.93      0.58      0.71       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.74      5556\n",
      "    macro avg       0.67      0.57      0.60      5556\n",
      " weighted avg       0.74      0.74      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 118.32304501533508 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.911, F1 Micro: 0.7365, F1 Macro: 0.5893\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 501.75866461685234\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 456.5606071949005 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4385, Accuracy: 0.8553, F1 Micro: 0.3691, F1 Macro: 0.1164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.333, Accuracy: 0.8854, F1 Micro: 0.6079, F1 Macro: 0.3086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2723, Accuracy: 0.8961, F1 Micro: 0.6381, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2354, Accuracy: 0.9032, F1 Micro: 0.7077, F1 Macro: 0.5067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2041, Accuracy: 0.9073, F1 Micro: 0.729, F1 Macro: 0.5447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1696, Accuracy: 0.9105, F1 Micro: 0.7368, F1 Macro: 0.5524\n",
      "Epoch 7/10, Train Loss: 0.1496, Accuracy: 0.912, F1 Micro: 0.7233, F1 Macro: 0.5483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1234, Accuracy: 0.9111, F1 Micro: 0.7393, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1101, Accuracy: 0.9115, F1 Micro: 0.741, F1 Macro: 0.6186\n",
      "Epoch 10/10, Train Loss: 0.0979, Accuracy: 0.9152, F1 Micro: 0.7406, F1 Macro: 0.6235\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9115, F1 Micro: 0.741, F1 Macro: 0.6186\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.72      0.66      0.69       732\n",
      "     HS_Group       0.63      0.70      0.66       402\n",
      "  HS_Religion       0.69      0.55      0.61       157\n",
      "      HS_Race       0.75      0.74      0.74       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.67      0.12      0.20        51\n",
      "     HS_Other       0.73      0.76      0.75       762\n",
      "      HS_Weak       0.68      0.63      0.65       689\n",
      "  HS_Moderate       0.53      0.60      0.57       331\n",
      "    HS_Strong       0.91      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.74      5556\n",
      "    macro avg       0.75      0.60      0.62      5556\n",
      " weighted avg       0.75      0.74      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 125.07750821113586 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4579, Accuracy: 0.8551, F1 Micro: 0.3558, F1 Macro: 0.112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3404, Accuracy: 0.8841, F1 Micro: 0.6262, F1 Macro: 0.312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2799, Accuracy: 0.8956, F1 Micro: 0.6693, F1 Macro: 0.3933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2426, Accuracy: 0.8967, F1 Micro: 0.7068, F1 Macro: 0.5024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2121, Accuracy: 0.9059, F1 Micro: 0.7205, F1 Macro: 0.5138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1745, Accuracy: 0.9094, F1 Micro: 0.7261, F1 Macro: 0.5328\n",
      "Epoch 7/10, Train Loss: 0.1523, Accuracy: 0.9104, F1 Micro: 0.7158, F1 Macro: 0.5488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1274, Accuracy: 0.9116, F1 Micro: 0.7321, F1 Macro: 0.5674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1128, Accuracy: 0.9129, F1 Micro: 0.7394, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0974, Accuracy: 0.9132, F1 Micro: 0.744, F1 Macro: 0.6129\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9132, F1 Micro: 0.744, F1 Macro: 0.6129\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.82      1134\n",
      "      Abusive       0.89      0.86      0.88       992\n",
      "HS_Individual       0.69      0.71      0.70       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.65      0.51      0.57       157\n",
      "      HS_Race       0.71      0.74      0.72       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.55      0.12      0.19        51\n",
      "     HS_Other       0.76      0.77      0.77       762\n",
      "      HS_Weak       0.67      0.70      0.68       689\n",
      "  HS_Moderate       0.57      0.52      0.54       331\n",
      "    HS_Strong       0.92      0.70      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.74      5556\n",
      "    macro avg       0.74      0.59      0.61      5556\n",
      " weighted avg       0.76      0.73      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 127.96618247032166 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.448, Accuracy: 0.8531, F1 Micro: 0.347, F1 Macro: 0.1096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3414, Accuracy: 0.8847, F1 Micro: 0.6226, F1 Macro: 0.3176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2774, Accuracy: 0.897, F1 Micro: 0.6534, F1 Macro: 0.388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2398, Accuracy: 0.9025, F1 Micro: 0.708, F1 Macro: 0.5098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2087, Accuracy: 0.9052, F1 Micro: 0.724, F1 Macro: 0.5364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1741, Accuracy: 0.9125, F1 Micro: 0.7356, F1 Macro: 0.5555\n",
      "Epoch 7/10, Train Loss: 0.1516, Accuracy: 0.9128, F1 Micro: 0.7294, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1274, Accuracy: 0.9119, F1 Micro: 0.7389, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1102, Accuracy: 0.9142, F1 Micro: 0.7499, F1 Macro: 0.622\n",
      "Epoch 10/10, Train Loss: 0.0948, Accuracy: 0.915, F1 Micro: 0.7467, F1 Macro: 0.6271\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9142, F1 Micro: 0.7499, F1 Macro: 0.622\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.87      0.88      0.87       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.68      0.62      0.65       402\n",
      "  HS_Religion       0.67      0.53      0.59       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.50      0.14      0.22        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.67      0.71      0.69       689\n",
      "  HS_Moderate       0.58      0.53      0.56       331\n",
      "    HS_Strong       0.93      0.65      0.76       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.74      0.60      0.62      5556\n",
      " weighted avg       0.75      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 127.21947288513184 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.913, F1 Micro: 0.745, F1 Macro: 0.6178\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 585.1014866006491\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 423.2147228717804 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4285, Accuracy: 0.8638, F1 Micro: 0.4405, F1 Macro: 0.1678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3143, Accuracy: 0.8913, F1 Micro: 0.6666, F1 Macro: 0.4293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2568, Accuracy: 0.9016, F1 Micro: 0.7003, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2214, Accuracy: 0.9073, F1 Micro: 0.7121, F1 Macro: 0.5188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1814, Accuracy: 0.9052, F1 Micro: 0.7353, F1 Macro: 0.5682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1607, Accuracy: 0.9115, F1 Micro: 0.7419, F1 Macro: 0.5716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1299, Accuracy: 0.9124, F1 Micro: 0.742, F1 Macro: 0.5876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1182, Accuracy: 0.9165, F1 Micro: 0.7468, F1 Macro: 0.6138\n",
      "Epoch 9/10, Train Loss: 0.1001, Accuracy: 0.9134, F1 Micro: 0.7443, F1 Macro: 0.6428\n",
      "Epoch 10/10, Train Loss: 0.0868, Accuracy: 0.907, F1 Micro: 0.7439, F1 Macro: 0.6452\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9165, F1 Micro: 0.7468, F1 Macro: 0.6138\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.89      0.86      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.75      0.58      0.65       402\n",
      "  HS_Religion       0.77      0.48      0.59       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.42      0.10      0.16        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.63      0.47      0.54       331\n",
      "    HS_Strong       0.92      0.61      0.74       114\n",
      "\n",
      "    micro avg       0.78      0.71      0.75      5556\n",
      "    macro avg       0.74      0.56      0.61      5556\n",
      " weighted avg       0.78      0.71      0.74      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 135.3291199207306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4493, Accuracy: 0.8599, F1 Micro: 0.4088, F1 Macro: 0.1407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.326, Accuracy: 0.8889, F1 Micro: 0.6428, F1 Macro: 0.3443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.268, Accuracy: 0.8982, F1 Micro: 0.6809, F1 Macro: 0.4524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.228, Accuracy: 0.9047, F1 Micro: 0.7043, F1 Macro: 0.4697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9038, F1 Micro: 0.7335, F1 Macro: 0.5591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.167, Accuracy: 0.9129, F1 Micro: 0.7348, F1 Macro: 0.5628\n",
      "Epoch 7/10, Train Loss: 0.1338, Accuracy: 0.913, F1 Micro: 0.7315, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.125, Accuracy: 0.9149, F1 Micro: 0.7407, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1078, Accuracy: 0.9138, F1 Micro: 0.7463, F1 Macro: 0.6344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0925, Accuracy: 0.916, F1 Micro: 0.7492, F1 Macro: 0.6183\n",
      "Model 2 - Iteration 5287: Accuracy: 0.916, F1 Micro: 0.7492, F1 Macro: 0.6183\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.89      0.85      0.87       992\n",
      "HS_Individual       0.73      0.68      0.70       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.76      0.46      0.58       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.42      0.10      0.16        51\n",
      "     HS_Other       0.74      0.79      0.77       762\n",
      "      HS_Weak       0.71      0.63      0.67       689\n",
      "  HS_Moderate       0.59      0.57      0.58       331\n",
      "    HS_Strong       0.90      0.72      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.71      0.59      0.62      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 138.05086469650269 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4382, Accuracy: 0.8574, F1 Micro: 0.396, F1 Macro: 0.126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3233, Accuracy: 0.8909, F1 Micro: 0.661, F1 Macro: 0.3774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2622, Accuracy: 0.9025, F1 Micro: 0.6844, F1 Macro: 0.4672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2224, Accuracy: 0.9078, F1 Micro: 0.709, F1 Macro: 0.5122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9039, F1 Micro: 0.7347, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1635, Accuracy: 0.9136, F1 Micro: 0.7449, F1 Macro: 0.5792\n",
      "Epoch 7/10, Train Loss: 0.133, Accuracy: 0.9126, F1 Micro: 0.7356, F1 Macro: 0.5717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1215, Accuracy: 0.9146, F1 Micro: 0.7456, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1035, Accuracy: 0.9183, F1 Micro: 0.7559, F1 Macro: 0.6441\n",
      "Epoch 10/10, Train Loss: 0.0901, Accuracy: 0.9161, F1 Micro: 0.7498, F1 Macro: 0.6373\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9183, F1 Micro: 0.7559, F1 Macro: 0.6441\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.73      0.68      0.71       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.80      0.72      0.75       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.52      0.22      0.31        51\n",
      "     HS_Other       0.79      0.74      0.76       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.61      0.57      0.59       331\n",
      "    HS_Strong       0.92      0.71      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.74      0.61      0.64      5556\n",
      " weighted avg       0.77      0.73      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 134.93536043167114 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9169, F1 Micro: 0.7506, F1 Macro: 0.6254\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 884.7821078499287\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 368.4058003425598 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4229, Accuracy: 0.8706, F1 Micro: 0.5013, F1 Macro: 0.2202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3041, Accuracy: 0.8956, F1 Micro: 0.6542, F1 Macro: 0.3941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9053, F1 Micro: 0.7042, F1 Macro: 0.4928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9105, F1 Micro: 0.7281, F1 Macro: 0.5407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1823, Accuracy: 0.9131, F1 Micro: 0.739, F1 Macro: 0.5592\n",
      "Epoch 6/10, Train Loss: 0.1496, Accuracy: 0.9088, F1 Micro: 0.7378, F1 Macro: 0.5583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1285, Accuracy: 0.9051, F1 Micro: 0.7414, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9153, F1 Micro: 0.749, F1 Macro: 0.6296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0891, Accuracy: 0.9161, F1 Micro: 0.7541, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0794, Accuracy: 0.9172, F1 Micro: 0.7567, F1 Macro: 0.6517\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9172, F1 Micro: 0.7567, F1 Macro: 0.6517\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.75      0.52      0.61       157\n",
      "      HS_Race       0.81      0.66      0.72       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.55      0.22      0.31        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.57      0.61      0.59       331\n",
      "    HS_Strong       0.93      0.69      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.74      0.62      0.65      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 144.99994826316833 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4414, Accuracy: 0.8615, F1 Micro: 0.4147, F1 Macro: 0.1452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3158, Accuracy: 0.8897, F1 Micro: 0.6183, F1 Macro: 0.3197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2563, Accuracy: 0.9024, F1 Micro: 0.6896, F1 Macro: 0.4546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.22, Accuracy: 0.9065, F1 Micro: 0.7017, F1 Macro: 0.486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1904, Accuracy: 0.9109, F1 Micro: 0.7183, F1 Macro: 0.5163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.156, Accuracy: 0.9129, F1 Micro: 0.7349, F1 Macro: 0.538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9133, F1 Micro: 0.744, F1 Macro: 0.5885\n",
      "Epoch 8/10, Train Loss: 0.1146, Accuracy: 0.9153, F1 Micro: 0.7431, F1 Macro: 0.6142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0983, Accuracy: 0.9173, F1 Micro: 0.7535, F1 Macro: 0.6353\n",
      "Epoch 10/10, Train Loss: 0.0845, Accuracy: 0.9159, F1 Micro: 0.7479, F1 Macro: 0.6323\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9173, F1 Micro: 0.7535, F1 Macro: 0.6353\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.69      0.60      0.65       402\n",
      "  HS_Religion       0.73      0.53      0.61       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       0.45      0.07      0.12        72\n",
      "    HS_Gender       0.53      0.18      0.26        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.68      0.68       689\n",
      "  HS_Moderate       0.60      0.52      0.56       331\n",
      "    HS_Strong       0.92      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.72      0.60      0.64      5556\n",
      " weighted avg       0.77      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 143.93138337135315 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4326, Accuracy: 0.8575, F1 Micro: 0.3791, F1 Macro: 0.1254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3129, Accuracy: 0.8937, F1 Micro: 0.6352, F1 Macro: 0.35\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2521, Accuracy: 0.905, F1 Micro: 0.7049, F1 Macro: 0.4882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2184, Accuracy: 0.9092, F1 Micro: 0.7201, F1 Macro: 0.5156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1876, Accuracy: 0.913, F1 Micro: 0.7292, F1 Macro: 0.5384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1534, Accuracy: 0.9108, F1 Micro: 0.7398, F1 Macro: 0.5567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1337, Accuracy: 0.9106, F1 Micro: 0.7464, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1138, Accuracy: 0.9158, F1 Micro: 0.7517, F1 Macro: 0.6255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.9175, F1 Micro: 0.7604, F1 Macro: 0.6509\n",
      "Epoch 10/10, Train Loss: 0.0853, Accuracy: 0.9115, F1 Micro: 0.7503, F1 Macro: 0.6496\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9175, F1 Micro: 0.7604, F1 Macro: 0.6509\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.70      0.55      0.62       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.58      0.10      0.17        72\n",
      "    HS_Gender       0.52      0.25      0.34        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.60      0.54      0.57       331\n",
      "    HS_Strong       0.92      0.70      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.72      0.62      0.65      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 145.64965200424194 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9173, F1 Micro: 0.7569, F1 Macro: 0.6459\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 620.2638938388784\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 331.5566749572754 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4172, Accuracy: 0.8752, F1 Micro: 0.5685, F1 Macro: 0.2703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2967, Accuracy: 0.898, F1 Micro: 0.6655, F1 Macro: 0.4288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.245, Accuracy: 0.9061, F1 Micro: 0.7125, F1 Macro: 0.5101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2065, Accuracy: 0.9087, F1 Micro: 0.7392, F1 Macro: 0.5604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9144, F1 Micro: 0.7411, F1 Macro: 0.58\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1435, Accuracy: 0.9156, F1 Micro: 0.7431, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.9153, F1 Micro: 0.7534, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1065, Accuracy: 0.9172, F1 Micro: 0.7549, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9183, F1 Micro: 0.7611, F1 Macro: 0.6492\n",
      "Epoch 10/10, Train Loss: 0.0764, Accuracy: 0.9142, F1 Micro: 0.758, F1 Macro: 0.6614\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9183, F1 Micro: 0.7611, F1 Macro: 0.6492\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.74      0.56      0.64       157\n",
      "      HS_Race       0.82      0.68      0.74       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.60      0.56      0.58       331\n",
      "    HS_Strong       0.90      0.66      0.76       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.75      0.62      0.65      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 153.71779417991638 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4344, Accuracy: 0.8717, F1 Micro: 0.5062, F1 Macro: 0.2177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3042, Accuracy: 0.8938, F1 Micro: 0.6555, F1 Macro: 0.3634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2531, Accuracy: 0.9034, F1 Micro: 0.7, F1 Macro: 0.4783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.212, Accuracy: 0.9096, F1 Micro: 0.7297, F1 Macro: 0.542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1781, Accuracy: 0.9124, F1 Micro: 0.7361, F1 Macro: 0.5606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.149, Accuracy: 0.9132, F1 Micro: 0.738, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.129, Accuracy: 0.9126, F1 Micro: 0.7479, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1104, Accuracy: 0.9144, F1 Micro: 0.7488, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.9187, F1 Micro: 0.7506, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.9165, F1 Micro: 0.7558, F1 Macro: 0.6338\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9165, F1 Micro: 0.7558, F1 Macro: 0.6338\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.67      0.64      0.65       402\n",
      "  HS_Religion       0.72      0.49      0.58       157\n",
      "      HS_Race       0.74      0.63      0.68       120\n",
      "  HS_Physical       0.43      0.04      0.08        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.57      0.56      0.57       331\n",
      "    HS_Strong       0.90      0.67      0.77       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.71      0.61      0.63      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 156.0118579864502 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4277, Accuracy: 0.8619, F1 Micro: 0.4002, F1 Macro: 0.1405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3028, Accuracy: 0.8968, F1 Micro: 0.6583, F1 Macro: 0.3875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9038, F1 Micro: 0.7021, F1 Macro: 0.4761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2093, Accuracy: 0.9074, F1 Micro: 0.738, F1 Macro: 0.5509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1755, Accuracy: 0.9138, F1 Micro: 0.7456, F1 Macro: 0.5814\n",
      "Epoch 6/10, Train Loss: 0.1467, Accuracy: 0.914, F1 Micro: 0.745, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1298, Accuracy: 0.9146, F1 Micro: 0.7506, F1 Macro: 0.6232\n",
      "Epoch 8/10, Train Loss: 0.1079, Accuracy: 0.9138, F1 Micro: 0.7482, F1 Macro: 0.6324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0927, Accuracy: 0.918, F1 Micro: 0.7585, F1 Macro: 0.6556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0796, Accuracy: 0.9181, F1 Micro: 0.7597, F1 Macro: 0.6596\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9181, F1 Micro: 0.7597, F1 Macro: 0.6596\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.71      0.61      0.66       402\n",
      "  HS_Religion       0.75      0.53      0.62       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.64      0.35      0.46        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.89      0.72      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 153.77846932411194 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9176, F1 Micro: 0.7589, F1 Macro: 0.6475\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1063.0426085885454\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 311.0910978317261 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4143, Accuracy: 0.8781, F1 Micro: 0.5701, F1 Macro: 0.271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2979, Accuracy: 0.8957, F1 Micro: 0.6347, F1 Macro: 0.4065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.9076, F1 Micro: 0.713, F1 Macro: 0.5137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2047, Accuracy: 0.9119, F1 Micro: 0.7423, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1718, Accuracy: 0.9102, F1 Micro: 0.7452, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1445, Accuracy: 0.9171, F1 Micro: 0.7458, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1226, Accuracy: 0.9191, F1 Micro: 0.7627, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9191, F1 Micro: 0.7647, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.089, Accuracy: 0.9186, F1 Micro: 0.7661, F1 Macro: 0.6439\n",
      "Epoch 10/10, Train Loss: 0.0789, Accuracy: 0.9195, F1 Micro: 0.7638, F1 Macro: 0.6461\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9186, F1 Micro: 0.7661, F1 Macro: 0.6439\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.70      0.59      0.64       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.47      0.16      0.24        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.66      0.77      0.71       689\n",
      "  HS_Moderate       0.65      0.53      0.59       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.73      0.63      0.64      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.57669758796692 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4327, Accuracy: 0.8719, F1 Micro: 0.4885, F1 Macro: 0.2103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3058, Accuracy: 0.8876, F1 Micro: 0.5867, F1 Macro: 0.3187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2495, Accuracy: 0.9038, F1 Micro: 0.6844, F1 Macro: 0.4373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2119, Accuracy: 0.9098, F1 Micro: 0.7283, F1 Macro: 0.5269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1793, Accuracy: 0.915, F1 Micro: 0.7442, F1 Macro: 0.5734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9174, F1 Micro: 0.7466, F1 Macro: 0.5916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1284, Accuracy: 0.9191, F1 Micro: 0.7567, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1089, Accuracy: 0.9178, F1 Micro: 0.7578, F1 Macro: 0.6297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0936, Accuracy: 0.9199, F1 Micro: 0.7669, F1 Macro: 0.6396\n",
      "Epoch 10/10, Train Loss: 0.0821, Accuracy: 0.9198, F1 Micro: 0.7609, F1 Macro: 0.6443\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9199, F1 Micro: 0.7669, F1 Macro: 0.6396\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.76      0.55      0.64       402\n",
      "  HS_Religion       0.71      0.53      0.61       157\n",
      "      HS_Race       0.80      0.63      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.74      0.81      0.78       762\n",
      "      HS_Weak       0.66      0.78      0.72       689\n",
      "  HS_Moderate       0.68      0.47      0.56       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.69      0.61      0.64      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 162.44525337219238 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4245, Accuracy: 0.8729, F1 Micro: 0.5237, F1 Macro: 0.2313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3031, Accuracy: 0.8944, F1 Micro: 0.6323, F1 Macro: 0.3577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9072, F1 Micro: 0.702, F1 Macro: 0.4905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2077, Accuracy: 0.9126, F1 Micro: 0.7338, F1 Macro: 0.5542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1745, Accuracy: 0.9102, F1 Micro: 0.7431, F1 Macro: 0.5791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1483, Accuracy: 0.9128, F1 Micro: 0.7519, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.127, Accuracy: 0.9189, F1 Micro: 0.7587, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.9194, F1 Micro: 0.7603, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0916, Accuracy: 0.9199, F1 Micro: 0.7651, F1 Macro: 0.6409\n",
      "Epoch 10/10, Train Loss: 0.0793, Accuracy: 0.9209, F1 Micro: 0.7643, F1 Macro: 0.6511\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9199, F1 Micro: 0.7651, F1 Macro: 0.6409\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.60      0.65       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.59      0.20      0.29        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.53      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.76      0.62      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 161.2303340435028 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9195, F1 Micro: 0.766, F1 Macro: 0.6415\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1124.3430385032052\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 287.4840462207794 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.406, Accuracy: 0.8778, F1 Micro: 0.5476, F1 Macro: 0.2546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2816, Accuracy: 0.8983, F1 Micro: 0.6589, F1 Macro: 0.4366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2404, Accuracy: 0.9038, F1 Micro: 0.7193, F1 Macro: 0.5136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2045, Accuracy: 0.912, F1 Micro: 0.7419, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.9195, F1 Micro: 0.7553, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9183, F1 Micro: 0.7561, F1 Macro: 0.6248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1181, Accuracy: 0.9186, F1 Micro: 0.7575, F1 Macro: 0.6277\n",
      "Epoch 8/10, Train Loss: 0.1018, Accuracy: 0.9187, F1 Micro: 0.7507, F1 Macro: 0.6301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9203, F1 Micro: 0.7599, F1 Macro: 0.647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9204, F1 Micro: 0.7702, F1 Macro: 0.6636\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9204, F1 Micro: 0.7702, F1 Macro: 0.6636\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.68      0.68       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.84      0.64      0.73       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.72      0.85      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.76      0.64      0.66      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 167.19563579559326 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4227, Accuracy: 0.8755, F1 Micro: 0.5345, F1 Macro: 0.2399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2907, Accuracy: 0.8952, F1 Micro: 0.656, F1 Macro: 0.3853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2475, Accuracy: 0.9046, F1 Micro: 0.7077, F1 Macro: 0.4812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9121, F1 Micro: 0.7323, F1 Macro: 0.5497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1775, Accuracy: 0.9163, F1 Micro: 0.7428, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1467, Accuracy: 0.9182, F1 Micro: 0.7505, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.124, Accuracy: 0.9137, F1 Micro: 0.7533, F1 Macro: 0.6333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9144, F1 Micro: 0.756, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0887, Accuracy: 0.9177, F1 Micro: 0.7591, F1 Macro: 0.6354\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9183, F1 Micro: 0.7559, F1 Macro: 0.6169\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9177, F1 Micro: 0.7591, F1 Macro: 0.6354\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.67      0.64      0.66       402\n",
      "  HS_Religion       0.68      0.57      0.62       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.50      0.16      0.24        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.54      0.57       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.71      0.61      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 165.12236332893372 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.416, Accuracy: 0.8692, F1 Micro: 0.4531, F1 Macro: 0.1995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2888, Accuracy: 0.8975, F1 Micro: 0.6722, F1 Macro: 0.3982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2452, Accuracy: 0.904, F1 Micro: 0.7152, F1 Macro: 0.492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2085, Accuracy: 0.9077, F1 Micro: 0.7408, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9191, F1 Micro: 0.7479, F1 Macro: 0.5945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.9197, F1 Micro: 0.7547, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1229, Accuracy: 0.9193, F1 Micro: 0.7629, F1 Macro: 0.6386\n",
      "Epoch 8/10, Train Loss: 0.1014, Accuracy: 0.9174, F1 Micro: 0.7623, F1 Macro: 0.6578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0873, Accuracy: 0.9196, F1 Micro: 0.7629, F1 Macro: 0.6535\n",
      "Epoch 10/10, Train Loss: 0.0775, Accuracy: 0.9174, F1 Micro: 0.7589, F1 Macro: 0.6549\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9196, F1 Micro: 0.7629, F1 Macro: 0.6535\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.70      0.62      0.66       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.56      0.29      0.38        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.56      0.58       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.76      0.63      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 161.56420874595642 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9193, F1 Micro: 0.7641, F1 Macro: 0.6508\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 943.4901969851643\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 251.01045894622803 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.406, Accuracy: 0.8738, F1 Micro: 0.488, F1 Macro: 0.2242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.8982, F1 Micro: 0.6868, F1 Macro: 0.4586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.9086, F1 Micro: 0.726, F1 Macro: 0.5415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2008, Accuracy: 0.9159, F1 Micro: 0.7389, F1 Macro: 0.5733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.9175, F1 Micro: 0.7604, F1 Macro: 0.6064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1396, Accuracy: 0.9174, F1 Micro: 0.7625, F1 Macro: 0.6211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.9191, F1 Micro: 0.7642, F1 Macro: 0.6369\n",
      "Epoch 8/10, Train Loss: 0.1033, Accuracy: 0.9167, F1 Micro: 0.7618, F1 Macro: 0.6476\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9178, F1 Micro: 0.7587, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9203, F1 Micro: 0.7676, F1 Macro: 0.6726\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9203, F1 Micro: 0.7676, F1 Macro: 0.6726\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.69      0.61      0.65       157\n",
      "      HS_Race       0.71      0.75      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 168.71271681785583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4244, Accuracy: 0.876, F1 Micro: 0.5297, F1 Macro: 0.2398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.8963, F1 Micro: 0.6683, F1 Macro: 0.4189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2403, Accuracy: 0.905, F1 Micro: 0.7171, F1 Macro: 0.4882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9131, F1 Micro: 0.7312, F1 Macro: 0.542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1702, Accuracy: 0.9134, F1 Micro: 0.754, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1468, Accuracy: 0.9174, F1 Micro: 0.763, F1 Macro: 0.6194\n",
      "Epoch 7/10, Train Loss: 0.1204, Accuracy: 0.9205, F1 Micro: 0.7563, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1072, Accuracy: 0.9204, F1 Micro: 0.7648, F1 Macro: 0.6376\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9204, F1 Micro: 0.7628, F1 Macro: 0.6537\n",
      "Epoch 10/10, Train Loss: 0.0765, Accuracy: 0.9213, F1 Micro: 0.7643, F1 Macro: 0.6604\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9204, F1 Micro: 0.7648, F1 Macro: 0.6376\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.70      0.63      0.66       402\n",
      "  HS_Religion       0.74      0.53      0.62       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.53      0.20      0.29        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.63      0.55      0.58       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.72      0.61      0.64      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 166.0510802268982 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.8728, F1 Micro: 0.4839, F1 Macro: 0.2194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2883, Accuracy: 0.8982, F1 Micro: 0.6829, F1 Macro: 0.4556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2361, Accuracy: 0.9042, F1 Micro: 0.7239, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9129, F1 Micro: 0.7321, F1 Macro: 0.5577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1677, Accuracy: 0.9141, F1 Micro: 0.7571, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1433, Accuracy: 0.9144, F1 Micro: 0.759, F1 Macro: 0.6254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9202, F1 Micro: 0.7591, F1 Macro: 0.6265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9173, F1 Micro: 0.7668, F1 Macro: 0.6579\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.918, F1 Micro: 0.7625, F1 Macro: 0.6606\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.9186, F1 Micro: 0.7558, F1 Macro: 0.6665\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9173, F1 Micro: 0.7668, F1 Macro: 0.6579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.66      0.70      0.68       402\n",
      "  HS_Religion       0.70      0.56      0.62       157\n",
      "      HS_Race       0.71      0.72      0.71       120\n",
      "  HS_Physical       0.67      0.08      0.15        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.71      0.84      0.77       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.72      0.65      0.66      5556\n",
      " weighted avg       0.74      0.79      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 168.03056025505066 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9193, F1 Micro: 0.7664, F1 Macro: 0.656\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 940.3432705214827\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 224.4458396434784 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.399, Accuracy: 0.8805, F1 Micro: 0.5596, F1 Macro: 0.2672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2796, Accuracy: 0.8991, F1 Micro: 0.6941, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.232, Accuracy: 0.9086, F1 Micro: 0.715, F1 Macro: 0.5095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.9154, F1 Micro: 0.742, F1 Macro: 0.5751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.9195, F1 Micro: 0.7536, F1 Macro: 0.6052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1374, Accuracy: 0.9204, F1 Micro: 0.7668, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9242, F1 Micro: 0.7698, F1 Macro: 0.6606\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.9234, F1 Micro: 0.7629, F1 Macro: 0.6626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.9229, F1 Micro: 0.7718, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9215, F1 Micro: 0.7762, F1 Macro: 0.6797\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9215, F1 Micro: 0.7762, F1 Macro: 0.6797\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.76      0.67      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 175.7029619216919 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4145, Accuracy: 0.8778, F1 Micro: 0.5438, F1 Macro: 0.2462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.8971, F1 Micro: 0.6636, F1 Macro: 0.3656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2402, Accuracy: 0.9083, F1 Micro: 0.7176, F1 Macro: 0.5011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9144, F1 Micro: 0.7386, F1 Macro: 0.5577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1693, Accuracy: 0.919, F1 Micro: 0.7541, F1 Macro: 0.6051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9199, F1 Micro: 0.7631, F1 Macro: 0.6261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1186, Accuracy: 0.9216, F1 Micro: 0.7648, F1 Macro: 0.6531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1045, Accuracy: 0.9217, F1 Micro: 0.7683, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0898, Accuracy: 0.9227, F1 Micro: 0.773, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9226, F1 Micro: 0.7735, F1 Macro: 0.6714\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9226, F1 Micro: 0.7735, F1 Macro: 0.6714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.69      0.68      0.69       402\n",
      "  HS_Religion       0.75      0.51      0.61       157\n",
      "      HS_Race       0.77      0.72      0.74       120\n",
      "  HS_Physical       0.67      0.08      0.15        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.61      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 179.55484580993652 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4074, Accuracy: 0.8817, F1 Micro: 0.5812, F1 Macro: 0.2817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2829, Accuracy: 0.8986, F1 Micro: 0.6925, F1 Macro: 0.4335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.9087, F1 Micro: 0.7191, F1 Macro: 0.5263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9143, F1 Micro: 0.7416, F1 Macro: 0.5633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1677, Accuracy: 0.9173, F1 Micro: 0.7579, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1396, Accuracy: 0.922, F1 Micro: 0.7646, F1 Macro: 0.627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9223, F1 Micro: 0.7713, F1 Macro: 0.6612\n",
      "Epoch 8/10, Train Loss: 0.101, Accuracy: 0.9233, F1 Micro: 0.767, F1 Macro: 0.6604\n",
      "Epoch 9/10, Train Loss: 0.0876, Accuracy: 0.9222, F1 Micro: 0.7694, F1 Macro: 0.6748\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9218, F1 Micro: 0.7648, F1 Macro: 0.6661\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9223, F1 Micro: 0.7713, F1 Macro: 0.6612\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.70      0.69      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.69      0.75      0.72       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.79      0.75      0.77       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.62      0.61      0.61       331\n",
      "    HS_Strong       0.90      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.64      0.66      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 174.1969780921936 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9221, F1 Micro: 0.7737, F1 Macro: 0.6708\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 907.8829534129962\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 204.6849410533905 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.8811, F1 Micro: 0.6009, F1 Macro: 0.2988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.9014, F1 Micro: 0.6967, F1 Macro: 0.484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.912, F1 Micro: 0.7378, F1 Macro: 0.5628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9168, F1 Micro: 0.755, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.92, F1 Micro: 0.7615, F1 Macro: 0.6144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9205, F1 Micro: 0.7708, F1 Macro: 0.6379\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.9184, F1 Micro: 0.7602, F1 Macro: 0.6271\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9231, F1 Micro: 0.7706, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.9234, F1 Micro: 0.7748, F1 Macro: 0.6886\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9213, F1 Micro: 0.7686, F1 Macro: 0.6705\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9234, F1 Micro: 0.7748, F1 Macro: 0.6886\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.73      0.68      0.70       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.62       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 174.088858127594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4129, Accuracy: 0.88, F1 Micro: 0.5761, F1 Macro: 0.2684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2889, Accuracy: 0.8985, F1 Micro: 0.6781, F1 Macro: 0.4224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.9085, F1 Micro: 0.7282, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2013, Accuracy: 0.9141, F1 Micro: 0.748, F1 Macro: 0.5736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.166, Accuracy: 0.9182, F1 Micro: 0.7536, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9198, F1 Micro: 0.7652, F1 Macro: 0.6241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1156, Accuracy: 0.9188, F1 Micro: 0.7653, F1 Macro: 0.63\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.9218, F1 Micro: 0.7594, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0876, Accuracy: 0.9233, F1 Micro: 0.7704, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9228, F1 Micro: 0.7708, F1 Macro: 0.6715\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9228, F1 Micro: 0.7708, F1 Macro: 0.6715\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.75      0.68      0.71       732\n",
      "     HS_Group       0.70      0.69      0.69       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.81      0.63      0.71       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.49      0.35      0.41        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.74      0.65      0.69       689\n",
      "  HS_Moderate       0.62      0.62      0.62       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 178.13490343093872 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4044, Accuracy: 0.8815, F1 Micro: 0.6131, F1 Macro: 0.3094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2837, Accuracy: 0.9005, F1 Micro: 0.6929, F1 Macro: 0.4826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2364, Accuracy: 0.9091, F1 Micro: 0.7307, F1 Macro: 0.5427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2015, Accuracy: 0.9138, F1 Micro: 0.7497, F1 Macro: 0.5743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9205, F1 Micro: 0.7569, F1 Macro: 0.5974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.921, F1 Micro: 0.7666, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9225, F1 Micro: 0.7668, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.9228, F1 Micro: 0.7695, F1 Macro: 0.6607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9222, F1 Micro: 0.7696, F1 Macro: 0.6729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0728, Accuracy: 0.9196, F1 Micro: 0.7704, F1 Macro: 0.6685\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9196, F1 Micro: 0.7704, F1 Macro: 0.6685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.84      0.94      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.71      0.57      0.63       157\n",
      "      HS_Race       0.78      0.65      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.57      0.41      0.48        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.90      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 180.14188861846924 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9219, F1 Micro: 0.772, F1 Macro: 0.6762\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 500.02913309468164\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 186.90974831581116 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8815, F1 Micro: 0.5592, F1 Macro: 0.2704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2782, Accuracy: 0.9042, F1 Micro: 0.6926, F1 Macro: 0.4815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2285, Accuracy: 0.913, F1 Micro: 0.7251, F1 Macro: 0.5384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9178, F1 Micro: 0.7503, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.9227, F1 Micro: 0.7688, F1 Macro: 0.6167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.9205, F1 Micro: 0.7707, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.921, F1 Micro: 0.771, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9226, F1 Micro: 0.7726, F1 Macro: 0.6587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9231, F1 Micro: 0.7729, F1 Macro: 0.6716\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9228, F1 Micro: 0.7694, F1 Macro: 0.6712\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9231, F1 Micro: 0.7729, F1 Macro: 0.6716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.73      0.61      0.67       402\n",
      "  HS_Religion       0.78      0.54      0.64       157\n",
      "      HS_Race       0.83      0.61      0.70       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.68      0.33      0.45        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 183.24101305007935 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4119, Accuracy: 0.8786, F1 Micro: 0.5444, F1 Macro: 0.2489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2864, Accuracy: 0.9007, F1 Micro: 0.6786, F1 Macro: 0.4238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.911, F1 Micro: 0.7252, F1 Macro: 0.5368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.191, Accuracy: 0.9147, F1 Micro: 0.7428, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9204, F1 Micro: 0.7604, F1 Macro: 0.6123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1385, Accuracy: 0.9199, F1 Micro: 0.7644, F1 Macro: 0.6385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9213, F1 Micro: 0.7659, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.097, Accuracy: 0.9233, F1 Micro: 0.775, F1 Macro: 0.6515\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9189, F1 Micro: 0.7631, F1 Macro: 0.656\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9231, F1 Micro: 0.7714, F1 Macro: 0.6755\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9233, F1 Micro: 0.775, F1 Macro: 0.6515\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.68      0.71      0.70       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.77      0.20      0.31        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.61      0.64      0.63       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.70      0.63      0.65      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 181.62935376167297 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4051, Accuracy: 0.8819, F1 Micro: 0.5575, F1 Macro: 0.2699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2827, Accuracy: 0.9026, F1 Micro: 0.6874, F1 Macro: 0.4722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.9121, F1 Micro: 0.7332, F1 Macro: 0.5391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9167, F1 Micro: 0.7519, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.9219, F1 Micro: 0.7645, F1 Macro: 0.612\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9153, F1 Micro: 0.7592, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.114, Accuracy: 0.9175, F1 Micro: 0.7661, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.9208, F1 Micro: 0.7722, F1 Macro: 0.6487\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9186, F1 Micro: 0.769, F1 Macro: 0.6659\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9228, F1 Micro: 0.7667, F1 Macro: 0.6691\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9208, F1 Micro: 0.7722, F1 Macro: 0.6487\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.80      0.75       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.74      0.57      0.64       157\n",
      "      HS_Race       0.71      0.66      0.68       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.72      0.25      0.38        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.78      0.73       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.87      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.63      0.65      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 179.96599650382996 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9224, F1 Micro: 0.7733, F1 Macro: 0.6573\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1123.8997679014653\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 167.2003207206726 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3983, Accuracy: 0.883, F1 Micro: 0.5996, F1 Macro: 0.3021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9059, F1 Micro: 0.6959, F1 Macro: 0.506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9143, F1 Micro: 0.739, F1 Macro: 0.5561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1896, Accuracy: 0.913, F1 Micro: 0.7544, F1 Macro: 0.5918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9206, F1 Micro: 0.7669, F1 Macro: 0.6216\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9198, F1 Micro: 0.7654, F1 Macro: 0.6429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9197, F1 Micro: 0.7729, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9244, F1 Micro: 0.7768, F1 Macro: 0.6773\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9237, F1 Micro: 0.773, F1 Macro: 0.6835\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.922, F1 Micro: 0.7688, F1 Macro: 0.6782\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9244, F1 Micro: 0.7768, F1 Macro: 0.6773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.75      0.77      0.76       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.72      0.72       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 183.69408082962036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.415, Accuracy: 0.8825, F1 Micro: 0.5895, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.9012, F1 Micro: 0.6667, F1 Macro: 0.4343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.9123, F1 Micro: 0.7228, F1 Macro: 0.5059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1936, Accuracy: 0.9085, F1 Micro: 0.7472, F1 Macro: 0.5882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9203, F1 Micro: 0.759, F1 Macro: 0.6094\n",
      "Epoch 6/10, Train Loss: 0.1311, Accuracy: 0.9195, F1 Micro: 0.7541, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1161, Accuracy: 0.9211, F1 Micro: 0.764, F1 Macro: 0.6311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9231, F1 Micro: 0.7733, F1 Macro: 0.662\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9225, F1 Micro: 0.7697, F1 Macro: 0.6728\n",
      "Epoch 10/10, Train Loss: 0.0723, Accuracy: 0.9219, F1 Micro: 0.7696, F1 Macro: 0.6653\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9231, F1 Micro: 0.7733, F1 Macro: 0.662\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.62      0.66       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.73      0.64      0.66      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 183.92978882789612 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4089, Accuracy: 0.8834, F1 Micro: 0.5824, F1 Macro: 0.2835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9037, F1 Micro: 0.6862, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.227, Accuracy: 0.9105, F1 Micro: 0.7327, F1 Macro: 0.5342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1936, Accuracy: 0.9129, F1 Micro: 0.7544, F1 Macro: 0.5896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9224, F1 Micro: 0.7678, F1 Macro: 0.6201\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9214, F1 Micro: 0.7646, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.921, F1 Micro: 0.771, F1 Macro: 0.641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9249, F1 Micro: 0.7723, F1 Macro: 0.6615\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9209, F1 Micro: 0.7717, F1 Macro: 0.674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9221, F1 Micro: 0.7731, F1 Macro: 0.6774\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9221, F1 Micro: 0.7731, F1 Macro: 0.6774\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.56      0.45      0.50        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 185.63173055648804 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9232, F1 Micro: 0.7744, F1 Macro: 0.6723\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 812.5273989045409\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 151.41556477546692 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3928, Accuracy: 0.8832, F1 Micro: 0.6077, F1 Macro: 0.3039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.8997, F1 Micro: 0.7148, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.9148, F1 Micro: 0.7444, F1 Macro: 0.5759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1893, Accuracy: 0.9174, F1 Micro: 0.7578, F1 Macro: 0.5984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9191, F1 Micro: 0.7654, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.9237, F1 Micro: 0.7664, F1 Macro: 0.6639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9211, F1 Micro: 0.769, F1 Macro: 0.6466\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9161, F1 Micro: 0.7677, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9218, F1 Micro: 0.7759, F1 Macro: 0.6855\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9217, F1 Micro: 0.7716, F1 Macro: 0.672\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9218, F1 Micro: 0.7759, F1 Macro: 0.6855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.68      0.66      0.67       157\n",
      "      HS_Race       0.76      0.66      0.71       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 189.77703428268433 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4078, Accuracy: 0.8819, F1 Micro: 0.5825, F1 Macro: 0.2862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2792, Accuracy: 0.8989, F1 Micro: 0.7062, F1 Macro: 0.4733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.912, F1 Micro: 0.7356, F1 Macro: 0.5583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1937, Accuracy: 0.9177, F1 Micro: 0.748, F1 Macro: 0.571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9183, F1 Micro: 0.757, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.9237, F1 Micro: 0.7695, F1 Macro: 0.653\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9216, F1 Micro: 0.7664, F1 Macro: 0.6445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9197, F1 Micro: 0.7714, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.082, Accuracy: 0.9231, F1 Micro: 0.7779, F1 Macro: 0.6744\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.9218, F1 Micro: 0.7724, F1 Macro: 0.6587\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9231, F1 Micro: 0.7779, F1 Macro: 0.6744\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.70      0.62      0.66       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.64      0.10      0.17        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.73       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 189.3251383304596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4021, Accuracy: 0.8818, F1 Micro: 0.5975, F1 Macro: 0.2905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9014, F1 Micro: 0.7158, F1 Macro: 0.5116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.9138, F1 Micro: 0.737, F1 Macro: 0.5624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9172, F1 Micro: 0.7533, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9192, F1 Micro: 0.7661, F1 Macro: 0.6199\n",
      "Epoch 6/10, Train Loss: 0.1313, Accuracy: 0.9239, F1 Micro: 0.7601, F1 Macro: 0.6473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.9227, F1 Micro: 0.7716, F1 Macro: 0.6485\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9191, F1 Micro: 0.7709, F1 Macro: 0.6533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9235, F1 Micro: 0.7818, F1 Macro: 0.6906\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9209, F1 Micro: 0.7716, F1 Macro: 0.6754\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9235, F1 Micro: 0.7818, F1 Macro: 0.6906\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.70      0.70      0.70       402\n",
      "  HS_Religion       0.69      0.64      0.67       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.75      0.73       689\n",
      "  HS_Moderate       0.60      0.63      0.61       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.77      0.80      0.78      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 187.250235080719 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9228, F1 Micro: 0.7785, F1 Macro: 0.6835\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 775.793518781818\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 135.8334505558014 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3882, Accuracy: 0.8843, F1 Micro: 0.6072, F1 Macro: 0.3168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.9042, F1 Micro: 0.7169, F1 Macro: 0.5294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2236, Accuracy: 0.9146, F1 Micro: 0.7375, F1 Macro: 0.5497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1872, Accuracy: 0.9191, F1 Micro: 0.7548, F1 Macro: 0.5919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9214, F1 Micro: 0.7665, F1 Macro: 0.6166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9222, F1 Micro: 0.7693, F1 Macro: 0.6435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9212, F1 Micro: 0.7776, F1 Macro: 0.6798\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9204, F1 Micro: 0.7734, F1 Macro: 0.6622\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.9216, F1 Micro: 0.7742, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0723, Accuracy: 0.9245, F1 Micro: 0.7799, F1 Macro: 0.6908\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9245, F1 Micro: 0.7799, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.78      0.75       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.74      0.57      0.65       157\n",
      "      HS_Race       0.73      0.78      0.75       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 191.28459787368774 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.405, Accuracy: 0.8819, F1 Micro: 0.5865, F1 Macro: 0.2813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.901, F1 Micro: 0.69, F1 Macro: 0.4731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2298, Accuracy: 0.9118, F1 Micro: 0.7236, F1 Macro: 0.5257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9183, F1 Micro: 0.7518, F1 Macro: 0.5718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9201, F1 Micro: 0.7549, F1 Macro: 0.6037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9227, F1 Micro: 0.7688, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1139, Accuracy: 0.9229, F1 Micro: 0.7786, F1 Macro: 0.679\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9229, F1 Micro: 0.7733, F1 Macro: 0.6486\n",
      "Epoch 9/10, Train Loss: 0.0832, Accuracy: 0.9197, F1 Micro: 0.7615, F1 Macro: 0.6715\n",
      "Epoch 10/10, Train Loss: 0.0748, Accuracy: 0.9226, F1 Micro: 0.7713, F1 Macro: 0.6734\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9229, F1 Micro: 0.7786, F1 Macro: 0.679\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.83      0.93      0.88       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.76      0.65      0.70       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.85      0.68      0.75       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.57      0.41      0.48        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 190.3876121044159 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3975, Accuracy: 0.8832, F1 Micro: 0.5866, F1 Macro: 0.2903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2743, Accuracy: 0.9032, F1 Micro: 0.7108, F1 Macro: 0.4994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2254, Accuracy: 0.9132, F1 Micro: 0.7391, F1 Macro: 0.5523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9174, F1 Micro: 0.7547, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9236, F1 Micro: 0.7653, F1 Macro: 0.6201\n",
      "Epoch 6/10, Train Loss: 0.1325, Accuracy: 0.9145, F1 Micro: 0.7619, F1 Macro: 0.6344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9225, F1 Micro: 0.7788, F1 Macro: 0.6693\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9233, F1 Micro: 0.7741, F1 Macro: 0.6528\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9211, F1 Micro: 0.7732, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9181, F1 Micro: 0.7711, F1 Macro: 0.6901\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9225, F1 Micro: 0.7788, F1 Macro: 0.6693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.70      0.69      0.69       402\n",
      "  HS_Religion       0.69      0.64      0.67       157\n",
      "      HS_Race       0.71      0.69      0.70       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.62      0.63      0.62       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 188.04635167121887 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9233, F1 Micro: 0.7791, F1 Macro: 0.6797\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 568.1352669492695\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 123.36039710044861 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3891, Accuracy: 0.8835, F1 Micro: 0.6172, F1 Macro: 0.3109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9031, F1 Micro: 0.667, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.915, F1 Micro: 0.7322, F1 Macro: 0.5658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.184, Accuracy: 0.9182, F1 Micro: 0.7575, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1528, Accuracy: 0.9226, F1 Micro: 0.7692, F1 Macro: 0.642\n",
      "Epoch 6/10, Train Loss: 0.1335, Accuracy: 0.9154, F1 Micro: 0.7647, F1 Macro: 0.6649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9222, F1 Micro: 0.7727, F1 Macro: 0.6682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9248, F1 Micro: 0.775, F1 Macro: 0.679\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9225, F1 Micro: 0.7712, F1 Macro: 0.6857\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9201, F1 Micro: 0.7707, F1 Macro: 0.697\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9248, F1 Micro: 0.775, F1 Macro: 0.679\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.78      0.58      0.67       402\n",
      "  HS_Religion       0.79      0.55      0.65       157\n",
      "      HS_Race       0.81      0.67      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.79      0.63      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 194.5818920135498 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4056, Accuracy: 0.8814, F1 Micro: 0.5995, F1 Macro: 0.2881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.8987, F1 Micro: 0.6527, F1 Macro: 0.3813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2283, Accuracy: 0.9129, F1 Micro: 0.7271, F1 Macro: 0.5459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9167, F1 Micro: 0.7516, F1 Macro: 0.5966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1544, Accuracy: 0.9214, F1 Micro: 0.7652, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1345, Accuracy: 0.9203, F1 Micro: 0.7682, F1 Macro: 0.6423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9244, F1 Micro: 0.7725, F1 Macro: 0.6569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9236, F1 Micro: 0.7757, F1 Macro: 0.6662\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.9225, F1 Micro: 0.7732, F1 Macro: 0.6764\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9209, F1 Micro: 0.7722, F1 Macro: 0.6799\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9236, F1 Micro: 0.7757, F1 Macro: 0.6662\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.82      0.67      0.73       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 195.48807835578918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.399, Accuracy: 0.8822, F1 Micro: 0.6008, F1 Macro: 0.2912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2742, Accuracy: 0.9042, F1 Micro: 0.6839, F1 Macro: 0.4562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2262, Accuracy: 0.9125, F1 Micro: 0.7196, F1 Macro: 0.548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.9173, F1 Micro: 0.7578, F1 Macro: 0.6006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1531, Accuracy: 0.9212, F1 Micro: 0.7692, F1 Macro: 0.6434\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.9138, F1 Micro: 0.764, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.925, F1 Micro: 0.7784, F1 Macro: 0.6588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9269, F1 Micro: 0.7817, F1 Macro: 0.681\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.9239, F1 Micro: 0.7747, F1 Macro: 0.6759\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9222, F1 Micro: 0.7763, F1 Macro: 0.6949\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9269, F1 Micro: 0.7817, F1 Macro: 0.681\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.75      0.75       732\n",
      "     HS_Group       0.77      0.61      0.68       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.81      0.69      0.74       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.73      0.37      0.49        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.73      0.72       689\n",
      "  HS_Moderate       0.70      0.54      0.61       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5556\n",
      "    macro avg       0.80      0.64      0.68      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.71988821029663 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9251, F1 Micro: 0.7775, F1 Macro: 0.6754\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1169.1263470267863\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 108.16773533821106 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3845, Accuracy: 0.8846, F1 Micro: 0.5932, F1 Macro: 0.2917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2656, Accuracy: 0.9037, F1 Micro: 0.7053, F1 Macro: 0.5074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9163, F1 Micro: 0.7438, F1 Macro: 0.5812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9201, F1 Micro: 0.7572, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9226, F1 Micro: 0.7654, F1 Macro: 0.6309\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9225, F1 Micro: 0.7603, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.924, F1 Micro: 0.7701, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.922, F1 Micro: 0.7733, F1 Macro: 0.679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.923, F1 Micro: 0.7753, F1 Macro: 0.6827\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9255, F1 Micro: 0.7752, F1 Macro: 0.6903\n",
      "Model 1 - Iteration 9216: Accuracy: 0.923, F1 Micro: 0.7753, F1 Macro: 0.6827\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.74      0.58      0.65       402\n",
      "  HS_Religion       0.80      0.54      0.65       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.79      0.15      0.26        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.3086018562317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3987, Accuracy: 0.8815, F1 Micro: 0.5718, F1 Macro: 0.2714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2726, Accuracy: 0.9012, F1 Micro: 0.6912, F1 Macro: 0.4743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2282, Accuracy: 0.9133, F1 Micro: 0.7345, F1 Macro: 0.5554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9187, F1 Micro: 0.7487, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9188, F1 Micro: 0.7667, F1 Macro: 0.6469\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9204, F1 Micro: 0.7561, F1 Macro: 0.6329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1104, Accuracy: 0.9247, F1 Micro: 0.774, F1 Macro: 0.6545\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.9204, F1 Micro: 0.7677, F1 Macro: 0.664\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9223, F1 Micro: 0.7716, F1 Macro: 0.6686\n",
      "Epoch 10/10, Train Loss: 0.0692, Accuracy: 0.9234, F1 Micro: 0.7724, F1 Macro: 0.6658\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9247, F1 Micro: 0.774, F1 Macro: 0.6545\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.80      0.54      0.64       157\n",
      "      HS_Race       0.84      0.64      0.73       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.65      0.22      0.32        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.61      0.65      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 195.1046359539032 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3949, Accuracy: 0.8845, F1 Micro: 0.5898, F1 Macro: 0.2861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.9035, F1 Micro: 0.7048, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.914, F1 Micro: 0.744, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1849, Accuracy: 0.92, F1 Micro: 0.7553, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.9201, F1 Micro: 0.7639, F1 Macro: 0.6288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.9233, F1 Micro: 0.7654, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9231, F1 Micro: 0.7676, F1 Macro: 0.6491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9189, F1 Micro: 0.7701, F1 Macro: 0.6677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.922, F1 Micro: 0.7718, F1 Macro: 0.6762\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9222, F1 Micro: 0.7702, F1 Macro: 0.678\n",
      "Model 3 - Iteration 9216: Accuracy: 0.922, F1 Micro: 0.7718, F1 Macro: 0.6762\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.77      0.65      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.75      0.35      0.48        51\n",
      "     HS_Other       0.74      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 200.52459192276 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9232, F1 Micro: 0.7737, F1 Macro: 0.6711\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 651.1915029315743\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 95.08323860168457 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3864, Accuracy: 0.8822, F1 Micro: 0.6516, F1 Macro: 0.3434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.9061, F1 Micro: 0.7066, F1 Macro: 0.5047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2212, Accuracy: 0.9131, F1 Micro: 0.7429, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9189, F1 Micro: 0.7547, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.9218, F1 Micro: 0.7599, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1269, Accuracy: 0.9216, F1 Micro: 0.7752, F1 Macro: 0.6744\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9242, F1 Micro: 0.7703, F1 Macro: 0.6577\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9235, F1 Micro: 0.7715, F1 Macro: 0.6773\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9204, F1 Micro: 0.773, F1 Macro: 0.6799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9222, F1 Micro: 0.7781, F1 Macro: 0.6872\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9222, F1 Micro: 0.7781, F1 Macro: 0.6872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.94      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.69      0.71      0.70       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       1.00      0.17      0.29        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.74      0.84      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.62      0.64      0.63       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 198.37438201904297 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4013, Accuracy: 0.8831, F1 Micro: 0.6271, F1 Macro: 0.3043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2739, Accuracy: 0.9023, F1 Micro: 0.6738, F1 Macro: 0.4297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2263, Accuracy: 0.9124, F1 Micro: 0.7349, F1 Macro: 0.5497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1861, Accuracy: 0.9185, F1 Micro: 0.7568, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1596, Accuracy: 0.9215, F1 Micro: 0.7659, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9226, F1 Micro: 0.7753, F1 Macro: 0.6605\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9233, F1 Micro: 0.7664, F1 Macro: 0.6467\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.923, F1 Micro: 0.7707, F1 Macro: 0.6661\n",
      "Epoch 9/10, Train Loss: 0.0826, Accuracy: 0.9218, F1 Micro: 0.7695, F1 Macro: 0.662\n",
      "Epoch 10/10, Train Loss: 0.0698, Accuracy: 0.9229, F1 Micro: 0.7724, F1 Macro: 0.6744\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9226, F1 Micro: 0.7753, F1 Macro: 0.6605\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.72      0.78      0.75       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.84      0.62      0.71       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.48      0.22      0.30        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.43982100486755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.395, Accuracy: 0.8847, F1 Micro: 0.6245, F1 Macro: 0.3099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.9058, F1 Micro: 0.6914, F1 Macro: 0.4824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.9125, F1 Micro: 0.7424, F1 Macro: 0.5585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1816, Accuracy: 0.9181, F1 Micro: 0.7605, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1606, Accuracy: 0.9209, F1 Micro: 0.7622, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9235, F1 Micro: 0.7721, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9259, F1 Micro: 0.7753, F1 Macro: 0.6771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9237, F1 Micro: 0.7765, F1 Macro: 0.6807\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9203, F1 Micro: 0.7717, F1 Macro: 0.6755\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9243, F1 Micro: 0.7745, F1 Macro: 0.6834\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9237, F1 Micro: 0.7765, F1 Macro: 0.6807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.71      0.60      0.65       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.63      0.63       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 200.65738892555237 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9228, F1 Micro: 0.7767, F1 Macro: 0.6762\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 836.0094916868775\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 97.26594352722168 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3821, Accuracy: 0.8844, F1 Micro: 0.5725, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2695, Accuracy: 0.9068, F1 Micro: 0.7086, F1 Macro: 0.5058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.9125, F1 Micro: 0.7482, F1 Macro: 0.5765\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.92, F1 Micro: 0.7436, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1515, Accuracy: 0.9224, F1 Micro: 0.7736, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9246, F1 Micro: 0.7771, F1 Macro: 0.6701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.9249, F1 Micro: 0.7795, F1 Macro: 0.6747\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9227, F1 Micro: 0.7748, F1 Macro: 0.6816\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9246, F1 Micro: 0.7788, F1 Macro: 0.6931\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9239, F1 Micro: 0.773, F1 Macro: 0.6866\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9249, F1 Micro: 0.7795, F1 Macro: 0.6747\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.78      0.56      0.65       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.66      0.56      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 200.0450358390808 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3972, Accuracy: 0.8817, F1 Micro: 0.571, F1 Macro: 0.2785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9044, F1 Micro: 0.697, F1 Macro: 0.4592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2236, Accuracy: 0.9128, F1 Micro: 0.7424, F1 Macro: 0.5571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9188, F1 Micro: 0.7434, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9197, F1 Micro: 0.7696, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1303, Accuracy: 0.9208, F1 Micro: 0.7706, F1 Macro: 0.6486\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9238, F1 Micro: 0.7703, F1 Macro: 0.6476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9215, F1 Micro: 0.7754, F1 Macro: 0.6695\n",
      "Epoch 9/10, Train Loss: 0.0821, Accuracy: 0.9229, F1 Micro: 0.7689, F1 Macro: 0.6716\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9228, F1 Micro: 0.7749, F1 Macro: 0.6787\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9215, F1 Micro: 0.7754, F1 Macro: 0.6695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.64      0.70      0.67       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.55      0.31      0.40        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.59      0.64      0.61       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 200.73160338401794 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3908, Accuracy: 0.8823, F1 Micro: 0.5543, F1 Macro: 0.2734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2722, Accuracy: 0.9074, F1 Micro: 0.7076, F1 Macro: 0.5081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9125, F1 Micro: 0.7508, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9194, F1 Micro: 0.7516, F1 Macro: 0.6078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.9184, F1 Micro: 0.7691, F1 Macro: 0.6277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9238, F1 Micro: 0.7699, F1 Macro: 0.6437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9239, F1 Micro: 0.7772, F1 Macro: 0.6648\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.908, F1 Micro: 0.7562, F1 Macro: 0.6654\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.9235, F1 Micro: 0.7756, F1 Macro: 0.6982\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9241, F1 Micro: 0.7747, F1 Macro: 0.6884\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9239, F1 Micro: 0.7772, F1 Macro: 0.6648\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.76      0.70      0.73       732\n",
      "     HS_Group       0.66      0.73      0.69       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.77      0.72      0.75       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.72      0.25      0.38        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.60      0.66      0.63       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.65      0.66      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 200.18335843086243 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9234, F1 Micro: 0.7774, F1 Macro: 0.6697\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 321.0965735392588\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 79.53537940979004 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3806, Accuracy: 0.8878, F1 Micro: 0.6104, F1 Macro: 0.3485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2559, Accuracy: 0.9091, F1 Micro: 0.7144, F1 Macro: 0.5273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9135, F1 Micro: 0.7491, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1753, Accuracy: 0.9193, F1 Micro: 0.7653, F1 Macro: 0.6157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.9246, F1 Micro: 0.7712, F1 Macro: 0.6558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9213, F1 Micro: 0.7746, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9247, F1 Micro: 0.7782, F1 Macro: 0.6799\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.9237, F1 Micro: 0.7738, F1 Macro: 0.6717\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.9237, F1 Micro: 0.7779, F1 Macro: 0.6814\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9234, F1 Micro: 0.7738, F1 Macro: 0.6873\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9247, F1 Micro: 0.7782, F1 Macro: 0.6799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.64373993873596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3953, Accuracy: 0.8807, F1 Micro: 0.5557, F1 Macro: 0.2791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.9044, F1 Micro: 0.7031, F1 Macro: 0.4835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9104, F1 Micro: 0.7417, F1 Macro: 0.5677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1827, Accuracy: 0.9194, F1 Micro: 0.7638, F1 Macro: 0.6132\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9227, F1 Micro: 0.7589, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1282, Accuracy: 0.9229, F1 Micro: 0.768, F1 Macro: 0.6451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9236, F1 Micro: 0.7703, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9228, F1 Micro: 0.7755, F1 Macro: 0.6618\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9243, F1 Micro: 0.7734, F1 Macro: 0.6644\n",
      "Epoch 10/10, Train Loss: 0.0693, Accuracy: 0.9218, F1 Micro: 0.7728, F1 Macro: 0.6864\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9228, F1 Micro: 0.7755, F1 Macro: 0.6618\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.76      0.54      0.63       157\n",
      "      HS_Race       0.74      0.67      0.70       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.63      0.24      0.34        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.73       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.63      0.66      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 204.20023608207703 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8845, F1 Micro: 0.5816, F1 Macro: 0.294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.908, F1 Micro: 0.7122, F1 Macro: 0.4933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9123, F1 Micro: 0.7469, F1 Macro: 0.5663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9176, F1 Micro: 0.7654, F1 Macro: 0.6103\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9223, F1 Micro: 0.7555, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.6596\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.922, F1 Micro: 0.7733, F1 Macro: 0.6743\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.9219, F1 Micro: 0.7729, F1 Macro: 0.6622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0792, Accuracy: 0.9246, F1 Micro: 0.7788, F1 Macro: 0.6908\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9244, F1 Micro: 0.7768, F1 Macro: 0.6982\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9246, F1 Micro: 0.7788, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.77      0.57      0.66       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.6362600326538 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.924, F1 Micro: 0.7775, F1 Macro: 0.6775\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 677.9680905644947\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 66.5665352344513 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8891, F1 Micro: 0.6351, F1 Macro: 0.343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.907, F1 Micro: 0.7033, F1 Macro: 0.507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2113, Accuracy: 0.9178, F1 Micro: 0.7566, F1 Macro: 0.6\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9229, F1 Micro: 0.7589, F1 Macro: 0.6104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.157, Accuracy: 0.9245, F1 Micro: 0.77, F1 Macro: 0.6467\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9243, F1 Micro: 0.7679, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9247, F1 Micro: 0.7829, F1 Macro: 0.6807\n",
      "Epoch 8/10, Train Loss: 0.0884, Accuracy: 0.9247, F1 Micro: 0.7763, F1 Macro: 0.6901\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9246, F1 Micro: 0.7773, F1 Macro: 0.6965\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9238, F1 Micro: 0.7795, F1 Macro: 0.7005\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9247, F1 Micro: 0.7829, F1 Macro: 0.6807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.79      0.75       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.79      0.60      0.68       157\n",
      "      HS_Race       0.75      0.67      0.71       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.68      0.37      0.48        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.69      0.78      0.73       689\n",
      "  HS_Moderate       0.65      0.55      0.60       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 205.48540687561035 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3922, Accuracy: 0.8859, F1 Micro: 0.6107, F1 Macro: 0.3141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2632, Accuracy: 0.9036, F1 Micro: 0.6906, F1 Macro: 0.4861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2179, Accuracy: 0.9152, F1 Micro: 0.7484, F1 Macro: 0.5702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.9215, F1 Micro: 0.7592, F1 Macro: 0.6198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.9222, F1 Micro: 0.765, F1 Macro: 0.6335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9226, F1 Micro: 0.7704, F1 Macro: 0.6551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.6652\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9225, F1 Micro: 0.7711, F1 Macro: 0.6825\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9225, F1 Micro: 0.77, F1 Macro: 0.677\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.922, F1 Micro: 0.7724, F1 Macro: 0.6829\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.6652\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.71      0.58      0.64       157\n",
      "      HS_Race       0.82      0.63      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.58      0.29      0.39        51\n",
      "     HS_Other       0.75      0.85      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.62      0.56      0.58       331\n",
      "    HS_Strong       0.91      0.75      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 206.39663457870483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.8886, F1 Micro: 0.6384, F1 Macro: 0.3461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.907, F1 Micro: 0.7057, F1 Macro: 0.4905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9153, F1 Micro: 0.7516, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9221, F1 Micro: 0.7634, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.9247, F1 Micro: 0.7742, F1 Macro: 0.6473\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9253, F1 Micro: 0.7678, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1076, Accuracy: 0.9248, F1 Micro: 0.7767, F1 Macro: 0.665\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9222, F1 Micro: 0.7762, F1 Macro: 0.6943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9256, F1 Micro: 0.7794, F1 Macro: 0.708\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9249, F1 Micro: 0.7732, F1 Macro: 0.7011\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9256, F1 Micro: 0.7794, F1 Macro: 0.708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.77      0.63      0.69       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.58      0.51      0.54        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 206.88837504386902 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6846\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 492.3308778980173\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 52.070767879486084 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3773, Accuracy: 0.8882, F1 Micro: 0.6407, F1 Macro: 0.3663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.9082, F1 Micro: 0.6965, F1 Macro: 0.5101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9173, F1 Micro: 0.75, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9178, F1 Micro: 0.7561, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9219, F1 Micro: 0.7735, F1 Macro: 0.6571\n",
      "Epoch 6/10, Train Loss: 0.1246, Accuracy: 0.9214, F1 Micro: 0.7733, F1 Macro: 0.6384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9216, F1 Micro: 0.7759, F1 Macro: 0.6762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9263, F1 Micro: 0.7766, F1 Macro: 0.6749\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.9215, F1 Micro: 0.773, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9216, F1 Micro: 0.7782, F1 Macro: 0.7014\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9216, F1 Micro: 0.7782, F1 Macro: 0.7014\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.68      0.79      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.76      0.75      0.75       120\n",
      "  HS_Physical       0.94      0.24      0.38        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.67      0.77      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.76      0.80      0.77      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 214.03554439544678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3918, Accuracy: 0.8869, F1 Micro: 0.6327, F1 Macro: 0.326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2643, Accuracy: 0.9022, F1 Micro: 0.6609, F1 Macro: 0.4491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9162, F1 Micro: 0.7392, F1 Macro: 0.5641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1783, Accuracy: 0.9186, F1 Micro: 0.757, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1535, Accuracy: 0.9213, F1 Micro: 0.7682, F1 Macro: 0.6435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.9238, F1 Micro: 0.7733, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.924, F1 Micro: 0.7756, F1 Macro: 0.673\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9219, F1 Micro: 0.7575, F1 Macro: 0.6535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9219, F1 Micro: 0.7775, F1 Macro: 0.6825\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9228, F1 Micro: 0.7736, F1 Macro: 0.6894\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9219, F1 Micro: 0.7775, F1 Macro: 0.6825\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.55      0.43      0.48        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.75      0.67      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 212.77659940719604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3849, Accuracy: 0.8864, F1 Micro: 0.6476, F1 Macro: 0.3513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.9071, F1 Micro: 0.6918, F1 Macro: 0.4897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2136, Accuracy: 0.9174, F1 Micro: 0.7477, F1 Macro: 0.576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9198, F1 Micro: 0.7651, F1 Macro: 0.6159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1524, Accuracy: 0.9227, F1 Micro: 0.7692, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9245, F1 Micro: 0.7696, F1 Macro: 0.6415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9259, F1 Micro: 0.7767, F1 Macro: 0.6761\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9246, F1 Micro: 0.7691, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9245, F1 Micro: 0.7782, F1 Macro: 0.6876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9237, F1 Micro: 0.7786, F1 Macro: 0.704\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9237, F1 Micro: 0.7786, F1 Macro: 0.704\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.93      0.91       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.68      0.75      0.72       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.74      0.76      0.75       120\n",
      "  HS_Physical       1.00      0.21      0.34        72\n",
      "    HS_Gender       0.57      0.51      0.54        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.59      0.68      0.63       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.69      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 215.39207196235657 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9224, F1 Micro: 0.7781, F1 Macro: 0.696\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 192.8873874199269\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 39.904847145080566 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3728, Accuracy: 0.8904, F1 Micro: 0.6418, F1 Macro: 0.3476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.252, Accuracy: 0.9085, F1 Micro: 0.7186, F1 Macro: 0.5197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9129, F1 Micro: 0.7497, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.9167, F1 Micro: 0.7599, F1 Macro: 0.6088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.92, F1 Micro: 0.7704, F1 Macro: 0.6291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1238, Accuracy: 0.9218, F1 Micro: 0.7768, F1 Macro: 0.6724\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9254, F1 Micro: 0.775, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9266, F1 Micro: 0.7796, F1 Macro: 0.6892\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.9231, F1 Micro: 0.7789, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.925, F1 Micro: 0.7797, F1 Macro: 0.707\n",
      "Model 1 - Iteration 10218: Accuracy: 0.925, F1 Micro: 0.7797, F1 Macro: 0.707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.76      0.71      0.73       732\n",
      "     HS_Group       0.69      0.71      0.70       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.73      0.75      0.74       120\n",
      "  HS_Physical       0.88      0.31      0.45        72\n",
      "    HS_Gender       0.51      0.43      0.47        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.75      0.69      0.72       689\n",
      "  HS_Moderate       0.61      0.63      0.62       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 216.92906665802002 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3878, Accuracy: 0.8875, F1 Micro: 0.6326, F1 Macro: 0.3291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9054, F1 Micro: 0.7005, F1 Macro: 0.4711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9109, F1 Micro: 0.7403, F1 Macro: 0.5732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9169, F1 Micro: 0.7526, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.922, F1 Micro: 0.7723, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.921, F1 Micro: 0.7739, F1 Macro: 0.6581\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9232, F1 Micro: 0.7656, F1 Macro: 0.6519\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9218, F1 Micro: 0.7697, F1 Macro: 0.6681\n",
      "Epoch 9/10, Train Loss: 0.0797, Accuracy: 0.9209, F1 Micro: 0.7726, F1 Macro: 0.6676\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9238, F1 Micro: 0.7732, F1 Macro: 0.6923\n",
      "Model 2 - Iteration 10218: Accuracy: 0.921, F1 Micro: 0.7739, F1 Macro: 0.6581\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1134\n",
      "      Abusive       0.84      0.94      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.72      0.52      0.60       157\n",
      "      HS_Race       0.73      0.76      0.74       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.55      0.24      0.33        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.65      0.66      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 213.51145124435425 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3796, Accuracy: 0.8885, F1 Micro: 0.6253, F1 Macro: 0.3396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2539, Accuracy: 0.9077, F1 Micro: 0.7119, F1 Macro: 0.5079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.21, Accuracy: 0.9124, F1 Micro: 0.7464, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9199, F1 Micro: 0.7581, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9188, F1 Micro: 0.7692, F1 Macro: 0.6282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1256, Accuracy: 0.921, F1 Micro: 0.7731, F1 Macro: 0.6694\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9247, F1 Micro: 0.7689, F1 Macro: 0.6655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.9202, F1 Micro: 0.7748, F1 Macro: 0.6842\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9197, F1 Micro: 0.7721, F1 Macro: 0.6718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.923, F1 Micro: 0.7752, F1 Macro: 0.7033\n",
      "Model 3 - Iteration 10218: Accuracy: 0.923, F1 Micro: 0.7752, F1 Macro: 0.7033\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.91      0.28      0.43        72\n",
      "    HS_Gender       0.55      0.53      0.54        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 217.3852207660675 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.923, F1 Micro: 0.7763, F1 Macro: 0.6894\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 47.695022263616785\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.880720376968384 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3638, Accuracy: 0.89, F1 Micro: 0.6371, F1 Macro: 0.352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9076, F1 Micro: 0.7122, F1 Macro: 0.5234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2024, Accuracy: 0.9196, F1 Micro: 0.7498, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9155, F1 Micro: 0.7569, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1403, Accuracy: 0.9215, F1 Micro: 0.7692, F1 Macro: 0.6409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.924, F1 Micro: 0.7797, F1 Macro: 0.672\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9214, F1 Micro: 0.7728, F1 Macro: 0.6735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9244, F1 Micro: 0.7807, F1 Macro: 0.6924\n",
      "Epoch 9/10, Train Loss: 0.0698, Accuracy: 0.9235, F1 Micro: 0.7733, F1 Macro: 0.6824\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9264, F1 Micro: 0.776, F1 Macro: 0.6946\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9244, F1 Micro: 0.7807, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.57      0.41      0.48        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.59      0.62       331\n",
      "    HS_Strong       0.91      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 218.34022283554077 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8877, F1 Micro: 0.6288, F1 Macro: 0.3257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9063, F1 Micro: 0.7, F1 Macro: 0.4834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2076, Accuracy: 0.9144, F1 Micro: 0.7341, F1 Macro: 0.5616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9168, F1 Micro: 0.7607, F1 Macro: 0.6125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.145, Accuracy: 0.9208, F1 Micro: 0.7694, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.9197, F1 Micro: 0.7742, F1 Macro: 0.6611\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.6763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.922, F1 Micro: 0.7771, F1 Macro: 0.6834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9251, F1 Micro: 0.7777, F1 Macro: 0.6866\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9195, F1 Micro: 0.7692, F1 Macro: 0.6846\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9251, F1 Micro: 0.7777, F1 Macro: 0.6866\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.55      0.33      0.41        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.66      0.60      0.63       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 219.42744493484497 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3727, Accuracy: 0.8886, F1 Micro: 0.6269, F1 Macro: 0.3274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2533, Accuracy: 0.9067, F1 Micro: 0.711, F1 Macro: 0.513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9174, F1 Micro: 0.7473, F1 Macro: 0.5801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9181, F1 Micro: 0.7653, F1 Macro: 0.6184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9229, F1 Micro: 0.7725, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9204, F1 Micro: 0.7748, F1 Macro: 0.6744\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9221, F1 Micro: 0.7731, F1 Macro: 0.6853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9202, F1 Micro: 0.7764, F1 Macro: 0.6898\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9232, F1 Micro: 0.7727, F1 Macro: 0.6967\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.924, F1 Micro: 0.7692, F1 Macro: 0.6938\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9202, F1 Micro: 0.7764, F1 Macro: 0.6898\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.91      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.68      0.82      0.74       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.66      0.64      0.65       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.73      0.85      0.79       762\n",
      "      HS_Weak       0.66      0.80      0.72       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.78      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 218.9598777294159 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9232, F1 Micro: 0.7783, F1 Macro: 0.6896\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 158.63955438209592\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 7.345273494720459 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3613, Accuracy: 0.8906, F1 Micro: 0.6304, F1 Macro: 0.3511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9111, F1 Micro: 0.7167, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1989, Accuracy: 0.9156, F1 Micro: 0.7533, F1 Macro: 0.5942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9218, F1 Micro: 0.7621, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1387, Accuracy: 0.9234, F1 Micro: 0.7652, F1 Macro: 0.6242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.922, F1 Micro: 0.7781, F1 Macro: 0.683\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9255, F1 Micro: 0.773, F1 Macro: 0.6752\n",
      "Epoch 8/10, Train Loss: 0.0811, Accuracy: 0.923, F1 Micro: 0.7757, F1 Macro: 0.6945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9238, F1 Micro: 0.7799, F1 Macro: 0.6966\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9213, F1 Micro: 0.7756, F1 Macro: 0.7073\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9238, F1 Micro: 0.7799, F1 Macro: 0.6966\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 220.28095364570618 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3767, Accuracy: 0.8863, F1 Micro: 0.6048, F1 Macro: 0.3073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.9043, F1 Micro: 0.6815, F1 Macro: 0.4544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9154, F1 Micro: 0.7474, F1 Macro: 0.5747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.9213, F1 Micro: 0.7579, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9236, F1 Micro: 0.7648, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1207, Accuracy: 0.923, F1 Micro: 0.7731, F1 Macro: 0.6606\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9241, F1 Micro: 0.7692, F1 Macro: 0.6645\n",
      "Epoch 8/10, Train Loss: 0.0833, Accuracy: 0.9237, F1 Micro: 0.7697, F1 Macro: 0.6777\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9236, F1 Micro: 0.7726, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.923, F1 Micro: 0.7783, F1 Macro: 0.7014\n",
      "Model 2 - Iteration 10535: Accuracy: 0.923, F1 Micro: 0.7783, F1 Macro: 0.7014\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.69      0.71      0.70       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.70      0.80      0.74       120\n",
      "  HS_Physical       0.69      0.25      0.37        72\n",
      "    HS_Gender       0.51      0.49      0.50        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.63      0.65      0.64       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.72      0.70      0.70      5556\n",
      " weighted avg       0.77      0.78      0.78      5556\n",
      "  samples avg       0.46      0.44      0.43      5556\n",
      "\n",
      "Training completed in 220.1190001964569 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3719, Accuracy: 0.8891, F1 Micro: 0.6144, F1 Macro: 0.3129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9083, F1 Micro: 0.7019, F1 Macro: 0.5121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9158, F1 Micro: 0.7519, F1 Macro: 0.5864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9222, F1 Micro: 0.7584, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1425, Accuracy: 0.925, F1 Micro: 0.7683, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.923, F1 Micro: 0.7746, F1 Macro: 0.6689\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9255, F1 Micro: 0.7744, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9212, F1 Micro: 0.7771, F1 Macro: 0.6907\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9237, F1 Micro: 0.774, F1 Macro: 0.7033\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9214, F1 Micro: 0.7769, F1 Macro: 0.7032\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9212, F1 Micro: 0.7771, F1 Macro: 0.6907\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.86      0.93      0.90       992\n",
      "HS_Individual       0.70      0.79      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.74      0.69      0.72       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.78      0.72       689\n",
      "  HS_Moderate       0.61      0.57      0.59       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.76      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 220.53279280662537 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9227, F1 Micro: 0.7784, F1 Macro: 0.6962\n",
      "Total sampling time: 6610.52 seconds\n",
      "Total runtime: 21269.782083034515 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACCoAAAHqCAYAAADInz0eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADtrElEQVR4nOzdd3SUZdrH8W96aInUUAVBUFEEpVmwoyAWQEDFQrG9umJDRVl7xbLLYsdVEAtFKYqAIohdKavYUAFBeq8JBEid949HiRFUAkkmGb6fc54z89S5bvec9Xbml+uOCoVCISRJkiRJkiRJkiRJkopBdLgLkCRJkiRJkiRJkiRJ+w+DCpIkSZIkSZIkSZIkqdgYVJAkSZIkSZIkSZIkScXGoIIkSZIkSZIkSZIkSSo2BhUkSZIkSZIkSZIkSVKxMaggSZIkSZIkSZIkSZKKjUEFSZIkSZIkSZIkSZJUbAwqSJIkSZIkSZIkSZKkYmNQQZIkSZIkSZIkSZIkFRuDCpIkSZIkqUTr1asX9erVC3cZkiRJkiSpkBhUkKR98OyzzxIVFUXr1q3DXYokSZK014YNG0ZUVNRut9tvv33ndVOmTOHyyy/niCOOICYmpsDhgd+eecUVV+z2/B133LHzmvXr1+/LkCRJkrQfcT4rSaVPbLgLkKTSbPjw4dSrV49Zs2axYMECDj744HCXJEmSJO21+++/n4MOOijfsSOOOGLn+xEjRvD6669z9NFHU7Nmzb36jMTERMaOHcuzzz5LfHx8vnMjR44kMTGRHTt25Dv+wgsvkJubu1efJ0mSpP1HSZ3PSpJ2ZUcFSdpLixYt4osvvmDgwIFUrVqV4cOHh7uk3UpPTw93CZIkSSolzjzzTC655JJ8W7NmzXaef/jhh0lLS+Pzzz+nadOme/UZ7du3Jy0tjXfffTff8S+++IJFixZx1lln7XJPXFwcCQkJe/V5v5ebm+uXxpIkSRGspM5ni5rfAUsqjQwqSNJeGj58OBUrVuSss86ia9euuw0qbN68mZtuuol69eqRkJBA7dq16dGjR762Xzt27ODee++lUaNGJCYmUqNGDc477zwWLlwIwEcffURUVBQfffRRvmcvXryYqKgohg0btvNYr169KF++PAsXLqRDhw5UqFCBiy++GIBPP/2Ubt26ceCBB5KQkECdOnW46aab2L59+y51z507l/PPP5+qVatSpkwZDjnkEO644w4APvzwQ6KionjzzTd3uW/EiBFERUUxffr0Av/zlCRJUslXs2ZN4uLi9ukZtWrV4sQTT2TEiBH5jg8fPpwmTZrk+4u33/Tq1WuXtry5ubk88cQTNGnShMTERKpWrUr79u358ssvd14TFRVFnz59GD58OIcffjgJCQlMnjwZgK+//pozzzyTpKQkypcvz2mnncaMGTP2aWySJEkq2cI1ny2s72YB7r33XqKiovjxxx+56KKLqFixIm3atAEgOzubBx54gAYNGpCQkEC9evX45z//SUZGxj6NWZKKgks/SNJeGj58OOeddx7x8fF0796d5557jv/973+0bNkSgK1bt3LCCSfw008/cdlll3H00Uezfv163n77bZYvX06VKlXIycnh7LPPZtq0aVx44YXccMMNbNmyhalTpzJnzhwaNGhQ4Lqys7Np164dbdq04V//+hdly5YFYPTo0Wzbto1rrrmGypUrM2vWLJ566imWL1/O6NGjd97/3XffccIJJxAXF8dVV11FvXr1WLhwIRMmTOChhx7i5JNPpk6dOgwfPpzOnTvv8s+kQYMGHHvssfvwT1aSJEnhkpqaustaulWqVCn0z7nooou44YYb2Lp1K+XLlyc7O5vRo0fTt2/fPe54cPnllzNs2DDOPPNMrrjiCrKzs/n000+ZMWMGLVq02HndBx98wBtvvEGfPn2oUqUK9erV44cffuCEE04gKSmJfv36ERcXx/PPP8/JJ5/Mxx9/TOvWrQt9zJIkSSp6JXU+W1jfzf5et27daNiwIQ8//DChUAiAK664gpdffpmuXbty8803M3PmTAYMGMBPP/202z88k6RwMqggSXvhq6++Yu7cuTz11FMAtGnThtq1azN8+PCdQYXHH3+cOXPmMG7cuHw/6N955507J46vvPIK06ZNY+DAgdx00007r7n99tt3XlNQGRkZdOvWjQEDBuQ7/uijj1KmTJmd+1dddRUHH3ww//znP1m6dCkHHnggANdddx2hUIjZs2fvPAbwyCOPAMFfpV1yySUMHDiQ1NRUkpOTAVi3bh1TpkzJl+6VJElS6dK2bdtdju3tvPSvdO3alT59+vDWW29xySWXMGXKFNavX0/37t156aWX/vb+Dz/8kGHDhnH99dfzxBNP7Dx+880371LvvHnz+P7772ncuPHOY507dyYrK4vPPvuM+vXrA9CjRw8OOeQQ+vXrx8cff1xII5UkSVJxKqnz2cL6bvb3mjZtmq+rw7fffsvLL7/MFVdcwQsvvADAP/7xD6pVq8a//vUvPvzwQ0455ZRC+2cgSfvKpR8kaS8MHz6clJSUnRO7qKgoLrjgAkaNGkVOTg4AY8eOpWnTprt0Hfjt+t+uqVKlCtddd92fXrM3rrnmml2O/X4inJ6ezvr16znuuOMIhUJ8/fXXQBA2+OSTT7jsssvyTYT/WE+PHj3IyMhgzJgxO4+9/vrrZGdnc8kll+x13ZIkSQqvZ555hqlTp+bbikLFihVp3749I0eOBIIlxI477jjq1q27R/ePHTuWqKgo7rnnnl3O/XEefdJJJ+ULKeTk5DBlyhQ6deq0M6QAUKNGDS666CI+++wz0tLS9mZYkiRJCrOSOp8tzO9mf3P11Vfn23/nnXcA6Nu3b77jN998MwCTJk0qyBAlqcjZUUGSCignJ4dRo0ZxyimnsGjRop3HW7duzb///W+mTZvGGWecwcKFC+nSpctfPmvhwoUccsghxMYW3v8dx8bGUrt27V2OL126lLvvvpu3336bTZs25TuXmpoKwC+//AKw23XUfu/QQw+lZcuWDB8+nMsvvxwIwhvHHHMMBx98cGEMQ5IkSWHQqlWrfMsmFKWLLrqISy+9lKVLl/LWW2/x2GOP7fG9CxcupGbNmlSqVOlvrz3ooIPy7a9bt45t27ZxyCGH7HLtYYcdRm5uLsuWLePwww/f43okSZJUMpTU+Wxhfjf7mz/Oc5csWUJ0dPQu389Wr16dAw44gCVLluzRcyWpuBhUkKQC+uCDD1i1ahWjRo1i1KhRu5wfPnw4Z5xxRqF93p91Vvitc8MfJSQkEB0dvcu1p59+Ohs3buS2227j0EMPpVy5cqxYsYJevXqRm5tb4Lp69OjBDTfcwPLly8nIyGDGjBk8/fTTBX6OJEmS9k/nnnsuCQkJ9OzZk4yMDM4///wi+Zzf//WaJEmSVFj2dD5bFN/Nwp/Pc/elU68kFSeDCpJUQMOHD6datWo888wzu5wbN24cb775JoMHD6ZBgwbMmTPnL5/VoEEDZs6cSVZWFnFxcbu9pmLFigBs3rw53/GCJGC///575s+fz8svv0yPHj12Hv9j67PfWt/+Xd0AF154IX379mXkyJFs376duLg4Lrjggj2uSZIkSfu3MmXK0KlTJ1577TXOPPNMqlSpssf3NmjQgPfee4+NGzfuUVeF36tatSply5Zl3rx5u5ybO3cu0dHR1KlTp0DPlCRJ0v5nT+ezRfHd7O7UrVuX3Nxcfv75Zw477LCdx9esWcPmzZv3eJk1SSou0X9/iSTpN9u3b2fcuHGcffbZdO3adZetT58+bNmyhbfffpsuXbrw7bff8uabb+7ynFAoBECXLl1Yv379bjsR/HZN3bp1iYmJ4ZNPPsl3/tlnn93jumNiYvI987f3TzzxRL7rqlatyoknnsjQoUNZunTpbuv5TZUqVTjzzDN57bXXGD58OO3bty/Ql8uSJEnSLbfcwj333MNdd91VoPu6dOlCKBTivvvu2+XcH+etfxQTE8MZZ5zB+PHjWbx48c7ja9asYcSIEbRp04akpKQC1SNJkqT9057MZ4viu9nd6dChAwCDBg3Kd3zgwIEAnHXWWX/7DEkqTnZUkKQCePvtt9myZQvnnnvubs8fc8wxVK1aleHDhzNixAjGjBlDt27duOyyy2jevDkbN27k7bffZvDgwTRt2pQePXrwyiuv0LdvX2bNmsUJJ5xAeno677//Pv/4xz/o2LEjycnJdOvWjaeeeoqoqCgaNGjAxIkTWbt27R7Xfeihh9KgQQNuueUWVqxYQVJSEmPHjt1lPTSAJ598kjZt2nD00Udz1VVXcdBBB7F48WImTZrEN998k+/aHj160LVrVwAeeOCBPf8HKUmSpFLpu+++4+233wZgwYIFpKam8uCDDwLQtGlTzjnnnAI9r2nTpjRt2rTAdZxyyilceumlPPnkk/z888+0b9+e3NxcPv30U0455RT69Onzl/c/+OCDTJ06lTZt2vCPf/yD2NhYnn/+eTIyMv5ybWFJkiSVbuGYzxbVd7O7q6Vnz57897//ZfPmzZx00knMmjWLl19+mU6dOnHKKacUaGySVNQMKkhSAQwfPpzExEROP/303Z6Pjo7mrLPOYvjw4WRkZPDpp59yzz338Oabb/Lyyy9TrVo1TjvtNGrXrg0Eadp33nmHhx56iBEjRjB27FgqV65MmzZtaNKkyc7nPvXUU2RlZTF48GASEhI4//zzefzxxzniiCP2qO64uDgmTJjA9ddfz4ABA0hMTKRz58706dNnl4l006ZNmTFjBnfddRfPPfccO3bsoG7durtdY+2cc86hYsWK5Obm/ml4Q5IkSZFj9uzZu/y12G/7PXv2LPAXu/vipZde4sgjj2TIkCHceuutJCcn06JFC4477ri/vffwww/n008/pX///gwYMIDc3Fxat27Na6+9RuvWrYuhekmSJIVDOOazRfXd7O68+OKL1K9fn2HDhvHmm29SvXp1+vfvzz333FPo45KkfRUV2pN+MZIk7UZ2djY1a9bknHPOYciQIeEuR5IkSZIkSZIkSaVAdLgLkCSVXm+99Rbr1q2jR48e4S5FkiRJkiRJkiRJpYQdFSRJBTZz5ky+++47HnjgAapUqcLs2bPDXZIkSZIkSZIkSZJKCTsqSJIK7LnnnuOaa66hWrVqvPLKK+EuR5IkSZIkSZIkSaWIHRUkSZIkSZIkSZIkSVKxsaOCJEmSJEmSJEmSJEkqNgYVJEmSJEmSJEmSJElSsYkNdwGFJTc3l5UrV1KhQgWioqLCXY4kSZKKUCgUYsuWLdSsWZPo6MjL3jq3lSRJ2n84t5UkSVKkKMjcNmKCCitXrqROnTrhLkOSJEnFaNmyZdSuXTvcZRQ657aSJEn7H+e2kiRJihR7MreNmKBChQoVgGDQSUlJYa5GkiRJRSktLY06dersnANGGue2kiRJ+w/ntpIkSYoUBZnbRkxQ4be2YUlJSU54JUmS9hOR2jrWua0kSdL+x7mtJEmSIsWezG0jb9EzSZIkSZIkSZIkSZJUYhlUkCRJkiRJkiRJkiRJxcaggiRJkiRJkiRJkiRJKjYGFSRJkiRJkiRJkiRJUrExqCBJkiRJkiRJkiRJkoqNQQVJkiRJkiRJkiRJklRsDCpIkiRJkiRJkiRJkqRiY1BBkiRJkiRJkiRJkiQVG4MKkiRJkiRJkrSfeOaZZ6hXrx6JiYm0bt2aWbNm/em1J598MlFRUbtsZ511VjFWLEmSpEhkUEGSJEmSJEmS9gOvv/46ffv25Z577mH27Nk0bdqUdu3asXbt2t1eP27cOFatWrVzmzNnDjExMXTr1q2YK5ckSVKkMaggSZIkSZIkSfuBgQMHcuWVV9K7d28aN27M4MGDKVu2LEOHDt3t9ZUqVaJ69eo7t6lTp1K2bFmDCpIkSdpnBhUkSZIkSZIkKcJlZmby1Vdf0bZt253HoqOjadu2LdOnT9+jZwwZMoQLL7yQcuXK/ek1GRkZpKWl5dskSZKkPzKoIEmSJEmSJEkRbv369eTk5JCSkpLveEpKCqtXr/7b+2fNmsWcOXO44oor/vK6AQMGkJycvHOrU6fOPtUtSZKkyGRQQZIkSZIkSZL0l4YMGUKTJk1o1arVX17Xv39/UlNTd27Lli0rpgolSZJUmsSGuwBJkqSi9NNPUKUKVK0a7kokSZKk/VQoFzZ9A5mb4IAjIdHJeThUqVKFmJgY1qxZk+/4mjVrqF69+l/em56ezqhRo7j//vv/9nMSEhJISEjYp1olSZIiyY7sHSTEJBAVFVWsnztp/iSio6I5s+GZxfq5e8qggiRJiljDh8Mll0DlyjBrFtSvH+6KJEmStF/JyYAda2DH2l9f/7BlbYWKTaHKsVDlGEioHO6KC0/6Mlg9FVZNgTXvQ8aGvHNl60Clo6Hi0XmvZWpAMX9xu7+Jj4+nefPmTJs2jU6dOgGQm5vLtGnT6NOnz1/eO3r0aDIyMrjkkkuKoVJJkqTIMfjLwdw4+UYaVW7EHSfcQdfGXYmJjinSzwyFQvzri39x2/u3USGhAl9d9RUHVzq4SD9zbxhUkCRJEWniROjZM3i/YQOccw588QUkJ4e3LkmSJEWorb/Aj49C6k+/CyKk/v19KyfmvU86BKoc92tw4ThIPgyiSsnKrVlbYe3HQTBh9RRIm5v/fGwFSKwGWxfCtmXBtnx83vnElLzgwm/hhXJ1Cz+8EMqFTV/D1kWQvQ1ytkPOr6/Z24ItNwNaPlO4n1tC9O3bl549e9KiRQtatWrFoEGDSE9Pp3fv3gD06NGDWrVqMWDAgHz3DRkyhE6dOlG5cgSFaSRJkopQTm4Ot0y5hUEzBwHw/drvuXDshRz68aH8s80/6d6kO7HRhf9T/Y7sHfzfxP/jlW9fAeDCwy/kwOQDC/1zCoNBBUmSFHE++QS6dYOcHOjSBaZPhx9/hAsvhAkTINYZkCRJkgpLZir88CDMexJyM3c9Hx0HCdWCH+ITU6BMSt5+TAJs/BLWfQFb5kPavGD75aXg3rjkoNPCb8GFKq0hLql4x/dncnNg0+xfgwlTYf0XkJuVdz4qGiq1ghpnQPXTg9qj4yArLVgGYuPsYNs0G9J+DXesejfYflO2dnBv9TOgeltIrLJ3tWalwaqpsHISrHwn+Ky/03xQUG+EueCCC1i3bh133303q1evplmzZkyePJmUlBQAli5dSnR0/nDMvHnz+Oyzz5gyZUo4SpYkSSp1tmRsofvY7kz6eRIA9550L1FRUQyaMYi56+fS460e3PvxvfRv058eTXsQHxNfKJ+7eutqOr/emRnLZxATFcOg9oO4tuW1xb7kxJ6KCoVCoXAXURjS0tJITk4mNTWVpKQS8h9skiSp2H39NZx8MqSlwdlnw7hx8N13cMIJsH073Hgj/Oc/4a5S+yrS536RPj5JkiJCbjYsfAG+uxsy1gfHqp8BDS6DxOpB94DEFIivuGddAXashw0zYP30X7eZwV/65xMFNdpBs0eh4pEFrzmUGyzBsH3l77ZVee8zNwdhg1B28PpX73N3QM6O/M8vd1AQTKhxBqScEox9T2Rvg83f5QUXNs6G1Dn5gw9EQcWj8p5f5bgg6PFn0ubDiolBOGHdp/mfFVs+WHIjphzEloWYMr++/u594/5QSF8Y/51In/tF+vgkSZJ+b8nmJZwz8hy+X/s9ibGJvNr5Vbo27gpAWkYaz/7vWf49/d+s3xb8N8SByQdy2/G3cdlRl5EYm7jXn/v1qq85d9S5LE9bzgGJBzC622ja1m9bKGMqiILM/QwqSJKkiPHzz9CmDaxdCyeeCJMnQ5kywbnRo+H884P3//0vXHll+OrUvov0uV+kj0+SpFJv5Xvw9c2Q+kOwn3QoHPVvqHlm4S1VkJsNm78POhWsnx50XUhf9OvJKKjfG458AMrW3P3921bAT/+G9MX5Qwmh7MKpD4LuDimnQY1fux5UaFB4z87eHgQMfuvYsPm7/OdjykK1k/KCC+UbwNpPgmDCikmwdUH+6ys0gppnQa2zoOoJxRZC2BORPveL9PFJkiT9ZsbyGXQc1ZG16WupXr46b1/4Ni1rtdzluvTMdP771X95/IvHWbV1FQA1ytfg1uNu5f9a/B9l48oW6HNH/zCanm/1ZHv2dg6pfAgTuk+gYeWGhTKmgjKo4IRXkqT9zvLlQUhhyRI46ij48ENITs5/zQMPwN13B0s/TJ0adF5Q6RTpc79IH58kSaVW6k8w++a85QniK0GT+6Dh/xXPMgFpP8N3d8LSN4L9mLLQuB8cdgvElst/7df94KfHd/+cxGqQWAPK1AyCDmV+3eIrBuOIigteo2Pz3kfF/nrs1/cx8VD2wOCa4rB9Fax+Py+48MflG6JiIJSTtx8dFwQZap4VbEnh+aJ2T0T63C/SxydJkgQwas4oer3Vi4ycDJpVb8bbF75NneQ6f3nPjuwdDJk9hEc/f5RlacsAqFq2Klc1v4qKiRXJCeWQG8olJzeHnFDObl/XbVvHq9+9CkD7g9szsstIDkg8oKiH+6cMKjjhlSRpv7JhQ9BB4ccfoWFD+OwzqFZt1+tCIbjoIhg1CipVgpkz4eCDi79e7btIn/tF+vgkSSp1dqyH7++FBYODH8Oj46DRdXDEnXu+vEFhWjc96OiwfnqwX6YGHPkgHNQTomOCY/+7Fn5+FmqeHSxH8VsYITGlRHUT2CuhUNBtYvUUWDUV1n0SLEORWB1qdgi6JlQ/HeIqhLvSPRLpc79IH58kSdq/hUIh7v/4fu79+F4Azj3kXIafN5zy8eX3+BmZOZm88u0rDPhsAL9s+mWv6uh7TF8eO/0xYn7774EwKcjcr5giz5IkSUVjyxbo0CEIKdSqFXRK2F1IAYIuvEOHwsKF8L//wTnnwIwZu3ZeKAqhEHz3XRCqyM6GnJz82x+P/bZ/yCFw0klFX58kSZJ2IycT5j8Fcx6ArNTgWO1O0Oyx8P6FftVj4fTPYdkY+Pq2YEmImZfDvCfgqH8FSzH8ptLRUKdz+GotClFRUPHIYDvslmCZiO0roHx9iIoOd3WSJEnaT+zI3sFl4y9j5JyRANxy7C080vaRAocF4mPiueLoK+jVrBcjvx/JlF+mEAqFiImOISbq1y36z1+Pr3M8ZzU6qyiGWKQMKkiSpFIrIwM6d4ZZs6ByZZgyBerW/et7ypSB8eOhZUuYOxcuuAAmTgyWgygK69bBq6/Ciy/CTz/t3TMeewxuvbVw65IkSdIfZG2BrYuCH/23Lg5eV0yErQuD8xWbwdEDIeWUcFaZJyoKDuwGtc6F+c8EYYrN38GHZ0CNM4GIaKK6Z2LLQAVbpUmSJKn4rNm6hs6vd2b68unERsfy3FnPccXRV+zTM2OjY7m06aVc2vTSQqqyZDOoIEmSSqXs7GAZh2nToHx5ePddaNx4z+6tUQMmTIA2beC99+Dmm+GJJwqvttxceP/9IJzw1luQlRUcL1sW6teHmJj8W2zsrsdiYmDHjmB8/foFXR+uuqrwapQkSSp2afNh45eQdCgkHw4xCcX7+Tk7IH3J78IIiyB9cd5+xobd35dYHZo+lH9ZhZIkJgEO6wv1ewVhhZ+fgVXvhrsqSZIkKWJ9t+Y7zh15LktSl1AxsSJjzx/LKQeVkEBzKWJQQZIklTqhEFx9NYwbB/HxQRigZcuCPeOoo4JOB126wJNPBiGH//u/fatr6VJ46aVgW7Ik73jLlnDFFXDhhVDQJVn794dHHgnGm5QUPEOSJKlUyc2CHx6BHx4I3gNExUJyY6h41K9bs2CL38c1ubK3Bx0Qtvz8u20+bFkA21f+/f3xlaD8QVDuoOC1QiOoewHEVdi3uopDQiVo/h9odC18czssGxscj0kMb12SJEkqlWYsn0HFxIo0qtyIqKiocJdTIizYuICHPn2IV799lZxQDg0rNWTiRRNpVLlRuEsrlQwqSJKkUue222DIEIiOhlGj4LTT9u45550HDz4Id94JffpAw4Zw6qkFe0ZmZtCd4cUXg+4MoV877B5wAFx6KVx+OTRtunf1ATz8MGzeDIMHB8+rUAHOKn3LjUmSpP3Vpm9hRm/Y9HWwf0AT2LYcMjcFyxRs/g4WvZx3ffn6v4YWjsp7LVMzWObgNzmZQQeE34cR0uYHr9uW8ZdLHsSWywsh/PZa/iAoVy94jStgqrQkqnAwnDAG1n0OKyZB/d7hrkiSJEmlyI7sHfR5pw9Dvh4CwEEHHET7g9vT/uD2nHrQqZSPLx/mCovf/A3zefCTBxn+/XByQ7kAnHvIubzU8SUqlakU5upKr6hQKBQRC9alpaWRnJxMamoqSQX9U0VJklRqPPoo3H578H7IELjssn17XigEl1wCI0ZAxYowc2YQWPg7P/wQdE545RVYty7v+CmnBN0TOneGMmX2rbbf5OYGIYURIyAxESZPhpNOKpxnl1aRPveL9PFJkvYDuVnww4BgKYJQdtCpoMVTULd7cH7bsiC8sOmbvNf0Jbt/VkLVILQQFROEEdIXQyjnzz87LhkqNPzd1ij48b58fUiokj/0IJUAkT73i/TxSZIUSZZsXkLX0V35cuWXREdFExsdS2ZO5s7zcdFxnFD3BM48+EzaH9yew6seHtHdFuaun8uDnzzIyDkjdwYUOjTswD0n3UOrWq3CXF3JVJC5n0EFSZJU4qWmwsSJMGZMsMwDwL/+BTffXDjP37EDTj45CCk0agQzZgShhT/auDHo4PDSS/Dll3nHq1eH3r2D0MTBBxdOTX+UlRUsUzFhApQvDx98UPDlLvbW9u2wYAFUrgw1axbPZ/6dSJ/7Rfr4JEkRbtO3MKNXED4AqN0JWj4HZar/9X0ZG38NLnzza3jha0ibu/tQQkzZvCBCUqP8wYSEqoYRVKpE+twv0scnSVKkmLpwKt3HdmfD9g1ULlOZUV1HcUztY/ho8Ue8+/O7vLvgXRZtXpTvntpJtWnfIOi20LZ+W5IT93EptxLip3U/8cAnDzBqzihCv3ZsO6fROdx90t20qNkizNWVbAYVnPBKklSsQiGYOhWefBI+/BAOPRSOOw6OPz7Y6tQp+DPXr4fx42HcuODZWVl55+64I1iyoTCtXg2tWsGyZTBgQF7XhpwcmDIFhg0LQhKZvwaIY2Ph7LODgEKHDsF+UduxI/isDz+ESpXgk0/g8MML59m5ubBiBcybt+u2dGnwv3FsLFx/PdxzD4R7uhXpc79IH58kKULlZMKPA2DOg7/rovA01L1w74MD2dshdU5e6OG3DgllahhGUMSI9LlfpI9PkqTSLhQK8chnj3Dnh3eSG8qlRc0WjD1/LAcmH7jLdQs2LuDdBe8yecFkPlz8ITuyd+w8HxMVwwl1T+CcRudwTqNzaFh5D9rWljA/rP2BBz55gDd+eGNnQKHjIR25+6S7ObrG0WGurnQwqOCEV5KkYrF1a7D0wVNPwdy5f35dnTr5gwtHHrn7H/ZXr4Y33ww6J3z8cRAS+M1hhwUdBbp2haZNC38sALfeGnRquOUWuPzyIJzw6quwcmXeNUceGYQTLr4YqlYtmjr+ypYt0LYtzJoFNWrAZ59B/foFe0YoBF99FXSp+OmnIIzw88+wbduf31OhQvDZEHSQePzx4J9BuH4fiPS5X6SPT5IUgTZ9A9N7weZvg/3anX/topASzqqkUiHS536RPj5JkkqztIw0er7Vk7fmvgXAFUddwVMdniIxNvFv792etZ1PlnzC5AWTeXfBu8zbMC/f+UMqHxKEFg45h+PqHEdsdDH8pdde+n7N99z/yf2M+XHMzmOdD+3M3SfdTbPqzcJXWClkUMEJryRJRWrhQnjmGRg6NFiWAYIfsi+7DC65BH75BT7/PNi++SZ/4ACgXDlo3ToILRxzDMyfD2PHBtf/fmbSrFkQTujSJQgqFLV+/YIf4H//ozwE3QsuvjgIKDRrFv4/3tu4EU46CebMCUIKn366Z0syrF8Pw4cH/7t9992u52NjoUEDOOSQvK1Ro+C1alWYPDnoqLBgQXB9mzbw9NNFFxz5K5E+94v08UmSIkhOJvzwMPzwUNBFIaEyNH8a6l4Q/kmTVEpE+twv0scnSVJp9eO6H+n8emfmb5hPfEw8T5/5NFc2v3Kvn/fLpl+YMG8CE+ZP4OMlH5Odm73zXMXEinRo2IFzGp1Du4PbcUDiAYUwgn23eutq/jntn7z0zUs7j3U5rAt3nXgXTauH4UvPCGBQwQmvJEmFLhSCadOC5R0mTswLFDRsGPx43bNn8AP/H23dCv/7X15wYfr0vHDD7rRuHQQTzjsv+NG8OPXvD488EryPiYH27YNwwtlnQ0JC8dbyd1atghNOCEIjjRsHy0BUrrzrdb8tXTF0aLCUxm9LaCQkQMeO0LJlXijhoIMgLu6vPzcjAwYODJbe2LYNoqPhH/+A+++HihULf5x/JtLnfpE+PklShPhjF4U650GLZ+2iIBVQpM/9In18kiTticycTD5a/BFv/vQmk36eROOqjRl7/ljKxZcLSz2jfxhN7/G9Sc9Kp05SHcacP4ZWtVoV2vNTd6Ty3sL3mDB/Au/8/A4bt2/ceS42OpYTDgyWiOh4aEfqV/z7drFv/PAGN0+5ma6HdeWW426hVlKtfaovMyeTJ2c+yf0f38+WzOAv1ro17sZdJ95Fk5Qm+/Ts/Z1BBSe8kiQVmvT0YPmDp56CH3/MO96+fRBQaNcu+LF6T+XmBs/5Lbjwv/9BSkoQTOjcOVgmIly+/x7uvDPoFHDJJcHSCiXZ4sVBrStWQIsWQZDkt2nQwoXw0kvB8hUrVuTd07x50Pmie/d9CxYsXRoskTF6dLD/W8eFo4tpqbZIn/tF+vgkSaVcTmbQQeGHh/O6KLR4Bg483y4K0l6I9LlfpI9PkqQ/k56ZznsL32PcT+OYOH8iqRn5/3qrbf22TOg+YY+WWSgs2bnZ3P7+7fx7+r8BOPWgUxnVZRRVyxXdGrfZudlMXzadCfMnMHH+RH5a/1O+80emHEnnQzvT+dDOHJlyJFG7+W+K3uN7M+ybYQDEx8TTq2kvbmtz2x6FHP5o0vxJ3PTeTfy88WcAWtZsyRPtn+DYOscWfHDahUEFJ7ySJO2zxYuDtv5DhsDmzcGx8uWhVy/o0yf4C3yF308/wYknBss6nHRSEEIYOhQ+/jjvmkqV4NJLg+4Qhb1Mw7RpcN11sGMH/PADlClTuM//M5E+94v08UmSSrGNX8OMXrD513Wk6nSBls9CYrWwliWVZpE+94v08UmS9Hsbt29kwrwJvDn3Td5b+B47snfsPJdSLoWOh3SkRc0W3PTeTaRnpdPxkI6M7jaauJi/aXNaCNZsXcOFYy/ko8UfAdDvuH48dNpDxEbHFvln/96CjQuYOH8ib897m0+WfEJOKG/d4PoV69PpkE50Pqwzx9Y+lpjoGCAvqFAxsSKbdmwCICYqhu5NutO/TX8aV238t587b/08bnrvJt5d8C4Q/O8x4LQB9GzWk+ioAvwlnv6SQQUnvJIkFUhGBnz7bdDd4H//g1mzYO7cvOUdGjQIfozu1QuSk8NaqnZj9mw45RRIS8s7FhUVdLu47DI499yiXboiMxOWLAmWASkukT73i/TxSZJKoZxM+OHBX7so5EBClaCLQt3zw12ZVOpF+twv0scnSdKKtBW8Nfct3pz7Jh8t/ijfD+8HHXBQ0C3gDz+8f7DoAzoM70BGTgYXNbmIVzq9svNcUfjfiv/R+fXOrNiygvLx5RnWcRhdGncpss/bUxu2bWDC/CDYMWXhlN0GOzof1plXv3uVEd+P4NG2j3JcneN46NOHmLxgMgBRRNH5sM7cccIdHF1j13avqTtSeeCTB3hi5hNk52YTFx3HjcfcyJ0n3klSgnOTwmZQwQmvJClCbN0aLJNQtWqwDEFiIXQBy8kJQgi/DyV8+y1kZe167emnww03wJlnFmx5BxW/Tz+Fc84Juidcdhn07BneZTSKWqTP/SJ9fJKkUmbj7F+7KHwf7NfpCi2fsYuCVEgife4X6eOTJO2fVqSt4LXvXuPNuW8yc8XMfOeaVGvCeYed95dLGUCwBEGn1zuRnZvNVUdfxeCzB//ptfvi/V/ep9OoTqRnpXNolUMZd/44Dqt6WKF/zr5Kz0xn8oLJvDn3zd0ulQHwaNtH6Xd8PwC+WvkVD336EG/OfXPn+TMPPpM7TriD4w88ntxQLsO+GUb/af1Zm74WgLMansXAdgNpVLlR8QxqP2RQwQmvJCkCpKUFLf2//TbvWMWKQWChZs38r398X7ZscH0oBEuXBmGE30IJX30VBCD+qHJlaNkSWrUKXlu2hJSU4hmrCkd2NsTE7B9LQ0f63C/SxydJ+7XMVFj7cdCRoELD4LUk/ct7x9pgeYdNv9u2BGu3klAlWObhwG7hrVGKMJE+94v08UmS9i+hUIiXvnmJGyffyJbMLTuPH1fnuKBzwqGdaVCpwR4/740f3qD72O7khnK5+dibefz0xws1rDDup3F0H9udzJxM2tZvy9jzx5aKLgKZOZl8uOhD3pz7JuPnjWf11tUAPHfWc1zd4up81/6w9gcGfDaAkXNGkhvKBeCkuieRnpXOlyu/BKBR5UYMajeIMxueWbwD2Q8ZVHDCK0kq5bKy4KyzYOrUvC4KO3b89T2/l5wcBBY2bIB163Y9X64cNG+eF0ho1Qrq1StZ35FLfyXS536RPj5J2m+tmgozesP2FXnH4pKCwEKFhlD+4F/fH1z0IYZQCNIXB0GE3wcTtq/c/fUHng8tnobEqkVTj7Qfi/S5X6SPT5K0/1i9dTVXTriSifMnAtCyZksuO+oyOh7SkRoVauz1c4d+PZTL374cgPtOvo+7T7q7UOod+vVQrpxwJbmhXLoc1oXh5w0nIbYI14ctIrmhXGYsn8GP637k4iYXUyauzG6vW7hxIY9+/ijDvhlGVm7QPjgpIYm7T7yb61pfR3xMfHGWvd8yqOCEV5JUioVC0Ls3vPxyECj4+GM4+mjYvBlWrYKVK/O//vHY9u35nxcXB0cemb9bwmGHBX95L5VWkT73i/TxSdJ+Jzsdvr4Nfn4m2C9TE6JiYdsy4C++lolLzgst5AsyNISEynseYsjNhrS5fwglfANZm3dzcVTw/IpHQaWjgteKRxlQkIpQpM/9In18kqT9w5gfx3D1xKvZsH0D8THxPHDKA9x87M3ERBfOl6xPznySGybfAMDAMwZy07E37dPz/vXFv7h16q0AXH7U5Tx/9vOFVmtJtyx1GU/OfJKs3Cz6t+lPSnnbBhengsz9YoupJkmStIfuvTcIKcTEwBtvBJ0PIFj2oWJFaNz4z+8NhYIlI34LLpQrB02b5nVlkCRJUjFbPwOm98hbPqFRH2j2CMSWg5wdsPWX4NyWn2HLgrz325ZBVips/CrY/iguOS+08PswQ7m6kL40r0PCxq8h9fvgs/4oOg6Sj8gLI1Q6Cg44EuIqFO0/E0mSJKmU2LR9E33e7cOI70cA0Kx6M17p9ApNUpoU6udc3/p6tmRs4c4P76TvlL6Ujy/Plc2vLPBzQqEQd3xwBwM+GwDArcfdyqNtHy3U5SRKujrJdXj8jMfDXYb2gEEFSZJKkCFD4P77g/fPPQcdOhTs/qioYNmH5OSga4IkSZLCJCcT5twPPw6AUC6UqQXHvAQ1Ts+7JiYRkhsH2x9lb88fYtj6+xDD8l9DDF8G256ILQ8Vm+UPJSQ1BtufSpIkSbv13oL3uPzty1mxZQXRUdH8s80/ueuku4psCYF/nvBP0jLSeOyLx/i/if9H+fjydG/SfY/vz8nN4dp3ruX5r54H4JHTHuG2NrcVSa1SYTCoIElSCTF5Mvzf/wXv77gDrix4YFaSJEklweY5MP3SYHkFgHqXQIsnIb7inj8jtgwccHiw/VH2dti6cPedGLavgMRqeYGE37YKDSAqulCGJ0mSJEWyrZlbuXXKrQz+ajAAjSo34pVOr9C6dusi/dyoqCgeafsIWzO38uyXz3Lpm5dSLr4c5x5y7t/em5mTSY83e/D6D68TRRSDzx7MVc2vKtJ6pX1lUEGSpBJg9mzo2hVycuDSS+GBB8JdkSRJkgosNwfm/Qe+vQNyMyGhMrQcDAd2LdzPiS0DBxwRbLvUkAVRsUGrLUmSJEkF8vnSz+n5Vk8WbloIwPWtrmdA2wGUjStbLJ8fFRXFUx2eYkvmFl797lW6je7GpIsm0bZ+2z+9Jz0zna6juzJ5wWTiouN47bzXOP/w84ulXmlfGFSQJCnMFi+Gs86C9HQ47TR48UW/V5YkSSp1ti6C6T1h3afBfs2zofULUKZ68dYRHVe8nydJkiRFgIzsDO7+8G4e/+JxQoSok1SHlzq+xGn1Tyv2WqKjohnacSjpWemM+2kcHUd1ZOqlUzmuznG7XLtp+ybOHnk2Xyz7grJxZRl3/jjaHdyu2GuW9oY9/yRJCqNNm6BDB1i9Gpo0gbFjId5lgiVJkkqPUAgWvAjvHBmEFGLLQ+sX4aS3iz+kIEmSJKnAvln9DS1faMljXzxGiBC9mvXi+2u+D0tI4Tex0bGMOG8E7Q9uz7asbXQY3oHZq2bnu2b11tWc/PLJfLHsCw5IPICpl041pKBSxaCCJElhkpEBnTrBTz9BrVrwzjuQnBzuqiRJkrTHtq+Gj8+BWVdC9laoegJ0+A4aXG6LLEmSJKmEy87N5qFPHqLVC634fu33VCtXjbcueIuXOr5EcmL4v6hNiE1g7PljOeHAE0jNSKXda+34ad1PACzatIg2Q9vw3ZrvqF6+Oh/3+ni3HRekksyggiRJYZCbCz17wiefQFISvPsu1K4d7qokSZK0x5aOgXeOgJWTIDoejvoXnPYhlD8o3JVJkiRJ+hvzN8ynzdA23PnhnWTlZnHeYecx55o5dDy0Y7hLy6dsXFkmXjSRFjVbsH7betq+2paJ8ydy/NDjWbhpIQcdcBCf9f6MI1OODHepUoHFhrsASZL2R7ffDq+/DnFxMG5csOyDJEmSSoHMTfDldbB4eLBf8Sg49hU44Ijw1iVJkiTpb+WGcnlm1jPc9v5tbM/eTnJCMk93eJqLm1xMVAntipaUkMTkiydz8ssnM2ftHM4ZeQ4Ah1c9nCmXTqFmhZrhLVDaS3vVUeGZZ56hXr16JCYm0rp1a2bNmvWn12ZlZXH//ffToEEDEhMTadq0KZMnT853zYABA2jZsiUVKlSgWrVqdOrUiXnz5u1NaZIklXjPPAOPPx68HzIETgvfUmeSJEnaU7k5sHIyTGoShBSiYuDwO+GMGYYUJEmSpBIsFAoxf8N8Xv32VU5/9XSun3w927O3c3r90/n+mu+55MhLSmxI4TeVy1ZmyiVTOLjSwQAcU/sYPun9iSEFlWoF7qjw+uuv07dvXwYPHkzr1q0ZNGgQ7dq1Y968eVSrVm2X6++8805ee+01XnjhBQ499FDee+89OnfuzBdffMFRRx0FwMcff8y1115Ly5Ytyc7O5p///CdnnHEGP/74I+XKldv3UUqSVEKMHw/XXx+8f/BBuPTS8NYjSZKkX+VmwbZlkL4Eti4OXrf9/v0yCGUH11ZoCMe+ClVah7NiSZIkSbuxYdsGZq2YxYzlM5i5YiazVsxi045NO8+XiS3Dv874F9e0uKbEBxR+r0aFGnxx2RdM/WUqHQ/pSLl4f0NV6RYVCoVCBbmhdevWtGzZkqeffhqA3Nxc6tSpw3XXXcftt9++y/U1a9bkjjvu4Nprr915rEuXLpQpU4bXXnttt5+xbt06qlWrxscff8yJJ564R3WlpaWRnJxMamoqSUlJBRmSJEnFYuZMOOUU2L4drrwSnn8eStE8WCpRIn3uF+njk6SwyN4O25YGwYNtS/ICCb+937YC+JuvSKLj4eD/g2aPQGzZYiha0v4g0ud+kT4+SVJ4ZWRn8O2ab5m5fCYzVwTbgo0LdrkuMTaRo2sczTG1juHqFlfTsHLDMFQrRb6CzP0K1FEhMzOTr776iv79++88Fh0dTdu2bZk+ffpu78nIyCAxMTHfsTJlyvDZZ5/96eekpqYCUKlSpYKUJ0lSibVgAZx9dhBS6NABnn3WkIIkSVKhykoLAgd/DCCkL4H0xbBj7d8/IyYRytWFsnWD1/L18r9PrAHRMUU7DkmSJEm7FQqF+GXTL0Eg4ddgwtervyYzJ3OXaxtVbkTrWq1pXas1x9Q+hiNTjiQuJi4MVUv6MwUKKqxfv56cnBxSUlLyHU9JSWHu3Lm7vaddu3YMHDiQE088kQYNGjBt2jTGjRtHTk7Obq/Pzc3lxhtv5Pjjj+eII/58jceMjAwyMjJ27qelpRVkKJIkFZt166B9e1i/Hpo3h9dfh9gCL74kSZKknbLTYfnbsPwt2DI/CCNkbvrb24gtD+XqBcGDna+/e59YzTSpJEmSVEJs3rGZWStm5euWsH7b+l2uq1ymMq1rt94ZTGhVqxUVy1QMQ8WSCqLIfyZ54oknuPLKKzn00EOJioqiQYMG9O7dm6FDh+72+muvvZY5c+b8ZccFgAEDBnDfffcVRcmSJBWabdvgnHNg4UKoVw8mToTy5cNdlSRJUimUkwGrJsOSUUFIIWfbrtfEV/rzEEK5uhBf0SCCJEmSVIJt3rGZN396k5FzRjJt0TRyQ7n5zsfHxNOserN83RLqV6xPlPN8qdQpUFChSpUqxMTEsGbNmnzH16xZQ/Xq1Xd7T9WqVXnrrbfYsWMHGzZsoGbNmtx+++3Ur19/l2v79OnDxIkT+eSTT6hdu/Zf1tK/f3/69u27cz8tLY06deoUZDiSJBWpnBy46CKYORMqVoR334U/+delJEmSdic3G9Z8GIQTlo2DrM1558o3gLoXQpVj84IIcRXCVqokSZKkvbMtaxsT509k5JyRvPPzO/mWcmhQsUG+bgnNqjcjITYhjNVKKiwFCirEx8fTvHlzpk2bRqdOnYBgqYZp06bRp0+fv7w3MTGRWrVqkZWVxdixYzn//PN3nguFQlx33XW8+eabfPTRRxx00EF/W0tCQgIJCf4fkSTtDzIy4NNPITo6+MH/ty0pqeT+QVwoBDfeCOPHQ0JC8HrooeGuSpIkqRQI5cL6GbBkJCx9A3aszTtXpiYceAHU6w6VWpTcyaAkSZJUwmTlZLElcwtbM7eyJWMLGTkZNK7amMTYxLDVM/WXqYycM5K35r7F1sytO881rtqYi464iAuPuJAGlRqEpT5JRa/ASz/07duXnj170qJFC1q1asWgQYNIT0+nd+/eAPTo0YNatWoxYMAAAGbOnMmKFSto1qwZK1as4N577yU3N5d+/frtfOa1117LiBEjGD9+PBUqVGD16tUAJCcnU6ZMmcIYpySplFqwALp2hW+/3fVcdDQccED+8MKebklJwf37KhSC3FzIzg62nJzg9YUX4Omng2teeQVOOGHfP0uSJClihUKw+VtYPDLonrBtad65hMpQpyvU7Q7VToCoQpjESZIkSSVYKBQiIyeDLRm/Bgsyt+zZ+98FEf54PiMnY5fPKRtXlrb123J2w7Pp0LADtZJqFem4ckO5fLb0M0Z8P4IxP45hw/YNO8/VTa5L9yO6071Jd5pUa+JSDtJ+oMBBhQsuuIB169Zx9913s3r1apo1a8bkyZNJSUkBYOnSpUT/7pefHTt2cOedd/LLL79Qvnx5OnTowKuvvsoBBxyw85rnnnsOgJNPPjnfZ7300kv06tWr4KOSJEWEceOgd29ISwvCBdWrw6ZNwZaREQQENm4MtoKKjobk5OC5BxwAMTF5YYPfBw7+6thv+3/lX/+C3zURkiRJ0u+lzQ86JywZBWlz847HlofanYPOCdXbQnRc+GqUJEmSilAoFOKNH95g4IyBrNqyamfYIDv3b7543EsJMQmUjy9PiBAbt2/k7Xlv8/a8twE4qvpRnNXwLM5qdBYta7YkJjpmnz8vFAoxe9VsRs4Zyes/vM7ytOU7z1UrV43zG5/PRU0u4pjaxxhOkPYzUaFQKBTuIgpDWloaycnJpKamkpSUFO5yJEn7ICsLbr8dBg4M9tu0gVGjoNbvAr3bt+eFFjZtgs2b8+//1bZ9e9GPIS4ObrsN7r/fjsRSUYj0uV+kj0/Sfi59WRBMWDISNn2ddzw6AWqdHXROqNkBYu2wKGn/EOlzv0gfnyTtiy9XfsmNk2/k82Wf/+k1ZePKUiG+AuXjy1MhocLu3//d+YS86+JighBwKBTi2zXfMnH+RCb9PImZy2cSIu8nw6plq3JmwzM5u+HZnNHgDJITkws0tnnr5zFyzkhGzhnJ/A3zdx5PTkjmvMPOo/sR3TnloFOIjS7w31RLKsEKMvczqCBJKlGWL4cLLoAvvgj2b7kFHn44+OG/sGRk7BpyCIUgNjZvi4nJv7+7Y3+1Hx1tQEEqSpE+94v08UnaD+1YC0vHBOGEdZ/lHY+KgepnQN0LoU4niPP/8yTtfyJ97hfp45OkvbFyy0ru+OAOhn0zDAjCCP2O68dZjc7KFy4oF1euULoa7Il16et4d8G7TPp5EpMXTCYtI23nudjoWE448ATOangWZzc6m0aVG+22+8Gy1GWMmjOKkXNG8vXqvFByYmwi5zQ6h4uaXET7g9uTGJtYLGOSVPwMKjjhlaRSaepUuOgiWL8+WJZh2DDo1CncVUkqiSJ97hfp45O0n8jcDMvfgsUjYc00COX8eiIKqp34azihKyRWCWORkhR+kT73i/TxSVJBbM/azn9m/IeHP32Y9Kx0AC458hIGnDaA2km1w1xdnqycLD5f9vnObgtz18/Nd75BxQY7QwuNqzbm7XlvM3LOSD5d+unOa2KjYzmjwRl0P6I7HQ/pSIWECsU9DElhYFDBCa8klSo5OfDgg3DffUFng6OOgtGjoUGDcFcmqaQqzLnfM888w+OPP87q1atp2rQpTz31FK1atdrttSeffDIff/zxLsc7dOjApEmTAOjVqxcvv/xyvvPt2rVj8uTJe1yTc1tJpVb2NlgxMeicsPIdyM3MO1epZRBOqHs+lC05X8JKUrhF+twv0scnSXsiFAox5scx3Dr1VpakLgGgda3WDGo/iGNqHxPm6v7ewo0LmfTzJCbOn8jHSz4mMyfzT689se6JdD+iO10bd6VKWUPJ0v6mIHM/F36RJIXVunVwySUwZUqwf9VV8MQTkGj3L0nF4PXXX6dv374MHjyY1q1bM2jQINq1a8e8efOoVq3aLtePGzeOzMy8/xjfsGEDTZs2pVu3bvmua9++PS+99NLO/YSEhKIbhCSFW04mrJ4SdE5YMR6y0/POJTeGut2DgEKFg8NXoyRJkhQms1fN5sbJN+7sNlCrQi0ebfso3Zt0JzoqOszV7ZkGlRpwfevrub719WzJ2ML7v7zPpJ8nMennSazeuprmNZrT/YjuXHDEBSWqM4Skks2ggiQpbL74As4/H1asgLJlYfBguPTScFclaX8ycOBArrzySnr37g3A4MGDmTRpEkOHDuX222/f5fpKlSrl2x81ahRly5bdJaiQkJBA9erVi65wSQq3bSth1XuwajKsmgJZm/POlauXF044oAnsZu1aSZIkKdKt3rqaf077J8O+GUaIEGViy9Dv+H7cetytlIsvF+7y9lqFhAp0PqwznQ/rTG4ol7SMNA5IPCDcZUkqhQwqSJKKXSgEgwZBv36QnQ2HHAJjx8Lhh4e7Mkn7k8zMTL766iv69++/81h0dDRt27Zl+vTpe/SMIUOGcOGFF1KuXP4vGD766COqVatGxYoVOfXUU3nwwQepXLnynz4nIyODjIyMnftpaWkFHI0kFbGcTFj/RRBMWDkZNn+b/3xidah7QRBOqNzacIIkSZL2Wzuyd/Cf6f/h4c8eZmvmVgAuanIRj5z2CHWS64S5usIVHRVtSEHSXjOoIEkqVqmpcNllMG5csH/hhfDf/0KFCuGtS9L+Z/369eTk5JCSkpLveEpKCnPnzv3b+2fNmsWcOXMYMmRIvuPt27fnvPPO46CDDmLhwoX885//5Mwzz2T69OnExMTs9lkDBgzgvvvu2/vBSFJRSF8ShBJWTYbV0yB7y+9ORkGlFlCzPdQ4Eyq3gujd/3+cJEmStD8IhUKM+2kct069lUWbFwHQqlYrBrUbxLF1jg1zdZJU8hhUkCQVm2++gW7dYMECiIuD//wH/vEP/+BOUuk0ZMgQmjRpQqtWrfIdv/DCC3e+b9KkCUceeSQNGjTgo48+4rTTTtvts/r370/fvn137qelpVGnTmT9lYWkUiBnB6z95NdwwruQ9ofQVkJVqNE+CCdUPx0Sq4anTkmSJKmE+XrV19z03k18vORjAGpWqMkjpz3CxUdeTHRUdJirk6SSyaCCJKlYDB0K114LO3bAgQfC6NHwh9/2JKlYValShZiYGNasWZPv+Jo1a6hevfpf3puens6oUaO4//77//Zz6tevT5UqVViwYMGfBhUSEhJISEjY8+IlqTCEQrBlwa/LObwLaz+CnO1556OiocqxQceEmu2h4lHBMUmSJEkArNm6hjs+uIOhXw8lRIjE2ERuPe5Wbjv+NsrFl/v7B0jSfsyggiSpSG3bFgQUhg0L9jt0gFdegb9Yql2SikV8fDzNmzdn2rRpdOrUCYDc3FymTZtGnz59/vLe0aNHk5GRwSWXXPK3n7N8+XI2bNhAjRo1CqNsSdo32emw5sMgmLBqMmz9Jf/5MrV+Xc6hPVQ/DeIrhqdOSZIkqQTLyM5g0IxBPPTpQ2zJDJZIu/CIC3m07aMcmHxgmKuTpNLBoIIkaa/l5sKmTbB+fbBt2JD3/rftiy9g3jyIjoYHH4TbbgveS1JJ0LdvX3r27EmLFi1o1aoVgwYNIj09nd69ewPQo0cPatWqxYABA/LdN2TIEDp16kTlP6Sutm7dyn333UeXLl2oXr06CxcupF+/fhx88MG0a9eu2MYlSTuFQpD6Y7CUw8rJsO5TyM3MOx8dB1VPyFvSIfkI1+WSJEmS/kQoFOLNuW9y69Rb+WVTEPptUbMFg9oN4vgDjw9zdZJUuhhUkCQBQehg8+Y/Dx3sLoSwaVNw399JSYGRI+GUU4p8GJJUIBdccAHr1q3j7rvvZvXq1TRr1ozJkyeTkpICwNKlS4n+Q7pq3rx5fPbZZ0yZMmWX58XExPDdd9/x8ssvs3nzZmrWrMkZZ5zBAw884NIOkopPZiqsmZbXNWHb8vzny9WDmmcG4YSUUyCuQljKlCRJkkqTb1d/y43v3chHiz8CoEb5GjzS9hEuOfISol0iTZIKLCoUCoXCXURhSEtLIzk5mdTUVJKSksJdjiSVCOnpsGpV3rZ6dbCtW7dr6GDjxj0LHexOcnKwlEOVKnnbb/vVqsG55wavklRYIn3uF+njk1TIQrmw6ZsglLByMqz/AkI5eedjEqHayb92TTgTKjS0a4IklSCRPveL9PFJinxr09dy5wd38uLsFwkRIiEmgVuOu4Xb29xO+fjy4S5PkkqUgsz97KggSaVMKBR0N1i9On8I4bcgwu/3t24t+POTkvIHDf4YPPjjfqVKEB9f+OOUJEnSX8hOh+UTgiUdVr0HO9bkP590SBBMqHEmVDsRYsuEp05JkiSplMrIzuDJmU/y4KcPkpaRBsD5h5/PY20fo+4BdcNcnSSVfgYVJKmEyMqCNWv+PHTw2/7q1cG1e6pcOahRA6pXD15r1Ai6G+wueFC5sqEDSZKkEisUgg0zYeFQWDIKsrfknYstBymn/bqkQzsof1D46pQkSZJKsVAoxPh547llyi0s3LQQgOY1mjOo/SDaHNgmzNVJUuQwqCBJxWz7dnjqKfjxx/xBhPXrC/acypXzgge/DyH8cb+CSw5LkiSVbttXw6JX4ZehkDY373j5BlDnvCCcUOV4iDFxKkmSJO2L79Z8x03v3cQHiz4AoHr56gw4bQA9mvYgOio6zNVJUmQxqCBJxWjdOujYEaZP3/352FhISfnzAMJvx6pXt/OBJElSRMvNgpXvBN0TVk6CUE5wPKYsHNgNGlwGVU+AqKjw1ilJkiSVQqFQiKWpS5m9ajZfr/462FZ9zYotKwBIiEng5mNv5vY2t1Mhwb8Ek6SiYFBBkorJzz9Dhw6wYAFUrAh9+0Lt2vlDCJUrQ7TBXEmSpP1X6o/wy0uw6BXYsTbveJVjof5lUPd8iEsKX32SJElSKZOTm8P8DfP5evXXecGEVV+zacemXa6NIoqujbvyaNtHOaiiy6lJUlEyqCBJxeCLL+Dcc2HDBqhXD959Fw49NNxVSZIkqUTISoMlrwfdEzbMyDuemAIH9YD6vSH5sPDVJ0mSJJUSGdkZzFk7Z2cYYfbq2Xy35ju2ZW3b5dq46DgOr3Y4R1U/iqOqH8XRNY7myJQj7aAgScXEoIIkFbHRo+HSSyEjA1q2hAkTguUdJEmStB8L5cLaT4LuCUtHQ8724HhUDNQ6O+ieUPNMiI4Lb52SJElSCbUlYwvfrP4m39INP6z7gezc7F2uLRtXlmbVm+0MJRxV4ygOr3o4CbEJYahckgQGFSSpyIRC8O9/w623BvsdO8Lw4VCuXHjrkiRJUhilL4NFLwcBha2/5B1POgwaXA71LoEyplolSZKk31ubvpavV32dL5Tw88afd3ttpTKVdnZI+C2U0LBSQ2KiY4q5aknSXzGoIElFIDsbbrgBnn022L/+ehg4EGKcC0uSJO1/cjJg+Xj4ZSismgKEguOxFaBe96B7QuVWEBUV1jIlSZKkcAuFQixNXcrXq79m9qrZO0MJK7as2O31tZNq51u64agaR1EnqQ5Rzq0lqcQzqCBJhSw9HS68ECZODL5rHjgQbrwx3FVJkiSp2G36BhYOhcXDIXNj3vFqJ0ODy6BOF4gtG67qJEmSpBJhedpynp71NF+u/JKvV3/Nxu0bd3tdo8qN8i3dcFT1o6harmoxVytJKiwGFSSpEK1eDWefDV99BYmJwVIP550X7qokSZJUbDI2wuIRQfeETV/nHS9bGw7qBfV7QYUG4apOkiRJKjFyQ7m88NUL3Dr1VrZkbtl5PDY6lsOrHp5v6YamKU2pkFAhjNVKkgqbQQVJKiQ//ggdOsCSJVClCkyYAMccE+6qJEmSVORyc2D1+0E4YflbkJsZHI+Oh9qdoX5vqN4WXBNXkiRJAmDBxgVcOeFKPlr8EQDH1D6Gy4+6nKNrHM3hVQ8nITYhvAVKkoqcQQVJKgQffgidO0NqKjRsCO++Cw38QzlJkqTItvUXWPgSLBoG25bnHa94FNS/DOpdBAmVwlaeJEmSVNJk52YzaMYg7vrwLnZk76BsXFkePvVh+rTqQ4zBXknarxhUkKR99NprcNllkJUFxx8P48dD5crhrkqSJElFInsbLBsLC4fC2o/yjsdXhHqXBN0TKh0VtvIkSZKkkur7Nd9z+duX87+V/wOgbf22/Pfs/3JQxYPCXJkkKRwMKkjSXgqF4MEH4e67g/3zz4eXX4bExPDWJUmSpEIWCsGGWcHSDktGQVbaryeioMYZQfeE2udCjBNBSZIk6Y8yczJ5+NOHefjTh8nKzSI5IZmB7QbSu1lvoqKiwl2eJClMDCpI0l7IyoKrr4ahQ4P9fv1gwACIjg5vXZIkSSpEWxfDkpGw+DVI/THvePn6QeeEg3pCuTphK0+SJEkq6WatmMVl4y/jh3U/ANDxkI48e9az1KxQM8yVSZLCzaCCJBVQWhp07QpTpwbBhGeeCUILkiRJigA71sHS0bBkBKz7PO94TBk4sFvQPaHaCRBlQlWSJEn6M9uytnHXB3cxaOYgckO5VCtXjafPfJqujbvaRUGSBBhUkKQCWb4cOnSA77+HcuXgjTeCfUmSJJViWVth+fggnLBqCoSyfz0RBSmnQr3uUKcrxCeHtUxJkiSpNPhw0YdcMeEKftn0CwCXHnkp/2n3HyqXrRzmyiRJJYlBBUnaQ998A2edBStXQvXqMGkSHH10uKuSJEnSXsnNglXvweIRQUghZ1veuUotoN5FcOAFUNaWtJIkSdKeSN2Ryq1Tb+WF2S8AUCepDs+f/TxnNjwzzJVJkkoigwqStAfeey9Y7mHrVjj8cHjnHTjwwHBXJUmSpAIJ5QbLOSweActGQ8aGvHPlD4Z6FwfdE5IOCV+NkiRJUik0Yd4Erp50NSu3rATgmhbX8EjbR0hKSApzZZKkksqggiT9jRdfhKuvhpwcOPVUGDsWDjgg3FVJkiRpj236LljWYfFI2LY073hidah7YdA9oVILcK1cSZIkqUDWpa/j+snXM2rOKAAaVmrIi+e+yIl1TwxzZZKkki463AVIUkkVCsEdd8CVVwYhhR494N13DSlIkiSVClsXww8DYFITeLcp/PhoEFKIrQD1e8GpU6HTcmj+H6jc0pCCJGm/8cwzz1CvXj0SExNp3bo1s2bN+svrN2/ezLXXXkuNGjVISEigUaNGvPPOO8VUraSSKhQKMeL7ERz2zGGMmjOK6Kho+h3Xj2+v/taQgiRpj9hRQZJ2IyMDLrsMRowI9u+5J9j8/lqSJKkE27Eelr4RdE9Y93ne8eh4qHlWsLRDzQ4QWyZ8NUqSFEavv/46ffv2ZfDgwbRu3ZpBgwbRrl075s2bR7Vq1Xa5PjMzk9NPP51q1aoxZswYatWqxZIlSzjAv+KQ9mvL05ZzzaRrmDh/IgBHphzJkHOH0KJmizBXJkkqTQwqSNIfbNwInTvDJ59AbCy88AL06hXuqiRJkrRbWVthxduweDismgKh7F9PREHKKcGyDnW6QPwB4axSkqQSYeDAgVx55ZX07t0bgMGDBzNp0iSGDh3K7bffvsv1Q4cOZePGjXzxxRfExcUBUK9eveIsWVIJkhvK5YWvXuDWqbeyJXML8THx3HXiXdx2/G3ExcSFuzxJUiljUEGSfmfRIujQAebOhaQkGDsW2rYNd1WSJEnKJzcrCCUsHg7Lx0POtrxzlZpD3Yug7oVQtmb4apQkqYTJzMzkq6++on///juPRUdH07ZtW6ZPn77be95++22OPfZYrr32WsaPH0/VqlW56KKLuO2224iJiSmu0iWVAAs2LuDKCVfy0eKPADim9jEMOXcIjas2Dm9hkqRSy6CCJP3qf/+Ds8+GtWuhdm145x1o0iTcVUmSJAmAUC6s+yJY1mHpG5CxIe9c+QbBsg51u0PyoeGrUZKkEmz9+vXk5OSQkpKS73hKSgpz587d7T2//PILH3zwARdffDHvvPMOCxYs4B//+AdZWVncc889u70nIyODjIyMnftpaWmFNwhJxS47N5tBMwZx14d3sSN7B2XjyvLwqQ/Tp1UfYqINLEmS9p5BBUkCxo+H7t1h+3Zo1gwmTYKa/gGeJElS+G3+HhaPgCUjIX1J3vHElKBrQt2LoHJLiIoKX42SJEWo3NxcqlWrxn//+19iYmJo3rw5K1as4PHHH//ToMKAAQO47777irlSSUXh+zXfc/nbl/O/lf8D4LSDTuO/5/yX+hXrh7kySVIkMKggab/31FNwww0QCkH79vDGG1ChQrirkiRJ2o+lL4HFI4PuCZu/zzseWwEO7BKEE1JOgWj/k1aSpD1VpUoVYmJiWLNmTb7ja9asoXr16ru9p0aNGsTFxeVb5uGwww5j9erVZGZmEh8fv8s9/fv3p2/fvjv309LSqFOnTiGNQlJxyMjO4OFPH+bhzx4mOzeb5IRkBrYbSO9mvYkyICxJKiR+qyNpv5WbC7fcAv/5T7B/1VXwzDMQ6/8zSpIkFb8d62HZaFg8HNZ9nnc8Oh5qngX1LgpeY8uEr0ZJkkqx+Ph4mjdvzrRp0+jUqRMQdEyYNm0affr02e09xx9/PCNGjCA3N5fo6GgA5s+fT40aNXYbUgBISEggISGhSMYgqejNXD6Ty9++nB/W/QBAx0M68uxZz1Kzgu1nJUmFy5/jJO2Xtm+HSy6BceOC/QED4Lbb7BgsSZJUrLLTYfn4YGmHVe9BKPvXE1GQcjLUuxjqnAfxFcNZpSRJEaNv37707NmTFi1a0KpVKwYNGkR6ejq9e/cGoEePHtSqVYsBAwYAcM011/D0009zww03cN111/Hzzz/z8MMPc/3114dzGJKKwLasbdz1wV0MmjmI3FAuVctW5ekOT9OtcTe7KEiSioRBBUn7nXXr4NxzYcYMiI+HYcOge/dwVyVJkrQfydkBs/vCLy9Dzra84xWPDjon1L0QytYKX32SJEWoCy64gHXr1nH33XezevVqmjVrxuTJk0lJSQFg6dKlOzsnANSpU4f33nuPm266iSOPPJJatWpxww03cNttt4VrCJKKwIeLPuSKCVfwy6ZfALjkyEsY1G4QlctWDnNlkqRIFhUKhULhLqIwpKWlkZycTGpqKklJSeEuR1IJNX8+dOgACxdCxYrw1ltw4onhrkqSVFCRPveL9PFpP5eVBh93hLUfBfvlG/waTugOyYeFtTRJksIh0ud+kT4+qTRL3ZHKrVNv5YXZLwBQO6k2z5/9PB0adghzZZKk0qogcz87KkjaLyxfHnRO+M9/YONGOOggeOcdOPTQcFcmSZK0H9m+Bj46EzZ9DbHl4fhRULOD629JkiRJxWzCvAlcPelqVm5ZCcA1La7hkbaPkJRgoEiSVDwMKkiKWFlZMHEiDBkC774LubnB8Vat4O234deuhpIkSSoOWxfBB2fA1gWQUBVOeRcqNQ93VZIkSdJ+ZV36Oq6ffD2j5owCoGGlhrx47oucWNe2s5Kk4mVQQVLEmTcvCCe8/DKsXZt3/MQT4Yor4PzzISEhfPVJkiTtdzZ9Bx+2gx2roVw9OGUKJDUMd1WSJEnSfiMUCvH6D6/T550+bNi+geioaG459hbuPfleysSVCXd5kqT9kEEFSREhPR3GjAkCCp9+mnc8JQV69YLLLoNGjcJWniRJ0v5r7afw8TmQlQrJR8Ap70HZmuGuSpIkSdpvrN66mn9M+gdvzn0TgCbVmjC041Ba1GwR5sokSfszgwqSSq1QCL76Cl58EUaOhLS04Hh0NHToEHRP6NAB4uLCW6ckSdJ+a/kE+Px8yNkBVY+HkyZAfMVwVyVJkiTtF0KhECPnjOS6d69j4/aNxEbHcucJd9L/hP7Ex8SHuzxJ0n7OoIKkUmfjRhg+PAgofPdd3vEGDYLOCT17Qq1a4atPkiRJwC/DYOYVEMqBmmdDm9chtmy4q5IkSZL2C6u3ruaaSdfw1ty3AGhWvRnDOg6jafWm4S1MkqRfGVSQVCrk5sJHHwXhhHHjICMjOJ6QAF27wuWXw0knBd0UJEmSFGY/Pg7f9AveH9QDWr8I0ba5kiRJkopaKBRixPcjuO7d69i0YxNx0XHcdeJd3N7mduJinJNLkkoOgwqSSrQVK2DYMBgyBBYtyjvetClceSVcdBFUtHuwJElSyRAKwTe3wU+PB/uH3gxHPQZRpkklSZKkorZqyyqunnQ1b897G4Cjqh/FsE7DODLlyDBXJknSrgwqSCpxsrJg0qSge8K77wbdFACSkuDii4PuCUcfDVFR4a1TkiRJv5ObDbOuDJZ8AGj2GDS+NawlSZIkSfuDUCjE8O+Hc/271+/sonD3SXdz2/G32UVBklRiGVSQVGLMmxd0TnjlFVizJu/4iSfCFVdAly5Q1mWNJUmSSp7s7fD5BbBiQtA9odWL0KB3uKuSJEmSIt7KLSu5euLVTJg/AYCjaxzNsI7DaJLSJMyVSZL01wwqSAqr9HQYMyYIKHz6ad7xlBTo1QsuuwwaNQpbeZIkSfo7mZvh43Ng3WcQkwjHvw61zw13VZIkSVJEC4VCvPrdq9ww+QY279hMXHQc95x0D/2O72cXBUlSqWBQQVKxC4Xgq6+CpR1GjoS0tOB4dDR06BB0T+jQAeKcT0uSJJVs21fBh+1g8/cQlwQnTYBqJ4a7KkmSJCmirdyykv+b+H9MnD8RgOY1mjOs0zCOqHZEmCuTJGnPGVSQVGw2boThw4PuCd9+m3e8fn24/HLo2RNq1QpffZIkSSqALQvgg9MhfTEkpsAp70HFpuGuSpIkSYpYoVCIV759hRvfu5HNOzYTHxPPvSfdy63H30pstD/3SJJKF//NJalI5ebCRx8F3RPGjYOMjOB4QgJ06RJ0TzjppKCbgiRJkkqJjV/DR+1hx1oo3wBOnQLl64e7KkmSJClirUhbwVUTr+Kdn98BoEXNFrzU8SW7KEiSSi2DCpKKxIoVMGwYDB0Kv/ySd7xp0yCccPHFULFi2MqTJEnS3lrzEXx8LmRvgYrN4OR3oUz1cFclSZIkRaRQKMTL377MjZNvJDUjlfiYeO47+T5uOe4WuyhIkko1/y0mqVBt3Qr33Qf/+Q/k5ATHkpLgoouCgMLRR0NUVHhrlCRJ0l5aNg4+7w65mVDtJDhxPMQnh7sqSZIkKSItT1vOVROu4t0F7wLQsmZLhnUaRuOqjcNcmSRJ+86ggqRCM348XHcdLFsW7LdpA1deCV27Qtmy4a1NkiRJ+2jBC/C/qyGUC7U7wfEjISYx3FVJkiRJEScUCvHSNy9x03s3kZaRRnxMPPeffD83H3ezXRQkSRHDf6NJ2mdLl8L11wdBBYB69eCZZ6BDh7CWJUmSpMIQCsGPA+DbO4L9BpdDy8HgF6SSJElSoVuWuoyrJl7F5AWTAWhVqxUvdXzJLgqSpIjjN0uS9lpWFjzxBNxzD2zbBrGxcOutcOeddlCQJEmKCKFcmN0X5j0R7DfuD00fci0vSZIkqZCFQiGGfj2UvlP6kpaRRkJMAg+c8gA3HXuTXRQkSREpem9ueuaZZ6hXrx6JiYm0bt2aWbNm/em1WVlZ3H///TRo0IDExESaNm3K5MmT813zySefcM4551CzZk2ioqJ466239qYsScVoxgxo0SIIJmzbFizz8M038PDDhhQkSZIiQm4WTO+RF1I4eiA0e9iQgiRJklTIlqYupf3w9lwx4QrSMtJoXas1X//f19x6/K2GFCRJEavAQYXXX3+dvn37cs899zB79myaNm1Ku3btWLt27W6vv/POO3n++ed56qmn+PHHH7n66qvp3LkzX3/99c5r0tPTadq0Kc8888zej0RSsdi0Ca6+Go47Dr77DipVgiFD4OOP4fDDw12dJEmSCkV2OnzcERYPh6hYOPZVOPSmcFclSZIkRZRQKMSLs1/kiGePYMrCKSTEJPD46Y/z+WWfc1jVw8JdniRJRSoqFAqFCnJD69atadmyJU8//TQAubm51KlTh+uuu47bb799l+tr1qzJHXfcwbXXXrvzWJcuXShTpgyvvfbargVFRfHmm2/SqVOnAg0kLS2N5ORkUlNTSUpKKtC9kv5eKAQjRkDfvvBbLqlXL3j8cahSJaylSZL2Q5E+94v08amEy9gIH50FG2ZATBloMwZqdQh3VZIkRaxIn/tF+vikvbU0dSlXvH0FU3+ZCsCxtY9laMehHFrl0DBXJknS3ivI3K9APYMyMzP56quv6N+//85j0dHRtG3blunTp+/2noyMDBITE/MdK1OmDJ999llBPnq3z83IyNi5n5aWtk/Pk/Tn5s+Hf/wDpk0L9g87DJ57Dk46Kbx1SZIkqZBtWw4ftoPUHyG+Ipw0CaoeG+6qJEmSpIgRCoV4YfYL3DLlFrZkbiExNpEHT3mQG4+5kZjomHCXJ0lSsSnQ0g/r168nJyeHlJSUfMdTUlJYvXr1bu9p164dAwcO5OeffyY3N5epU6cybtw4Vq1atfdVAwMGDCA5OXnnVqdOnX16nqRd7dgB990HTZoEIYXERHjoIfjmG0MKkiRJESd1Lkw5LggplKkJbT81pCBJkiQVoiWbl3DGa2fwfxP/jy2ZWziuznF883/fcPNxNxtSkCTtdwoUVNgbTzzxBA0bNuTQQw8lPj6ePn360Lt3b6Kj9+2j+/fvT2pq6s5t2bJlhVSxJID334cjj4R774XMTGjfHn74Af75T4iPD3d1kiRJKlQb/gfvt4Fty6BCIzjjCzjg8HBXJUmSJEWEUCjE818+zxHPHcH7v7xPYmwiA88YyCe9PuGQKoeEuzxJksKiQEs/VKlShZiYGNasWZPv+Jo1a6hevfpu76latSpvvfUWO3bsYMOGDdSsWZPbb7+d+vXr733VQEJCAgkJCfv0DEm7WrMGbr4Zhg8P9mvUgCeegK5dISoqvLVJkiSpCKyaCp92hux0qNQCTn4HEquGuypJkiQpIizevJgr3r6CaYuCdXWPr3M8QzsOpVHlRmGuTJKk8CpQW4P4+HiaN2/OtN8Wqgdyc3OZNm0axx771y1BExMTqVWrFtnZ2YwdO5aOHTvuXcWSikRuLjz/PBx6aBBSiIqC666Dn36Cbt0MKUiSJEWkJW/Ax2cFIYWU0+C0DwwpSJIkSYUgN5TL4C8H0+S5JkxbNI0ysWX4T7v/8HGvjw0pSJJEATsqAPTt25eePXvSokULWrVqxaBBg0hPT6d3794A9OjRg1q1ajFgwAAAZs6cyYoVK2jWrBkrVqzg3nvvJTc3l379+u185tatW1mwYMHO/UWLFvHNN99QqVIlDjzwwH0do6S/8e23cPXVMGNGsH/00UFooUWL8NYlSZKkIjT/WfiyDxCCA7vBsa9CjF3rJEmSpH21aNMirphwBR8s+gCANge2Yei5Q2lYuWGYK5MkqeQocFDhggsuYN26ddx9992sXr2aZs2aMXnyZFJSUgBYunQp0dF5jRp27NjBnXfeyS+//EL58uXp0KEDr776KgcccMDOa7788ktOOeWUnft9+/YFoGfPngwbNmwvhybp72zdCvfeC4MGQU4OVKgADz4I114LMTHhrk6SJElFIhSC7++DOfcF+w2vgeZPQbQTQEmSJGlf/NZFod/UfqRnpVMmtgyPtH2EPq36EB1VoAbXkiRFvKhQKBQKdxGFIS0tjeTkZFJTU0lKSgp3OVKJN358sLTDsmXBfteuQWChVq2wliVJ0h6J9LlfpI9PYZSbA19dDz8/G+wfcQ80ucd1viRJCqNIn/tF+vik3yzatIjL376cDxd/CMCJdU9kyLlDOLjSwWGuTJKk4lOQuV+BOypIKt2WLoXrrw+CCgD16sEzz0CHDmEtS5IkSUUtJwOm94ClbwBR0PxJOKRPuKuSJEmSSrVQKMTgLwdz69RbSc9Kp2xcWR457RGubXWtXRQkSfoLBhWk/URWFjzxBNxzD2zbBrGxcOutcOedULZsuKuTJElSkcraAp+eB6vfh+g4OOYVqHdhuKuSJEmSSrUtGVu4YsIVvPHDGwCcVPckhpw7hAaVGoS5MkmSSj6DCtJ+YMYM+L//g+++C/bbtIHBg+Hww8NblyRJkorBjnXw0Vmw8X8QWw5OGAc1zgh3VZIkSVKp9tO6n+jyRhd+Wv8TsdGxPH7641zf+nq7KEiStIcMKkgRbNMm6N8f/vtfCIWgUiV4/HHo1QuinS9LkiRFvvQl8GE7SJsHCZXhpHegSqtwVyVJkiSVamN+HEPv8b3ZmrmVmhVqMrrbaI6rc1y4y5IkqVQxqCBFoFAIRoyAvn1h7drgWK9eQUihSpWwliZJkqTisvmHIKSwfQWUrQOnTIHkQ8NdlSRJklRqZeVkcfv7tzNwxkAATq53MqO6jCKlfEqYK5MkqfQxqCBFmPnz4R//gGnTgv3DDoPnnoOTTgpvXZIkSSpG66bDx2dB5iZIOgxOnQJla4e7KkmSJKnUWrVlFReMuYBPl34KQL/j+vHQaQ8RG+3PLJIk7Q3/DSpFiB074JFHYMAAyMyExES46y645RaIjw93dZIkSSo2K9+FT7tCzjao3BpOnhQs+yBJkiRpr3y65FPOH3M+q7eupkJ8BV7u9DKdD+sc7rIkSSrVDCpIEeD994MuCj//HOy3awfPPAMNGoS3LkmSJBWzRcNhRi8IZUONdnDCWIgtF+6qJEmSpFIpFAoxaMYgbp16KzmhHI6odgRjzx9Lo8qNwl2aJEmlnkEFqRRbswZuvhmGDw/2a9SAQYOgWzeIigpraZIkSSpuc5+A2TcG7+teBMe8BDG21pIkSZL2xpaMLVz+9uWM/nE0ABc1uYj/nv1fysUbBJYkqTAYVJBKodxceOEFuP122Lw5CCX06QMPPADJyeGuTpIkScUqFILv7oQfHg72G10Pzf8DUdHhrUuSJEkqpX5a9xPnvXEec9fPJS46joHtBnJty2uJ8q/DJEkqNAYVpFLm22/h6qthxoxg/+ij4fnnoUWL8NYlSZKkMMjNhv9dAwtfDPaPfBAO/6fttSRJkqS99MYPb3DZ+MtIz0qnVoVajO42mmPrHBvusiRJijgGFaRSIhSCe+6Bhx+GnByoUAEefBCuvRZiYsJdnSRJkopdzg74/CJY/mbQPaHlc3DwVeGuSpIkSSqVsnKy6De1H4NmDgLglHqnMKrrKKqVqxbewiRJilD2ApVKif/+N1jaIScHunaFn36C6683pCBJ0r565plnqFevHomJibRu3ZpZs2b96bUnn3wyUVFRu2xnnXXWzmtCoRB33303NWrUoEyZMrRt25aff/65OIai/Un2dvikcxBSiI6H498wpCBJkiTtpVVbVnHqK6fuDCncfvztTLl0iiEFSZKKkEEFqRT46qsglADwyCMwejTUqhXemiRJigSvv/46ffv25Z577mH27Nk0bdqUdu3asXbt2t1eP27cOFatWrVzmzNnDjExMXTr1m3nNY899hhPPvkkgwcPZubMmZQrV4527dqxY8eO4hqWIl32NvjkXFg1GWLKwsnvwIFdwl2VJEmSVCp9suQTjnr+KD5b+hlJCUm8ecGbDGg7gNhoG1JLklSUDCpIJdzGjUEHhcxM6NQJ+vULd0WSJEWOgQMHcuWVV9K7d28aN27M4MGDKVu2LEOHDt3t9ZUqVaJ69eo7t6lTp1K2bNmdQYVQKMSgQYO488476dixI0ceeSSvvPIKK1eu5K233irGkSliZafDx2fD6vchthyc8i5UPy3cVUmSJEmlTigU4t9f/JtTXz6VNelrOKLaEXx55Zd0OrRTuEuTJGm/YFBBKsFyc6FnT1i8GOrXh5degqiocFclSVJkyMzM5KuvvqJt27Y7j0VHR9O2bVumT5++R88YMmQIF154IeXKlQNg0aJFrF69Ot8zk5OTad269R4/U/pTWVvgwzNhzYcQWwFOeQ+qnRjuqiRJkqRSZ0vGFs4fcz63TL2FnFAOlxx5CTMun0HDyg3DXZokSfsNexdJJdhjj8HEiZCQAGPGwAEHhLsiSZIix/r168nJySElJSXf8ZSUFObOnfu398+aNYs5c+YwZMiQncdWr1698xl/fOZv53YnIyODjIyMnftpaWl7NAbtR7LSgpDC+i8gLikIKVQ5JtxVSZIkSaXOj+t+5LzXz2PehnnERccxqP0grmlxDVH+hZgkScXKjgpSCfXRR3DHHcH7p5+Go44KazmSJOkPhgwZQpMmTWjVqtU+P2vAgAEkJyfv3OrUqVMIFSpiZG6GD07/NaRwAJw6zZCCJEmStBden/M6rV5oxbwN86idVJtPen/CP1r+w5CCJElhYFBBKoFWrYILL8xb+uHyy8NdkSRJkadKlSrExMSwZs2afMfXrFlD9erV//Le9PR0Ro0axeV/+Jf0b/cV9Jn9+/cnNTV157Zs2bKCDEWRLGMjfNAWNsyC+Epw2gdQuUW4q5IkSZJKlaycLG6cfCMXjr2Q9Kx0Tj3oVGZfNZtjahsAliQpXAwqSCVMdnYQUlizBpo0gWefBQO9kiQVvvj4eJo3b860adN2HsvNzWXatGkce+yxf3nv6NGjycjI4JJLLsl3/KCDDqJ69er5npmWlsbMmTP/8pkJCQkkJSXl2yR2rIcPToONX0FClSCkUMk2W5IkSVJBrNyyklNePoUnZj4BQP82/ZlyyRSqlqsa5sokSdq/xYa7AEn53XEHfPIJVKgAY8ZA2bLhrkiSpMjVt29fevbsSYsWLWjVqhWDBg0iPT2d3r17A9CjRw9q1arFgAED8t03ZMgQOnXqROXKlfMdj4qK4sYbb+TBBx+kYcOGHHTQQdx1113UrFmTTp06FdewFAl2rA06KWz+HhKrBcs9HHBEuKuSJEmSSpWPF3/MBWMuYE36GpISknil0yt0PLRjuMuSJEkYVJBKlPHj4bHHgvdDh0KjRuGtR5KkSHfBBRewbt067r77blavXk2zZs2YPHkyKSkpACxdupTo6PxNyObNm8dnn33GlClTdvvMfv36kZ6ezlVXXcXmzZtp06YNkydPJjExscjHowixfXXQSSH1R0isHnRSSD4s3FVJkiRJpUYoFOLf0//N7e/fTk4ohyNTjmTs+WM5uNLB4S5NkiT9KioUCoXCXURhSEtLIzk5mdTUVFvlqlT65Rc4+mhITYUbb4T//CfcFUmSVHJF+twv0senv7BtJXxwKqTNgzK1gpBCkulVSZIiWaTP/SJ9fCp50jLSuGz8ZYz9aSwAlx55KYPPHkzZOFvXSpJU1Aoy97OjglQC7NgBXbsGIYVjj4VHHw13RZIkSSp225bDtFNhy89Qtg6c9iFUaBDuqiRJkqRS44e1P9DljS7M2zCPuOg4nmj/BFe3uJqoqKhwlyZJkv7AoIJUAtxwA3z9NVSpAq+/DvHx4a5IkiRJxSp9SRBS2PoLlKsXdFIof1C4q5IkSZJKjVFzRnH525ezLWsbtZNqM6bbGFrXbh3usiRJ0p8wqCCF2SuvwH//C1FRMHw41KkT7ookSZJUrLYugmmnBGGF8vWDTgrlDgx3VZIkSVKpkJmTya1TbuXJWU8C0LZ+W0acN4Kq5aqGuTJJkvRXDCpIYfT993D11cH7e+6BM84Ibz2SJEkqZlsWBJ0Uti2D8gdD2w+hbO1wVyVJkiSVCivSVnD+mPP5YtkXANxxwh3cd/J9xETHhLkySZL0dwwqSGGSlgZdusD27dCuHdx1V7grkiRJUrFKmx90Uti+EpIOgVM/gLI1w12VJEmSVCp8tPgjLhhzAWvT15KckMyrnV/lnEPOCXdZkiRpDxlUkMIgFIIrroCff4bateG11yA6OtxVSZIkqdik/hR0UtixGpIbw6nToEz1cFclSZIklXihUIh/ffEv+k/rT04ohyNTjmTs+WM5uNLB4S5NkiQVgEEFKQyeegpGj4bY2OC1SpVwVyRJkqRis3kOfHAa7FgLBzSBU9+HxGrhrkqSJEkq8dIy0ug9vjfjfhoHQI+mPXjurOcoG1c2zJVJkqSCMqggFbPp0+Hmm4P3//43HHNMeOuRJElSMdr0XRBSyFgPFZvBKVMh0dSqJEmS9Hd+WPsD571xHvM3zCc+Jp4n2z/JVc2vIioqKtylSZKkvWBQQSpG69fD+edDdjZ06wbXXRfuiiRJklRsNn4NH7SFzI1QqTmcMgUSKoW7KkmSJKnEG/n9SK6YcAXbsrZRJ6kOY84fQ6tarcJdliRJ2gcGFaRikpMDF18My5dDo0bw4otg2FeSJGk/seFL+OB0yNoMlVvBKe9B/AHhrkqSJEkq0TJzMrllyi08NespAE6vfzojuoygSlm7kkmSVNoZVJCKyYMPwpQpUKYMjBkDSUnhrkiSJEnFYv1M+LAdZKVClWPh5HchPjncVUmSJEkl2oq0FXQb3Y3py6cDcOcJd3LvyfcSEx0T5sokSVJhMKggFYMpU+C++4L3zz8PTZqEtx5JkiQVk3Wfw4dnQvYWqHoCnDwJ4iqEuypJkiSpRPtw0YdcOPZC1qav5YDEA3i186uc3ejscJclSZIKkUEFqYgtWwYXXQShEFx1FVx6abgrkiRJUrFY+wl81AGy06HayXDyRIgtF+6qJEmSpBIrFArx+BeP039af3JDuTRNacq4C8ZRv2L9cJcmSZIKmUEFqQhlZsIFF8CGDXDUUfDEE+GuSJIkScVi9Qfw8TmQsw2qt4UTx0Ns2XBXJUmSJJVYqTtS6TW+F2/NfQuAXs168WyHZykTVya8hUmSpCJhUEEqQv36wfTpkJwMY8ZAYmK4K5IkSVKRWzUVPjkXcnZAjfZwwjiI9ctVSZIk6c8sS13Gaa+cxs8bfyY+Jp6nznyKK4++kqioqHCXJkmSiohBBamIjB6d10HhlVegvt3JJEmSIt/Kd+GTzpCbATXPghPGQIxpVUmSJOnP5IZy6TW+Fz9v/JkDkw9kTLcxtKzVMtxlSZKkImZQQSoC8+bBZZcF7/v1g3PPDW89kiRJKgbLJ8BnXSE3E2p3hOPfgJj4cFclSZIklWjPf/k8Hyz6gDKxZZjWYxoHVzo43CVJkqRiEB3uAqRIs20bdO0KW7fCiSfCQw+FuyJJkiQVuWVvwmddgpBCnS7QZrQhBUmSJOlv/LLpF26deisAj7Z91JCCJEn7EYMKUiEKheCaa2DOHEhJgVGjINa+JZIkSZFt6Rj47HzIzYIDL4DjR0J0XLirkiRJkkq03FAul42/jPSsdE6qexLXtro23CVJkqRiZFBBKkRDhsArr0B0dBBSqFEj3BVJkiSpSC0eBZ9fCKFsqHcxHPeaIQVJkiRpDzz7v2f5eMnHlIsrx9COQ4mO8ucKSZL2J/6bXyoks2dDnz7B+4cegpNPDms5kiRJKmqLXoPpF0MoB+r3gmNehmjbaUmSJEl/Z+HGhdz2/m0APHb6Y9SvWD/MFUmSpOJmUEEqBJs2QdeukJEBZ58N/fqFuyJJkiQVqYUvwfQeEMqFBldA6yEQHRPuqiRJkqQSLzeUS+/xvdmWtY1T6p3C1S2uDndJkiQpDAwqSPsoFIJevWDRIqhXD15+OVj6QZIkSRFqwQsw8zIgBA2vgVbPg21qJUmSpD3y1Myn+HTpp5SPL++SD5Ik7cecAUj76F//grffhvh4GD0aKlUKd0WSJEkqMvOfhVlXBe8bXQctnjGkIEmSSpVnnnmGevXqkZiYSOvWrZk1a9afXjts2DCioqLybYmJicVYrSLNzxt+pv+0/gD86/R/Ue+AeuEtSJIkhY3fqEn74JNPoH8wr+bJJ6FFi/DWI0mSpCI070n48trg/aF9ofkTEBUV3pokSZIK4PXXX6dv377cc889zJ49m6ZNm9KuXTvWrl37p/ckJSWxatWqnduSJUuKsWJFkpzcHHqP78327O20rd+Wq5pfFe6SJElSGBlUkPbS6tVwwQWQkwOXXAJXOa+WJEmKXD8NhK9uCN43vg2O+pchBUmSVOoMHDiQK6+8kt69e9O4cWMGDx5M2bJlGTp06J/eExUVRfXq1XduKSkpxVixIskTM5/g82WfUyG+Ai+e8yJRzqclSdqvGVSQ9kJ2NnTvHoQVDj8cBg/2e2pJkqSI9eOj8PXNwfvD74SmA5z8SZKkUiczM5OvvvqKtm3b7jwWHR1N27ZtmT59+p/et3XrVurWrUudOnXo2LEjP/zww19+TkZGBmlpafk2ad76edzxwR0A/PuMf1P3gLphrkiSJIWbQQVpL9x9N3z0EZQrB2PGBK+SJEmKQHMehG9uD943uQ+aPmBIQZIklUrr168nJydnl44IKSkprF69erf3HHLIIQwdOpTx48fz2muvkZuby3HHHcfy5cv/9HMGDBhAcnLyzq1OnTqFOg6VPjm5OfQa34sd2Ts4o8EZXHH0FeEuSZIklQAGFaQCmjgRBgwI3r/4Ihx6aHjrkSRJUhEIheC7e+G7u4L9pg9Bk7vDWZEkSVKxO/bYY+nRowfNmjXjpJNOYty4cVStWpXnn3/+T+/p378/qampO7dly5YVY8UqiQZOH8iM5TNISkhyyQdJkrRTbLgLkEqTRYvg0kuD9336wIUXhrceSZIkFYFQCL67E354ONhv9hg0vjW8NUmSJO2jKlWqEBMTw5o1a/IdX7NmDdWrV9+jZ8TFxXHUUUexYMGCP70mISGBhISEfapVkeOndT9x14dB+Pc/7f5DnWQ7bEiSpIAdFaQ9lJEB3brB5s3QqhX861/hrkiSJEmFLhSCb27LCykcPdCQgiRJigjx8fE0b96cadOm7TyWm5vLtGnTOPbYY/foGTk5OXz//ffUqFGjqMpUBMnOzabX+F5k5GRw5sFn0rtZ73CXJEmSShA7Kkh76Kab4KuvoFIlGD0aDIZLkiRFmFAIZveFeYOC/eZPwSF9wlqSJElSYerbty89e/akRYsWtGrVikGDBpGenk7v3sEPyD169KBWrVoM+HXd0/vvv59jjjmGgw8+mM2bN/P444+zZMkSrrjiinAOQ6XEv7/4N7NWzCI5IZkXznnBJR8kSVI+BhWkPTB8ODz3HERFBe8PPDDcFUmSJKlQhULw1fUw/+lgv+Vz0PDq8NYkSZJUyC644ALWrVvH3XffzerVq2nWrBmTJ08mJSUFgKVLlxIdndeEd9OmTVx55ZWsXr2aihUr0rx5c7744gsaN24criGolPhh7Q/c/dHdADzR/glqJdUKc0WSJKmkiQqFQqFwF1EY0tLSSE5OJjU1laSkpHCXowjyww/BUg/btsFdd8H994e7IkmSFOlzv0gfX4kTyoX//QMWPA9EQav/wsH+laAkSSoekT73i/TxaVfZudkcO+RYvlz5JWc1PIsJ3SfYTUGSpP1EQeZ+dlSQ/sKWLdClSxBSOO00uOeecFckSZKkQhXKhVlXwcIhQBQc8xLU7xnuqiRJkqRS67HPH+PLlV9yQOIB/Pec/xpSkCRJu2VQQfoToRBcdRXMmwc1a8KIERATE+6qJEmSVGhyc2DmZbDoFYiKhmNegYMuDndVkiRJUqn1/ZrvufejewF46synqFmhZngLkiRJJZZBBelPPPssjBoFsbHwxhtQrVq4K5IkSVKhyc2G6T1hyQiIioHjhkPdC8JdlSRJklRqZeVk0Wt8L7Jyszj3kHO5uIkhYEmS9OcMKki7MWsW3HRT8P6xx+D448NbjyRJkgpRbhZ8cQksfQOiYuH4UXBgl3BXJUmSJJVqj3z2CLNXzaZSmUo8f/bzLvkgSZL+kkEF6Q82bIBu3SArC847D268MdwVSZIkqdDkZMIX3WHZOIiOgzajoXbHcFclSZIklWrfrv6W+z+5HwiWfKhevnqYK5IkSSWdQQXpd3Jz4ZJLYOlSOPhgGDoUDP5KkiRFiNxs+Pz/27vv8CjK9Y3j9256AkmoSUgCEZDeWygiKAiIImDjAAqiolKUYgOl2cCCEFCkeKRYwUY5giAicFR6t9ClS0JPIEAC2ff3x57sjzUJkJBkUr6f68q1k9133rlnMrt5gIeZB6XD8yW7t9T8Gyn8bqtTAQAAAPlackqyHpn/iC47Lqtzlc7qWqOr1ZEAAEA+YM/KSpMmTVJUVJR8fX0VHR2tdevWZTj20qVLevXVV1WhQgX5+vqqdu3aWrx48Q3NCeSU0aOlxYslX1/p66+loCCrEwEAACDbHJ77vyYFH+nW+TQpAAAAANlg9M+jtSV2i0r4ldDkuyZzywcAAHBdMt2oMGfOHA0ePFgjR47Upk2bVLt2bbVt21bHjh1Ld/ywYcM0depUvffee/rzzz/11FNPqXPnztq8eXOW5wRywo8/SiNGOJc/+ECqXdvaPAAAAMhmh+Y6HysPkMq0szYLAAAAUABsPrpZb/z8hiRpUvtJCikSYnEiAACQX2S6UWHcuHHq3bu3evXqpWrVqmnKlCny9/fX9OnT0x3/ySef6KWXXlL79u1Vvnx59enTR+3bt9e7776b5TmB7HbkiNStm2SM9OijUq9eVicCAABAtkpJlv5e6FyO7GxtFgAAAKAASE5JVs95PXXZcVn3V7tfD1Z/0OpIAAAgH8lUo0JycrI2btyo1q1b//8Edrtat26t1atXp7tOUlKSfH193Z7z8/PTL7/8kuU5gex06ZLUpYt0/LjzKgrvv291IgAAAGS7uOXSpQTJL0wq0cjqNAAAAEC+99rK1/Tbsd9U0r+kJrWfxC0fAABApmSqUeHEiRNKSUlRSIj75ZtCQkIUGxub7jpt27bVuHHjtHv3bjkcDi1dulTffvutjh49muU5JWcDREJCgtsXkBVDhki//ioFBkpffy35+VmdCAAAANnu8P9u+xDeUbJl+sJyAAAAAK6w8e+NGvPLGEnSB+0/UOmA0hYnAgAA+U2O/w3dhAkTdPPNN6tKlSry9vZW//791atXL9ntN7bpMWPGKCgoyPUVGRmZTYlRmHz7rTRunHN55kypYkVL4wAAACAnGId0eL5zmds+AAAAADck6XKSes7rqRSToi7Vu+iB6g9YHQkAAORDmeoWKFmypDw8PBQXF+f2fFxcnEJDQ9Ndp1SpUpo3b54SExN14MAB7dixQ0WKFFH58uWzPKckDR06VPHx8a6vQ4cOZWZXAO3eLfXq5Vx+9lmpM39nDQAAUDCdWCtdjJW8gqTSLa1OAwAAAORrr658VX8c/0OlA0rr/fbcRxcAAGRNphoVvL29Vb9+fS1btsz1nMPh0LJly9SkSZOrruvr66vw8HBdvnxZ33zzjTp27HhDc/r4+CgwMNDtC7heFy5I998vJSRIzZpJY8ZYnQgAAAA5JvW2D2Xukjy8rc0CAAAA5GPrj6zXm7++KUmactcUlfQvaXEiAACQX3lmdoXBgwerZ8+eatCggRo1aqSYmBglJiaq1//+a3qPHj0UHh6uMf/7l9+1a9fqyJEjqlOnjo4cOaJRo0bJ4XDohRdeuO45gezWv7+0bZtUqpQ0Z47k5WV1IgAAAOQIY6RD/2tUiOxkaRQAAAAgP7t4+aJ6zusph3Goa42u6lyVS9QCAICsy3SjQpcuXXT8+HGNGDFCsbGxqlOnjhYvXqyQkBBJ0sGDB2W3//+FGi5evKhhw4bpr7/+UpEiRdS+fXt98sknCg4Ovu45gew0fbrzy2aTvvhCCg+3OhEAAAByTPyf0rk9kt1HCmtndRoAAAAg3xq1YpS2n9iukIAQvXfne1bHAQAA+ZzNGGOsDpEdEhISFBQUpPj4eG4DgQz99pvUqJF08aL02mvSsGFWJwIAAFlR0Gu/gr5/uer3N6Rtw5y3fWj5ndVpAAAA0ijotV9B37/CYs3hNWo2vZkcxqF5XeapY5WOVkcCAAB5UGZqP/tVXwUKkIsXpW7dnI/t2kkvvWR1IgAAAOS4w6m3feCytAAAAEBWXLh0Qb3m95LDOPRQrYdoUgAAANmCRgUUGkOGSL//LpUuLc2cKdk5+wEAAAq2xEPSqY2SzS6Fd7A6DQAAAJAvjVg+QjtO7FBYkTBNaDfB6jgAAKCA4J9qUSgsWSJN+F8NPWOGFBJibR4AAADkgsPznI8lm0m+pS2NAgAAAORHqw6t0rur35UkTeswTcX9ilucCAAAFBQ0KqDAO35ceuQR53K/flL79pbGAQAAQG5JbVSI6GRlCgAAACBfOn/pvB6Z94iMjHrW7qm7K91tdSQAAFCA0KiAAs0YqXdvKTZWqlpVeucdqxMBAAAgVySdlI6tdC5HdrI0CgAAAJAfDftpmHaf2q0yRcsopl2M1XEAAEABQ6MCCrQPP5Tmz5e8vKTPP5f8/KxOBAAAgFxxZKFkUqTgWlKR8lanAQAAAPKVXw7+opg1MZKkDzt8qGDfYEvzAACAgodGBRRYO3dKgwY5l0ePlurUsTQOAAAActPhuc7HiM7W5gAAAADymcTkRPWa30tGRo/WeVTtb+ZeugAAIPvRqIACKTlZ6t5dOn9euv12afBgqxMBAAAg11w+Lx1d4lzmtg8AAABApry07CXtObVHEYERGtd2nNVxAABAAUWjAgqkkSOljRulYsWkWbMkO2c6AABA4XH0BynlghQQJQXXtjoNAAAAkG+s3L9SE9dNlCT9u8O/FeQbZHEiAABQUPHPtyhwVq6U3nrLufzhh1JEhLV5AAAAkMsOz3M+RnSSbDYrkwAAAAD5RmJyoh5d8Kgk6fG6j6ttxbYWJwIAAAUZjQooUE6flh5+WDJG6tVLuu8+qxMBAAAgVzkuS0f+41yO7GxtFgAAACAfGfLjEP11+i9FBkbq3bbvWh0HAAAUcDQqoMAwRurTRzp0SKpQQZowwepEAAAAyHXHf5aST0k+JaWSzaxOAwAAAOQLy/ct1/vr35ckTe84XYE+gRYnAgAABR2NCigwPv1UmjNH8vCQPvtMKlrU6kQAAADIdYfmOh/D75HsHtZmAQAAAPKBc8nnXLd8eLL+k2pdvrXFiQAAQGFAowIKhH37pH79nMsjR0rR0dbmAQAAgAWMkQ7Pcy5HdLIyCQAAAJBvvLD0Be0/s1/lgsrpnTvesToOAAAoJGhUQL53+bL00EPS2bNS06bS0KFWJwIAAIAlTm+Szh+SPAOkUP4XGAAAAHAty/5apskbJkty3vKhqA+XqQUAALmDRgXke2PGSKtWOW/18Omnkqen1YkAAABgiUPznI9h7SRPP0ujAAAAAHldQlKC65YPfRv01e033W5xIgAAUJjQqIB8bc0a6ZVXnMuTJkk33WRtHgAAAFjo8FznY0Rna3MAAAAA+cDzPzyvg/EHdVPwTXrrjresjgMAAAoZGhWQb50967zlQ0qK9K9/OZcBAABQSCXsluL/kGyeUnh7q9MAAAAAedoPe3/QtE3TJDlv+VDEu4jFiQAAQGFDowLyrQEDpL17pchIafJkyWazOhEAAAAsc3ie8zHkNsm7mKVRAAAAgLws/mK8Hl/wuCTp6UZPq2VUS2sDAQCAQolGBeRLX38tzZjhbE745BMpONjqRAAAALBUaqNCRCcrUwAAAAB53rM/PKtDCYdUoVgFjWk1xuo4AACgkKJRAfnO4cPSE084l198UWrRwto8AAAAsNiFWOnEaudyREdrswAAAAB52OI9i/XR5o9kk00zOs5QgHeA1ZEAAEAhRaMC8hWHQ+rZUzp9WqpfX3rlFasTAQAAwHJHFkgyUolGkn+41WkAAACAPOnMxTOuWz48E/2MmpdrbnEiAABQmNGogHxl3Djpp58kf3/ps88kb2+rEwEAAMByh+Y6HyM6W5sDAAAAyMMGLRmkI2ePqGLxihrdarTVcQAAQCFHowLyjS1bpJdeci6PHy9VrmxpHAAAAOQFyfFS3DLnckQnS6MAAAAAedXCXQs1c8tM2WTTzI4z5e/lb3UkAABQyNGogHzh/HmpWzfp0iWpY0epd2+rEwEAgIJi0qRJioqKkq+vr6Kjo7Vu3bqrjj9z5oz69eunsLAw+fj4qFKlSlq0aJHr9VGjRslms7l9ValSJad3o/D6+3vJcUkKrCIFcZwBAACAfzp94bR6/8f5F6qDGg9Ss7LNLE4EAAAgeVodALgeL7wgbd8uhYZKH34o2WxWJwIAAAXBnDlzNHjwYE2ZMkXR0dGKiYlR27ZttXPnTpUuXTrN+OTkZN1xxx0qXbq0vv76a4WHh+vAgQMKDg52G1e9enX9+OOPru89PSm7c8zh1Ns+dLI0BgAAAJBXDVwyUEfPHVWlEpX0+u2vWx0HAABAEo0KyAcWLpQmTXIuz5wplSplaRwAAFCAjBs3Tr1791avXr0kSVOmTNHChQs1ffp0DRkyJM346dOn69SpU1q1apW8vLwkSVFRUWnGeXp6KjQ0NEezQ1JKkvT3/65mEdHZ2iwAAABAHrRg5wJ9vPVj2W12zew4U35eflZHAgAAkMStH5DHxcVJjz7qXB4wQGrb1to8AACg4EhOTtbGjRvVunVr13N2u12tW7fW6tWr011nwYIFatKkifr166eQkBDVqFFDo0ePVkpKitu43bt3q0yZMipfvry6d++ugwcPXjVLUlKSEhIS3L5wHWKXSZfPSX5lpBINrE4DAAAA5CmnLpzSk989KUl6tsmzahLZxOJEAAAA/49GBeRZxjibFI4dk2rUkN580+pEAACgIDlx4oRSUlIUEhLi9nxISIhiY2PTXeevv/7S119/rZSUFC1atEjDhw/Xu+++q9df///Lp0ZHR2vmzJlavHixJk+erH379ql58+Y6e/ZshlnGjBmjoKAg11dkZGT27GRBd3ie8zGik2TjjzYAAADAlZ75/hnFnotVlZJV9Optr1odBwAAwA23fkCeNXmytGiR5OMjff655OtrdSIAAFDYORwOlS5dWtOmTZOHh4fq16+vI0eO6J133tHIkSMlSXfeeadrfK1atRQdHa1y5crpyy+/1GOPPZbuvEOHDtXgwYNd3yckJNCscC2OFOnIfOdyRCdLowAAAAB5zbwd8/TZb5/JbrNrVqdZ8vXkL1cBAEDeQqMC8qQ//5Sefda5/NZbUs2a1uYBAAAFT8mSJeXh4aG4uDi35+Pi4hQaGpruOmFhYfLy8pKHh4fruapVqyo2NlbJycny9vZOs05wcLAqVaqkPXv2ZJjFx8dHPj4+WdyTQurkGuniMckrSAppaXUaAAAAIM84cf6E65YPLzR9QY3CG1mcCAAAIC2uj4o8JylJ6t5dunhRatNGevppqxMBAICCyNvbW/Xr19eyZctczzkcDi1btkxNmqR/79ZmzZppz549cjgcrud27dqlsLCwdJsUJOncuXPau3evwsLCsncHCrtDc52P4XdLdi9rswAAAAB5yNPfP61jicdUrVQ1jWo5yuo4AAAA6aJRAXnOsGHSli1SiRLSzJmSnbMUAADkkMGDB+vDDz/UrFmztH37dvXp00eJiYnq1auXJKlHjx4aOnSoa3yfPn106tQpDRgwQLt27dLChQs1evRo9evXzzXmueee08qVK7V//36tWrVKnTt3loeHh7p27Zrr+1dgGSMdnudcjuhsaRQAAAAgL/nmz280+/fZ8rB5aGbHmfLx5MptAAAgb+LWD8hTli2Txo51Ln/0kcR/PAQAADmpS5cuOn78uEaMGKHY2FjVqVNHixcvVkhIiCTp4MGDsl/RNRkZGaklS5Zo0KBBqlWrlsLDwzVgwAC9+OKLrjGHDx9W165ddfLkSZUqVUq33HKL1qxZo1KlSuX6/hVY8b9L5/ZKHr5SmXZWpwEAAADyhOOJx9VnYR9J0pBbhqhheEOLEwEAAGSMRgXkGadOST17OpefeELq2NHaPAAAoHDo37+/+vfvn+5rK1asSPNckyZNtGbNmgznmz17dnZFQ0YOzXM+ht4heQZYGgUAAADIK/ot6qfj54+rRukaGn7rcKvjAAAAXBUX1UeeYIyzOeHIEalSJWncOKsTAQAAIM86PNf5yG0fAAAAAEnSl398qa/+/EoeNg/N6jSLWz4AAIA8j0YF5AkzZ0rffCN5ekqffSYF8B/jAAAAkJ7EA9LpzZLNLoXfbXUaAAAAwHJx5+LUd2FfSdJLzV9SvbB6FicCAAC4NhoVYLk9e6RnnnEuv/qq1KCBtXkAAACQh6Xe9qFUc8m3lKVRAAAAAKsZY9R3UV+dvHBStUJqaditw6yOBAAAcF1oVIClLl2SHnpIOndOuvVW6YUXrE4EAACAPO3wPOdjRCcrUwAAAAB5wpw/5ujb7d/K0+6pWZ1mydvD2+pIAAAA14VGBVjq9deltWuloCDpk08kDw+rEwEAACDPunhCOv5f5zKNCgAAACjkYs/Fqt+ifpKkYc2HqU5oHWsDAQAAZAKNCrDMr786GxUkacoUqWxZa/MAAAAgj/v7O8k4pGJ1pCJRVqcBAAAALGOM0VPfPaVTF06pTmgdvdT8JasjAQAAZAqNCrBEQoLzlg8Oh/PxX/+yOhEAAADyvENznY8Rna3NAQAAAFhs0e5Fmr9zvrzsXprVaZa8PLysjgQAAJApNCrAEv37S/v3S1FR0vvvW50GAAAAed7lRCn2B+cyt30AAABAIffu6nclSc9EP6NaIbUsTgMAAJB5NCog182eLX3yiWS3S59+KgUFWZ0IAAAAed7RJVLKRalIeSm4ptVpAAAAAMtsjd2q5fuXy8PmoWein7E6DgAAQJbQqIBcdfCg9NRTzuWXX5aaNbM2DwAAAPKJQ/OcjxGdJJvNyiQAAACApSasnSBJuq/afSobVNbiNAAAAFlDowJyTUqK1KOHFB8vNWokDR9udSIAAADkC45L0pH/OJcjOlubBQAAALDQscRj+uy3zyRJgxoPsjgNAABA1tGogFzzzjvSypVSQID02WeSl5fViQAAAJAvHPuvdOmM5FNKKtnE6jQAAACAZSavn6zklGRFh0ercURjq+MAAABkGY0KyBUbN/7/FRQmTpQqVrQ2DwAAAPKRQ3OdjxEdJbuHtVkAAAAAiyRdTtIHGz6QJA1sPNDaMAAAADeIRgXkuMREqVs36fJl6b77pF69rE4EAACAfMMY6fA853JEJyuTAAAAAJaa/ftsHUs8pojACN1X9T6r4wAAANwQGhWQ4559Vtq1SypTRpo6VbLZrE4EAACAfOPUBunCEcmziBTayuo0AAAAgCWMMRq/ZrwkqX/D/vLy4L66AAAgf6NRATlq/nxnc4IkffyxVKKEtXkAAACQz6ReTaHMnZKHr6VRAAAAAKusPLBSW+O2ys/TT73r97Y6DgAAwA2jUQE55uhR6fHHncvPPiu14j/AAQAAILMOzXU+RnS2NgcAAABgoZg1MZKknrV7qrhfcWvDAAAAZAMaFZAjHA6pVy/pxAmpdm3pjTesTgQAAIB8J2GnlLBdsntJZdpbnQYAAACwxN5Te7Vg5wJJ0oDGAyxOAwAAkD1oVECOeP99ackSyddX+vxzycfH6kQAAADId1Jv+xByu+QdZGkUAAAAwCoT106UkdGdFe9UlZJVrI4DAACQLWhUQLb77TfphRecy2PHStWqWZsHAAAA+ZTrtg+dLI0BAAAAWCX+Yrymb5kuSRrYeKC1YQAAALIRjQrIVhcvSt27S0lJUvv2Ut++VicCAABAvnT+b+nkWudyREdrswAAAAAWmb55us4ln1O1UtV0R/k7rI4DAACQbWhUQLYaOtR5RYVSpaTp0yWbzepEAAAAyJeOzHc+lmgs+YVZmwUAAACwQIojRRPXTZQkDYweKBt/2QoAAAoQGhWQbX74QYqJcS5Pny6FhFgaBwAAAPnZoXnOx8jOlsYAAAAArDJ/53ztP7NfJfxK6KFaD1kdBwAAIFvRqIBsceKE1LOnc7lvX+nuu63NAwAAgHws+YwU95NzOaKTlUkAAAAKnEmTJikqKkq+vr6Kjo7WunXrrmu92bNny2azqVOnTjkbEC4xa2IkSU/Wf1J+Xn7WhgEAAMhmNCrghhkjPf64FBsrVakivfOO1YkAAACQr/29SDKXpaBqUmAlq9MAAAAUGHPmzNHgwYM1cuRIbdq0SbVr11bbtm117Nixq663f/9+Pffcc2revHkuJcXGvzfq54M/y9Puqb4N+1odBwAAINvRqIAb9u9/S/PnS15e0uefS/7+VicCAABAvnZorvORqykAAABkq3Hjxql3797q1auXqlWrpilTpsjf31/Tp0/PcJ2UlBR1795dr7zyisqXL5+LaQu3CWsnSJK6VO+i8MBwi9MAAABkPxoVcEN27ZIGDnQuv/GGVLeupXEAAACQ36VclI5+71yO6GxtFgAAgAIkOTlZGzduVOvWrV3P2e12tW7dWqtXr85wvVdffVWlS5fWY489lhsxIeno2aOa/ftsSdLAxgOtDQMAAJBDPK0OgPzr0iWpe3fp/Hnp9tulZ5+1OhEAAADyvdgfpcuJkn+EVLy+1WkAAAAKjBMnTiglJUUhISFuz4eEhGjHjh3prvPLL7/oo48+0pYtW657O0lJSUpKSnJ9n5CQkKW8hdkH6z/QJcclNYtspgZlGlgdBwAAIEdwRQVk2ahR0oYNUrFi0qxZkp2zCQAAADfq8DznY0QnyWazMgkAAEChdvbsWT388MP68MMPVbJkyeteb8yYMQoKCnJ9RUZG5mDKgufCpQuasnGKJGlQ40EWpwEAAMg5Wfqn5UmTJikqKkq+vr6Kjo7WunXrrjo+JiZGlStXlp+fnyIjIzVo0CBdvHjR9frZs2c1cOBAlStXTn5+fmratKnWr1+flWjIJf/9rzRmjHN56lQpIsLaPAAAACgAHCnS4QXO5YhOlkYBAAAoaEqWLCkPDw/FxcW5PR8XF6fQ0NA04/fu3av9+/erQ4cO8vT0lKenpz7++GMtWLBAnp6e2rt3b7rbGTp0qOLj411fhw4dypH9Kag+/+1znTh/QuWCyqljlY5WxwEAAMgxmW5UmDNnjgYPHqyRI0dq06ZNql27ttq2batjx46lO/7zzz/XkCFDNHLkSG3fvl0fffSR5syZo5deesk15vHHH9fSpUv1ySef6LffflObNm3UunVrHTlyJOt7hhxz5oz08MOSMdIjj0gPPGB1IgAAABQIJ1ZJSccl72JS6VutTgMAAFCgeHt7q379+lq2bJnrOYfDoWXLlqlJkyZpxlepUkW//fabtmzZ4vq65557dNttt2nLli0ZXinBx8dHgYGBbl+4PsYYjV8zXpL0dKOn5Wnnzs0AAKDgynSjwrhx49S7d2/16tVL1apV05QpU+Tv76/p06enO37VqlVq1qyZunXrpqioKLVp00Zdu3Z1XYXhwoUL+uabb/T222/r1ltvVcWKFTVq1ChVrFhRkydPvrG9Q47o21c6eFAqX16aONHqNAAAACgwDs11Ppa5W7J7WZsFAACgABo8eLA+/PBDzZo1S9u3b1efPn2UmJioXr16SZJ69OihoUOHSpJ8fX1Vo0YNt6/g4GAVLVpUNWrUkLe3t5W7UiAt27dMfxz/QwFeAXqs3mNWxwEAAMhRmWrJTE5O1saNG13FqiTZ7Xa1bt1aq1evTnedpk2b6tNPP9W6devUqFEj/fXXX1q0aJEefvhhSdLly5eVkpIiX19ft/X8/Pz0yy+/ZJglKSlJSUlJru8TEhIysyvIos8+k774QvLwcC4XLWp1IgAAABQIxkiH5zmXIztbGgUAAKCg6tKli44fP64RI0YoNjZWderU0eLFixUSEiJJOnjwoOz2LN0tGNkgZk2MJOnRuo8q2DfY0iwAAAA5LVONCidOnFBKSoqrcE0VEhKiHTt2pLtOt27ddOLECd1yyy0yxujy5ct66qmnXLd+KFq0qJo0aaLXXntNVatWVUhIiL744gutXr1aFStWzDDLmDFj9Morr2QmPm7Q/v3OqylI0ogRUuPGlsYBAABAQXJmm5S4T/LwlcLaWJ0GAACgwOrfv7/69++f7msrVqy46rozZ87M/kCQJO08sVMLdy+UTTY93ehpq+MAAADkuBxvj12xYoVGjx6tDz74QJs2bdK3336rhQsX6rXXXnON+eSTT2SMUXh4uHx8fDRx4kR17dr1qt27Q4cOVXx8vOvr0KFDOb0rhdrly9JDD0kJCVLTptL/+kwAAACA7JF6NYWwtpJngKVRAAAAgNw2ca3zHrt3V7pbN5e42eI0AAAAOS9TV1QoWbKkPDw8FBcX5/Z8XFycQkND011n+PDhevjhh/X4449LkmrWrKnExEQ98cQTevnll2W321WhQgWtXLlSiYmJSkhIUFhYmLp06aLy5ctnmMXHx0c+Pj6ZiY8b8Oab0q+/Om/18MknkmemzhwAAADgGg7NdT5GdLI0BgAAAJDbTl84rZlbZ0qSBjYeaGkWAACA3JKpKyp4e3urfv36WrZsmes5h8OhZcuWqUmTJumuc/78+TRXRvDw8JAkGWPcng8ICFBYWJhOnz6tJUuWqGPHjpmJhxyybp00apRz+f33pav0jwAAAACZd26fdGarZLNL4R2sTgMAAADkqn9v+rfOXzqvWiG1dFvUbVbHAQAAyBWZ/n/xgwcPVs+ePdWgQQM1atRIMTExSkxMVK9evSRJPXr0UHh4uMaMGSNJ6tChg8aNG6e6desqOjpae/bs0fDhw9WhQwdXw8KSJUtkjFHlypW1Z88ePf/886pSpYprTlgnMVHq3l1KSZG6dJEeftjqRAAAAChwUm/7UOpWyaeEpVEAAACA3HTZcVnvrXtPkjQweqBsNpvFiQAAAHJHphsVunTpouPHj2vEiBGKjY1VnTp1tHjxYoWEhEiSDh486HYFhWHDhslms2nYsGE6cuSISpUqpQ4dOuiNN95wjYmPj9fQoUN1+PBhFS9eXPfdd5/eeOMNeXl5ZcMu4ka89Za0Z48UESFNnixRJwMAACDbpTYqRHa2NAYAAACQ277d/q0OJRxSKf9S6lqzq9VxAAAAco3N/PP+C/lUQkKCgoKCFB8fr8DAQKvjFAgHDkhVqkgXL0rffCPde6/ViQAAAJwKeu1X0PfPzcXj0txQyTikjgekgLJWJwIAAMhVBb32K+j7d6OaftRUqw+v1ohbR+iV216xOg4AAMANyUztZ7/qqyjUXnjB2aTQsqXUmf/cBgAAgJxw5D/OJoVi9WhSAAAAQKGy9vBarT68Wt4e3urTsI/VcQAAAHIVjQpI188/S19+KdntUkwMt3wAAABADjk01/nIbR8AAABQyMSsjZEkda3RVaFFQq0NAwAAkMtoVEAaKSnSgAHO5d69pdq1rc0DAACAAurSOSl2qXM5opOlUQAAAIDcdDjhsL764ytJ0oDoARanAQAAyH00KiCNmTOlzZuloCDptdesTgMAAIAC6+hiyZEkFakoBVW3Og0AAACQayatm6QUk6IW5Vqoblhdq+MAAADkOhoV4CYhQXrpJefyiBFSqVLW5gEAAEABdnie8zGyE/caAwAAQKFx/tJ5Td04VZI0qPEgi9MAAABYg0YFuHn9denYMalSJal/f6vTAAAAoMBKSZaOfOdcjuhsbRYAAAAgF3289WOdvnha5YuV192V7rY6DgAAgCVoVIDLnj1STIxzedw4ydvb0jgAAAAoyI6tkC7FS74hUsnGVqcBAAAAcoXDODRh7QRJ0jONnpGH3cPiRAAAANagUQEuzz0nXboktW0rtW9vdRoAAAAUaKm3fYjoKNn4YwkAAAAKhx/2/qAdJ3Yo0CdQj9Z91Oo4AAAAluFvBCFJ+vFHaf58ycNDGj+eWwQDAAAgBxnHFY0KnaxMAgAAAOSq8WvGS5Ieq/uYivoUtTgNAACAdWhUgC5flgYOdC736ydVrWppHAAAABR0J9dLF45KnkWlkNutTgMAAADkij+O/aEf9v4gu82upxs9bXUcAAAAS9GoAE2bJv3xh1S8uDRypNVpAAAAUOAdnut8LNNe8vCxNgsAAACQSyaunShJ6li5o24qdpPFaQAAAKxFo0Ihd+qUNHy4c/m115zNCgAAAECOSr3tQ2RnS2MAAAAAueXk+ZP6eNvHkqRBjQdZnAYAAMB6NCoUcq+84mxWqFFDeuIJq9MAAACgwIvfLiXslOzeUpk7rU4DAAAA5IqpG6fq4uWLqhdWT7eUvcXqOAAAAJajUaEQ+/NPadIk5/L48ZKnp7V5AAAAUAikXk0hpJXkFWhpFAAAACA3JKcka9J651/EDoweKJvNZnEiAAAA69GoUEgZIw0eLKWkSB07Sq1bW50IAAAAhcKhuc7HyE6WxgAAAAByy9d/fq2/z/6t0CKh6lKji9VxAAAA8gQaFQqpRYukJUskLy9p7Fir0wAAAKBQOH9YOrVekk0Kv8fqNAAAAECOM8Zo/JrxkqR+DfvJ28Pb4kQAAAB5A40KhVBysvNqCpI0cKBUsaKlcQAAAFBYHJ7vfCzZRPILtTYLAAAAkAtWHVqlDX9vkI+Hj56s/6TVcQAAAPIMGhUKofffl3btkkqXloYNszoNAAAACo3D85yPkZ0tjQEAAADklpi1MZKkh2o9pFIBpawNAwAAkIfQqFDIHD8uvfqqc3n0aCkw0No8AAAAKCSST0txK5zLEZ2sTAIAAADkigNnDujb7d9KkgZED7A4DQAAQN5Co0IhM3y4FB8v1a0rPfKI1WkAAABQaBxZKJnLUlANqSj3HgMAAEDB9/669+UwDrUu31o1Q2paHQcAACBPoVGhENm6VfrwQ+fyhAmSh4e1eQAAAFCIHJ7rfORqCgAAACgEziWf04ebnH8ZOzB6oLVhAAAA8iAaFQoJY6SBAyWHQ3rwQal5c6sTAQAAoNC4fEH6e7FzObKztVkAAACAXDBzy0zFJ8WrUolKuvPmO62OAwAAkOfQqFBIzJ0rrVgh+fpKb79tdRoAAAAUKrFLpZTzkn9ZqVhdq9MAAAAAOcphHJqwdoIkaUD0ANlt/DU8AADAP1EhFQIXL0rPPedcfu45qVw5a/MAAACgkDk8z/kY0Umy2axMAgAAAOS4RbsXac+pPQr2DVaP2j2sjgMAAJAn0ahQCIwfL+3bJ4WHS0OGWJ0GAAAAhYrjsnRkgXM5spOlUQAAAIDcMH7NeElS73q9VcS7iMVpAAAA8iYaFQq4o0elN95wLr/5phQQYG0eAAAAFDLHf5WSTkrexaVSza1OAwAAAOSobXHb9NO+n+Rh81D/Rv2tjgMAAJBn0ahQwL30kpSYKEVHS926WZ0GAAAAhc7huc7H8A6S3dPaLAAAAEAOm7BmgiTpvmr3qWxQWYvTAAAA5F00KhRg69dLM2c6lydMkOz8tAEAAJCbjJEOz3MuR3a2NAoAAACQ044lHtNnv30mSRoYPdDaMAAAAHkc/3RdQBkjDRzoXH74YecVFQAAAIBcdXqLlHhA8vCTQu+wOg0AAACQo6ZsmKKklCQ1Cm+kxhGNrY4DAACQp9GoUEDNni2tWiX5+0tjxlidBgAAAIVS6tUUwtpJnv6WRgEAAAByUtLlJH2w/gNJzqsp2Gw2ixMBAADkbTQqFECJidILLziXX3pJCg+3Ng8AAAAKqcNznY8RnSyNAQAAAOS0OX/MUVxinMKLhuv+avdbHQcAACDPo1GhAHrnHenwYalcOWnwYKvTAAAAoFA6u1c685tk85DC77Y6DQAAAJBjjDEav2a8JKl/o/7y8vCyOBEAAEDeR6NCAXPwoPT2287ld96R/PyszQMAAIBCKvW2D6VbSD7FLY0CAAAA5KT/HvivtsRukZ+nn3rX6211HAAAgHyBRoUC5sUXpQsXpFtvle7nCmMAAACwSmqjQkRnS2MAAAAAOS1mbYwkqUftHirhX8LaMAAAAPkEjQoFyK+/SrNnSzabFBPjfAQAAMDVTZo0SVFRUfL19VV0dLTWrVt31fFnzpxRv379FBYWJh8fH1WqVEmLFi26oTkLnAtx0vFfncsRHa3NAgAAAOSgv07/pfk75kuSBkQPsDgNAABA/kGjQgHhcEgD/lcHP/aYVLeutXkAAADygzlz5mjw4MEaOXKkNm3apNq1a6tt27Y6duxYuuOTk5N1xx13aP/+/fr666+1c+dOffjhhwoPD8/ynAXSkQWSjFS8gRQQaXUaAAAAIMdMXDtRRkbtKrZT1VJVrY4DAACQb9CoUEDMmiVt3CgFBkqvv251GgAAgPxh3Lhx6t27t3r16qVq1appypQp8vf31/Tp09MdP336dJ06dUrz5s1Ts2bNFBUVpRYtWqh27dpZnrNAct32oZOVKQAAAIAclZCUoOmbnXX+wOiB1oYBAADIZ2hUKADOnpVeesm5PHy4FBJibR4AAID8IDk5WRs3blTr1q1dz9ntdrVu3VqrV69Od50FCxaoSZMm6tevn0JCQlSjRg2NHj1aKSkpWZ6zwLmUIMX+6FyO7GxtFgAAACAHTd88XWeTz6pqyapqU6GN1XEAAADyFU+rA+DGjR4txcZKFStKzzxjdRoAAID84cSJE0pJSVHIP7o8Q0JCtGPHjnTX+euvv/TTTz+pe/fuWrRokfbs2aO+ffvq0qVLGjlyZJbmlKSkpCQlJSW5vk9ISLiBPbPY34slR7JUtJIUyKVvAQAAUDClOFI0ce1ESdLAxgNls9ksTgQAAJC/cEWFfG7vXmncOOfyuHGSt7e1eQAAAAoyh8Oh0qVLa9q0aapfv766dOmil19+WVOmTLmheceMGaOgoCDXV2RkZDYltsDhuc7HiE4Sf1kLAACAAmrBzgXad2afivsV10O1HrI6DgAAQL5Do0I+9/zzUnKydMcd0t13W50GAAAg/yhZsqQ8PDwUFxfn9nxcXJxCQ0PTXScsLEyVKlWSh4eH67mqVasqNjZWycnJWZpTkoYOHar4+HjX16FDh25gzyyUkiQdWehc5rYPAAAAKMBi1sZIkp6s/6T8vfytDQMAAJAP0aiQj/30kzR3ruThIY0fz39YAwAAyAxvb2/Vr19fy5Ytcz3ncDi0bNkyNWnSJN11mjVrpj179sjhcLie27Vrl8LCwuTt7Z2lOSXJx8dHgYGBbl/5Utxy6fJZyS9MKtHI6jQAAABAjth0dJP+e+C/8rR7ql/DflbHAQAAyJdoVMinLl+WBg50LvfpI1WvbmkcAACAfGnw4MH68MMPNWvWLG3fvl19+vRRYmKievXqJUnq0aOHhg4d6hrfp08fnTp1SgMGDNCuXbu0cOFCjR49Wv369bvuOQu0w/Ocj+EdJRt/1AAAAEDBNGHtBEnSg9UfVHhguMVpAAAA8idPqwMga/79b+m336RixaRRo6xOAwAAkD916dJFx48f14gRIxQbG6s6depo8eLFCgkJkSQdPHhQdvv//4N7ZGSklixZokGDBqlWrVoKDw/XgAED9OKLL173nAWWcUiH5zuXIzpZGgUAAADIKUfPHtUXv30hSRoQPcDiNAAAAPmXzRhjrA6RHRISEhQUFKT4+Pj8e6nc63T6tHTzzdLJk9LEidLTT1udCAAAIHcV9NovX+7f8dXS0qaSV6B073HJw9vqRAAAAPlCvqz9MqGg7d+I5SP02n9fU9PIpvr10V+tjgMAAJCnZKb243qs+dCrrzqbFKpVk556yuo0AAAAgKTDc52PZe6iSQEAAAAF0sXLFzV5w2RJ0qDGgyxOAwAAkL/RqJDP7Nghvf++c3n8eMnLy9o8AAAAgIyRDv2vUSGys7VZAAAAgBzy2bbPdOL8CZUNKqtOVTpZHQcAACBfo1Ehn3n2WenyZenuu6U2baxOAwAAAEiK/1M6t0ey+0hh7axOAwAAAGQ7Y4xi1sZIkp5u9LQ87Z7WBgIAAMjnaFTIR77/Xlq0yHkVhXfftToNAAAA8D+H5zkfQ1tLXkUtjQIAAADkhJ/2/aTfj/2uAK8APV7vcavjAAAA5Hs0KuQTly5Jgwc7l595RqpUydo8AAAAgMvh/932IaKTpTEAAACAnJJ6NYVedXop2DfY0iwAAAAFAY0K+cQHH0g7dkilSknDh1udBgAAAPifxEPSqY2SbFLEPVanAQAAALLdrpO79N2u7yRJz0Q/Y3EaAACAgoFGhXzgxAlp1Cjn8htvSEFBlsYBAAAA/l/qbR9KNZN8S1saBQAAAMgJE9dOlCTdXelu3VziZovTAAAAFAw0KuQDI0ZIZ85ItWtLjz5qdRoAAADgCqmNChGdLY0BAAAA5ITTF05rxpYZkqSB0QOtDQMAAFCA0KiQx/32mzR1qnN5wgTJw8PaPAAAAIBL0knp2ErncmQnS6MAAAAAOeGjzR/p/KXzqlm6pm6/6Xar4wAAABQYNCrkYcZIAwdKDod0//1SixZWJwIAAACucGShZFKk4FpSkfJWpwEAAACy1WXHZb237j1J0sDGA2Wz2SxOBAAAUHDQqJCHzZ8v/fST5OMjvfOO1WkAAACAfzg81/kY0cnSGAAAAEBOmLt9rg7GH1Qp/1LqVrOb1XEAAAAKFBoV8qikJOm555zLzz4rRUVZGgcAAABwd/m8dHSJczmys7VZAAAAgBwQszZGktSnQR/5evpaGwYAAKCAoVEhj5owQdq7VwoLk4YOtToNAAAA8A9Hf5BSLkgB5aTg2lanAQAAALLVuiPrtOrQKnnZvdSnYR+r4wAAABQ4NCrkQbGx0muvOZfffFMqUsTaPAAAAEAah+c5HyM6S9yrFwAAAAVMzJoYSVLXml0VWiTU2jAAAAAFEI0KedDLL0vnzkmNGkkPPWR1GgAAAOAfHJelI/9xLkd0sjQKAAAAkN0OJxzWV39+JUkaGD3Q2jAAAAAFFI0KeczGjdKMGc7lmBjJzk8IAAAAec3xn6XkU5JPSalUM6vTAAAAANnqg/Uf6LLjslqUa6G6YXWtjgMAAFAg8c/geYgx0oABzsfu3aUmTaxOBAAAAKTj0FznY3gHye5pbRYAAAAgG52/dF5TN06VJA1sPNDaMAAAAAUYjQp5yJdfSr/+Kvn7S2++aXUaAAAAIB3GSIfnOZcjOlsaBQAAAMhun2z9RKcunNJNwTepQ6UOVscBAAAosLLUqDBp0iRFRUXJ19dX0dHRWrdu3VXHx8TEqHLlyvLz81NkZKQGDRqkixcvul5PSUnR8OHDddNNN8nPz08VKlTQa6+9JmNMVuLlS+fPSy+84FweMkSKiLA2DwAAAJCu05uk84ckzwAptLXVaQAAAIBs4zAOxayNkSQ9E/2MPOwe1gYCAAAowDJ9ndY5c+Zo8ODBmjJliqKjoxUTE6O2bdtq586dKl26dJrxn3/+uYYMGaLp06eradOm2rVrlx555BHZbDaNGzdOkvTWW29p8uTJmjVrlqpXr64NGzaoV69eCgoK0jPPPHPje5kPjB0rHTwolS0rPfec1WkAAACADKTe9iGsneTpZ20WAAAAIBst3btUO07sUFHvonq07qNWxwEAACjQMn1FhXHjxql3797q1auXqlWrpilTpsjf31/Tp09Pd/yqVavUrFkzdevWTVFRUWrTpo26du3qdhWGVatWqWPHjrrrrrsUFRWl+++/X23atLnmlRoKisOHpbfeci6//bbkx9/3AgAAIK9y3fahk5UpAAAAgGw3fs14SdJjdR9ToE+gxWkAAAAKtkw1KiQnJ2vjxo1q3fr/L/Fqt9vVunVrrV69Ot11mjZtqo0bN7qaDv766y8tWrRI7du3dxuzbNky7dq1S5K0detW/fLLL7rzzjszvUP50ZAhzls/3HKL9OCDVqcBAAAAMpCwW4r/Q7J5SuF3WZ0GAAAAyDZ/Hv9TS/YukU02PR39tNVxAAAACrxM3frhxIkTSklJUUhIiNvzISEh2rFjR7rrdOvWTSdOnNAtt9wiY4wuX76sp556Si+99JJrzJAhQ5SQkKAqVarIw8NDKSkpeuONN9S9e/cMsyQlJSkpKcn1fUJCQmZ2Jc9YvVr67DPJZpMmTHA+AgAAAHlS6tUUQlpK3sWsTAIAAABkq4lrJ0qSOlbpqPLFylucBgAAoODL9K0fMmvFihUaPXq0PvjgA23atEnffvutFi5cqNdee8015ssvv9Rnn32mzz//XJs2bdKsWbM0duxYzZo1K8N5x4wZo6CgINdXZGRkTu9KtnM4pAEDnMu9ekn16lmbBwAAALiqw3OdjxGdrc0BAAAAZKOT50/q460fS5IGNR5kcRoAAIDCIVNXVChZsqQ8PDwUFxfn9nxcXJxCQ0PTXWf48OF6+OGH9fjjj0uSatasqcTERD3xxBN6+eWXZbfb9fzzz2vIkCH617/+5Rpz4MABjRkzRj179kx33qFDh2rw4MGu7xMSEvJds8Knn0rr10tFi0pvvGF1GgAAAOAqLhyVTqxxLkd0tDYLAAAAkI2mbZymC5cvqG5oXTUv29zqOAAAAIVCpq6o4O3trfr162vZsmWu5xwOh5YtW6YmTZqku8758+dlt7tvxsPDQ5JkjLnqGIfDkWEWHx8fBQYGun3lJ+fOSUOGOJeHDZMy6PMAAAAA8obDCyQZqUQjyT/c6jQAAADIokmTJikqKkq+vr6Kjo7WunXrMhz77bffqkGDBgoODlZAQIDq1KmjTz75JBfT5rxLKZf0/vr3JUkDGw+UjXvzAgAA5IpMXVFBkgYPHqyePXuqQYMGatSokWJiYpSYmKhevXpJknr06KHw8HCNGTNGktShQweNGzdOdevWVXR0tPbs2aPhw4erQ4cOroaFDh066I033lDZsmVVvXp1bd68WePGjdOjjz6ajbuat4wZIx09KlWo8P+3fwAAAADyrMPznI8RnaxMAQAAgBswZ84cDR48WFOmTFF0dLRiYmLUtm1b7dy5U6VLl04zvnjx4nr55ZdVpUoVeXt767vvvlOvXr1UunRptW3b1oI9yH5f//m1/j77t0KLhKpL9S5WxwEAACg0Mt2o0KVLFx0/flwjRoxQbGys6tSpo8WLFyskJESSdPDgQberIwwbNkw2m03Dhg3TkSNHVKpUKVdjQqr33ntPw4cPV9++fXXs2DGVKVNGTz75pEaMGJENu5j37Nsnvfuuc3nsWMnHx9o8AAAAwFUlx0tx/7uqWkRna7MAAAAgy8aNG6fevXu7/tPZlClTtHDhQk2fPl1DUi//eoWWLVu6fT9gwADNmjVLv/zyS4FoVDDGaPya8ZKkvg36yseTv6gFAADILTaTev+FfC4hIUFBQUGKj4/P87eBuP9+6ZtvpFatpKVLJa4mBgAAkDn5qfbLijy3f/tnS6u6SoGVpbt3WJ0GAACgQMmt2i85OVn+/v76+uuv1alTJ9fzPXv21JkzZzR//vyrrm+M0U8//aR77rlH8+bN0x133JHuuKSkJCUlJbm+T0hIUGRkZN6pba+w6tAqNZveTD4ePjo46KBKB6S9qgQAAACuX2ZqW/tVX0W2W7nS2aRgt0vjx9OkAAAAgHzg8FznI1dTAAAAyLdOnDihlJQU15VxU4WEhCg2NjbD9eLj41WkSBF5e3vrrrvu0nvvvZdhk4IkjRkzRkFBQa6vyMjIbNuH7BazJkaS1L1md5oUAAAAchmNCrkoJUUaMMC5/OSTUs2a1uYBAAAAriklSfp7kXM5opOlUQAAAJD7ihYtqi1btmj9+vV64403NHjwYK1YsSLD8UOHDlV8fLzr69ChQ7kXNhMOnDmgb7Z/I0ka2HigtWEAAAAKIU+rAxQmH30kbd0qBQdLr75qdRoAAADgOsQuky6fk/zKSCUaWp0GAAAAWVSyZEl5eHgoLi7O7fm4uDiFhoZmuJ7dblfFihUlSXXq1NH27ds1ZswYtWzZMt3xPj4+8vHxybbcOWXS+klyGIda3dRKNUP4H2UAAAC5jSsq5JL4eGnYMOfyqFFSyZKWxgEAAACuz+F5zseITpKNPz4AAADkV97e3qpfv76WLVvmes7hcGjZsmVq0qTJdc/jcDiUlJSUExFzzbnkc5q2cZokrqYAAABgFa6okEtee006flyqUkXq29fqNAAAAMB1cKRIR+Y7l7ntAwAAQL43ePBg9ezZUw0aNFCjRo0UExOjxMRE9erVS5LUo0cPhYeHa8yYMZKkMWPGqEGDBqpQoYKSkpK0aNEiffLJJ5o8ebKVu3HDZm2ZpfikeN1c/Ga1v7m91XEAAAAKJRoVcsGuXdKECc7l8eMlLy9r8wAAAADX5eQa6eIxyStICmlpdRoAAADcoC5duuj48eMaMWKEYmNjVadOHS1evFghISGSpIMHD8pu//+raCUmJqpv3746fPiw/Pz8VKVKFX366afq0qWLVbtwwxzGoQlrnX9ZOyB6gOxcNQwAAMASNCrkgmeflS5fltq3l9q1szoNAAAAcJ0OzXU+ht8t2em2BQAAKAj69++v/v37p/vaihUr3L5//fXX9frrr+dCqtyzaPci7T61W0E+QepZp6fVcQAAAAot2kVz2JIl0nffSZ6e0rhxVqcBAAAArpMx0uF5zuWIzpZGAQAAALJLzJoYSVLver1VxLuItWEAAAAKMRoVctClS9KgQc7lp5+WKle2Ng8AAABw3eJ/l87tlew+Ulhbq9MAAAAAN+y3uN+0bN8y2W129W+U/lUlAAAAkDtoVMhBU6ZI27dLJUtKI0ZYnQYAAADIhEPznI9hbSQv/qcZAAAA8r8JaydIku6rep/KBZezOA0AAEDhRqNCDjl5Uho50rn82mtScLClcQAAAIDMOTzX+RjRydIYAAAAQHY4lnhMn277VJI0sPFAa8MAAACARoWcMnKkdPq0VKuW1Lu31WkAAACATEg8IJ3eLNnsUngHq9MAAAAAN2zqhqlKSklSwzIN1SSiidVxAAAACj0aFXLAH384b/sgSTExkoeHpXEAAACAzEm97UOpWyTfUpZGAQAAAG5U0uUkfbDhA0nSoMaDZLPZLE4EAAAAGhWymTHSoEFSSorUubN0221WJwIAAAAy6fA852NEZ0tjAAAAANnhyz++VOy5WJUpWkb3V7vf6jgAAAAQjQrZ7rvvpKVLJW9vaexYq9MAAAAAmXTxhHT8v87liE6WRgEAAABulDFG49eMlyT1b9hfXh5eFicCAACARKNCtkpKkgYPdi4PHiyVL29tHgAAACDT/v5OMg6pWB2pSJTVaQAAAIAb8vPBn7U5drP8PP30RP0nrI4DAACA/6FRIRu99560Z48UGiq99JLVaQAAAIAsODTX+cjVFAAAAFAAxKyJkST1qN1DJfxLWBsGAAAALjQqZJO4OOm115zLY8ZIRYtamwcAAADItMuJUuwPzuWIztZmAQAAAG7QX6f/0rwd8yRJz0Q/Y20YAAAAuKFRIZsMGyYlJEgNGkg9elidBgAAAMiCo0uklItSwE1ScE2r0wAAAAA35L2178nIqG2FtqpWqprVcQAAAHAFGhWywebN0kcfOZdjYiQ7RxUAAAD5UeptHyI7SzabtVkAAACAG5CQlKCPNjv/0nZg44HWhgEAAEAa/JP6DTJGGjjQ+di1q9SsmdWJAAAAgCxwXJKOfOdcjuhkaRQAAADgRs3YPENnk8+qasmqaluhrdVxAAAA8A80Ktygr7+W/vtfyc9Peustq9MAAAAAWXRspXTpjORTSirZ1Oo0AAAAQJalOFI0cd1ESdKA6AGycbUwAACAPIdGhRtw4YL0/PPO5RdekCIjrc0DAAAAZNmhec7HiHsku4elUQAAAIAb8Z9d/9Ffp/9SMd9ierj2w1bHAQAAQDpoVLgB48ZJBw5IERHORgUAAAAgXzIO6fA853JEZ0ujAAAAADcqZk2MJOnJ+k/K38vf2jAAAABIF40KWXTkiDR6tHP57bclf+pdAAAA5FenNkoXjkieRaTQVlanAQAAALJs89HNWnlgpTztnurXqJ/VcQAAAJABGhWyKCZGOn9eatpU+te/rE4DAAAA3IBDc52PZe6UPHytzQIAAADcgA/WfyBJeqDaA4oIjLA4DQAAADLiaXWA/OqNN6SwMOnWWyWbzeo0AAAAwA2oPEAqcpNU9GarkwAAAAA3ZGybsapWqppaRLWwOgoAAACugkaFLPL2lgYPtjoFAAAAkA38QqSKva1OAQAAANywIN8gDWoyyOoYAAAAuAZu/QAAAAAAAAAAAAAAAHINjQoAAAAAAAAAAAAAACDX0KgAAAAAAAAAAAAAAAByDY0KAAAAAAAAAAAAAAAg19CoAAAAAAAAAAAAAAAAcg2NCgAAAAAAAAAAAAAAINfQqAAAAAAAAAAAAAAAAHINjQoAAAAAAAAAAAAAACDX0KgAAAAAAAAAAAAAAAByDY0KAAAAAAAAAAAAAAAg19CoAAAAAAAAAAAAAAAAcg2NCgAAAAAAAAAAAAAAINfQqAAAAAAAAAAAAAAAAHINjQoAAAAAAAAAAAAAACDX0KgAAAAAAAAAAAAAAAByjafVAbKLMUaSlJCQYHESAAAA5LTUmi+1BixoqG0BAAAKD2pbAAAAFBSZqW0LTKPC2bNnJUmRkZEWJwEAAEBuOXv2rIKCgqyOke2obQEAAAofalsAAAAUFNdT29pMAWnVdTgc+vvvv1W0aFHZbLZc2WZCQoIiIyN16NAhBQYG5so2rVDQ9jM/709+yp5Xs+aVXFbmyO1tZ8f2cjpzTsyfXXPeyDxWrJuV9TKzTk7PL0lHjhxRtWrV9Oeffyo8PDxb585L47Nzbis+04wxOnv2rMqUKSO7veDdzYzaNucUtP3Mz/uTn7Ln1ax5JRe1be7PkdvzU9vmz9o2M3VtVvLkpfHUtnkbtW3OKWj7mZ/3Jz9lz6tZ80ouatvcnyO356e2pbbN6+MLU21bYK6oYLfbFRERYcm2AwMD89Qv9JxS0PYzP+9PfsqeV7PmlVxW5sjtbWfH9nI6c07Mn11z3sg8VqyblfUys05Ozp96aaqiRYvmWJ68ND47587tz5WC+L/NUlHb5ryCtp/5eX/yU/a8mjWv5KK2zf05cnt+atucWSen5s9KXZuVPHlpPLVt3kRtm/MK2n7m5/3JT9nzata8kovaNvfnyO35qW1zZh1q2+wbXxhq24LXogsAAAAAAAAAAAAAAPIsGhUAAAAAAAAAAAAAAECuoVHhBvj4+GjkyJHy8fGxOkqOKmj7mZ/3Jz9lz6tZ80ouK3Pk9razY3s5nTkn5s+uOW9kHivWzcp6mVknp+eXnJfBatGixXVdCiuzc+el8dk5d175bMWNKSw/x4K2n/l5f/JT9ryaNa/korbN/Tlye35q2/xZ22amrs1Knrw0ntoW/1RYfo4FbT/z8/7kp+x5NWteyUVtm/tz5Pb81LbUtnl9fGGqbW3GGGN1CAAAAAAAAAAAAAAAUDhwRQUAAAAAAAAAAAAAAJBraFQAAAAAAAAAAAAAAAC5hkYFAAAAAAAAAAAAAACQa2hUyMCoUaNks9ncvqpUqXLVdb766itVqVJFvr6+qlmzphYtWpRLaa/ff//7X3Xo0EFlypSRzWbTvHnzXK9dunRJL774omrWrKmAgACVKVNGPXr00N9//33VObNyrLLL1fZHkuLi4vTII4+oTJky8vf3V7t27bR79+6rzvnhhx+qefPmKlasmIoVK6bWrVtr3bp12Z59zJgxatiwoYoWLarSpUurU6dO2rlzp9uYli1bpjm2Tz311FXnHTVqlKpUqaKAgABX/rVr12Y55+TJk1WrVi0FBgYqMDBQTZo00ffff+96/eLFi+rXr59KlCihIkWK6L777lNcXNxV5zx37pz69++viIgI+fn5qVq1apoyZUq25srKsfvn+NSvd95557pzvfnmm7LZbBo4cKDruawco2+//VZt2rRRiRIlZLPZtGXLlixtO5UxRnfeeWe675Osbvuf29u/f3+Gx/Crr75yrZfeZ0Z6XwEBAdd9vIwxGjFihIoUKXLVz6Mnn3xSFSpUkJ+fn0qVKqWOHTtqx44dV5175MiRaeYsX7686/XMnGvX2vcRI0bo4YcfVmhoqAICAlSvXj198803rvWPHDmihx56SCVKlJCfn59q1qypadOmuX0OPvjggwoLC5Ofn59at27t+sxLb90NGzZIkiZOnKigoCDZ7XZ5eHioVKlSrs//q60nSe3bt5eXl5dsNps8PT1Vp04dtWvXLsPxjzzySJr99vT0lL+/f7rjJWn79u265557FBQU5NqWr69vuuPPnTunvn37KigoKMPjXLNmTUnSmTNnVLNmzWuei/369ZMkTZs2TS1btpSnp+d1jX/yySdVvHjx654/9VwePnz4dY1dvXq1br/9dvn7+191/NXem+mNT0lJUf/+/RUQEOB63sPDQ35+fmrYsKEOHjzoes9dea59/vnnV/2dLEmTJk1SVFSUfH19FR0dnSO/X5E+altqW2pbJ2pbaltqW2pbaltqW2rb/I/altqW2taJ2pbaltqW2pba9vpr2yvr2goVKrjyXs/8qedxaGgotW02o1HhKqpXr66jR4+6vn755ZcMx65atUpdu3bVY489ps2bN6tTp07q1KmTfv/991xMfG2JiYmqXbu2Jk2alOa18+fPa9OmTRo+fLg2bdqkb7/9Vjt37tQ999xzzXkzc6yy09X2xxijTp066a+//tL8+fO1efNmlStXTq1bt1ZiYmKGc65YsUJdu3bV8uXLtXr1akVGRqpNmzY6cuRItmZfuXKl+vXrpzVr1mjp0qW6dOmS2rRpkyZb79693Y7t22+/fdV5K1WqpPfff1+//fabfvnlF0VFRalNmzY6fvx4lnJGRETozTff1MaNG7Vhwwbdfvvt6tixo/744w9J0qBBg/Sf//xHX331lVauXKm///5b995771XnHDx4sBYvXqxPP/1U27dv18CBA9W/f38tWLAg23JJmT92V449evSopk+fLpvNpvvuu++6Mq1fv15Tp05VrVq13J7PyjFKTEzULbfcorfeeuuGtp0qJiZGNpvtuua6nm2nt73IyMg0x/CVV15RkSJFdOedd7qtf+VnxtatW/X777+7vm/ZsqUkaerUqdd9vN5++21NnDhRd999typUqKA2bdooMjJS+/btc/s8ql+/vmbMmKHt27dryZIlMsaoTZs2SklJyXDuX3/9VXa7XTNmzNCyZctc4y9evOgak5lzrXr16tq6davr6/fff3eda8uXL9fOnTu1YMEC/fbbb7r33nv14IMPavPmzTp9+rSaNWsmLy8vff/99/rzzz/17rvvytPT0+1zcOHChZoyZYrWrl2rgIAAtW3bVkePHk133WLFimnOnDl67rnnFBERobFjx+q+++7TxYsX9fvvv6t9+/YZridJc+bM0Q8//KABAwZo8eLFat++vbZu3aply5bp888/TzM+1c0336xixYppypQpCgsLU5MmTSRJL7zwQprxe/fu1S233KIqVaro7bffljFGAQEBateuXbrzDx48WF988YW8vLz0+uuvuwpEDw8PPfPMM5Kkxx57TJLUrFkzbd++XQ8++KB8fX3l7+8vf39/bd26Vdu2bdPSpUslSQ888IAk5+/Jo0ePus6XiRMnqlSpUvLw8NCOHTvSjK9fv746duyom2++WUuWLFHLli0VEhKibdu26ejRo2nGp57LY8eOdRXlderUUWRkpBYuXOg2dvXq1WrXrp3q168vLy8vdevWTS+//LJWrFihmTNn6ssvv3SNT31vfvrppxowYIA++ugjSZKPj4/27NmTJstrr72myZMnq3LlyipSpIjrD3XFixfXyy+/LF9fX9d77spz7dlnn1X16tXT/Z2cer4MHjxYI0eO1KZNm1S7dm21bdtWx44dy/D9guxFbUttS21LbUtte/3bo7altqW2pbalts3bqG2pbaltqW2pba9/e9S21LaFrbaNiYlx1bZz5851G5t6rvXr10/ly5dXmzZtFBISok2bNrnO93/On3oe33XXXYqOjpYklShRQvv27Uszlto2kwzSNXLkSFO7du3rHv/ggw+au+66y+256Oho8+STT2ZzsuwjycydO/eqY9atW2ckmQMHDmQ4JrPHKqf8c3927txpJJnff//d9VxKSoopVaqU+fDDD6973suXL5uiRYuaWbNmZWfcNI4dO2YkmZUrV7qea9GihRkwYMANzRsfH28kmR9//PEGE/6/YsWKmX//+9/mzJkzxsvLy3z11Veu17Zv324kmdWrV2e4fvXq1c2rr77q9ly9evXMyy+/nC25jMmeY9exY0dz++23X9fYs2fPmptvvtksXbrUbdtZPUap9u3bZySZzZs3Z3rbqTZv3mzCw8PN0aNHr+t9f61tX2t7V6pTp4559NFH3Z672mfGmTNnjM1mMzVq1HA9d63j5XA4TGhoqHnnnXdcc585c8b4+PiYL7744qr7uHXrViPJ7NmzJ8O5AwICTFhYmFvGK+fOzLmW0b6nnmsBAQHm448/dnutePHi5sMPPzQvvviiueWWWzKc2+FwGEmmZ8+eabLec889Ga7bqFEj069fP9f3KSkppkyZMqZv375GkmnYsGGG2/znui+88ILx8vK66mdOz549TUhIiHn00Ufd9unee+813bt3TzO+S5cu5qGHHjJnz541xYoVMzVq1LjqMa9evbopUqSIef/9913P1atXz1SuXNkUK1bMeHp6mpSUFHPgwAEjyQwePNjMmDHDBAUFmYULFxpJrt8RAwYMMBUqVDAOh8N1bOx2u2ncuLGRZE6fPu2a5+mnn04z3hj3n/k/z7d/jnc4HKZEiRImKCjI9X799NNPjY+Pj2nXrp3b2OjoaDNs2DDX8fmn9LJcSZJp1apVuuMbNWpkJJl7773XNXeHDh2MJLN06VK391yqf74v0vusyehcGzNmTLoZkb2obZ2obalt00Ntmxa1bfqobd1R21LbUttS21qF2taJ2pbaNj3UtmlR26aP2tYdtW3BrW1r166dbi2Z+jNP71y7cv7U83jgwIFu71dPT0/zxRdfpMlCbZs5XFHhKnbv3q0yZcqofPny6t69uw4ePJjh2NWrV6t169Zuz7Vt21arV6/O6Zg5Kj4+XjabTcHBwVcdl5ljlVuSkpIkSb6+vq7n7Ha7fHx8MtU5fP78eV26dEnFixfP9oxXio+Pl6Q02/nss89UsmRJ1ahRQ0OHDtX58+eve87k5GRNmzZNQUFBql279g1nTElJ0ezZs5WYmKgmTZpo48aNunTpktu5X6VKFZUtW/aq537Tpk21YMECHTlyRMYYLV++XLt27VKbNm2yJVeqGzl2cXFxWrhwoauD71r69eunu+66K83nQFaPUWZktG3Jef5269ZNkyZNUmhoaI5v70obN27Uli1b0j2GGX1m/PjjjzLGuDoopWsfr3379ik2NtaVZ/fu3apatapsNptGjRqV4edRYmKiZsyYoZtuukmRkZEZzp2YmKjTp0+78vbt21e1a9d2y5OZc+2f+75x40bXuda0aVPNmTNHp06dksPh0OzZs3Xx4kW1bNlSCxYsUIMGDfTAAw+odOnSqlu3rj788EO3rJLc3utBQUGKjo7Wzz//nO66ycnJ2rhxo9vP0m63q3Xr1tq8ebMkqWHDhuluM711FyxYoGLFislms+lf//pXmoyp4uPjNXPmTI0bN07x8fFq2bKl5s6dq19++cVtvMPh0MKFC1WpUiVVqlRJZ86c0fHjx7V582ZNmzYt3fmbNm2qCxcu6MKFC26fL2FhYTp9+rRuu+022e1212XtUs+1c+fOqU+fPpKkYcOGacuWLfr000/16KOPurra//vf/8rhcOiOO+5wba9s2bIKCgrSihUr0oy/8mceGhqq5s2bKyAgQMYYJScnpxn/559/6uTJkxo5cqTr/RoQEKCGDRtqxYoVrrHHjh3T2rVrVapUKX311VeaO3euihcvrmLFiik6OlpfffVVhlkk53tTkutn988slSpVkiR9//33qlSpkpo2barvvvtOkvTvf/87zXvuynMto/fp1c61/F4r5SfUttS2ErXtlahtM0Ztmxa1bfqobaltqW2dqG1zH7Utta1EbXslatuMUdumRW2bPmrbglfbBgYG6vfff8+wlty1a5eaNm0qT09Pvfzyyzp48GCaejL1PJ4/f77b+7VSpUr65Zdf3MZS22ZBjrdC5FOLFi0yX375pdm6datZvHixadKkiSlbtqxJSEhId7yXl5f5/PPP3Z6bNGmSKV26dG7EzRJdo0PvwoULpl69eqZbt25XnSezxyqn/HN/kpOTTdmyZc0DDzxgTp06ZZKSksybb75pJJk2bdpc97x9+vQx5cuXNxcuXMiB1E4pKSnmrrvuMs2aNXN7furUqWbx4sVm27Zt5tNPPzXh4eGmc+fO15zvP//5jwkICDA2m82UKVPGrFu37obybdu2zQQEBBgPDw9X95oxxnz22WfG29s7zfiGDRuaF154IcP5Ll68aHr06OHqOvP29s5S53NGuYzJ+rFL9dZbb5lixYpd18/9iy++MDVq1HCNvbJrMKvHKNW1OnOvtm1jjHniiSfMY4895vr+Wu/7a237Wtu7Up8+fUzVqlXTPH+1z4x//etfRlKa43614/Xrr78aSebvv/92m7t58+amRIkSaT6PJk2aZAICAowkU7ly5Qy7cq+ce+rUqW55/f39XedTZs619PY9ODjYBAcHmwsXLpjTp0+bNm3auN4bgYGBZsmSJcYYY3x8fIyPj48ZOnSo2bRpk5k6darx9fU1M2fOdMv60UcfuW3zgQceMHa7Pd11x48fbySZVatWua0zaNAg4+/vn+F6M2fONEeOHHGtm/qZI8lIMiVKlEg3ozHOc2ju3Lnm0UcfdY2XZPr27ZtmfGp3qo+PjwkNDTXe3t7G09PT1VWa3vwXL140UVFRbp8vzz//vPHw8DCSzMaNG40xxtV5bIwxq1atMrNmzTKbN282vr6+Jjg42Pj5+RkPDw9z5MgR19xTpkxxde7qf525xhgTERFhSpQokWZ86nZ8fHyMJBMREWHq1q1rypYta2bOnJlmfMeOHV3nsjH//35t3LixsdlsrrGrV682kkyxYsWMJOPr62tuvfVW4+XlZZ599lkjydjt9jRZUvXp08fts2DOnDluWWJjY423t7frZ2Oz2UzNmjVd37///vtuOa881x588EG37KmuPF+u9Pzzz5tGjRqlmxPZi9qW2jYVtS217bVQ2w5Id31q27SobaltqW2pba1CbUttm4raltr2WqhtB6S7PrVtWtS2BbO2LV68uJGUppacNGmS8fX1NZJMVFSUmT59uut8/2dtm/rz69q1q2t9SaZp06amSZMmbmOpbTOPRoXrdPr0aRMYGOi6PNE/FbSCNzk52XTo0MHUrVvXxMfHZ2reax2rnJLe/mzYsMHUrl3bSDIeHh6mbdu25s477zTt2rW7rjnHjBljihUrZrZu3ZoDif/fU089ZcqVK2cOHTp01XHLli0zUsaXO0p17tw5s3v3brN69Wrz6KOPmqioKBMXF5flfElJSWb37t1mw4YNZsiQIaZkyZLmjz/+yHIx984775hKlSqZBQsWmK1bt5r33nvPFClSxCxdujRbcqXneo9dqsqVK5v+/ftfc9zBgwdN6dKl3c6R3Cp4r7Xt+fPnm4oVK5qzZ8+6Xr+Rgvda27vS+fPnTVBQkBk7duw1t3PlZ0ZYWJix2+1pxlxvwXulBx54wHTq1CnN59GZM2fMrl27zMqVK02HDh1MvXr1MvyDTXpznz592nh6epoGDRqku05mzrXTp08bu93uulRd//79TaNGjcyPP/5otmzZYkaNGmWCgoLMtm3bjJeXl2nSpInb+k8//bRp3LixW9aMCt701q1Xr16aIiQ5OdlUqFDB+Pv7X3WbVxYwqZ85np6ext/f33h7e7s+c67MmOqLL74wERERxsPDw1StWtVIMkWLFjUzZ850G5+6DR8fH7N161ZXnhIlSphKlSqlO/8777xjKlSoYKKjo43NZnN9pV7aLNWVBe+VAgICTMOGDY2fn5+5+eab3V672l/m+vr6mrvvvjvNfP8832rXrm2KFi1qqlev7jZ+/vz5JiIiIt2/zA0JCXG7jF3qz7p///5uRXLNmjXNkCFDTKlSpUyZMmXSZDHm/9+bV34WtGnTxi3LF1984SriUwteb29vU65cOVOuXDnTunXrfFfwIi1q2+tHbZt51LbUthmhtnWitqW2pbaltkX2ora9ftS2mUdtS22bEWpbJ2pbatu8XNv6+PgYX1/fNHOld64dPXrUBAYGpqltUxvpdu/e7XoutVEhJCTEbSy1beZx64frFBwcrEqVKmnPnj3pvh4aGqq4uDi35+Li4rLtkj256dKlS3rwwQd14MABLV26VIGBgZla/1rHKjfVr19fW7Zs0ZkzZ3T06FEtXrxYJ0+eVPny5a+57tixY/Xmm2/qhx9+UK1atXIsY//+/fXdd99p+fLlioiIuOrY6OhoSbrmsQ0ICFDFihXVuHFjffTRR/L09NRHH32U5Yze3t6qWLGi6tevrzFjxqh27dqaMGGCQkNDlZycrDNnzriNv9q5f+HCBb300ksaN26cOnTooFq1aql///7q0qWLxo4dmy250nO9x06Sfv75Z+3cuVOPP/74Ncdu3LhRx44dU7169eTp6SlPT0+tXLlSEydOlKenp0JCQjJ9jK7Xtba9dOlS7d27V8HBwa7XJem+++5Ty5Yts317KSkprrFff/21zp8/rx49elxz3tTPjOXLl+vo0aNyOByZOl6pz6f3GVy2bNk0n0dBQUG6+eabdeutt+rrr7/Wjh07NHfu3OueOzg4WL6+vnL+Tk8rM+fab7/9JofDoaioKO3du1fvv/++pk+frlatWql27doaOXKkGjRooEmTJiksLEzVqlVzW79q1aquS6SlZk29HOGVxyEgICDddWNjY+Xh4eHav9TP/1OnTunWW2+96jZLlizpWjf1M6dMmTIqU6aMvLy8XJ85V2ZM9fzzz2vIkCEKDw9X06ZNVbJkSd12220aM2aM2/jUbSQlJalevXq6dOmS1qxZo5MnT2rXrl3y9PRU5cqVXeNTP18mTJigNWvW6Pz58zp06JDat2+vS5cuqWTJkq4Mqb8HDhw44Jbt4sWLCg4O1oULFxQSEuL2WuXKlSUpzf7Ex8fr4sWL6X5m/PN82717t4KCgvTnn3+6jf/pp5905MgRSVJkZKTr/XrvvfcqLi5O9erVc40NCwuT5Pwd5+np6foZVa1aVdu3b9eJEycy/N2d+t5MdeDAAf34449uWZ5//nmNGDFCnp6eGjJkiE6dOqXhw4fr8OHDioqK0qlTpySl/57L6H165flyvesgZ1HbXj9q28yhtqW2zSpqWydqW2pbaltqW2Qete31o7bNHGpbatusorZ1oraltrWytj1w4ICSkpLSPT/TO9eWL1+ucuXKpaltd+zYIcl5q5Mr36+rVq1SXFyc21hq28yjUeE6nTt3Tnv37nWdZP/UpEkTLVu2zO25pUuXut13KT9I/bDbvXu3fvzxR5UoUSLTc1zrWFkhKChIpUqV0u7du7VhwwZ17NjxquPffvttvfbaa1q8eLEaNGiQI5mMMerfv7/mzp2rn376STfddNM119myZYskZfrYOhwO173fskPqfPXr15eXl5fbub9z504dPHgww3P/0qVLunTpkux2948fDw8PORyObMmVnswcu48++kj169e/rvvDtWrVSr/99pu2bNni+mrQoIG6d+/uWs7sMbpe19r2yy+/rG3btrm9Lknjx4/XjBkzsn17Hh4errEfffSR7rnnHpUqVeqa86Z+ZuzevVt16tTJ9PG66aabFBoa6rZOQkKC1q5dq7p1617188g4ryyU4XmT3tx///23zp07pxo1aqS7TmbOtSlTpsjDw0O1a9d2FSEZvTeaNWumnTt3ur22a9culStXzpVVkrZt2+Z6PfU41KxZM8N169evr2XLlrl9/vv4+KhFixZX3aa3t7dr3VRNmzbVwYMH5ePj4zqmV2ZMdf78edntdjVr1kzbtm3TyZMnFRQUJIfD4TY+dRt33323tmzZojvvvFN169ZVcHCwoqKitGXLFu3Zs8c1/p+fL76+vgoPD3fd26tXr16uDA888IAk6f3333c99/333yslJUXe3t7y8PBQ/fr13XLfeuutstvtWrp0qeu5w4cP6+zZs/L399ddd92lq0k932JjY1W0aFG38UOGDNHWrVtVokQJDRw40HUetWrVSpLUtWtX19ioqCiVKVNGe/fuVcOGDV0/o127dunkyZPy9vbO8PMr9b2ZasaMGSpdurRblvPnz8vb21sNGzbU4cOHFRwcrP379yslJUWenp6qVKlShu+5jN6n6Z0vDodDy5Yty3e1UkFBbXv9qG2vD7UttS21rRO1LbUttS21LXIfte31o7a9PtS21LbUtk7UttS2+bm2TW2Out66Nj4+Xrt3705T244ePdqtrk09j2w2mwIDA93GUttmQY5fsyGfevbZZ82KFSvMvn37zK+//mpat25tSpYsaY4dO2aMMebhhx82Q4YMcY3/9ddfjaenpxk7dqzZvn27GTlypPHy8jK//fabVbuQrrNnz5rNmzebzZs3G0lm3LhxZvPmzebAgQMmOTnZ3HPPPSYiIsJs2bLFHD161PWVlJTkmuP222837733nuv7ax0rq/bHGGO+/PJLs3z5crN3714zb948U65cOXPvvfe6zfHPn+Wbb75pvL29zddff+12DK68DFN26NOnjwkKCjIrVqxw28758+eNMcbs2bPHvPrqq2bDhg1m3759Zv78+aZ8+fLm1ltvdZuncuXK5ttvvzXGOC8dNnToULN69Wqzf/9+s2HDBtOrVy/j4+Njfv/99yzlHDJkiFm5cqXZt2+f2bZtmxkyZIix2Wzmhx9+MMY4L39WtmxZ89NPP5kNGzaYJk2apLnk0JUZjXFedqp69epm+fLl5q+//jIzZswwvr6+5oMPPsiWXFk5dqni4+ONv7+/mTx5cmYPldv+XXlZrawco5MnT5rNmzebhQsXGklm9uzZZvPmzebo0aOZ2vY/KZ1LiN3IttPb3u7du43NZjPff/99uhmKFStmXnvtNbfPjBIlShg/Pz8zefLkLB2vN9980wQHB5tOnTqZ6dOnmzvuuMOEhYWZ22+/3fV5tHfvXjN69GizYcMGc+DAAfPrr7+aDh06mOLFi7tdYu+fczdv3twUKVLETJs2zXz88cemVKlSxm63m4MHD2b6XLvy8/KHH34wdrvdFClSxBw7dswkJyebihUrmubNm5u1a9eaPXv2mLFjxxqbzWYWLlxo1q1bZzw9PU358uXNiBEjzGeffWb8/f3Nv//9b7fPQT8/PzN+/HizZMkS07FjR3PTTTeZn3/+2Xh6epo33njDNG7c2PTs2dP4+/ubTz/91MyePdt4e3ubunXrmtDQUHPfffeZwMBAs23bNvP999+71tu9e7epVq2a8fb2Np9++qkxxrju1zVs2DCzdOlS07JlS9clGxctWuTKWK1aNfPee++Zs2fPmueee860b9/ehISEmKeeesp1+bDg4GBz9913u403xphvv/3WeHl5mWnTpplvvvnG2O12I8m0a9fONX+zZs1cn+MtWrQw5cuXN6+88opZsWKFefHFF12ZUi/5lfq5X61aNdflJV944QUTEBBg/Pz8jL+/v/Hw8DB//PGH8fb2dl2+7ujRo6Zp06auS2u9+uqrrkttpb4PUn9Hpp5vDz30kJkzZ475+uuvTbNmzYynp6ex2+3m6aefvuq5PH/+fCPJeHt7m6CgINdl7lLHjx8/3gQGBprnnnvOeHp6mrvuuss11mazmZ9//jnN7+stW7YYm83mulfZ2LFjTWhoqOnTp4/b3D179jTFihUzPXv2NB4eHub22283NpvNlC1b1nh4eJiff/7ZvPnmm8bT09M88cQTZtu2baZjx44mKirKrFmzxnUu3nzzzebFF190/U6ePXu28fHxMTNnzjR//vmneeKJJ0xwcLCJjY1N97MC2YvaltqW2taJ2jbzqG2pbaltqW2pbalt8xpqW2pbalsnatvMo7altqW2LRy17ahRo4zdbjc2m8019+23325GjhzpOtd69+5t3n//fdOqVSsTGBhomjdv7qptr1bXbtu2zUgydrvdPPvss2nOJWrbzKFRIQNdunQxYWFhxtvb24SHh5suXbq43bemRYsWpmfPnm7rfPnll6ZSpUrG29vbVK9e3SxcuDCXU1/b8uXLXW/UK7969uzpuq9Rel/Lly93zVGuXDkzcuRI1/fXOlZW7Y8xxkyYMMFEREQYLy8vU7ZsWTNs2DC34t2YtD/LcuXKpTvnlfucHTI61jNmzDDGOO8rdeutt5rixYsbHx8fU7FiRfP888+nuffcletcuHDBdO7c2ZQpU8Z4e3ubsLAwc88995h169ZlOeejjz5qypUrZ7y9vU2pUqVMq1atXMVu6jb79u1rihUrZvz9/U3nzp3TFEZXZjTG+UvjkUceMWXKlDG+vr6mcuXK5t133zUOhyNbcmXl2KWaOnWq8fPzM2fOnLnuLP/0zyIwK8doxowZWToPs1Lw3si209ve0KFDTWRkpElJSckwQ3BwsNtnxuuvv+467lk5Xg6HwwwfPtz4+Pi47s0UEhLi9nl05MgRc+edd5rSpUsbLy8vExERYbp162Z27Nhx1bm7dOliihQp4joOpUuXdt2XL7Pn2pWfl8HBwcbDw8PtPna7du0y9957ryldurTx9/c3tWrVMh9//LHr9f/85z/Gy8vLeHh4mCpVqphp06Zl+Dlot9tNq1atzM6dO13r1qhRw0gyJUuWNNOmTXPNO2rUqAw/k0aPHm1q1KhhfHx8jKenp9s9sS5cuGBq1aplPDw8jCTj5eVlqlWrZipUqGB8fHxcGVN/b5w/f960adPGlCxZ0tjtduPh4WHsdrtrnypXruw2PtVHH31kKlasaHx9fc1NN91kfHx83I7BlZ/jR48eNe3atTOenp5u+/HZZ5+55ksdf/r0adcxSf0qWrSo2/tEknnssceMMcaMHDkyw+OUepxTs6eeb6nnZOofRho0aOA2PqNzOSQkxLXe4sWL0z0/x4wZYyIiIoy3t7fx9fV17fOkSZPcsqTq1q1butk7derkNndCQoKpX7++6w8Xqe+pGjVqmHnz5rlyBgUFmYCAAOPj42NatWplPv7446v+TjbGmPfee8+ULVvWeHt7m0aNGpk1a9YY5A5qW2pbalsnatvMo7altqW2pbaltqW2zWuobaltqW2dqG0zj9qW2pbatnDVtr169XLNXa5cOTN48GDXuWa3211fpUuXNi1atHDVtlera6+siVN/hv88P6ltr5/tfzsIAAAAAAAAAAAAAACQ4+zXHgIAAAAAAAAAAAAAAJA9aFQAAAAAAAAAAAAAAAC5hkYFAAAAAAAAAAAAAACQa2hUAAAAAAAAAAAAAAAAuYZGBQAAAAAAAAAAAAAAkGtoVAAAAAAAAAAAAAAAALmGRgUAAAAAAAAAAAAAAJBraFQAAAAAAAAAAAAAAAC5hkYFACjgRo0apZCQENlsNs2bN++61lmxYoVsNpvOnDmTo9nykqioKMXExFgdAwAAAFdBbXt9qG0BAADyPmrb60NtCxRcNCoAyHWPPPKIbDabbDabvL29VbFiRb366qu6fPmy1dGuKTNFY16wfft2vfLKK5o6daqOHj2qO++8M8e21bJlSw0cODDH5gcAAMiLqG1zD7UtAABAzqK2zT3UtgAgeVodAEDh1K5dO82YMUNJSUlatGiR+vXrJy8vLw0dOjTTc6WkpMhms8lup/fqn/bu3StJ6tixo2w2m8VpAAAACiZq29xBbQsAAJDzqG1zB7UtAHBFBQAW8fHxUWhoqMqVK6c+ffqodevWWrBggSQpKSlJzz33nMLDwxUQEKDo6GitWLHCte7MmTMVHBysBQsWqFq1avLx8dHBgweVlJSkF198UZGRkfLx8VHFihX10Ucfudb7/fffdeedd6pIkSIKCQnRww8/rBMnTrheb9mypZ555hm98MILKl68uEJDQzVq1CjX61FRUZKkzp07y2azub7fu3evOnbsqJCQEBUpUkQNGzbUjz/+6La/R48e1V133SU/Pz/ddNNN+vzzz9NcsurMmTN6/PHHVapUKQUGBur222/X1q1br3ocf/vtN91+++3y8/NTiRIl9MQTT+jcuXOSnJcO69ChgyTJbrdfteBdtGiRKlWqJD8/P912223av3+/2+snT55U165dFR4eLn9/f9WsWVNffPGF6/VHHnlEK1eu1IQJE1xd1/v371dKSooee+wx3XTTTfLz81PlypU1YcKEq+5T6s/3SvPmzXPLv3XrVt12220qWrSoAgMDVb9+fW3YsMH1+i+//KLmzZvLz89PkZGReuaZZ5SYmOh6/dixY+rQoYPr5/HZZ59dNRMAAMDVUNtS22aE2hYAAOQ31LbUthmhtgWQ3WhUAJAn+Pn5KTk5WZLUv39/rV69WrNnz9a2bdv0wAMPqF27dtq9e7dr/Pnz5/XWW2/p3//+t/744w+VLl1aPXr00BdffKGJEydq+/btmjp1qooUKSLJWUzefvvtqlu3rjZs2KDFixcrLi5ODz74oFuOWbNmKSAgQGvXrtXbb7+tV199VUuXLpUkrV+/XpI0Y8YMHT161PX9uXPn1L59ey1btkybN29Wu3bt1KFDBx08eNA1b48ePfT3339rxYoV+uabbzRt2jQdO3bMbdsPPPCAjh07pu+//14bN25UvXr11KpVK506dSrdY5aYmKi2bduqWLFiWr9+vb766iv9+OOP6t+/vyTpueee04wZMyQ5C+6jR4+mO8+hQ4d07733qkOHDtqyZYsef/xxDRkyxG3MxYsXVb9+fS1cuFC///67nnjiCT388MNat26dJGnChAlq0qSJevfu7dpWZGSkHA6HIiIi9NVXX+nPP//UiBEj9NJLL+nLL79MN8v16t69uyIiIrR+/Xpt3LhRQ4YMkZeXlyTnH0DatWun++67T9u2bdOcOXP0yy+/uI6L5CzQDx06pOXLl+vrr7/WBx98kObnAQAAkFXUttS2mUFtCwAA8jJqW2rbzKC2BZApBgByWc+ePU3Hjh2NMcY4HA6zdOlS4+PjY5577jlz4MAB4+HhYY4cOeK2TqtWrczQoUONMcbMmDHDSDJbtmxxvb5z504jySxdujTdbb722mumTZs2bs8dOnTISDI7d+40xhjTokULc8stt7iNadiwoXnxxRdd30syc+fOveY+Vq9e3bz33nvGGGO2b99uJJn169e7Xt+9e7eRZMaPH2+MMebnn382gYGB5uLFi27zVKhQwUydOjXdbUybNs0UK1bMnDt3zvXcwoULjd1uN7GxscYYY+bOnWuu9VE/dOhQU61aNbfnXnzxRSPJnD59OsP17rrrLvPss8+6vm/RooUZMGDAVbdljDH9+vUz9913X4avz5gxwwQFBbk998/9KFq0qJk5c2a66z/22GPmiSeecHvu559/Nna73Vy4cMF1rqxbt871eurPKPXnAQAAcL2obaltqW0BAEBBQW1LbUttCyA3eeZ4JwQApOO7775TkSJFdOnSJTkcDnXr1k2jRo3SihUrlJKSokqVKrmNT0pKUokSJVzfe3t7q1atWq7vt2zZIg8PD7Vo0SLd7W3dulXLly93depeae/eva7tXTmnJIWFhV2zY/PcuXMaNWqUFi5cqKNHj+ry5cu6cOGCqzN3586d8vT0VL169VzrVKxYUcWKFXPLd+7cObd9lKQLFy647lf2T9u3b1ft2rUVEBDgeq5Zs2ZyOBzauXOnQkJCrpr7ynmio6PdnmvSpInb9ykpKRo9erS+/PJLHTlyRMnJyUpKSpK/v/815580aZKmT5+ugwcP6sKFC0pOTladOnWuK1tGBg8erMcff1yffPKJWrdurQceeEAVKlSQ5DyW27Ztc7ssmDFGDodD+/bt065du+Tp6an69eu7Xq9SpUqay5YBAABcL2pbatsbQW0LAADyEmpbatsbQW0LIDNoVABgidtuu02TJ0+Wt7e3ypQpI09P58fRuXPn5OHhoY0bN8rDw8NtnSuLVT8/P7d7X/n5+V11e+fOnVOHDh301ltvpXktLCzMtZx6GapUNptNDofjqnM/99xzWrp0qcaOHauKFSvKz89P999/v+uSaNfj3LlzCgsLc7unW6q8UIi98847mjBhgmJiYlSzZk0FBARo4MCB19zH2bNn67nnntO7776rJk2aqGjRonrnnXe0du3aDNex2+0yxrg9d+nSJbfvR40apW7dumnhwoX6/vvvNXLkSM2ePVudO3fWuXPn9OSTT+qZZ55JM3fZsmW1a9euTOw5AADAtVHbps1HbetEbQsAAPIbatu0+ahtnahtAWQ3GhUAWCIgIEAVK1ZM83zdunWVkpKiY8eOqXnz5tc9X82aNeVwOLRy5Uq1bt06zev16tXTN998o6ioKFdxnRVeXl5KSUlxe+7XX3/VI488os6dO0tyFq/79+93vV65cmVdvnxZmzdvdnWD7tmzR6dPn3bLFxsbK09PT0VFRV1XlqpVq2rmzJlKTEx0def++uuvstvtqly58nXvU9WqVbVgwQK359asWZNmHzt27KiHHnpIkuRwOLRr1y5Vq1bNNcbb2zvdY9O0aVP17dvX9VxGncapSpUqpbNnz7rt15YtW9KMq1SpkipVqqRBgwapa9eumjFjhjp37qx69erpzz//TPf8kpxduJcvX9bGjRvVsGFDSc7u6TNnzlw1FwAAQEaobaltM0JtCwAA8htqW2rbjFDbAshudqsDAMCVKlWqpO7du6tHjx769ttvtW/fPq1bt05jxozRwoULM1wvKipKPXv21KOPPqp58+Zp3759WrFihb788ktJUr9+/XTq1Cl17dpV69ev1969e7VkyRL16tUrTZF2NVFRUVq2bJliY2NdBevNN9+sb7/9Vlu2bNHWrVvVrVs3t27eKlWqqHXr1nriiSe0bt06bd68WU888YRbd3Hr1q3VpEkTderUST/88IP279+vVatW6eWXX9aGDRvSzdK9e3f5+vqqZ8+e+v3337V8+XI9/fTTevjhh6/78mGS9NRTT2n37t16/vnntXPnTn3++eeaOXOm25ibb75ZS5cu1apVq7R9+3Y9+eSTiouLS3Ns1q5dq/379+vEiRNyOBy6+eabtWHDBi1ZskS7du3S8OHDtX79+qvmiY6Olr+/v1566SXt3bs3TZ4LFy6of//+WrFihQ4cOKBff/1V69evV9WqVSVJL774olatWqX+/ftry5Yt2r17t+bPn6/+/ftLcv4BpF27dnryySe1du1abdy4UY8//vg1u7sBAAAyi9qW2pbaFgAAFBTUttS21LYAshuNCgDynBkzZqhHjx569tlnVblyZXXq1Enr169X2bJlr7re5MmTdf/996tv376qUqWKevfurcTERElSmTJl9OuvvyolJUVt2rRRzZo1NXDgQAUHB8tuv/6PwnfffVdLly5VZGSk6tatK0kaN26cihUrpqZNm6pDhw5q27at233NJOnjjz9WSEiIbr31VnXu3Fm9e/dW0aJF5evrK8l5qbJFixbp1ltvVa9evVSpUiX961//0oEDBzIsXv39/bVkyRKdOnVKDRs21P33369WrVrp/fffv+79kZyX1frmm280b9481a5dW1OmTNHo0aPdxgwbNkz16tVT27Zt1bJlS4WGhqpTp05uY5577jl5eHioWrVqKlWqlA4ePKgnn3xS9957r7p06aLo6GidPHnSrUs3PcWLF9enn36qRYsWqWbNmvriiy80atQo1+seHh46efKkevTooUqVKunBBx/UnXfeqVdeeUWS8351K1eu1K5du9S8eXPVrVtXI0aMUJkyZVxzzJgxQ2XKlFGLFi1077336oknnlDp0qUzddwAAACuB7UttS21LQAAKCiobaltqW0BZCeb+ecNZQAAOe7w4cOKjIzUjz/+qFatWlkdBwAAAMgyalsAAAAUFNS2AJB7aFQAgFzw008/6dy5c6pZs6aOHj2qF154QUeOHNGuXbvk5eVldTwAAADgulHbAgAAoKCgtgUA63haHQAACoNLly7ppZde0l9//aWiRYuqadOm+uyzzyh2AQAAkO9Q2wIAAKCgoLYFAOtwRQUAAAAAAAAAAAAAAJBr7FYHAAAAAAAAAAAAAAAAhQeNCgAAAAAAAAAAAAAAINfQqAAAAAAAAAAAAAAAAHINjQoAAAAAAAAAAAAAACDX0KgAAAAAAAAAAAAAAAByDY0KAAAAAAAAAAAAAAAg19CoAAAAAAAAAAAAAAAAcg2NCgAAAAAAAAAAAAAAINfQqAAAAAAAAAAAAAAAAHLN/wFvjMkYhK6nqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3405f0a",
   "metadata": {
    "papermill": {
     "duration": 0.012153,
     "end_time": "2025-06-08T08:00:31.451814",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.439661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2cb21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6119, Accuracy: 0.83, F1 Micro: 0.1445, F1 Macro: 0.045\n",
      "Epoch 2/10, Train Loss: 0.4724, Accuracy: 0.8287, F1 Micro: 0.0209, F1 Macro: 0.0091\n",
      "Epoch 3/10, Train Loss: 0.3967, Accuracy: 0.8346, F1 Micro: 0.1256, F1 Macro: 0.0415\n",
      "Epoch 4/10, Train Loss: 0.4024, Accuracy: 0.8353, F1 Micro: 0.122, F1 Macro: 0.0414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3867, Accuracy: 0.8389, F1 Micro: 0.1605, F1 Macro: 0.0558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3732, Accuracy: 0.8487, F1 Micro: 0.2747, F1 Macro: 0.0919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3557, Accuracy: 0.8622, F1 Micro: 0.4166, F1 Macro: 0.1524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3336, Accuracy: 0.8704, F1 Micro: 0.4692, F1 Macro: 0.2104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3008, Accuracy: 0.8767, F1 Micro: 0.5572, F1 Macro: 0.2597\n",
      "Epoch 10/10, Train Loss: 0.2662, Accuracy: 0.8768, F1 Micro: 0.5415, F1 Macro: 0.2556\n",
      "Model 1 - Iteration 658: Accuracy: 0.8767, F1 Micro: 0.5572, F1 Macro: 0.2597\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.69      0.74      1134\n",
      "      Abusive       0.80      0.73      0.77       992\n",
      "HS_Individual       0.66      0.44      0.53       732\n",
      "     HS_Group       0.25      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.53      0.59       762\n",
      "      HS_Weak       0.62      0.38      0.47       689\n",
      "  HS_Moderate       0.11      0.01      0.01       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.45      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.59      0.45      0.50      5556\n",
      "  samples avg       0.38      0.27      0.28      5556\n",
      "\n",
      "Training completed in 57.48400664329529 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5875, Accuracy: 0.8215, F1 Micro: 0.2912, F1 Macro: 0.1065\n",
      "Epoch 2/10, Train Loss: 0.4609, Accuracy: 0.829, F1 Micro: 0.0373, F1 Macro: 0.0153\n",
      "Epoch 3/10, Train Loss: 0.3929, Accuracy: 0.8345, F1 Micro: 0.1121, F1 Macro: 0.041\n",
      "Epoch 4/10, Train Loss: 0.3973, Accuracy: 0.838, F1 Micro: 0.1566, F1 Macro: 0.0553\n",
      "Epoch 5/10, Train Loss: 0.381, Accuracy: 0.8457, F1 Micro: 0.2304, F1 Macro: 0.0814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3626, Accuracy: 0.8547, F1 Micro: 0.3324, F1 Macro: 0.1095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3452, Accuracy: 0.8677, F1 Micro: 0.4694, F1 Macro: 0.1905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3219, Accuracy: 0.872, F1 Micro: 0.4893, F1 Macro: 0.2195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2911, Accuracy: 0.8765, F1 Micro: 0.5515, F1 Macro: 0.2656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2588, Accuracy: 0.8769, F1 Micro: 0.5702, F1 Macro: 0.2909\n",
      "Model 2 - Iteration 658: Accuracy: 0.8769, F1 Micro: 0.5702, F1 Macro: 0.2909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.71      0.75      1134\n",
      "      Abusive       0.84      0.69      0.76       992\n",
      "HS_Individual       0.64      0.48      0.55       732\n",
      "     HS_Group       0.55      0.15      0.23       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.55      0.59       762\n",
      "      HS_Weak       0.62      0.42      0.50       689\n",
      "  HS_Moderate       0.32      0.07      0.11       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.47      0.57      5556\n",
      "    macro avg       0.37      0.26      0.29      5556\n",
      " weighted avg       0.62      0.47      0.53      5556\n",
      "  samples avg       0.35      0.27      0.28      5556\n",
      "\n",
      "Training completed in 53.17448377609253 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5819, Accuracy: 0.8291, F1 Micro: 0.0282, F1 Macro: 0.0117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.8332, F1 Micro: 0.1093, F1 Macro: 0.0373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4155, Accuracy: 0.8362, F1 Micro: 0.1859, F1 Macro: 0.0522\n",
      "Epoch 4/10, Train Loss: 0.4114, Accuracy: 0.8302, F1 Micro: 0.0304, F1 Macro: 0.0132\n",
      "Epoch 5/10, Train Loss: 0.3932, Accuracy: 0.8364, F1 Micro: 0.1256, F1 Macro: 0.0443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3778, Accuracy: 0.8429, F1 Micro: 0.1953, F1 Macro: 0.07\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3695, Accuracy: 0.8553, F1 Micro: 0.335, F1 Macro: 0.1159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.348, Accuracy: 0.8654, F1 Micro: 0.4275, F1 Macro: 0.1796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3138, Accuracy: 0.874, F1 Micro: 0.5643, F1 Macro: 0.2599\n",
      "Epoch 10/10, Train Loss: 0.2796, Accuracy: 0.8757, F1 Micro: 0.5374, F1 Macro: 0.2532\n",
      "Model 3 - Iteration 658: Accuracy: 0.874, F1 Micro: 0.5643, F1 Macro: 0.2599\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.77      0.76      1134\n",
      "      Abusive       0.76      0.70      0.73       992\n",
      "HS_Individual       0.63      0.48      0.54       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.58      0.60       762\n",
      "      HS_Weak       0.63      0.40      0.49       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.47      0.56      5556\n",
      "    macro avg       0.28      0.24      0.26      5556\n",
      " weighted avg       0.54      0.47      0.50      5556\n",
      "  samples avg       0.37      0.27      0.29      5556\n",
      "\n",
      "Training completed in 57.322962522506714 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8758, F1 Micro: 0.5639, F1 Macro: 0.2702\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 324.8998854448068\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 629.0547597408295 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.8296, F1 Micro: 0.032, F1 Macro: 0.0136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4011, Accuracy: 0.8346, F1 Micro: 0.095, F1 Macro: 0.0361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3755, Accuracy: 0.8554, F1 Micro: 0.3514, F1 Macro: 0.1126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3398, Accuracy: 0.8692, F1 Micro: 0.4449, F1 Macro: 0.1997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3063, Accuracy: 0.8792, F1 Micro: 0.5958, F1 Macro: 0.287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2812, Accuracy: 0.8848, F1 Micro: 0.6278, F1 Macro: 0.3338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2468, Accuracy: 0.8884, F1 Micro: 0.6373, F1 Macro: 0.3499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2143, Accuracy: 0.8888, F1 Micro: 0.6432, F1 Macro: 0.3835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1997, Accuracy: 0.8907, F1 Micro: 0.6606, F1 Macro: 0.4193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1757, Accuracy: 0.893, F1 Micro: 0.6699, F1 Macro: 0.4663\n",
      "Model 1 - Iteration 1646: Accuracy: 0.893, F1 Micro: 0.6699, F1 Macro: 0.4663\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.79      0.80      1134\n",
      "      Abusive       0.90      0.75      0.82       992\n",
      "HS_Individual       0.65      0.62      0.63       732\n",
      "     HS_Group       0.61      0.54      0.57       402\n",
      "  HS_Religion       0.59      0.28      0.38       157\n",
      "      HS_Race       0.84      0.39      0.53       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.71      0.69       762\n",
      "      HS_Weak       0.62      0.59      0.61       689\n",
      "  HS_Moderate       0.46      0.43      0.44       331\n",
      "    HS_Strong       1.00      0.06      0.12       114\n",
      "\n",
      "    micro avg       0.71      0.63      0.67      5556\n",
      "    macro avg       0.60      0.43      0.47      5556\n",
      " weighted avg       0.71      0.63      0.66      5556\n",
      "  samples avg       0.36      0.34      0.33      5556\n",
      "\n",
      "Training completed in 79.65512561798096 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5168, Accuracy: 0.8304, F1 Micro: 0.0566, F1 Macro: 0.0222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3965, Accuracy: 0.84, F1 Micro: 0.1645, F1 Macro: 0.0618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3699, Accuracy: 0.8565, F1 Micro: 0.3658, F1 Macro: 0.1151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3347, Accuracy: 0.8672, F1 Micro: 0.4329, F1 Macro: 0.1916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.301, Accuracy: 0.8798, F1 Micro: 0.6076, F1 Macro: 0.3218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2748, Accuracy: 0.8846, F1 Micro: 0.6163, F1 Macro: 0.3577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2414, Accuracy: 0.8894, F1 Micro: 0.6355, F1 Macro: 0.3846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2096, Accuracy: 0.8907, F1 Micro: 0.649, F1 Macro: 0.4338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.193, Accuracy: 0.8921, F1 Micro: 0.667, F1 Macro: 0.4446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1736, Accuracy: 0.8925, F1 Micro: 0.6677, F1 Macro: 0.4981\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8925, F1 Micro: 0.6677, F1 Macro: 0.4981\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.79      0.80      1134\n",
      "      Abusive       0.88      0.76      0.82       992\n",
      "HS_Individual       0.69      0.56      0.62       732\n",
      "     HS_Group       0.57      0.61      0.59       402\n",
      "  HS_Religion       0.59      0.24      0.34       157\n",
      "      HS_Race       0.78      0.47      0.59       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.69      0.69       762\n",
      "      HS_Weak       0.65      0.53      0.58       689\n",
      "  HS_Moderate       0.43      0.46      0.44       331\n",
      "    HS_Strong       0.86      0.38      0.52       114\n",
      "\n",
      "    micro avg       0.71      0.63      0.67      5556\n",
      "    macro avg       0.58      0.46      0.50      5556\n",
      " weighted avg       0.70      0.63      0.66      5556\n",
      "  samples avg       0.37      0.34      0.34      5556\n",
      "\n",
      "Training completed in 79.75913619995117 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5277, Accuracy: 0.8334, F1 Micro: 0.1225, F1 Macro: 0.0401\n",
      "Epoch 2/10, Train Loss: 0.4137, Accuracy: 0.8297, F1 Micro: 0.0238, F1 Macro: 0.0104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3825, Accuracy: 0.8441, F1 Micro: 0.2219, F1 Macro: 0.0756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3539, Accuracy: 0.8597, F1 Micro: 0.3594, F1 Macro: 0.147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3178, Accuracy: 0.878, F1 Micro: 0.5945, F1 Macro: 0.2855\n",
      "Epoch 6/10, Train Loss: 0.2904, Accuracy: 0.8822, F1 Micro: 0.5837, F1 Macro: 0.296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2537, Accuracy: 0.886, F1 Micro: 0.641, F1 Macro: 0.3691\n",
      "Epoch 8/10, Train Loss: 0.218, Accuracy: 0.8872, F1 Micro: 0.6256, F1 Macro: 0.3824\n",
      "Epoch 9/10, Train Loss: 0.203, Accuracy: 0.8895, F1 Micro: 0.6289, F1 Macro: 0.4058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1805, Accuracy: 0.8913, F1 Micro: 0.6701, F1 Macro: 0.4623\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8913, F1 Micro: 0.6701, F1 Macro: 0.4623\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.80      0.80      1134\n",
      "      Abusive       0.85      0.80      0.82       992\n",
      "HS_Individual       0.64      0.65      0.65       732\n",
      "     HS_Group       0.59      0.51      0.55       402\n",
      "  HS_Religion       0.59      0.28      0.38       157\n",
      "      HS_Race       0.88      0.29      0.44       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.70      0.69       762\n",
      "      HS_Weak       0.61      0.62      0.62       689\n",
      "  HS_Moderate       0.43      0.37      0.40       331\n",
      "    HS_Strong       0.82      0.12      0.21       114\n",
      "\n",
      "    micro avg       0.70      0.64      0.67      5556\n",
      "    macro avg       0.57      0.43      0.46      5556\n",
      " weighted avg       0.69      0.64      0.65      5556\n",
      "  samples avg       0.37      0.35      0.34      5556\n",
      "\n",
      "Training completed in 71.43134331703186 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8923, F1 Micro: 0.6692, F1 Macro: 0.4756\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 803.8612324245428\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 566.2328510284424 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.487, Accuracy: 0.8332, F1 Micro: 0.0894, F1 Macro: 0.0328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3799, Accuracy: 0.8528, F1 Micro: 0.3075, F1 Macro: 0.1063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3364, Accuracy: 0.8794, F1 Micro: 0.5667, F1 Macro: 0.2608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.294, Accuracy: 0.8854, F1 Micro: 0.6306, F1 Macro: 0.3198\n",
      "Epoch 5/10, Train Loss: 0.2427, Accuracy: 0.8917, F1 Micro: 0.6266, F1 Macro: 0.328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2315, Accuracy: 0.8956, F1 Micro: 0.6665, F1 Macro: 0.3777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2043, Accuracy: 0.8979, F1 Micro: 0.6715, F1 Macro: 0.4697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1867, Accuracy: 0.9014, F1 Micro: 0.6925, F1 Macro: 0.4602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1586, Accuracy: 0.9015, F1 Micro: 0.7123, F1 Macro: 0.52\n",
      "Epoch 10/10, Train Loss: 0.1381, Accuracy: 0.9025, F1 Micro: 0.709, F1 Macro: 0.5172\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9015, F1 Micro: 0.7123, F1 Macro: 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.86      0.82      1134\n",
      "      Abusive       0.83      0.88      0.85       992\n",
      "HS_Individual       0.67      0.70      0.68       732\n",
      "     HS_Group       0.64      0.58      0.61       402\n",
      "  HS_Religion       0.71      0.43      0.53       157\n",
      "      HS_Race       0.84      0.49      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.77      0.72       762\n",
      "      HS_Weak       0.64      0.67      0.66       689\n",
      "  HS_Moderate       0.49      0.47      0.48       331\n",
      "    HS_Strong       1.00      0.15      0.26       114\n",
      "\n",
      "    micro avg       0.72      0.71      0.71      5556\n",
      "    macro avg       0.61      0.50      0.52      5556\n",
      " weighted avg       0.70      0.71      0.70      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 90.61585783958435 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.475, Accuracy: 0.8355, F1 Micro: 0.1232, F1 Macro: 0.0455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3746, Accuracy: 0.8526, F1 Micro: 0.3083, F1 Macro: 0.1043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3319, Accuracy: 0.8783, F1 Micro: 0.5614, F1 Macro: 0.2615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2887, Accuracy: 0.8863, F1 Micro: 0.646, F1 Macro: 0.3674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2396, Accuracy: 0.8944, F1 Micro: 0.6539, F1 Macro: 0.3973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2276, Accuracy: 0.8966, F1 Micro: 0.6623, F1 Macro: 0.4059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1999, Accuracy: 0.8997, F1 Micro: 0.6713, F1 Macro: 0.4745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1829, Accuracy: 0.9022, F1 Micro: 0.6888, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1522, Accuracy: 0.9048, F1 Micro: 0.7094, F1 Macro: 0.5366\n",
      "Epoch 10/10, Train Loss: 0.1364, Accuracy: 0.903, F1 Micro: 0.6964, F1 Macro: 0.5205\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9048, F1 Micro: 0.7094, F1 Macro: 0.5366\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.81      0.82      1134\n",
      "      Abusive       0.88      0.82      0.85       992\n",
      "HS_Individual       0.69      0.67      0.68       732\n",
      "     HS_Group       0.67      0.56      0.61       402\n",
      "  HS_Religion       0.74      0.32      0.45       157\n",
      "      HS_Race       0.83      0.48      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.74      0.72       762\n",
      "      HS_Weak       0.66      0.63      0.65       689\n",
      "  HS_Moderate       0.51      0.44      0.47       331\n",
      "    HS_Strong       0.88      0.43      0.58       114\n",
      "\n",
      "    micro avg       0.75      0.67      0.71      5556\n",
      "    macro avg       0.62      0.49      0.54      5556\n",
      " weighted avg       0.73      0.67      0.70      5556\n",
      "  samples avg       0.40      0.37      0.37      5556\n",
      "\n",
      "Training completed in 91.56987190246582 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4888, Accuracy: 0.8339, F1 Micro: 0.098, F1 Macro: 0.0353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3881, Accuracy: 0.8528, F1 Micro: 0.3182, F1 Macro: 0.104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.348, Accuracy: 0.8774, F1 Micro: 0.5595, F1 Macro: 0.2589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2998, Accuracy: 0.8849, F1 Micro: 0.6208, F1 Macro: 0.3173\n",
      "Epoch 5/10, Train Loss: 0.2475, Accuracy: 0.8886, F1 Micro: 0.6152, F1 Macro: 0.3314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2329, Accuracy: 0.8952, F1 Micro: 0.6732, F1 Macro: 0.4122\n",
      "Epoch 7/10, Train Loss: 0.2048, Accuracy: 0.8951, F1 Micro: 0.6481, F1 Macro: 0.4669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1856, Accuracy: 0.8997, F1 Micro: 0.6765, F1 Macro: 0.46\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.154, Accuracy: 0.9027, F1 Micro: 0.7042, F1 Macro: 0.5347\n",
      "Epoch 10/10, Train Loss: 0.1373, Accuracy: 0.9031, F1 Micro: 0.6932, F1 Macro: 0.5327\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9027, F1 Micro: 0.7042, F1 Macro: 0.5347\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.84      0.84      0.84       992\n",
      "HS_Individual       0.70      0.64      0.67       732\n",
      "     HS_Group       0.64      0.58      0.61       402\n",
      "  HS_Religion       0.69      0.50      0.58       157\n",
      "      HS_Race       0.84      0.51      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.70      0.70       762\n",
      "      HS_Weak       0.67      0.62      0.64       689\n",
      "  HS_Moderate       0.49      0.46      0.47       331\n",
      "    HS_Strong       0.94      0.29      0.44       114\n",
      "\n",
      "    micro avg       0.74      0.67      0.70      5556\n",
      "    macro avg       0.61      0.50      0.53      5556\n",
      " weighted avg       0.72      0.67      0.69      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 87.87291026115417 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.903, F1 Micro: 0.7087, F1 Macro: 0.5304\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 690.0587280671243\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 509.61470103263855 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4689, Accuracy: 0.8394, F1 Micro: 0.1698, F1 Macro: 0.0619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3605, Accuracy: 0.8776, F1 Micro: 0.5468, F1 Macro: 0.2516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3035, Accuracy: 0.8864, F1 Micro: 0.6133, F1 Macro: 0.3142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2596, Accuracy: 0.8943, F1 Micro: 0.6628, F1 Macro: 0.4118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2255, Accuracy: 0.8997, F1 Micro: 0.6895, F1 Macro: 0.4555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.9034, F1 Micro: 0.6944, F1 Macro: 0.4943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1685, Accuracy: 0.9044, F1 Micro: 0.717, F1 Macro: 0.5181\n",
      "Epoch 8/10, Train Loss: 0.1465, Accuracy: 0.9035, F1 Micro: 0.7012, F1 Macro: 0.5249\n",
      "Epoch 9/10, Train Loss: 0.1255, Accuracy: 0.9062, F1 Micro: 0.7117, F1 Macro: 0.5414\n",
      "Epoch 10/10, Train Loss: 0.1082, Accuracy: 0.9068, F1 Micro: 0.7055, F1 Macro: 0.5416\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9044, F1 Micro: 0.717, F1 Macro: 0.5181\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.85      0.82      1134\n",
      "      Abusive       0.85      0.87      0.86       992\n",
      "HS_Individual       0.68      0.70      0.69       732\n",
      "     HS_Group       0.67      0.59      0.63       402\n",
      "  HS_Religion       0.75      0.37      0.50       157\n",
      "      HS_Race       0.86      0.50      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.77      0.72       762\n",
      "      HS_Weak       0.64      0.67      0.65       689\n",
      "  HS_Moderate       0.56      0.47      0.51       331\n",
      "    HS_Strong       0.93      0.11      0.20       114\n",
      "\n",
      "    micro avg       0.73      0.70      0.72      5556\n",
      "    macro avg       0.62      0.49      0.52      5556\n",
      " weighted avg       0.72      0.70      0.70      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 102.90155673027039 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4602, Accuracy: 0.8469, F1 Micro: 0.2587, F1 Macro: 0.0876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3558, Accuracy: 0.8774, F1 Micro: 0.5488, F1 Macro: 0.2517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2982, Accuracy: 0.8884, F1 Micro: 0.6251, F1 Macro: 0.3491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2572, Accuracy: 0.8959, F1 Micro: 0.6615, F1 Macro: 0.419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2246, Accuracy: 0.8993, F1 Micro: 0.7033, F1 Macro: 0.5257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1877, Accuracy: 0.9062, F1 Micro: 0.7085, F1 Macro: 0.5304\n",
      "Epoch 7/10, Train Loss: 0.1653, Accuracy: 0.9049, F1 Micro: 0.7013, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1452, Accuracy: 0.907, F1 Micro: 0.7208, F1 Macro: 0.5582\n",
      "Epoch 9/10, Train Loss: 0.1245, Accuracy: 0.9041, F1 Micro: 0.7207, F1 Macro: 0.5661\n",
      "Epoch 10/10, Train Loss: 0.1076, Accuracy: 0.9082, F1 Micro: 0.7121, F1 Macro: 0.5708\n",
      "Model 2 - Iteration 3335: Accuracy: 0.907, F1 Micro: 0.7208, F1 Macro: 0.5582\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.83      1134\n",
      "      Abusive       0.89      0.83      0.86       992\n",
      "HS_Individual       0.68      0.70      0.69       732\n",
      "     HS_Group       0.69      0.57      0.63       402\n",
      "  HS_Religion       0.66      0.55      0.60       157\n",
      "      HS_Race       0.73      0.55      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.72      0.72       762\n",
      "      HS_Weak       0.65      0.66      0.66       689\n",
      "  HS_Moderate       0.54      0.49      0.52       331\n",
      "    HS_Strong       0.98      0.41      0.58       114\n",
      "\n",
      "    micro avg       0.75      0.70      0.72      5556\n",
      "    macro avg       0.61      0.53      0.56      5556\n",
      " weighted avg       0.73      0.70      0.71      5556\n",
      "  samples avg       0.40      0.39      0.38      5556\n",
      "\n",
      "Training completed in 103.17700600624084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4729, Accuracy: 0.8303, F1 Micro: 0.0308, F1 Macro: 0.0133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3708, Accuracy: 0.8647, F1 Micro: 0.4169, F1 Macro: 0.1733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.313, Accuracy: 0.8833, F1 Micro: 0.5931, F1 Macro: 0.2905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.265, Accuracy: 0.8935, F1 Micro: 0.6594, F1 Macro: 0.3957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2277, Accuracy: 0.899, F1 Micro: 0.6943, F1 Macro: 0.4763\n",
      "Epoch 6/10, Train Loss: 0.1931, Accuracy: 0.9021, F1 Micro: 0.694, F1 Macro: 0.5017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1689, Accuracy: 0.9045, F1 Micro: 0.7014, F1 Macro: 0.5093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1492, Accuracy: 0.9062, F1 Micro: 0.7218, F1 Macro: 0.5641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1272, Accuracy: 0.9054, F1 Micro: 0.7239, F1 Macro: 0.5637\n",
      "Epoch 10/10, Train Loss: 0.1088, Accuracy: 0.9093, F1 Micro: 0.7202, F1 Macro: 0.5617\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9054, F1 Micro: 0.7239, F1 Macro: 0.5637\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.83      1134\n",
      "      Abusive       0.82      0.90      0.86       992\n",
      "HS_Individual       0.69      0.69      0.69       732\n",
      "     HS_Group       0.64      0.61      0.62       402\n",
      "  HS_Religion       0.73      0.54      0.62       157\n",
      "      HS_Race       0.78      0.57      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.75      0.73       762\n",
      "      HS_Weak       0.66      0.66      0.66       689\n",
      "  HS_Moderate       0.50      0.52      0.51       331\n",
      "    HS_Strong       0.91      0.45      0.60       114\n",
      "\n",
      "    micro avg       0.73      0.72      0.72      5556\n",
      "    macro avg       0.60      0.54      0.56      5556\n",
      " weighted avg       0.71      0.72      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 104.47721409797668 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9056, F1 Micro: 0.7206, F1 Macro: 0.5467\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 729.4260133922525\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 455.36017060279846 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4579, Accuracy: 0.8415, F1 Micro: 0.2019, F1 Macro: 0.0685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3482, Accuracy: 0.8779, F1 Micro: 0.6074, F1 Macro: 0.2811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2871, Accuracy: 0.8902, F1 Micro: 0.6094, F1 Macro: 0.3152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2555, Accuracy: 0.8952, F1 Micro: 0.6505, F1 Macro: 0.3822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2189, Accuracy: 0.9042, F1 Micro: 0.7035, F1 Macro: 0.4909\n",
      "Epoch 6/10, Train Loss: 0.1901, Accuracy: 0.906, F1 Micro: 0.6931, F1 Macro: 0.507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1663, Accuracy: 0.9062, F1 Micro: 0.7259, F1 Macro: 0.56\n",
      "Epoch 8/10, Train Loss: 0.1439, Accuracy: 0.9107, F1 Micro: 0.7226, F1 Macro: 0.5476\n",
      "Epoch 9/10, Train Loss: 0.1227, Accuracy: 0.9109, F1 Micro: 0.7231, F1 Macro: 0.5508\n",
      "Epoch 10/10, Train Loss: 0.1025, Accuracy: 0.9111, F1 Micro: 0.7256, F1 Macro: 0.5744\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9062, F1 Micro: 0.7259, F1 Macro: 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.83      1134\n",
      "      Abusive       0.83      0.90      0.87       992\n",
      "HS_Individual       0.72      0.64      0.68       732\n",
      "     HS_Group       0.61      0.67      0.64       402\n",
      "  HS_Religion       0.67      0.59      0.63       157\n",
      "      HS_Race       0.72      0.60      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.77      0.74       762\n",
      "      HS_Weak       0.67      0.63      0.65       689\n",
      "  HS_Moderate       0.51      0.58      0.54       331\n",
      "    HS_Strong       0.83      0.35      0.49       114\n",
      "\n",
      "    micro avg       0.73      0.72      0.73      5556\n",
      "    macro avg       0.59      0.55      0.56      5556\n",
      " weighted avg       0.72      0.72      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 115.05375695228577 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4484, Accuracy: 0.8513, F1 Micro: 0.3016, F1 Macro: 0.1003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3414, Accuracy: 0.8788, F1 Micro: 0.6046, F1 Macro: 0.2861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2813, Accuracy: 0.8917, F1 Micro: 0.6116, F1 Macro: 0.3849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2507, Accuracy: 0.9005, F1 Micro: 0.6714, F1 Macro: 0.452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2154, Accuracy: 0.9031, F1 Micro: 0.6914, F1 Macro: 0.4701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9068, F1 Micro: 0.6931, F1 Macro: 0.5207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1655, Accuracy: 0.9077, F1 Micro: 0.729, F1 Macro: 0.5679\n",
      "Epoch 8/10, Train Loss: 0.1419, Accuracy: 0.9095, F1 Micro: 0.7265, F1 Macro: 0.5623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1202, Accuracy: 0.91, F1 Micro: 0.7304, F1 Macro: 0.5727\n",
      "Epoch 10/10, Train Loss: 0.0992, Accuracy: 0.9104, F1 Micro: 0.7263, F1 Macro: 0.5851\n",
      "Model 2 - Iteration 4055: Accuracy: 0.91, F1 Micro: 0.7304, F1 Macro: 0.5727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.83      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.70      0.68      0.69       732\n",
      "     HS_Group       0.69      0.59      0.64       402\n",
      "  HS_Religion       0.71      0.52      0.60       157\n",
      "      HS_Race       0.80      0.54      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.75      0.74       762\n",
      "      HS_Weak       0.67      0.65      0.66       689\n",
      "  HS_Moderate       0.57      0.51      0.54       331\n",
      "    HS_Strong       0.92      0.53      0.67       114\n",
      "\n",
      "    micro avg       0.75      0.71      0.73      5556\n",
      "    macro avg       0.62      0.54      0.57      5556\n",
      " weighted avg       0.74      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 116.81678366661072 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4623, Accuracy: 0.8381, F1 Micro: 0.1591, F1 Macro: 0.053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3587, Accuracy: 0.8774, F1 Micro: 0.5485, F1 Macro: 0.2481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2912, Accuracy: 0.8897, F1 Micro: 0.6242, F1 Macro: 0.3374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2563, Accuracy: 0.896, F1 Micro: 0.6467, F1 Macro: 0.3748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2183, Accuracy: 0.9019, F1 Micro: 0.676, F1 Macro: 0.4663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.191, Accuracy: 0.9073, F1 Micro: 0.7051, F1 Macro: 0.5317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1672, Accuracy: 0.9074, F1 Micro: 0.7284, F1 Macro: 0.5738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1451, Accuracy: 0.9085, F1 Micro: 0.7339, F1 Macro: 0.5763\n",
      "Epoch 9/10, Train Loss: 0.1202, Accuracy: 0.9104, F1 Micro: 0.7254, F1 Macro: 0.5692\n",
      "Epoch 10/10, Train Loss: 0.1019, Accuracy: 0.9114, F1 Micro: 0.7271, F1 Macro: 0.5886\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9085, F1 Micro: 0.7339, F1 Macro: 0.5763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1134\n",
      "      Abusive       0.84      0.89      0.86       992\n",
      "HS_Individual       0.72      0.69      0.70       732\n",
      "     HS_Group       0.62      0.65      0.64       402\n",
      "  HS_Religion       0.73      0.47      0.57       157\n",
      "      HS_Race       0.72      0.61      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.78      0.74       762\n",
      "      HS_Weak       0.69      0.66      0.67       689\n",
      "  HS_Moderate       0.50      0.56      0.53       331\n",
      "    HS_Strong       0.88      0.59      0.71       114\n",
      "\n",
      "    micro avg       0.74      0.73      0.73      5556\n",
      "    macro avg       0.60      0.56      0.58      5556\n",
      " weighted avg       0.72      0.73      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 117.31110572814941 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9082, F1 Micro: 0.7301, F1 Macro: 0.5697\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 483.89445474200727\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 410.11161708831787 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4451, Accuracy: 0.8461, F1 Micro: 0.2437, F1 Macro: 0.0843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.33, Accuracy: 0.8831, F1 Micro: 0.5845, F1 Macro: 0.2732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2781, Accuracy: 0.8959, F1 Micro: 0.6733, F1 Macro: 0.3962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2381, Accuracy: 0.9029, F1 Micro: 0.6901, F1 Macro: 0.4809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2019, Accuracy: 0.9039, F1 Micro: 0.728, F1 Macro: 0.5523\n",
      "Epoch 6/10, Train Loss: 0.1747, Accuracy: 0.9082, F1 Micro: 0.7073, F1 Macro: 0.5247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1438, Accuracy: 0.911, F1 Micro: 0.737, F1 Macro: 0.5669\n",
      "Epoch 8/10, Train Loss: 0.123, Accuracy: 0.91, F1 Micro: 0.7288, F1 Macro: 0.5713\n",
      "Epoch 9/10, Train Loss: 0.1064, Accuracy: 0.9115, F1 Micro: 0.7353, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0912, Accuracy: 0.9119, F1 Micro: 0.7401, F1 Macro: 0.5993\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9119, F1 Micro: 0.7401, F1 Macro: 0.5993\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.69      0.70      0.70       732\n",
      "     HS_Group       0.67      0.62      0.64       402\n",
      "  HS_Religion       0.72      0.55      0.62       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.73      0.76      0.75       762\n",
      "      HS_Weak       0.66      0.68      0.67       689\n",
      "  HS_Moderate       0.56      0.52      0.54       331\n",
      "    HS_Strong       0.90      0.63      0.74       114\n",
      "\n",
      "    micro avg       0.75      0.73      0.74      5556\n",
      "    macro avg       0.70      0.57      0.60      5556\n",
      " weighted avg       0.75      0.73      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 125.74894618988037 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.8487, F1 Micro: 0.2648, F1 Macro: 0.0919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3249, Accuracy: 0.8829, F1 Micro: 0.5761, F1 Macro: 0.2857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2739, Accuracy: 0.897, F1 Micro: 0.6795, F1 Macro: 0.4126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2349, Accuracy: 0.9041, F1 Micro: 0.6946, F1 Macro: 0.5004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1975, Accuracy: 0.9055, F1 Micro: 0.7261, F1 Macro: 0.5708\n",
      "Epoch 6/10, Train Loss: 0.1686, Accuracy: 0.9106, F1 Micro: 0.7208, F1 Macro: 0.5547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9114, F1 Micro: 0.7362, F1 Macro: 0.5837\n",
      "Epoch 8/10, Train Loss: 0.1188, Accuracy: 0.9096, F1 Micro: 0.7289, F1 Macro: 0.5703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1024, Accuracy: 0.9124, F1 Micro: 0.7386, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0901, Accuracy: 0.9136, F1 Micro: 0.7418, F1 Macro: 0.5996\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9136, F1 Micro: 0.7418, F1 Macro: 0.5996\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.86      0.89      0.87       992\n",
      "HS_Individual       0.71      0.68      0.70       732\n",
      "     HS_Group       0.67      0.62      0.65       402\n",
      "  HS_Religion       0.73      0.50      0.59       157\n",
      "      HS_Race       0.82      0.65      0.73       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.57      0.54      0.55       331\n",
      "    HS_Strong       0.90      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.71      0.57      0.60      5556\n",
      " weighted avg       0.76      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 127.31407189369202 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4502, Accuracy: 0.8445, F1 Micro: 0.2288, F1 Macro: 0.0803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3395, Accuracy: 0.8807, F1 Micro: 0.5775, F1 Macro: 0.2711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2811, Accuracy: 0.8964, F1 Micro: 0.6717, F1 Macro: 0.4323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2399, Accuracy: 0.9045, F1 Micro: 0.6978, F1 Macro: 0.507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2026, Accuracy: 0.9067, F1 Micro: 0.7289, F1 Macro: 0.5574\n",
      "Epoch 6/10, Train Loss: 0.1746, Accuracy: 0.9092, F1 Micro: 0.7161, F1 Macro: 0.5473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1439, Accuracy: 0.9118, F1 Micro: 0.7309, F1 Macro: 0.5761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1261, Accuracy: 0.9114, F1 Micro: 0.7353, F1 Macro: 0.5841\n",
      "Epoch 9/10, Train Loss: 0.1047, Accuracy: 0.9115, F1 Micro: 0.729, F1 Macro: 0.5766\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.9133, F1 Micro: 0.7316, F1 Macro: 0.5885\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9114, F1 Micro: 0.7353, F1 Macro: 0.5841\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.73      0.66      0.69       732\n",
      "     HS_Group       0.64      0.65      0.65       402\n",
      "  HS_Religion       0.70      0.56      0.62       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.74      0.74       762\n",
      "      HS_Weak       0.70      0.62      0.66       689\n",
      "  HS_Moderate       0.53      0.59      0.56       331\n",
      "    HS_Strong       0.87      0.58      0.69       114\n",
      "\n",
      "    micro avg       0.76      0.71      0.74      5556\n",
      "    macro avg       0.62      0.56      0.58      5556\n",
      " weighted avg       0.74      0.71      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 125.57581067085266 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9123, F1 Micro: 0.7391, F1 Macro: 0.5943\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 792.2592145988601\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 366.82312870025635 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.438, Accuracy: 0.8578, F1 Micro: 0.4076, F1 Macro: 0.1362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3205, Accuracy: 0.8883, F1 Micro: 0.6111, F1 Macro: 0.3095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2663, Accuracy: 0.8991, F1 Micro: 0.6662, F1 Macro: 0.4074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2281, Accuracy: 0.9059, F1 Micro: 0.702, F1 Macro: 0.518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1876, Accuracy: 0.91, F1 Micro: 0.7198, F1 Macro: 0.5262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1625, Accuracy: 0.9086, F1 Micro: 0.7429, F1 Macro: 0.5784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1392, Accuracy: 0.9123, F1 Micro: 0.7434, F1 Macro: 0.5857\n",
      "Epoch 8/10, Train Loss: 0.1195, Accuracy: 0.9133, F1 Micro: 0.7374, F1 Macro: 0.6035\n",
      "Epoch 9/10, Train Loss: 0.1019, Accuracy: 0.9174, F1 Micro: 0.7419, F1 Macro: 0.5951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0836, Accuracy: 0.9168, F1 Micro: 0.7537, F1 Macro: 0.6186\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9168, F1 Micro: 0.7537, F1 Macro: 0.6186\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.73      0.68      0.71       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.76      0.47      0.58       157\n",
      "      HS_Race       0.86      0.63      0.73       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.50      0.04      0.07        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.60      0.56      0.58       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.74      0.59      0.62      5556\n",
      " weighted avg       0.77      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 138.31112456321716 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4298, Accuracy: 0.8609, F1 Micro: 0.44, F1 Macro: 0.1539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3167, Accuracy: 0.8891, F1 Micro: 0.6283, F1 Macro: 0.3612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2609, Accuracy: 0.9004, F1 Micro: 0.6825, F1 Macro: 0.4795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2246, Accuracy: 0.9063, F1 Micro: 0.71, F1 Macro: 0.5356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1887, Accuracy: 0.909, F1 Micro: 0.7101, F1 Macro: 0.5074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1609, Accuracy: 0.9137, F1 Micro: 0.7298, F1 Macro: 0.5622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1377, Accuracy: 0.913, F1 Micro: 0.7335, F1 Macro: 0.5635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1184, Accuracy: 0.9111, F1 Micro: 0.7434, F1 Macro: 0.6026\n",
      "Epoch 9/10, Train Loss: 0.0977, Accuracy: 0.9149, F1 Micro: 0.7427, F1 Macro: 0.6015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0832, Accuracy: 0.9175, F1 Micro: 0.7492, F1 Macro: 0.6118\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9175, F1 Micro: 0.7492, F1 Macro: 0.6118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.92      0.84      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.75      0.58      0.65       402\n",
      "  HS_Religion       0.70      0.47      0.56       157\n",
      "      HS_Race       0.89      0.59      0.71       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.75      0.78      0.76       762\n",
      "      HS_Weak       0.69      0.68      0.69       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5556\n",
      "    macro avg       0.73      0.57      0.61      5556\n",
      " weighted avg       0.78      0.72      0.74      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 139.4426794052124 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4441, Accuracy: 0.8526, F1 Micro: 0.329, F1 Macro: 0.1075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3296, Accuracy: 0.8861, F1 Micro: 0.6158, F1 Macro: 0.3173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2702, Accuracy: 0.8999, F1 Micro: 0.6732, F1 Macro: 0.4194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2301, Accuracy: 0.9063, F1 Micro: 0.7024, F1 Macro: 0.5238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1915, Accuracy: 0.9099, F1 Micro: 0.7281, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1637, Accuracy: 0.9142, F1 Micro: 0.7377, F1 Macro: 0.5754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1367, Accuracy: 0.912, F1 Micro: 0.7426, F1 Macro: 0.585\n",
      "Epoch 8/10, Train Loss: 0.1176, Accuracy: 0.9156, F1 Micro: 0.7401, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1009, Accuracy: 0.9148, F1 Micro: 0.7438, F1 Macro: 0.5952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0837, Accuracy: 0.9162, F1 Micro: 0.7483, F1 Macro: 0.6046\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9162, F1 Micro: 0.7483, F1 Macro: 0.6046\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.83      1134\n",
      "      Abusive       0.89      0.86      0.88       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.70      0.61      0.65       402\n",
      "  HS_Religion       0.73      0.50      0.59       157\n",
      "      HS_Race       0.76      0.64      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.70      0.67      0.68       689\n",
      "  HS_Moderate       0.61      0.51      0.56       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.75      5556\n",
      "    macro avg       0.69      0.58      0.60      5556\n",
      " weighted avg       0.76      0.72      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 139.1884388923645 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9168, F1 Micro: 0.7504, F1 Macro: 0.6117\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 970.0281522319038\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 330.96873664855957 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4309, Accuracy: 0.8616, F1 Micro: 0.4552, F1 Macro: 0.1563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3135, Accuracy: 0.8919, F1 Micro: 0.6522, F1 Macro: 0.3508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.259, Accuracy: 0.9018, F1 Micro: 0.6976, F1 Macro: 0.4827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9089, F1 Micro: 0.723, F1 Macro: 0.5322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1857, Accuracy: 0.9117, F1 Micro: 0.7269, F1 Macro: 0.5603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1618, Accuracy: 0.9129, F1 Micro: 0.7427, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1379, Accuracy: 0.9153, F1 Micro: 0.7435, F1 Macro: 0.5952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1137, Accuracy: 0.9167, F1 Micro: 0.7557, F1 Macro: 0.6345\n",
      "Epoch 9/10, Train Loss: 0.0942, Accuracy: 0.9148, F1 Micro: 0.7537, F1 Macro: 0.665\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9171, F1 Micro: 0.7555, F1 Macro: 0.6491\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9167, F1 Micro: 0.7557, F1 Macro: 0.6345\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.74      0.52      0.61       157\n",
      "      HS_Race       0.86      0.57      0.69       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.67      0.71      0.69       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.91      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.75      0.60      0.63      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 146.994553565979 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4225, Accuracy: 0.8641, F1 Micro: 0.4902, F1 Macro: 0.1863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.31, Accuracy: 0.8934, F1 Micro: 0.652, F1 Macro: 0.3867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2578, Accuracy: 0.9026, F1 Micro: 0.6977, F1 Macro: 0.4762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2249, Accuracy: 0.9093, F1 Micro: 0.7233, F1 Macro: 0.5402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1877, Accuracy: 0.9103, F1 Micro: 0.7338, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.16, Accuracy: 0.9138, F1 Micro: 0.7434, F1 Macro: 0.5858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1343, Accuracy: 0.9161, F1 Micro: 0.7554, F1 Macro: 0.6117\n",
      "Epoch 8/10, Train Loss: 0.1173, Accuracy: 0.9146, F1 Micro: 0.7377, F1 Macro: 0.5976\n",
      "Epoch 9/10, Train Loss: 0.0985, Accuracy: 0.9161, F1 Micro: 0.7521, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0836, Accuracy: 0.9174, F1 Micro: 0.7588, F1 Macro: 0.6375\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9174, F1 Micro: 0.7588, F1 Macro: 0.6375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.72      0.59      0.65       402\n",
      "  HS_Religion       0.76      0.54      0.63       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.58      0.10      0.17        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.66      0.73      0.70       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.73      0.61      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 146.35637545585632 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4351, Accuracy: 0.8593, F1 Micro: 0.4237, F1 Macro: 0.1474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3183, Accuracy: 0.8912, F1 Micro: 0.6318, F1 Macro: 0.3351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2617, Accuracy: 0.9027, F1 Micro: 0.6925, F1 Macro: 0.4868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2279, Accuracy: 0.9095, F1 Micro: 0.7234, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1869, Accuracy: 0.9133, F1 Micro: 0.7311, F1 Macro: 0.5794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1592, Accuracy: 0.9143, F1 Micro: 0.7506, F1 Macro: 0.5996\n",
      "Epoch 7/10, Train Loss: 0.1372, Accuracy: 0.9155, F1 Micro: 0.7404, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1184, Accuracy: 0.9183, F1 Micro: 0.7557, F1 Macro: 0.617\n",
      "Epoch 9/10, Train Loss: 0.0949, Accuracy: 0.9186, F1 Micro: 0.7552, F1 Macro: 0.6436\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9175, F1 Micro: 0.7538, F1 Macro: 0.6427\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9183, F1 Micro: 0.7557, F1 Macro: 0.617\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.73      0.70      0.72       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.81      0.52      0.64       157\n",
      "      HS_Race       0.88      0.53      0.66       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.83      0.10      0.18        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.71      0.68      0.70       689\n",
      "  HS_Moderate       0.62      0.53      0.57       331\n",
      "    HS_Strong       0.89      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.80      0.58      0.62      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 144.43431162834167 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9175, F1 Micro: 0.7567, F1 Macro: 0.6297\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1006.2589840381314\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 301.8124017715454 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.8658, F1 Micro: 0.4177, F1 Macro: 0.1797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3064, Accuracy: 0.8907, F1 Micro: 0.6039, F1 Macro: 0.3553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2473, Accuracy: 0.9045, F1 Micro: 0.6949, F1 Macro: 0.4768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2179, Accuracy: 0.9114, F1 Micro: 0.7248, F1 Macro: 0.5405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1781, Accuracy: 0.911, F1 Micro: 0.7428, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1518, Accuracy: 0.9139, F1 Micro: 0.7488, F1 Macro: 0.5958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.131, Accuracy: 0.916, F1 Micro: 0.7501, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9171, F1 Micro: 0.757, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0927, Accuracy: 0.9185, F1 Micro: 0.7599, F1 Macro: 0.6589\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.9148, F1 Micro: 0.7507, F1 Macro: 0.6533\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9185, F1 Micro: 0.7599, F1 Macro: 0.6589\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.70      0.60      0.64       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.55      0.24      0.33        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.60      0.54      0.57       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.73      0.63      0.66      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 158.21017050743103 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4152, Accuracy: 0.8667, F1 Micro: 0.428, F1 Macro: 0.178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3024, Accuracy: 0.8929, F1 Micro: 0.613, F1 Macro: 0.3828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9045, F1 Micro: 0.6833, F1 Macro: 0.4758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.217, Accuracy: 0.9102, F1 Micro: 0.7182, F1 Macro: 0.5499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1773, Accuracy: 0.9139, F1 Micro: 0.7411, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1526, Accuracy: 0.917, F1 Micro: 0.7494, F1 Macro: 0.5969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1304, Accuracy: 0.9137, F1 Micro: 0.751, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1127, Accuracy: 0.916, F1 Micro: 0.758, F1 Macro: 0.6276\n",
      "Epoch 9/10, Train Loss: 0.0916, Accuracy: 0.9165, F1 Micro: 0.7539, F1 Macro: 0.6498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.916, F1 Micro: 0.7595, F1 Macro: 0.6594\n",
      "Model 2 - Iteration 6285: Accuracy: 0.916, F1 Micro: 0.7595, F1 Macro: 0.6594\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.68      0.74      0.71       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.67      0.14      0.23        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.58      0.54      0.56       331\n",
      "    HS_Strong       0.85      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.72      0.64      0.66      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 157.41811776161194 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.428, Accuracy: 0.8636, F1 Micro: 0.4244, F1 Macro: 0.1707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3128, Accuracy: 0.8862, F1 Micro: 0.5794, F1 Macro: 0.3372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2479, Accuracy: 0.9034, F1 Micro: 0.6823, F1 Macro: 0.4698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.912, F1 Micro: 0.725, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1777, Accuracy: 0.9124, F1 Micro: 0.7484, F1 Macro: 0.5985\n",
      "Epoch 6/10, Train Loss: 0.1521, Accuracy: 0.9142, F1 Micro: 0.7463, F1 Macro: 0.592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.13, Accuracy: 0.916, F1 Micro: 0.7486, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9179, F1 Micro: 0.7592, F1 Macro: 0.6362\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9193, F1 Micro: 0.7538, F1 Macro: 0.6406\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9138, F1 Micro: 0.7553, F1 Macro: 0.6442\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9179, F1 Micro: 0.7592, F1 Macro: 0.6362\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.67      0.66      0.67       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.57      0.06      0.10        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.69      0.67      0.68       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.83      0.85      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.73      0.62      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.42      0.42      0.40      5556\n",
      "\n",
      "Training completed in 153.29454231262207 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9175, F1 Micro: 0.7595, F1 Macro: 0.6515\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 935.629372117215\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 270.20811700820923 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4159, Accuracy: 0.8767, F1 Micro: 0.5762, F1 Macro: 0.2647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3003, Accuracy: 0.8957, F1 Micro: 0.6672, F1 Macro: 0.4251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2439, Accuracy: 0.9028, F1 Micro: 0.6827, F1 Macro: 0.4357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9108, F1 Micro: 0.7216, F1 Macro: 0.5176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9116, F1 Micro: 0.7482, F1 Macro: 0.5794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1547, Accuracy: 0.917, F1 Micro: 0.7552, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1295, Accuracy: 0.9178, F1 Micro: 0.7561, F1 Macro: 0.6233\n",
      "Epoch 8/10, Train Loss: 0.1089, Accuracy: 0.9154, F1 Micro: 0.7525, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0912, Accuracy: 0.9165, F1 Micro: 0.7578, F1 Macro: 0.646\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.919, F1 Micro: 0.7569, F1 Macro: 0.6684\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9165, F1 Micro: 0.7578, F1 Macro: 0.646\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.68      0.77      0.72       732\n",
      "     HS_Group       0.73      0.58      0.65       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.82      0.63      0.71       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.53      0.20      0.29        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.65      0.75      0.70       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.92      0.74      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.62      0.65      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 159.78386068344116 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4088, Accuracy: 0.8764, F1 Micro: 0.5726, F1 Macro: 0.2619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2968, Accuracy: 0.8942, F1 Micro: 0.6441, F1 Macro: 0.4333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2418, Accuracy: 0.903, F1 Micro: 0.6804, F1 Macro: 0.4631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.9107, F1 Micro: 0.7243, F1 Macro: 0.535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1763, Accuracy: 0.9115, F1 Micro: 0.7471, F1 Macro: 0.5775\n",
      "Epoch 6/10, Train Loss: 0.1502, Accuracy: 0.9176, F1 Micro: 0.7457, F1 Macro: 0.5983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1304, Accuracy: 0.9168, F1 Micro: 0.751, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1079, Accuracy: 0.9191, F1 Micro: 0.7635, F1 Macro: 0.6351\n",
      "Epoch 9/10, Train Loss: 0.0886, Accuracy: 0.9186, F1 Micro: 0.7581, F1 Macro: 0.647\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9205, F1 Micro: 0.7623, F1 Macro: 0.653\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9191, F1 Micro: 0.7635, F1 Macro: 0.6351\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.66      0.62      0.64       157\n",
      "      HS_Race       0.76      0.70      0.73       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.55      0.12      0.19        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.59      0.57      0.58       331\n",
      "    HS_Strong       0.92      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.75      0.62      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 157.74866652488708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4232, Accuracy: 0.8719, F1 Micro: 0.5266, F1 Macro: 0.2326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3043, Accuracy: 0.8937, F1 Micro: 0.6605, F1 Macro: 0.4296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.245, Accuracy: 0.9023, F1 Micro: 0.6761, F1 Macro: 0.4551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9105, F1 Micro: 0.7183, F1 Macro: 0.5319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1786, Accuracy: 0.9128, F1 Micro: 0.75, F1 Macro: 0.582\n",
      "Epoch 6/10, Train Loss: 0.1493, Accuracy: 0.9161, F1 Micro: 0.7497, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1303, Accuracy: 0.9163, F1 Micro: 0.7504, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9149, F1 Micro: 0.7534, F1 Macro: 0.6204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0919, Accuracy: 0.915, F1 Micro: 0.7544, F1 Macro: 0.6315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9186, F1 Micro: 0.7597, F1 Macro: 0.6512\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9186, F1 Micro: 0.7597, F1 Macro: 0.6512\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.72      0.60      0.65       402\n",
      "  HS_Religion       0.84      0.55      0.67       157\n",
      "      HS_Race       0.80      0.56      0.66       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.78      0.61      0.65      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 160.63550209999084 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9181, F1 Micro: 0.7603, F1 Macro: 0.6441\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 920.4946594412834\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 249.64208221435547 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8784, F1 Micro: 0.5476, F1 Macro: 0.251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2934, Accuracy: 0.8972, F1 Micro: 0.6804, F1 Macro: 0.3802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.24, Accuracy: 0.9075, F1 Micro: 0.7165, F1 Macro: 0.5088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9141, F1 Micro: 0.7257, F1 Macro: 0.5422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.9166, F1 Micro: 0.7442, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1447, Accuracy: 0.919, F1 Micro: 0.7526, F1 Macro: 0.6135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1233, Accuracy: 0.9205, F1 Micro: 0.7599, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.9224, F1 Micro: 0.7677, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0865, Accuracy: 0.9195, F1 Micro: 0.7686, F1 Macro: 0.6561\n",
      "Epoch 10/10, Train Loss: 0.0753, Accuracy: 0.9212, F1 Micro: 0.7641, F1 Macro: 0.6698\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9195, F1 Micro: 0.7686, F1 Macro: 0.6561\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.75      0.73       732\n",
      "     HS_Group       0.69      0.64      0.67       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.60      0.55      0.58       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.76      0.63      0.66      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 167.51456665992737 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4021, Accuracy: 0.8797, F1 Micro: 0.5646, F1 Macro: 0.2575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2913, Accuracy: 0.8987, F1 Micro: 0.682, F1 Macro: 0.4665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2379, Accuracy: 0.9079, F1 Micro: 0.7146, F1 Macro: 0.539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2045, Accuracy: 0.9137, F1 Micro: 0.7368, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1803, Accuracy: 0.9174, F1 Micro: 0.746, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1467, Accuracy: 0.9192, F1 Micro: 0.7526, F1 Macro: 0.6065\n",
      "Epoch 7/10, Train Loss: 0.1188, Accuracy: 0.9158, F1 Micro: 0.7503, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9187, F1 Micro: 0.7623, F1 Macro: 0.6432\n",
      "Epoch 9/10, Train Loss: 0.0875, Accuracy: 0.9133, F1 Micro: 0.7594, F1 Macro: 0.6411\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9172, F1 Micro: 0.7608, F1 Macro: 0.6539\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9187, F1 Micro: 0.7623, F1 Macro: 0.6432\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.68      0.65      0.67       402\n",
      "  HS_Religion       0.74      0.62      0.67       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.71      0.68      0.69       689\n",
      "  HS_Moderate       0.60      0.57      0.58       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.75      0.62      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 164.8583562374115 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4162, Accuracy: 0.8722, F1 Micro: 0.4876, F1 Macro: 0.2165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2994, Accuracy: 0.895, F1 Micro: 0.6618, F1 Macro: 0.3647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2425, Accuracy: 0.907, F1 Micro: 0.7051, F1 Macro: 0.5228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2069, Accuracy: 0.9121, F1 Micro: 0.7182, F1 Macro: 0.5521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1799, Accuracy: 0.9164, F1 Micro: 0.749, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.146, Accuracy: 0.9176, F1 Micro: 0.7545, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1245, Accuracy: 0.9193, F1 Micro: 0.762, F1 Macro: 0.6223\n",
      "Epoch 8/10, Train Loss: 0.105, Accuracy: 0.9156, F1 Micro: 0.7612, F1 Macro: 0.6411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0887, Accuracy: 0.9187, F1 Micro: 0.7633, F1 Macro: 0.6403\n",
      "Epoch 10/10, Train Loss: 0.0759, Accuracy: 0.9181, F1 Micro: 0.7605, F1 Macro: 0.661\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9187, F1 Micro: 0.7633, F1 Macro: 0.6403\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.78      0.59      0.67       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.53      0.20      0.29        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.59      0.56      0.57       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.72      0.62      0.64      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 165.24087691307068 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.919, F1 Micro: 0.7648, F1 Macro: 0.6465\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 669.0419089840249\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 224.10717225074768 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4129, Accuracy: 0.8786, F1 Micro: 0.5479, F1 Macro: 0.2502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2897, Accuracy: 0.8991, F1 Micro: 0.6807, F1 Macro: 0.4555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.908, F1 Micro: 0.7206, F1 Macro: 0.5277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1964, Accuracy: 0.9153, F1 Micro: 0.735, F1 Macro: 0.5528\n",
      "Epoch 5/10, Train Loss: 0.1703, Accuracy: 0.9174, F1 Micro: 0.7309, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.9189, F1 Micro: 0.7698, F1 Macro: 0.6359\n",
      "Epoch 7/10, Train Loss: 0.1202, Accuracy: 0.9227, F1 Micro: 0.7662, F1 Macro: 0.6615\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9225, F1 Micro: 0.7668, F1 Macro: 0.6516\n",
      "Epoch 9/10, Train Loss: 0.0854, Accuracy: 0.9221, F1 Micro: 0.7611, F1 Macro: 0.6643\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9236, F1 Micro: 0.7683, F1 Macro: 0.6808\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9189, F1 Micro: 0.7698, F1 Macro: 0.6359\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.67      0.81      0.73       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.72      0.65      0.68       157\n",
      "      HS_Race       0.76      0.76      0.76       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.33      0.06      0.10        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.65      0.79      0.71       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.86      0.71      0.78       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.71      0.63      0.64      5556\n",
      " weighted avg       0.75      0.79      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 167.01829266548157 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.8797, F1 Micro: 0.5739, F1 Macro: 0.2662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2876, Accuracy: 0.8996, F1 Micro: 0.6797, F1 Macro: 0.4644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.232, Accuracy: 0.9083, F1 Micro: 0.7095, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1967, Accuracy: 0.9152, F1 Micro: 0.745, F1 Macro: 0.5869\n",
      "Epoch 5/10, Train Loss: 0.1694, Accuracy: 0.9178, F1 Micro: 0.7437, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1417, Accuracy: 0.9158, F1 Micro: 0.7609, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9155, F1 Micro: 0.763, F1 Macro: 0.6362\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9204, F1 Micro: 0.7607, F1 Macro: 0.6417\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.9215, F1 Micro: 0.7589, F1 Macro: 0.6649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9193, F1 Micro: 0.7652, F1 Macro: 0.6655\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9193, F1 Micro: 0.7652, F1 Macro: 0.6655\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.72      0.59      0.65       402\n",
      "  HS_Religion       0.69      0.60      0.64       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.3427288532257 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4192, Accuracy: 0.8768, F1 Micro: 0.5675, F1 Macro: 0.2582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.294, Accuracy: 0.8991, F1 Micro: 0.6835, F1 Macro: 0.474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2375, Accuracy: 0.9091, F1 Micro: 0.7179, F1 Macro: 0.5268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1977, Accuracy: 0.915, F1 Micro: 0.7372, F1 Macro: 0.5794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1677, Accuracy: 0.9184, F1 Micro: 0.7416, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1441, Accuracy: 0.9164, F1 Micro: 0.7623, F1 Macro: 0.6163\n",
      "Epoch 7/10, Train Loss: 0.1197, Accuracy: 0.9196, F1 Micro: 0.7612, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.9209, F1 Micro: 0.7636, F1 Macro: 0.6625\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9213, F1 Micro: 0.7601, F1 Macro: 0.6532\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9209, F1 Micro: 0.7636, F1 Macro: 0.6625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.57      0.64       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.58      0.10      0.17        72\n",
      "    HS_Gender       0.53      0.33      0.41        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.64      0.51      0.57       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.78      0.74      0.76      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 172.06424283981323 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9197, F1 Micro: 0.7662, F1 Macro: 0.6546\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 859.1993409161347\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 200.77397108078003 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4091, Accuracy: 0.8817, F1 Micro: 0.5868, F1 Macro: 0.2703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.899, F1 Micro: 0.6921, F1 Macro: 0.4467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2384, Accuracy: 0.9094, F1 Micro: 0.7332, F1 Macro: 0.5467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9178, F1 Micro: 0.7491, F1 Macro: 0.5679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.918, F1 Micro: 0.7617, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.919, F1 Micro: 0.7672, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1204, Accuracy: 0.9175, F1 Micro: 0.7692, F1 Macro: 0.6532\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9225, F1 Micro: 0.7656, F1 Macro: 0.6601\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9141, F1 Micro: 0.7649, F1 Macro: 0.6747\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9188, F1 Micro: 0.7674, F1 Macro: 0.6872\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9175, F1 Micro: 0.7692, F1 Macro: 0.6532\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.92      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.67      0.81      0.73       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.44      0.16      0.23        51\n",
      "     HS_Other       0.71      0.85      0.77       762\n",
      "      HS_Weak       0.66      0.79      0.72       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.86      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.80      0.77      5556\n",
      "    macro avg       0.72      0.65      0.65      5556\n",
      " weighted avg       0.74      0.80      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 176.7043194770813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4024, Accuracy: 0.8818, F1 Micro: 0.5964, F1 Macro: 0.2783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2839, Accuracy: 0.9012, F1 Micro: 0.6947, F1 Macro: 0.4507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2357, Accuracy: 0.9104, F1 Micro: 0.7346, F1 Macro: 0.5527\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9163, F1 Micro: 0.729, F1 Macro: 0.5602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1631, Accuracy: 0.9173, F1 Micro: 0.7542, F1 Macro: 0.5995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1417, Accuracy: 0.9198, F1 Micro: 0.7622, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9183, F1 Micro: 0.7658, F1 Macro: 0.6456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0968, Accuracy: 0.9222, F1 Micro: 0.7688, F1 Macro: 0.653\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9178, F1 Micro: 0.7643, F1 Macro: 0.6602\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9167, F1 Micro: 0.7602, F1 Macro: 0.6612\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9222, F1 Micro: 0.7688, F1 Macro: 0.653\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.75      0.72      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.55      0.12      0.19        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.76      0.63      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 175.35577511787415 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4109, Accuracy: 0.8814, F1 Micro: 0.5838, F1 Macro: 0.2694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2907, Accuracy: 0.9007, F1 Micro: 0.6877, F1 Macro: 0.4282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.9084, F1 Micro: 0.7288, F1 Macro: 0.5397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2, Accuracy: 0.9161, F1 Micro: 0.7333, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1651, Accuracy: 0.9163, F1 Micro: 0.7583, F1 Macro: 0.6051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1432, Accuracy: 0.9167, F1 Micro: 0.7608, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1179, Accuracy: 0.9182, F1 Micro: 0.7665, F1 Macro: 0.6464\n",
      "Epoch 8/10, Train Loss: 0.0952, Accuracy: 0.9202, F1 Micro: 0.7622, F1 Macro: 0.6453\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9208, F1 Micro: 0.7581, F1 Macro: 0.6616\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9207, F1 Micro: 0.7615, F1 Macro: 0.6722\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9182, F1 Micro: 0.7665, F1 Macro: 0.6464\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.74      0.55      0.63       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.52      0.24      0.32        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.58      0.55      0.57       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5556\n",
      "    macro avg       0.71      0.64      0.65      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 175.32207107543945 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9193, F1 Micro: 0.7682, F1 Macro: 0.6508\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 644.9016579236261\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 181.75559377670288 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.406, Accuracy: 0.8769, F1 Micro: 0.5178, F1 Macro: 0.2368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.8978, F1 Micro: 0.6744, F1 Macro: 0.4263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.237, Accuracy: 0.9107, F1 Micro: 0.725, F1 Macro: 0.5158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2018, Accuracy: 0.916, F1 Micro: 0.7469, F1 Macro: 0.5735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9184, F1 Micro: 0.7558, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.9204, F1 Micro: 0.758, F1 Macro: 0.6183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1202, Accuracy: 0.9194, F1 Micro: 0.7685, F1 Macro: 0.6465\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.918, F1 Micro: 0.764, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0842, Accuracy: 0.9236, F1 Micro: 0.7718, F1 Macro: 0.6779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0735, Accuracy: 0.9215, F1 Micro: 0.7722, F1 Macro: 0.6824\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9215, F1 Micro: 0.7722, F1 Macro: 0.6824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.68      0.62      0.65       157\n",
      "      HS_Race       0.81      0.67      0.73       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.77      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 183.96980357170105 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3991, Accuracy: 0.8765, F1 Micro: 0.5113, F1 Macro: 0.2352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2819, Accuracy: 0.9, F1 Micro: 0.686, F1 Macro: 0.471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.911, F1 Micro: 0.7259, F1 Macro: 0.5462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1991, Accuracy: 0.9165, F1 Micro: 0.7422, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9173, F1 Micro: 0.7468, F1 Macro: 0.5747\n",
      "Epoch 6/10, Train Loss: 0.1376, Accuracy: 0.9195, F1 Micro: 0.7452, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1177, Accuracy: 0.9204, F1 Micro: 0.7612, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.9187, F1 Micro: 0.762, F1 Macro: 0.6278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.9209, F1 Micro: 0.7639, F1 Macro: 0.6567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0736, Accuracy: 0.922, F1 Micro: 0.7657, F1 Macro: 0.6661\n",
      "Model 2 - Iteration 7901: Accuracy: 0.922, F1 Micro: 0.7657, F1 Macro: 0.6661\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.77      0.56      0.65       402\n",
      "  HS_Religion       0.81      0.58      0.67       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.45      0.14      0.21        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.81      0.74      0.77       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.70      0.45      0.55       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5556\n",
      "    macro avg       0.74      0.62      0.67      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 183.62854719161987 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4123, Accuracy: 0.8759, F1 Micro: 0.5127, F1 Macro: 0.2359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2869, Accuracy: 0.8983, F1 Micro: 0.6708, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2361, Accuracy: 0.9107, F1 Micro: 0.7273, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9156, F1 Micro: 0.7448, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.917, F1 Micro: 0.7577, F1 Macro: 0.5951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9198, F1 Micro: 0.7593, F1 Macro: 0.6095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1168, Accuracy: 0.9166, F1 Micro: 0.7619, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.917, F1 Micro: 0.7657, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0846, Accuracy: 0.9186, F1 Micro: 0.7665, F1 Macro: 0.6714\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9191, F1 Micro: 0.7626, F1 Macro: 0.6712\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9186, F1 Micro: 0.7665, F1 Macro: 0.6714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.74      0.60      0.66       157\n",
      "      HS_Race       0.80      0.66      0.72       120\n",
      "  HS_Physical       0.67      0.14      0.23        72\n",
      "    HS_Gender       0.59      0.33      0.42        51\n",
      "     HS_Other       0.73      0.83      0.77       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.64      0.54      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 184.1170220375061 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9207, F1 Micro: 0.7681, F1 Macro: 0.6733\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 807.5979934176687\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 168.5835371017456 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4055, Accuracy: 0.8805, F1 Micro: 0.5721, F1 Macro: 0.2682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.283, Accuracy: 0.9006, F1 Micro: 0.7042, F1 Macro: 0.4678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9091, F1 Micro: 0.7321, F1 Macro: 0.5348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9174, F1 Micro: 0.7432, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1626, Accuracy: 0.9224, F1 Micro: 0.767, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9232, F1 Micro: 0.7673, F1 Macro: 0.6182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.921, F1 Micro: 0.7689, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9235, F1 Micro: 0.769, F1 Macro: 0.6521\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9222, F1 Micro: 0.7594, F1 Macro: 0.6721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9225, F1 Micro: 0.772, F1 Macro: 0.6885\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9225, F1 Micro: 0.772, F1 Macro: 0.6885\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.84      0.22      0.35        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.63      0.51      0.57       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.20368885993958 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.881, F1 Micro: 0.5954, F1 Macro: 0.2843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9004, F1 Micro: 0.7055, F1 Macro: 0.4796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2256, Accuracy: 0.9095, F1 Micro: 0.7256, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9187, F1 Micro: 0.7491, F1 Macro: 0.5921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1606, Accuracy: 0.9183, F1 Micro: 0.7549, F1 Macro: 0.6042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9222, F1 Micro: 0.7596, F1 Macro: 0.6131\n",
      "Epoch 7/10, Train Loss: 0.1148, Accuracy: 0.921, F1 Micro: 0.7593, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1, Accuracy: 0.9217, F1 Micro: 0.7631, F1 Macro: 0.654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9214, F1 Micro: 0.767, F1 Macro: 0.6727\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9188, F1 Micro: 0.7664, F1 Macro: 0.6741\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9214, F1 Micro: 0.767, F1 Macro: 0.6727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.69      0.74      0.72       732\n",
      "     HS_Group       0.76      0.56      0.64       402\n",
      "  HS_Religion       0.75      0.56      0.64       157\n",
      "      HS_Race       0.77      0.72      0.75       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.62      0.35      0.45        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.67      0.72      0.69       689\n",
      "  HS_Moderate       0.69      0.48      0.56       331\n",
      "    HS_Strong       0.90      0.84      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 185.6016595363617 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4119, Accuracy: 0.8795, F1 Micro: 0.5721, F1 Macro: 0.2674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8997, F1 Micro: 0.6965, F1 Macro: 0.4608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2289, Accuracy: 0.9074, F1 Micro: 0.7245, F1 Macro: 0.5247\n",
      "Epoch 4/10, Train Loss: 0.1988, Accuracy: 0.9142, F1 Micro: 0.7207, F1 Macro: 0.5674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9195, F1 Micro: 0.7626, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.138, Accuracy: 0.9215, F1 Micro: 0.7651, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1168, Accuracy: 0.9214, F1 Micro: 0.7657, F1 Macro: 0.6381\n",
      "Epoch 8/10, Train Loss: 0.0987, Accuracy: 0.9212, F1 Micro: 0.7653, F1 Macro: 0.6471\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.92, F1 Micro: 0.7595, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0734, Accuracy: 0.9222, F1 Micro: 0.7677, F1 Macro: 0.6831\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9222, F1 Micro: 0.7677, F1 Macro: 0.6831\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.74      0.69      0.72       732\n",
      "     HS_Group       0.68      0.66      0.67       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.59      0.47      0.52        51\n",
      "     HS_Other       0.79      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.67      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.90      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 184.3309669494629 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.922, F1 Micro: 0.7689, F1 Macro: 0.6814\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1250.7615902380335\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 149.5310094356537 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4026, Accuracy: 0.8815, F1 Micro: 0.5711, F1 Macro: 0.2657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2829, Accuracy: 0.9019, F1 Micro: 0.6989, F1 Macro: 0.4774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9109, F1 Micro: 0.7243, F1 Macro: 0.541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9191, F1 Micro: 0.7565, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1627, Accuracy: 0.9225, F1 Micro: 0.765, F1 Macro: 0.619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.9201, F1 Micro: 0.7707, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1145, Accuracy: 0.9231, F1 Micro: 0.7753, F1 Macro: 0.677\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9256, F1 Micro: 0.7747, F1 Macro: 0.6798\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9243, F1 Micro: 0.774, F1 Macro: 0.6882\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.922, F1 Micro: 0.7741, F1 Macro: 0.6971\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9231, F1 Micro: 0.7753, F1 Macro: 0.677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.72      0.63      0.67       157\n",
      "      HS_Race       0.82      0.68      0.75       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.0923047065735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.8831, F1 Micro: 0.6076, F1 Macro: 0.295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.9029, F1 Micro: 0.6917, F1 Macro: 0.4714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9096, F1 Micro: 0.7234, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.915, F1 Micro: 0.7536, F1 Macro: 0.5971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.9205, F1 Micro: 0.7664, F1 Macro: 0.6136\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.92, F1 Micro: 0.7629, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1139, Accuracy: 0.9221, F1 Micro: 0.7682, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9242, F1 Micro: 0.7711, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9217, F1 Micro: 0.7719, F1 Macro: 0.6769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9245, F1 Micro: 0.7758, F1 Macro: 0.6748\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9245, F1 Micro: 0.7758, F1 Macro: 0.6748\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.78      0.60      0.68       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.70      0.27      0.39        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.70      0.51      0.59       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.80      0.63      0.67      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 192.11112141609192 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4096, Accuracy: 0.881, F1 Micro: 0.5955, F1 Macro: 0.2803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2864, Accuracy: 0.9027, F1 Micro: 0.6894, F1 Macro: 0.4748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.911, F1 Micro: 0.7324, F1 Macro: 0.5635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1858, Accuracy: 0.9171, F1 Micro: 0.7582, F1 Macro: 0.5955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1623, Accuracy: 0.9235, F1 Micro: 0.7696, F1 Macro: 0.6152\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9204, F1 Micro: 0.765, F1 Macro: 0.6402\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9233, F1 Micro: 0.7688, F1 Macro: 0.6593\n",
      "Epoch 8/10, Train Loss: 0.1003, Accuracy: 0.9205, F1 Micro: 0.7648, F1 Macro: 0.6576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.923, F1 Micro: 0.776, F1 Macro: 0.6796\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.922, F1 Micro: 0.7725, F1 Macro: 0.6668\n",
      "Model 3 - Iteration 8402: Accuracy: 0.923, F1 Micro: 0.776, F1 Macro: 0.6796\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.89      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 186.84009051322937 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9235, F1 Micro: 0.7757, F1 Macro: 0.6771\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 259.9886650665045\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 134.58943676948547 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3949, Accuracy: 0.8809, F1 Micro: 0.5474, F1 Macro: 0.2516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2748, Accuracy: 0.9009, F1 Micro: 0.6989, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9121, F1 Micro: 0.7292, F1 Macro: 0.564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9212, F1 Micro: 0.7596, F1 Macro: 0.5941\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9204, F1 Micro: 0.7591, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1324, Accuracy: 0.9238, F1 Micro: 0.7675, F1 Macro: 0.6464\n",
      "Epoch 7/10, Train Loss: 0.1103, Accuracy: 0.9225, F1 Micro: 0.7645, F1 Macro: 0.6521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9246, F1 Micro: 0.7731, F1 Macro: 0.6793\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.923, F1 Micro: 0.7709, F1 Macro: 0.692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9214, F1 Micro: 0.7732, F1 Macro: 0.6989\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9214, F1 Micro: 0.7732, F1 Macro: 0.6989\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.68      0.68       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.71      0.69      0.70       120\n",
      "  HS_Physical       0.77      0.28      0.41        72\n",
      "    HS_Gender       0.70      0.41      0.52        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.59      0.61      0.60       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.8462450504303 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.8793, F1 Micro: 0.5391, F1 Macro: 0.2566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2717, Accuracy: 0.9032, F1 Micro: 0.7033, F1 Macro: 0.528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2256, Accuracy: 0.913, F1 Micro: 0.7247, F1 Macro: 0.5729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9193, F1 Micro: 0.7556, F1 Macro: 0.5952\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9188, F1 Micro: 0.7426, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1326, Accuracy: 0.9227, F1 Micro: 0.7639, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9231, F1 Micro: 0.7683, F1 Macro: 0.6512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.923, F1 Micro: 0.774, F1 Macro: 0.6677\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.922, F1 Micro: 0.7678, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0701, Accuracy: 0.9239, F1 Micro: 0.7763, F1 Macro: 0.6964\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9239, F1 Micro: 0.7763, F1 Macro: 0.6964\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.76      0.70      0.73       732\n",
      "     HS_Group       0.67      0.72      0.69       402\n",
      "  HS_Religion       0.68      0.66      0.67       157\n",
      "      HS_Race       0.68      0.79      0.73       120\n",
      "  HS_Physical       0.71      0.21      0.32        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.68      0.71       689\n",
      "  HS_Moderate       0.58      0.64      0.61       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 194.31729578971863 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4022, Accuracy: 0.8784, F1 Micro: 0.5429, F1 Macro: 0.2518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.9024, F1 Micro: 0.6956, F1 Macro: 0.5062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9129, F1 Micro: 0.7241, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9179, F1 Micro: 0.7585, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9193, F1 Micro: 0.7613, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9213, F1 Micro: 0.7688, F1 Macro: 0.6395\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9169, F1 Micro: 0.7624, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0966, Accuracy: 0.9221, F1 Micro: 0.7743, F1 Macro: 0.6802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9227, F1 Micro: 0.7762, F1 Macro: 0.6852\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9232, F1 Micro: 0.7664, F1 Macro: 0.6761\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9227, F1 Micro: 0.7762, F1 Macro: 0.6852\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.80      0.60      0.69       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.58      0.41      0.48        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.59      0.60       331\n",
      "    HS_Strong       0.91      0.82      0.87       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 193.94543743133545 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9226, F1 Micro: 0.7752, F1 Macro: 0.6935\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 630.9481436268669\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 121.31928491592407 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3964, Accuracy: 0.8804, F1 Micro: 0.5556, F1 Macro: 0.2608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.9041, F1 Micro: 0.7078, F1 Macro: 0.4902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2283, Accuracy: 0.9088, F1 Micro: 0.7454, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9159, F1 Micro: 0.76, F1 Macro: 0.6087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.913, F1 Micro: 0.7604, F1 Macro: 0.6202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9234, F1 Micro: 0.7669, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1105, Accuracy: 0.9233, F1 Micro: 0.7673, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9219, F1 Micro: 0.7719, F1 Macro: 0.6827\n",
      "Epoch 9/10, Train Loss: 0.0831, Accuracy: 0.9201, F1 Micro: 0.7678, F1 Macro: 0.6687\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9245, F1 Micro: 0.7692, F1 Macro: 0.6905\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9219, F1 Micro: 0.7719, F1 Macro: 0.6827\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.88      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.14930152893066 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3891, Accuracy: 0.8788, F1 Micro: 0.5308, F1 Macro: 0.2512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9047, F1 Micro: 0.7129, F1 Macro: 0.5156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2265, Accuracy: 0.9086, F1 Micro: 0.7405, F1 Macro: 0.5721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1937, Accuracy: 0.9191, F1 Micro: 0.762, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9198, F1 Micro: 0.765, F1 Macro: 0.6056\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.9205, F1 Micro: 0.7534, F1 Macro: 0.6022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9212, F1 Micro: 0.773, F1 Macro: 0.6466\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9205, F1 Micro: 0.7698, F1 Macro: 0.6611\n",
      "Epoch 9/10, Train Loss: 0.083, Accuracy: 0.9206, F1 Micro: 0.7688, F1 Macro: 0.6627\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9216, F1 Micro: 0.7701, F1 Macro: 0.6937\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9212, F1 Micro: 0.773, F1 Macro: 0.6466\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.80      0.67      0.73       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.63      0.65      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 193.06909918785095 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4036, Accuracy: 0.8814, F1 Micro: 0.5761, F1 Macro: 0.2728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2784, Accuracy: 0.9028, F1 Micro: 0.707, F1 Macro: 0.5052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9089, F1 Micro: 0.7417, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9161, F1 Micro: 0.7571, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.9214, F1 Micro: 0.7653, F1 Macro: 0.6188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9234, F1 Micro: 0.7686, F1 Macro: 0.6304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1106, Accuracy: 0.9227, F1 Micro: 0.7713, F1 Macro: 0.6756\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9224, F1 Micro: 0.7707, F1 Macro: 0.6721\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9226, F1 Micro: 0.769, F1 Macro: 0.6634\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9226, F1 Micro: 0.7685, F1 Macro: 0.6824\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9227, F1 Micro: 0.7713, F1 Macro: 0.6756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.76      0.68      0.72       732\n",
      "     HS_Group       0.67      0.71      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.74      0.65      0.69       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 194.30216598510742 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9219, F1 Micro: 0.7721, F1 Macro: 0.6683\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1005.632979297863\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 108.94246578216553 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3945, Accuracy: 0.8857, F1 Micro: 0.6145, F1 Macro: 0.3006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.9054, F1 Micro: 0.7077, F1 Macro: 0.5173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9134, F1 Micro: 0.73, F1 Macro: 0.5438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9189, F1 Micro: 0.7629, F1 Macro: 0.6021\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9224, F1 Micro: 0.7581, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9227, F1 Micro: 0.7678, F1 Macro: 0.6482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1107, Accuracy: 0.923, F1 Micro: 0.7711, F1 Macro: 0.6668\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9223, F1 Micro: 0.7696, F1 Macro: 0.6799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9214, F1 Micro: 0.7725, F1 Macro: 0.695\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9203, F1 Micro: 0.7713, F1 Macro: 0.6927\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9214, F1 Micro: 0.7725, F1 Macro: 0.695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.77      0.64      0.70       120\n",
      "  HS_Physical       0.63      0.26      0.37        72\n",
      "    HS_Gender       0.53      0.51      0.52        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 199.55951976776123 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8856, F1 Micro: 0.6127, F1 Macro: 0.31\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2652, Accuracy: 0.9052, F1 Micro: 0.7099, F1 Macro: 0.5294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.224, Accuracy: 0.9136, F1 Micro: 0.7234, F1 Macro: 0.5543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9187, F1 Micro: 0.7579, F1 Macro: 0.5996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9221, F1 Micro: 0.7679, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1293, Accuracy: 0.9231, F1 Micro: 0.7693, F1 Macro: 0.6378\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9229, F1 Micro: 0.7677, F1 Macro: 0.6588\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9189, F1 Micro: 0.7689, F1 Macro: 0.6652\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9227, F1 Micro: 0.7626, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9225, F1 Micro: 0.7747, F1 Macro: 0.6988\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9225, F1 Micro: 0.7747, F1 Macro: 0.6988\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.72      0.60      0.65       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.74      0.28      0.40        72\n",
      "    HS_Gender       0.57      0.49      0.53        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.63      0.53      0.57       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 199.13798832893372 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4037, Accuracy: 0.8827, F1 Micro: 0.5912, F1 Macro: 0.2834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2704, Accuracy: 0.9035, F1 Micro: 0.7058, F1 Macro: 0.5209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2268, Accuracy: 0.9131, F1 Micro: 0.7292, F1 Macro: 0.5409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9173, F1 Micro: 0.7581, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1562, Accuracy: 0.9202, F1 Micro: 0.7681, F1 Macro: 0.6202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9214, F1 Micro: 0.769, F1 Macro: 0.6589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9223, F1 Micro: 0.7695, F1 Macro: 0.6653\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9168, F1 Micro: 0.7627, F1 Macro: 0.6637\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9206, F1 Micro: 0.7671, F1 Macro: 0.6784\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9211, F1 Micro: 0.769, F1 Macro: 0.6875\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9223, F1 Micro: 0.7695, F1 Macro: 0.6653\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.77      0.66      0.71       732\n",
      "     HS_Group       0.65      0.73      0.69       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.72      0.69      0.70       120\n",
      "  HS_Physical       0.57      0.06      0.10        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.76      0.64      0.69       689\n",
      "  HS_Moderate       0.58      0.66      0.62       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 199.90842986106873 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9221, F1 Micro: 0.7722, F1 Macro: 0.6864\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 601.2406184345664\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 95.45694422721863 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3956, Accuracy: 0.8845, F1 Micro: 0.594, F1 Macro: 0.2849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9054, F1 Micro: 0.7015, F1 Macro: 0.4968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9151, F1 Micro: 0.7446, F1 Macro: 0.5665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1882, Accuracy: 0.9181, F1 Micro: 0.7612, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.155, Accuracy: 0.9251, F1 Micro: 0.7658, F1 Macro: 0.6286\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9241, F1 Micro: 0.7658, F1 Macro: 0.6705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1094, Accuracy: 0.9252, F1 Micro: 0.7741, F1 Macro: 0.6838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6908\n",
      "Epoch 9/10, Train Loss: 0.0797, Accuracy: 0.9256, F1 Micro: 0.7728, F1 Macro: 0.6939\n",
      "Epoch 10/10, Train Loss: 0.0701, Accuracy: 0.925, F1 Micro: 0.7728, F1 Macro: 0.6957\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.67      0.70      0.69       402\n",
      "  HS_Religion       0.69      0.64      0.67       157\n",
      "      HS_Race       0.76      0.74      0.75       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.66      0.41      0.51        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.58      0.65      0.61       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.1124722957611 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3882, Accuracy: 0.8856, F1 Micro: 0.6071, F1 Macro: 0.3005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2654, Accuracy: 0.9054, F1 Micro: 0.705, F1 Macro: 0.4965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.219, Accuracy: 0.9126, F1 Micro: 0.7445, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1871, Accuracy: 0.9122, F1 Micro: 0.7492, F1 Macro: 0.5971\n",
      "Epoch 5/10, Train Loss: 0.1562, Accuracy: 0.9213, F1 Micro: 0.7478, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9242, F1 Micro: 0.7684, F1 Macro: 0.6507\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9219, F1 Micro: 0.7666, F1 Macro: 0.6548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.923, F1 Micro: 0.7701, F1 Macro: 0.6763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.925, F1 Micro: 0.7722, F1 Macro: 0.6779\n",
      "Epoch 10/10, Train Loss: 0.0692, Accuracy: 0.9216, F1 Micro: 0.767, F1 Macro: 0.6955\n",
      "Model 2 - Iteration 9216: Accuracy: 0.925, F1 Micro: 0.7722, F1 Macro: 0.6779\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.82      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.81      0.58      0.67       157\n",
      "      HS_Race       0.78      0.61      0.68       120\n",
      "  HS_Physical       1.00      0.21      0.34        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.79      0.76      0.78       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.79      0.62      0.68      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 202.2888617515564 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4003, Accuracy: 0.8819, F1 Micro: 0.5696, F1 Macro: 0.2697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.9042, F1 Micro: 0.7077, F1 Macro: 0.5078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9135, F1 Micro: 0.7402, F1 Macro: 0.5639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9155, F1 Micro: 0.7536, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1565, Accuracy: 0.9239, F1 Micro: 0.7623, F1 Macro: 0.6171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9251, F1 Micro: 0.7665, F1 Macro: 0.6674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9232, F1 Micro: 0.7709, F1 Macro: 0.6662\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9136, F1 Micro: 0.7595, F1 Macro: 0.6718\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9231, F1 Micro: 0.7624, F1 Macro: 0.6713\n",
      "Epoch 10/10, Train Loss: 0.0699, Accuracy: 0.9213, F1 Micro: 0.7694, F1 Macro: 0.687\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9232, F1 Micro: 0.7709, F1 Macro: 0.6662\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.81      0.51      0.62       157\n",
      "      HS_Race       0.81      0.62      0.70       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.72      0.35      0.47        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.80      0.63      0.67      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 203.06713604927063 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9238, F1 Micro: 0.7729, F1 Macro: 0.6783\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 795.3795061451219\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 84.91964888572693 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3931, Accuracy: 0.8821, F1 Micro: 0.557, F1 Macro: 0.2644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.9065, F1 Micro: 0.7167, F1 Macro: 0.5241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9117, F1 Micro: 0.7453, F1 Macro: 0.5771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.914, F1 Micro: 0.7605, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1528, Accuracy: 0.9217, F1 Micro: 0.7734, F1 Macro: 0.6397\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.921, F1 Micro: 0.7729, F1 Macro: 0.6738\n",
      "Epoch 7/10, Train Loss: 0.108, Accuracy: 0.9237, F1 Micro: 0.7732, F1 Macro: 0.6549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9231, F1 Micro: 0.7764, F1 Macro: 0.6869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9248, F1 Micro: 0.7774, F1 Macro: 0.6972\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9204, F1 Micro: 0.7756, F1 Macro: 0.7085\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9248, F1 Micro: 0.7774, F1 Macro: 0.6972\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.91      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.70      0.41      0.52        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.22557520866394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3857, Accuracy: 0.8805, F1 Micro: 0.5484, F1 Macro: 0.2775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.9078, F1 Micro: 0.7111, F1 Macro: 0.5306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.22, Accuracy: 0.9147, F1 Micro: 0.7414, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1858, Accuracy: 0.9106, F1 Micro: 0.7559, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9236, F1 Micro: 0.7712, F1 Macro: 0.6329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.9228, F1 Micro: 0.7742, F1 Macro: 0.6474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1049, Accuracy: 0.9246, F1 Micro: 0.7757, F1 Macro: 0.6652\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.9213, F1 Micro: 0.7724, F1 Macro: 0.6736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.9253, F1 Micro: 0.7774, F1 Macro: 0.6954\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9216, F1 Micro: 0.7719, F1 Macro: 0.6891\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9253, F1 Micro: 0.7774, F1 Macro: 0.6954\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.69      0.56      0.62       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 204.9848804473877 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.878, F1 Micro: 0.534, F1 Macro: 0.2502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9069, F1 Micro: 0.7098, F1 Macro: 0.5279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2208, Accuracy: 0.9139, F1 Micro: 0.749, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.9152, F1 Micro: 0.7586, F1 Macro: 0.5983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9209, F1 Micro: 0.7703, F1 Macro: 0.6277\n",
      "Epoch 6/10, Train Loss: 0.1276, Accuracy: 0.9189, F1 Micro: 0.7687, F1 Macro: 0.6509\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9236, F1 Micro: 0.7655, F1 Macro: 0.645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9237, F1 Micro: 0.7757, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9247, F1 Micro: 0.7762, F1 Macro: 0.6921\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9189, F1 Micro: 0.7686, F1 Macro: 0.6855\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9247, F1 Micro: 0.7762, F1 Macro: 0.6921\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.80      0.61      0.69       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.67      0.43      0.52        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.74      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 204.23448038101196 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9249, F1 Micro: 0.777, F1 Macro: 0.6949\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 832.9631161363214\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 83.53290963172913 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3883, Accuracy: 0.8827, F1 Micro: 0.5664, F1 Macro: 0.2676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2677, Accuracy: 0.9045, F1 Micro: 0.7042, F1 Macro: 0.4563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9084, F1 Micro: 0.7412, F1 Macro: 0.5739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9214, F1 Micro: 0.7608, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.9198, F1 Micro: 0.7699, F1 Macro: 0.6444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.9215, F1 Micro: 0.7731, F1 Macro: 0.6563\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9235, F1 Micro: 0.7723, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6887\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9221, F1 Micro: 0.7718, F1 Macro: 0.6941\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9214, F1 Micro: 0.7724, F1 Macro: 0.7004\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6887\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.91      0.88      0.89       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.71      0.64      0.68       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 206.48807668685913 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.8846, F1 Micro: 0.581, F1 Macro: 0.2902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9056, F1 Micro: 0.7069, F1 Macro: 0.4934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9129, F1 Micro: 0.7442, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9203, F1 Micro: 0.752, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9184, F1 Micro: 0.7636, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9182, F1 Micro: 0.7645, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9239, F1 Micro: 0.7702, F1 Macro: 0.647\n",
      "Epoch 8/10, Train Loss: 0.0911, Accuracy: 0.9216, F1 Micro: 0.7612, F1 Macro: 0.6631\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9187, F1 Micro: 0.7673, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.9235, F1 Micro: 0.7742, F1 Macro: 0.6958\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9235, F1 Micro: 0.7742, F1 Macro: 0.6958\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.75      0.59      0.66       402\n",
      "  HS_Religion       0.74      0.60      0.66       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.78      0.29      0.42        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 208.22979497909546 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3955, Accuracy: 0.8812, F1 Micro: 0.558, F1 Macro: 0.2626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.9046, F1 Micro: 0.7038, F1 Macro: 0.4871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2233, Accuracy: 0.9098, F1 Micro: 0.7419, F1 Macro: 0.5826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9209, F1 Micro: 0.7621, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9219, F1 Micro: 0.772, F1 Macro: 0.6368\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.9195, F1 Micro: 0.7659, F1 Macro: 0.6352\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9219, F1 Micro: 0.7597, F1 Macro: 0.6607\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9229, F1 Micro: 0.7639, F1 Macro: 0.6652\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9216, F1 Micro: 0.7668, F1 Macro: 0.6693\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9237, F1 Micro: 0.7695, F1 Macro: 0.6877\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9219, F1 Micro: 0.772, F1 Macro: 0.6368\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.78      0.61      0.69       402\n",
      "  HS_Religion       0.77      0.59      0.66       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.92      0.75      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.61      0.64      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 202.66077160835266 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9234, F1 Micro: 0.7743, F1 Macro: 0.6738\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 519.8476910920091\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 71.65338611602783 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.8819, F1 Micro: 0.5468, F1 Macro: 0.2626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2692, Accuracy: 0.9025, F1 Micro: 0.6695, F1 Macro: 0.4406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9138, F1 Micro: 0.7201, F1 Macro: 0.5556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9191, F1 Micro: 0.7635, F1 Macro: 0.605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.9202, F1 Micro: 0.7683, F1 Macro: 0.6569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1276, Accuracy: 0.9205, F1 Micro: 0.7729, F1 Macro: 0.6688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9221, F1 Micro: 0.773, F1 Macro: 0.6818\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9254, F1 Micro: 0.7712, F1 Macro: 0.6889\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9238, F1 Micro: 0.7678, F1 Macro: 0.6926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9252, F1 Micro: 0.7794, F1 Macro: 0.7038\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9252, F1 Micro: 0.7794, F1 Macro: 0.7038\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.91      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.70      0.71      0.70       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.76      0.70      0.73       120\n",
      "  HS_Physical       0.95      0.28      0.43        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.71       689\n",
      "  HS_Moderate       0.62      0.62      0.62       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 212.81872582435608 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.884, F1 Micro: 0.5728, F1 Macro: 0.3071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2653, Accuracy: 0.905, F1 Micro: 0.6889, F1 Macro: 0.4706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9133, F1 Micro: 0.7234, F1 Macro: 0.5729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.918, F1 Micro: 0.7579, F1 Macro: 0.5972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9209, F1 Micro: 0.765, F1 Macro: 0.6242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.9196, F1 Micro: 0.7684, F1 Macro: 0.6646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9177, F1 Micro: 0.7688, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.923, F1 Micro: 0.7708, F1 Macro: 0.6904\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.9233, F1 Micro: 0.7694, F1 Macro: 0.6925\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9252, F1 Micro: 0.7679, F1 Macro: 0.697\n",
      "Model 2 - Iteration 9618: Accuracy: 0.923, F1 Micro: 0.7708, F1 Macro: 0.6904\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 212.77161717414856 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.395, Accuracy: 0.882, F1 Micro: 0.5602, F1 Macro: 0.2698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2693, Accuracy: 0.9052, F1 Micro: 0.6899, F1 Macro: 0.4707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.9143, F1 Micro: 0.7255, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.9158, F1 Micro: 0.7576, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9216, F1 Micro: 0.7695, F1 Macro: 0.6451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9195, F1 Micro: 0.7697, F1 Macro: 0.6714\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.919, F1 Micro: 0.7692, F1 Macro: 0.6626\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9212, F1 Micro: 0.7646, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0785, Accuracy: 0.9209, F1 Micro: 0.7722, F1 Macro: 0.6872\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9228, F1 Micro: 0.7685, F1 Macro: 0.6862\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9209, F1 Micro: 0.7722, F1 Macro: 0.6872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.73      0.61      0.67       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.76      0.18      0.29        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 211.06055545806885 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.923, F1 Micro: 0.7742, F1 Macro: 0.6938\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 530.111341097416\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 60.58690166473389 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8878, F1 Micro: 0.6183, F1 Macro: 0.3014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.9085, F1 Micro: 0.7167, F1 Macro: 0.5217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2162, Accuracy: 0.9148, F1 Micro: 0.7497, F1 Macro: 0.5766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1791, Accuracy: 0.9189, F1 Micro: 0.7659, F1 Macro: 0.6159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1495, Accuracy: 0.9233, F1 Micro: 0.7752, F1 Macro: 0.683\n",
      "Epoch 6/10, Train Loss: 0.1271, Accuracy: 0.9244, F1 Micro: 0.7748, F1 Macro: 0.6753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1104, Accuracy: 0.9241, F1 Micro: 0.7814, F1 Macro: 0.7009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.925, F1 Micro: 0.7824, F1 Macro: 0.7109\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9236, F1 Micro: 0.7807, F1 Macro: 0.707\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9238, F1 Micro: 0.7723, F1 Macro: 0.7081\n",
      "Model 1 - Iteration 9818: Accuracy: 0.925, F1 Micro: 0.7824, F1 Macro: 0.7109\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.70      0.78      0.74       120\n",
      "  HS_Physical       0.87      0.28      0.42        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.80      0.80      0.80       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.70      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 213.33240842819214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.381, Accuracy: 0.888, F1 Micro: 0.6398, F1 Macro: 0.3406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2604, Accuracy: 0.9074, F1 Micro: 0.7165, F1 Macro: 0.5182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.914, F1 Micro: 0.749, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.9204, F1 Micro: 0.7607, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.9234, F1 Micro: 0.7616, F1 Macro: 0.647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9221, F1 Micro: 0.7672, F1 Macro: 0.6625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9181, F1 Micro: 0.7723, F1 Macro: 0.678\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9244, F1 Micro: 0.7722, F1 Macro: 0.6948\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9234, F1 Micro: 0.772, F1 Macro: 0.6914\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9243, F1 Micro: 0.772, F1 Macro: 0.7053\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9181, F1 Micro: 0.7723, F1 Macro: 0.678\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.91      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.67      0.81      0.74       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.72      0.56      0.63       157\n",
      "      HS_Race       0.70      0.69      0.69       120\n",
      "  HS_Physical       0.79      0.21      0.33        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.72      0.86      0.78       762\n",
      "      HS_Weak       0.66      0.79      0.72       689\n",
      "  HS_Moderate       0.60      0.57      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.74      0.81      0.77      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.74      0.81      0.77      5556\n",
      "  samples avg       0.45      0.45      0.44      5556\n",
      "\n",
      "Training completed in 211.61802744865417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3948, Accuracy: 0.8863, F1 Micro: 0.6239, F1 Macro: 0.3172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.9086, F1 Micro: 0.7188, F1 Macro: 0.5226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9133, F1 Micro: 0.742, F1 Macro: 0.575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9182, F1 Micro: 0.7633, F1 Macro: 0.6052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1495, Accuracy: 0.9229, F1 Micro: 0.7675, F1 Macro: 0.6582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.9212, F1 Micro: 0.7716, F1 Macro: 0.6681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9211, F1 Micro: 0.7732, F1 Macro: 0.6786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0916, Accuracy: 0.922, F1 Micro: 0.7758, F1 Macro: 0.681\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9189, F1 Micro: 0.7702, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9204, F1 Micro: 0.7669, F1 Macro: 0.6887\n",
      "Model 3 - Iteration 9818: Accuracy: 0.922, F1 Micro: 0.7758, F1 Macro: 0.681\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.71      0.71      0.71       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.67      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 213.9066824913025 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9217, F1 Micro: 0.7768, F1 Macro: 0.69\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 723.6496373017274\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 46.97857356071472 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3797, Accuracy: 0.886, F1 Micro: 0.6126, F1 Macro: 0.3132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.9095, F1 Micro: 0.7181, F1 Macro: 0.5215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9185, F1 Micro: 0.7491, F1 Macro: 0.5736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.92, F1 Micro: 0.7641, F1 Macro: 0.6206\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.921, F1 Micro: 0.7609, F1 Macro: 0.6487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9238, F1 Micro: 0.7787, F1 Macro: 0.6846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9241, F1 Micro: 0.7826, F1 Macro: 0.7049\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9248, F1 Micro: 0.7811, F1 Macro: 0.7029\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9258, F1 Micro: 0.7816, F1 Macro: 0.6998\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9225, F1 Micro: 0.7766, F1 Macro: 0.7081\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9241, F1 Micro: 0.7826, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.80      0.74       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.68      0.69      0.69       157\n",
      "      HS_Race       0.76      0.71      0.73       120\n",
      "  HS_Physical       0.78      0.29      0.42        72\n",
      "    HS_Gender       0.58      0.41      0.48        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.68      0.78      0.73       689\n",
      "  HS_Moderate       0.65      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 215.49315476417542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3735, Accuracy: 0.8867, F1 Micro: 0.6074, F1 Macro: 0.3707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9086, F1 Micro: 0.7149, F1 Macro: 0.5412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9179, F1 Micro: 0.7492, F1 Macro: 0.593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9187, F1 Micro: 0.7624, F1 Macro: 0.6138\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.92, F1 Micro: 0.7613, F1 Macro: 0.622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1226, Accuracy: 0.9219, F1 Micro: 0.772, F1 Macro: 0.6477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9251, F1 Micro: 0.7753, F1 Macro: 0.6866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9214, F1 Micro: 0.7767, F1 Macro: 0.6889\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9231, F1 Micro: 0.7741, F1 Macro: 0.6907\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9228, F1 Micro: 0.7753, F1 Macro: 0.6973\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9214, F1 Micro: 0.7767, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.77      0.74       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.69      0.63      0.66       157\n",
      "      HS_Race       0.71      0.75      0.73       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.71       689\n",
      "  HS_Moderate       0.61      0.58      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 217.92676424980164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8848, F1 Micro: 0.6097, F1 Macro: 0.3132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.9075, F1 Micro: 0.7192, F1 Macro: 0.5331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2106, Accuracy: 0.9184, F1 Micro: 0.7578, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.92, F1 Micro: 0.7617, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.9215, F1 Micro: 0.7637, F1 Macro: 0.6385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9228, F1 Micro: 0.7765, F1 Macro: 0.6642\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9232, F1 Micro: 0.7668, F1 Macro: 0.6767\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9228, F1 Micro: 0.7726, F1 Macro: 0.6806\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9218, F1 Micro: 0.7716, F1 Macro: 0.6828\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9218, F1 Micro: 0.7733, F1 Macro: 0.6859\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9228, F1 Micro: 0.7765, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.74      0.67      0.70       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.70      0.76      0.73       689\n",
      "  HS_Moderate       0.64      0.52      0.57       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.77      0.64      0.66      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.4164843559265 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9228, F1 Micro: 0.7786, F1 Macro: 0.686\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 40.165892888970795\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 35.41716527938843 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3812, Accuracy: 0.8876, F1 Micro: 0.6365, F1 Macro: 0.3336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.907, F1 Micro: 0.7091, F1 Macro: 0.4757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2137, Accuracy: 0.9137, F1 Micro: 0.7456, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9206, F1 Micro: 0.7664, F1 Macro: 0.6005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.9244, F1 Micro: 0.7719, F1 Macro: 0.6431\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.9249, F1 Micro: 0.7709, F1 Macro: 0.677\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.9179, F1 Micro: 0.7718, F1 Macro: 0.6856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0883, Accuracy: 0.925, F1 Micro: 0.7826, F1 Macro: 0.7087\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9237, F1 Micro: 0.7728, F1 Macro: 0.7012\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9261, F1 Micro: 0.7796, F1 Macro: 0.7103\n",
      "Model 1 - Iteration 10218: Accuracy: 0.925, F1 Micro: 0.7826, F1 Macro: 0.7087\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.70      0.70      0.70       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.86      0.67      0.75       120\n",
      "  HS_Physical       0.94      0.24      0.38        72\n",
      "    HS_Gender       0.65      0.47      0.55        51\n",
      "     HS_Other       0.77      0.83      0.80       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.60      0.64      0.62       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.78      0.68      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 218.27107119560242 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3724, Accuracy: 0.887, F1 Micro: 0.6643, F1 Macro: 0.3982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9057, F1 Micro: 0.7076, F1 Macro: 0.4782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.9157, F1 Micro: 0.7486, F1 Macro: 0.573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9202, F1 Micro: 0.7613, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9231, F1 Micro: 0.7712, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.9236, F1 Micro: 0.7757, F1 Macro: 0.6782\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9195, F1 Micro: 0.773, F1 Macro: 0.6724\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9234, F1 Micro: 0.774, F1 Macro: 0.6778\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9223, F1 Micro: 0.7661, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9228, F1 Micro: 0.7757, F1 Macro: 0.7066\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9228, F1 Micro: 0.7757, F1 Macro: 0.7066\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.71      0.61      0.65       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.81      0.31      0.44        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 218.85191416740417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3862, Accuracy: 0.8851, F1 Micro: 0.6421, F1 Macro: 0.3327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.9065, F1 Micro: 0.7078, F1 Macro: 0.4868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9144, F1 Micro: 0.7451, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1795, Accuracy: 0.92, F1 Micro: 0.764, F1 Macro: 0.5989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1449, Accuracy: 0.923, F1 Micro: 0.7677, F1 Macro: 0.629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9222, F1 Micro: 0.7679, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1046, Accuracy: 0.9207, F1 Micro: 0.7713, F1 Macro: 0.6751\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9208, F1 Micro: 0.7691, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.6957\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9197, F1 Micro: 0.7725, F1 Macro: 0.6952\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.6957\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.67      0.67      0.67       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 220.90843629837036 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9238, F1 Micro: 0.7781, F1 Macro: 0.7037\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 17.211136328867198\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.264196395874023 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3757, Accuracy: 0.8886, F1 Micro: 0.6227, F1 Macro: 0.306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.9046, F1 Micro: 0.7209, F1 Macro: 0.5112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.917, F1 Micro: 0.7451, F1 Macro: 0.5778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9204, F1 Micro: 0.7616, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.924, F1 Micro: 0.772, F1 Macro: 0.6455\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9253, F1 Micro: 0.7698, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9239, F1 Micro: 0.7766, F1 Macro: 0.6819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0848, Accuracy: 0.9248, F1 Micro: 0.7831, F1 Macro: 0.7022\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.925, F1 Micro: 0.7791, F1 Macro: 0.7156\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9244, F1 Micro: 0.7755, F1 Macro: 0.7025\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9248, F1 Micro: 0.7831, F1 Macro: 0.7022\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.75      0.65      0.70       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.72      0.41      0.53        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 223.94108653068542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3694, Accuracy: 0.889, F1 Micro: 0.6288, F1 Macro: 0.3247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2494, Accuracy: 0.9049, F1 Micro: 0.7132, F1 Macro: 0.525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9167, F1 Micro: 0.7433, F1 Macro: 0.5798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1739, Accuracy: 0.9187, F1 Micro: 0.7535, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.143, Accuracy: 0.9221, F1 Micro: 0.768, F1 Macro: 0.6338\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9235, F1 Micro: 0.7617, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9233, F1 Micro: 0.7705, F1 Macro: 0.6695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0854, Accuracy: 0.9247, F1 Micro: 0.7764, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9231, F1 Micro: 0.7795, F1 Macro: 0.7096\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9214, F1 Micro: 0.7696, F1 Macro: 0.6966\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9231, F1 Micro: 0.7795, F1 Macro: 0.7096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.70      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.91      0.28      0.43        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.60      0.63      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.76      0.69      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 225.5673713684082 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3826, Accuracy: 0.8863, F1 Micro: 0.6081, F1 Macro: 0.2952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9048, F1 Micro: 0.7181, F1 Macro: 0.5141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9165, F1 Micro: 0.7426, F1 Macro: 0.582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9177, F1 Micro: 0.7567, F1 Macro: 0.6011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9239, F1 Micro: 0.7727, F1 Macro: 0.6372\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9235, F1 Micro: 0.7696, F1 Macro: 0.6538\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9212, F1 Micro: 0.7681, F1 Macro: 0.6713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9247, F1 Micro: 0.7763, F1 Macro: 0.6817\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9241, F1 Micro: 0.7732, F1 Macro: 0.6882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9234, F1 Micro: 0.7767, F1 Macro: 0.6979\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9234, F1 Micro: 0.7767, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.65      0.66      0.66       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.94      0.24      0.38        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 223.7559826374054 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9238, F1 Micro: 0.7798, F1 Macro: 0.7032\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -11.857158479412178\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 6.745861768722534 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3715, Accuracy: 0.8888, F1 Micro: 0.6357, F1 Macro: 0.3223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2481, Accuracy: 0.9076, F1 Micro: 0.712, F1 Macro: 0.5015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9139, F1 Micro: 0.7471, F1 Macro: 0.5885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9216, F1 Micro: 0.7668, F1 Macro: 0.6171\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9226, F1 Micro: 0.7561, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1204, Accuracy: 0.9225, F1 Micro: 0.7754, F1 Macro: 0.6717\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9227, F1 Micro: 0.7696, F1 Macro: 0.6606\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9228, F1 Micro: 0.7741, F1 Macro: 0.691\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9239, F1 Micro: 0.7744, F1 Macro: 0.6964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9234, F1 Micro: 0.776, F1 Macro: 0.7049\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9234, F1 Micro: 0.776, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.66      0.71      0.68       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.84      0.29      0.43        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.60      0.63      0.61       331\n",
      "    HS_Strong       0.88      0.87      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 224.42929363250732 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3662, Accuracy: 0.8899, F1 Micro: 0.6348, F1 Macro: 0.3469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.246, Accuracy: 0.9063, F1 Micro: 0.7094, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.205, Accuracy: 0.9164, F1 Micro: 0.74, F1 Macro: 0.5777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9203, F1 Micro: 0.767, F1 Macro: 0.6211\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9208, F1 Micro: 0.748, F1 Macro: 0.595\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9227, F1 Micro: 0.7649, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9218, F1 Micro: 0.7722, F1 Macro: 0.6472\n",
      "Epoch 8/10, Train Loss: 0.08, Accuracy: 0.9179, F1 Micro: 0.7694, F1 Macro: 0.6808\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9237, F1 Micro: 0.7715, F1 Macro: 0.6884\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.922, F1 Micro: 0.7652, F1 Macro: 0.6992\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9218, F1 Micro: 0.7722, F1 Macro: 0.6472\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.79      0.55      0.65       402\n",
      "  HS_Religion       0.81      0.53      0.64       157\n",
      "      HS_Race       0.82      0.67      0.73       120\n",
      "  HS_Physical       1.00      0.12      0.22        72\n",
      "    HS_Gender       0.50      0.08      0.14        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.66      0.79      0.72       689\n",
      "  HS_Moderate       0.73      0.46      0.56       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.78      0.62      0.65      5556\n",
      " weighted avg       0.78      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 223.22318720817566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3772, Accuracy: 0.887, F1 Micro: 0.6261, F1 Macro: 0.3165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9078, F1 Micro: 0.7155, F1 Macro: 0.5029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.9155, F1 Micro: 0.7473, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9207, F1 Micro: 0.7631, F1 Macro: 0.6134\n",
      "Epoch 5/10, Train Loss: 0.141, Accuracy: 0.9215, F1 Micro: 0.7571, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9212, F1 Micro: 0.7727, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9247, F1 Micro: 0.7737, F1 Macro: 0.6538\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.9191, F1 Micro: 0.769, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9227, F1 Micro: 0.7747, F1 Macro: 0.6756\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9234, F1 Micro: 0.7693, F1 Macro: 0.6961\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9227, F1 Micro: 0.7747, F1 Macro: 0.6756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.62      0.66       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.82      0.87      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 226.36350798606873 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9226, F1 Micro: 0.7743, F1 Macro: 0.6759\n",
      "Total sampling time: 5886.99 seconds\n",
      "Total runtime: 20673.792259693146 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050d685",
   "metadata": {
    "papermill": {
     "duration": 0.012516,
     "end_time": "2025-06-08T08:00:31.506771",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.494255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0896d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:00:31.532644Z",
     "iopub.status.busy": "2025-06-08T08:00:31.532428Z",
     "iopub.status.idle": "2025-06-08T13:34:49.359332Z",
     "shell.execute_reply": "2025-06-08T13:34:49.358377Z"
    },
    "papermill": {
     "duration": 20057.841316,
     "end_time": "2025-06-08T13:34:49.360715",
     "exception": false,
     "start_time": "2025-06-08T08:00:31.519399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6215, Accuracy: 0.8191, F1 Micro: 0.1615, F1 Macro: 0.0752\n",
      "Epoch 2/10, Train Loss: 0.4665, Accuracy: 0.8316, F1 Micro: 0.073, F1 Macro: 0.0276\n",
      "Epoch 3/10, Train Loss: 0.3955, Accuracy: 0.8331, F1 Micro: 0.0914, F1 Macro: 0.0331\n",
      "Epoch 4/10, Train Loss: 0.3996, Accuracy: 0.8322, F1 Micro: 0.0663, F1 Macro: 0.0261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3852, Accuracy: 0.8423, F1 Micro: 0.1862, F1 Macro: 0.0687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3697, Accuracy: 0.8528, F1 Micro: 0.3211, F1 Macro: 0.1051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3565, Accuracy: 0.8617, F1 Micro: 0.408, F1 Macro: 0.1432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3315, Accuracy: 0.8738, F1 Micro: 0.5215, F1 Macro: 0.2324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2962, Accuracy: 0.8754, F1 Micro: 0.5639, F1 Macro: 0.2627\n",
      "Epoch 10/10, Train Loss: 0.263, Accuracy: 0.8768, F1 Micro: 0.5586, F1 Macro: 0.2751\n",
      "Model 1 - Iteration 658: Accuracy: 0.8754, F1 Micro: 0.5639, F1 Macro: 0.2627\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.72      0.75      1134\n",
      "      Abusive       0.82      0.72      0.77       992\n",
      "HS_Individual       0.62      0.48      0.54       732\n",
      "     HS_Group       0.44      0.01      0.02       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.62      0.55      0.58       762\n",
      "      HS_Weak       0.59      0.43      0.49       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.47      0.56      5556\n",
      "    macro avg       0.32      0.24      0.26      5556\n",
      " weighted avg       0.58      0.47      0.50      5556\n",
      "  samples avg       0.36      0.27      0.28      5556\n",
      "\n",
      "Training completed in 49.1267671585083 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9daa7b2fae6491082569bd477f6b4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6202, Accuracy: 0.8216, F1 Micro: 0.2374, F1 Macro: 0.0537\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.8289, F1 Micro: 0.0206, F1 Macro: 0.009\n",
      "Epoch 3/10, Train Loss: 0.3926, Accuracy: 0.8333, F1 Micro: 0.0952, F1 Macro: 0.0349\n",
      "Epoch 4/10, Train Loss: 0.4, Accuracy: 0.8342, F1 Micro: 0.1006, F1 Macro: 0.0364\n",
      "Epoch 5/10, Train Loss: 0.3855, Accuracy: 0.8385, F1 Micro: 0.1788, F1 Macro: 0.0548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.372, Accuracy: 0.8472, F1 Micro: 0.2544, F1 Macro: 0.0879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3573, Accuracy: 0.8604, F1 Micro: 0.3983, F1 Macro: 0.1601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3352, Accuracy: 0.8711, F1 Micro: 0.4901, F1 Macro: 0.2237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3016, Accuracy: 0.8749, F1 Micro: 0.5685, F1 Macro: 0.264\n",
      "Epoch 10/10, Train Loss: 0.2699, Accuracy: 0.8753, F1 Micro: 0.533, F1 Macro: 0.2503\n",
      "Model 2 - Iteration 658: Accuracy: 0.8749, F1 Micro: 0.5685, F1 Macro: 0.264\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.74      0.75      1134\n",
      "      Abusive       0.76      0.72      0.74       992\n",
      "HS_Individual       0.64      0.52      0.58       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.54      0.58       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.48      0.57      5556\n",
      "    macro avg       0.28      0.25      0.26      5556\n",
      " weighted avg       0.54      0.48      0.51      5556\n",
      "  samples avg       0.39      0.28      0.30      5556\n",
      "\n",
      "Training completed in 47.46734571456909 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5865, Accuracy: 0.8278, F1 Micro: 0.0554, F1 Macro: 0.0267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4603, Accuracy: 0.8362, F1 Micro: 0.1378, F1 Macro: 0.0461\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8367, F1 Micro: 0.1344, F1 Macro: 0.0452\n",
      "Epoch 4/10, Train Loss: 0.3978, Accuracy: 0.8345, F1 Micro: 0.0947, F1 Macro: 0.0352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3828, Accuracy: 0.8479, F1 Micro: 0.2674, F1 Macro: 0.091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3636, Accuracy: 0.8522, F1 Micro: 0.3052, F1 Macro: 0.1024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3506, Accuracy: 0.8657, F1 Micro: 0.4335, F1 Macro: 0.1769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3283, Accuracy: 0.8714, F1 Micro: 0.4787, F1 Macro: 0.2164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2903, Accuracy: 0.8776, F1 Micro: 0.5532, F1 Macro: 0.2618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.264, Accuracy: 0.8781, F1 Micro: 0.5728, F1 Macro: 0.2893\n",
      "Model 3 - Iteration 658: Accuracy: 0.8781, F1 Micro: 0.5728, F1 Macro: 0.2893\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.71      0.75      1134\n",
      "      Abusive       0.82      0.71      0.76       992\n",
      "HS_Individual       0.67      0.45      0.54       732\n",
      "     HS_Group       0.46      0.12      0.19       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.54      0.60       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.43      0.06      0.11       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.47      0.57      5556\n",
      "    macro avg       0.37      0.25      0.29      5556\n",
      " weighted avg       0.62      0.47      0.53      5556\n",
      "  samples avg       0.35      0.27      0.28      5556\n",
      "\n",
      "Training completed in 54.042015075683594 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5684, F1 Macro: 0.272\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 558.8451098179937\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 614.2014443874359 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5289, Accuracy: 0.8314, F1 Micro: 0.0675, F1 Macro: 0.026\n",
      "Epoch 2/10, Train Loss: 0.3978, Accuracy: 0.8305, F1 Micro: 0.0373, F1 Macro: 0.016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3774, Accuracy: 0.855, F1 Micro: 0.3371, F1 Macro: 0.1096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3388, Accuracy: 0.8742, F1 Micro: 0.5025, F1 Macro: 0.2272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3023, Accuracy: 0.8808, F1 Micro: 0.5993, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2625, Accuracy: 0.8874, F1 Micro: 0.6259, F1 Macro: 0.334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2356, Accuracy: 0.8883, F1 Micro: 0.6516, F1 Macro: 0.377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2134, Accuracy: 0.8903, F1 Micro: 0.6758, F1 Macro: 0.416\n",
      "Epoch 9/10, Train Loss: 0.1955, Accuracy: 0.891, F1 Micro: 0.6511, F1 Macro: 0.4228\n",
      "Epoch 10/10, Train Loss: 0.1756, Accuracy: 0.8915, F1 Micro: 0.6757, F1 Macro: 0.4631\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8903, F1 Micro: 0.6758, F1 Macro: 0.416\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.86      0.81      1134\n",
      "      Abusive       0.82      0.85      0.84       992\n",
      "HS_Individual       0.64      0.65      0.65       732\n",
      "     HS_Group       0.57      0.56      0.57       402\n",
      "  HS_Religion       0.50      0.15      0.23       157\n",
      "      HS_Race       1.00      0.08      0.15       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.75      0.70       762\n",
      "      HS_Weak       0.61      0.62      0.61       689\n",
      "  HS_Moderate       0.44      0.43      0.44       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.66      0.68      5556\n",
      "    macro avg       0.50      0.41      0.42      5556\n",
      " weighted avg       0.66      0.66      0.65      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 69.00389361381531 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.8285, F1 Micro: 0.0143, F1 Macro: 0.0063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3963, Accuracy: 0.8351, F1 Micro: 0.1101, F1 Macro: 0.0396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3761, Accuracy: 0.8498, F1 Micro: 0.2897, F1 Macro: 0.1015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3393, Accuracy: 0.8745, F1 Micro: 0.5072, F1 Macro: 0.2361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3047, Accuracy: 0.8807, F1 Micro: 0.6117, F1 Macro: 0.2904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.267, Accuracy: 0.8865, F1 Micro: 0.6258, F1 Macro: 0.3251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2396, Accuracy: 0.8896, F1 Micro: 0.6513, F1 Macro: 0.3666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2181, Accuracy: 0.8905, F1 Micro: 0.6648, F1 Macro: 0.3979\n",
      "Epoch 9/10, Train Loss: 0.1981, Accuracy: 0.8901, F1 Micro: 0.654, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1762, Accuracy: 0.8911, F1 Micro: 0.6662, F1 Macro: 0.4492\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8911, F1 Micro: 0.6662, F1 Macro: 0.4492\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.80      0.80      1134\n",
      "      Abusive       0.86      0.79      0.82       992\n",
      "HS_Individual       0.67      0.56      0.61       732\n",
      "     HS_Group       0.58      0.64      0.61       402\n",
      "  HS_Religion       0.52      0.24      0.33       157\n",
      "      HS_Race       0.84      0.32      0.46       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.68      0.69       762\n",
      "      HS_Weak       0.64      0.54      0.58       689\n",
      "  HS_Moderate       0.44      0.55      0.49       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.63      0.67      5556\n",
      "    macro avg       0.50      0.43      0.45      5556\n",
      " weighted avg       0.68      0.63      0.65      5556\n",
      "  samples avg       0.39      0.35      0.35      5556\n",
      "\n",
      "Training completed in 71.7653911113739 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5144, Accuracy: 0.8348, F1 Micro: 0.1027, F1 Macro: 0.0381\n",
      "Epoch 2/10, Train Loss: 0.3965, Accuracy: 0.8353, F1 Micro: 0.1002, F1 Macro: 0.0391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3735, Accuracy: 0.8552, F1 Micro: 0.3463, F1 Macro: 0.1169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.336, Accuracy: 0.8751, F1 Micro: 0.5022, F1 Macro: 0.2314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3, Accuracy: 0.8822, F1 Micro: 0.5761, F1 Macro: 0.2885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2618, Accuracy: 0.8855, F1 Micro: 0.6529, F1 Macro: 0.3711\n",
      "Epoch 7/10, Train Loss: 0.2367, Accuracy: 0.8884, F1 Micro: 0.6179, F1 Macro: 0.3651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.214, Accuracy: 0.8907, F1 Micro: 0.6664, F1 Macro: 0.4058\n",
      "Epoch 9/10, Train Loss: 0.1943, Accuracy: 0.8912, F1 Micro: 0.6399, F1 Macro: 0.4068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1755, Accuracy: 0.8907, F1 Micro: 0.6822, F1 Macro: 0.4464\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8907, F1 Micro: 0.6822, F1 Macro: 0.4464\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.85      0.81      1134\n",
      "      Abusive       0.83      0.85      0.84       992\n",
      "HS_Individual       0.62      0.69      0.66       732\n",
      "     HS_Group       0.58      0.55      0.57       402\n",
      "  HS_Religion       0.56      0.31      0.40       157\n",
      "      HS_Race       1.00      0.16      0.27       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.75      0.70       762\n",
      "      HS_Weak       0.59      0.67      0.63       689\n",
      "  HS_Moderate       0.46      0.44      0.45       331\n",
      "    HS_Strong       1.00      0.02      0.03       114\n",
      "\n",
      "    micro avg       0.68      0.68      0.68      5556\n",
      "    macro avg       0.59      0.44      0.45      5556\n",
      " weighted avg       0.68      0.68      0.66      5556\n",
      "  samples avg       0.39      0.38      0.37      5556\n",
      "\n",
      "Training completed in 68.7573447227478 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8907, F1 Micro: 0.6747, F1 Macro: 0.4372\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1100.3654183553197\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples:889 \n",
      "Sampling duration: 550.5447084903717 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4927, Accuracy: 0.8332, F1 Micro: 0.0872, F1 Macro: 0.0325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.858, F1 Micro: 0.3884, F1 Macro: 0.1214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3287, Accuracy: 0.8788, F1 Micro: 0.6072, F1 Macro: 0.2941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2894, Accuracy: 0.8881, F1 Micro: 0.6141, F1 Macro: 0.318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2402, Accuracy: 0.8934, F1 Micro: 0.6643, F1 Macro: 0.4082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2146, Accuracy: 0.8939, F1 Micro: 0.6889, F1 Macro: 0.459\n",
      "Epoch 7/10, Train Loss: 0.1944, Accuracy: 0.896, F1 Micro: 0.6576, F1 Macro: 0.4563\n",
      "Epoch 8/10, Train Loss: 0.1664, Accuracy: 0.8995, F1 Micro: 0.6738, F1 Macro: 0.4716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1452, Accuracy: 0.9009, F1 Micro: 0.6981, F1 Macro: 0.4854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1275, Accuracy: 0.903, F1 Micro: 0.7062, F1 Macro: 0.5388\n",
      "Model 1 - Iteration 2535: Accuracy: 0.903, F1 Micro: 0.7062, F1 Macro: 0.5388\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.81      1134\n",
      "      Abusive       0.86      0.85      0.86       992\n",
      "HS_Individual       0.68      0.63      0.65       732\n",
      "     HS_Group       0.65      0.61      0.63       402\n",
      "  HS_Religion       0.65      0.48      0.55       157\n",
      "      HS_Race       0.81      0.57      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.70      0.71       762\n",
      "      HS_Weak       0.64      0.60      0.62       689\n",
      "  HS_Moderate       0.53      0.52      0.52       331\n",
      "    HS_Strong       0.94      0.29      0.44       114\n",
      "\n",
      "    micro avg       0.74      0.68      0.71      5556\n",
      "    macro avg       0.61      0.50      0.54      5556\n",
      " weighted avg       0.72      0.68      0.70      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 85.72259402275085 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4917, Accuracy: 0.834, F1 Micro: 0.0993, F1 Macro: 0.0387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.8571, F1 Micro: 0.4042, F1 Macro: 0.1391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.334, Accuracy: 0.8791, F1 Micro: 0.5994, F1 Macro: 0.2797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2968, Accuracy: 0.8873, F1 Micro: 0.6274, F1 Macro: 0.3341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2457, Accuracy: 0.8918, F1 Micro: 0.6629, F1 Macro: 0.3958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2185, Accuracy: 0.8943, F1 Micro: 0.6833, F1 Macro: 0.4321\n",
      "Epoch 7/10, Train Loss: 0.1958, Accuracy: 0.8962, F1 Micro: 0.6462, F1 Macro: 0.4192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1672, Accuracy: 0.9013, F1 Micro: 0.6965, F1 Macro: 0.4871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1491, Accuracy: 0.8974, F1 Micro: 0.7118, F1 Macro: 0.5309\n",
      "Epoch 10/10, Train Loss: 0.1258, Accuracy: 0.9028, F1 Micro: 0.7066, F1 Macro: 0.5199\n",
      "Model 2 - Iteration 2535: Accuracy: 0.8974, F1 Micro: 0.7118, F1 Macro: 0.5309\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.88      0.82      1134\n",
      "      Abusive       0.84      0.88      0.86       992\n",
      "HS_Individual       0.64      0.70      0.67       732\n",
      "     HS_Group       0.58      0.68      0.63       402\n",
      "  HS_Religion       0.70      0.45      0.55       157\n",
      "      HS_Race       0.74      0.66      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.79      0.72       762\n",
      "      HS_Weak       0.62      0.67      0.65       689\n",
      "  HS_Moderate       0.47      0.58      0.52       331\n",
      "    HS_Strong       0.90      0.16      0.27       114\n",
      "\n",
      "    micro avg       0.69      0.74      0.71      5556\n",
      "    macro avg       0.58      0.54      0.53      5556\n",
      " weighted avg       0.68      0.74      0.70      5556\n",
      "  samples avg       0.41      0.41      0.39      5556\n",
      "\n",
      "Training completed in 84.97384738922119 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4837, Accuracy: 0.8356, F1 Micro: 0.1116, F1 Macro: 0.0399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3803, Accuracy: 0.853, F1 Micro: 0.3107, F1 Macro: 0.1048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3275, Accuracy: 0.8819, F1 Micro: 0.5914, F1 Macro: 0.2949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2862, Accuracy: 0.8866, F1 Micro: 0.5986, F1 Macro: 0.3286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.239, Accuracy: 0.8939, F1 Micro: 0.6674, F1 Macro: 0.413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2121, Accuracy: 0.8941, F1 Micro: 0.6787, F1 Macro: 0.423\n",
      "Epoch 7/10, Train Loss: 0.1925, Accuracy: 0.8961, F1 Micro: 0.6408, F1 Macro: 0.423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1669, Accuracy: 0.8987, F1 Micro: 0.6819, F1 Macro: 0.473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1504, Accuracy: 0.8996, F1 Micro: 0.7125, F1 Macro: 0.5089\n",
      "Epoch 10/10, Train Loss: 0.1269, Accuracy: 0.9027, F1 Micro: 0.7086, F1 Macro: 0.5208\n",
      "Model 3 - Iteration 2535: Accuracy: 0.8996, F1 Micro: 0.7125, F1 Macro: 0.5089\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.87      0.82      1134\n",
      "      Abusive       0.83      0.90      0.86       992\n",
      "HS_Individual       0.65      0.70      0.68       732\n",
      "     HS_Group       0.64      0.64      0.64       402\n",
      "  HS_Religion       0.67      0.39      0.50       157\n",
      "      HS_Race       0.79      0.45      0.57       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.78      0.72       762\n",
      "      HS_Weak       0.62      0.69      0.65       689\n",
      "  HS_Moderate       0.50      0.52      0.51       331\n",
      "    HS_Strong       0.91      0.09      0.16       114\n",
      "\n",
      "    micro avg       0.70      0.72      0.71      5556\n",
      "    macro avg       0.59      0.50      0.51      5556\n",
      " weighted avg       0.69      0.72      0.70      5556\n",
      "  samples avg       0.42      0.41      0.39      5556\n",
      "\n",
      "Training completed in 84.99183940887451 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9, F1 Micro: 0.7102, F1 Macro: 0.5262\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 758.5561158982824\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 498.4967861175537 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4616, Accuracy: 0.8344, F1 Micro: 0.0878, F1 Macro: 0.0355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3589, Accuracy: 0.8755, F1 Micro: 0.5262, F1 Macro: 0.235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2997, Accuracy: 0.8889, F1 Micro: 0.625, F1 Macro: 0.316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2541, Accuracy: 0.8959, F1 Micro: 0.6723, F1 Macro: 0.4001\n",
      "Epoch 5/10, Train Loss: 0.2245, Accuracy: 0.8987, F1 Micro: 0.6665, F1 Macro: 0.4205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1879, Accuracy: 0.9039, F1 Micro: 0.7063, F1 Macro: 0.5166\n",
      "Epoch 7/10, Train Loss: 0.164, Accuracy: 0.9031, F1 Micro: 0.6836, F1 Macro: 0.4909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1455, Accuracy: 0.9079, F1 Micro: 0.7197, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.117, Accuracy: 0.9043, F1 Micro: 0.7235, F1 Macro: 0.5528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1072, Accuracy: 0.909, F1 Micro: 0.7256, F1 Macro: 0.5708\n",
      "Model 1 - Iteration 3335: Accuracy: 0.909, F1 Micro: 0.7256, F1 Macro: 0.5708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.86      0.86      0.86       992\n",
      "HS_Individual       0.68      0.69      0.68       732\n",
      "     HS_Group       0.69      0.54      0.60       402\n",
      "  HS_Religion       0.70      0.46      0.56       157\n",
      "      HS_Race       0.79      0.60      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.75      0.74      0.75       762\n",
      "      HS_Weak       0.64      0.66      0.65       689\n",
      "  HS_Moderate       0.59      0.45      0.51       331\n",
      "    HS_Strong       0.93      0.46      0.61       114\n",
      "\n",
      "    micro avg       0.75      0.70      0.73      5556\n",
      "    macro avg       0.71      0.53      0.57      5556\n",
      " weighted avg       0.75      0.70      0.71      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 99.59758806228638 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4629, Accuracy: 0.8322, F1 Micro: 0.0688, F1 Macro: 0.0271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3632, Accuracy: 0.874, F1 Micro: 0.5218, F1 Macro: 0.2371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.305, Accuracy: 0.8863, F1 Micro: 0.5902, F1 Macro: 0.2836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2623, Accuracy: 0.8931, F1 Micro: 0.6765, F1 Macro: 0.4224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2296, Accuracy: 0.9005, F1 Micro: 0.684, F1 Macro: 0.4428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1937, Accuracy: 0.9029, F1 Micro: 0.6956, F1 Macro: 0.4765\n",
      "Epoch 7/10, Train Loss: 0.1665, Accuracy: 0.9052, F1 Micro: 0.6939, F1 Macro: 0.493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1485, Accuracy: 0.9054, F1 Micro: 0.7199, F1 Macro: 0.5308\n",
      "Epoch 9/10, Train Loss: 0.1249, Accuracy: 0.9078, F1 Micro: 0.7182, F1 Macro: 0.534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1085, Accuracy: 0.9076, F1 Micro: 0.723, F1 Macro: 0.5534\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9076, F1 Micro: 0.723, F1 Macro: 0.5534\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.85      0.88      0.86       992\n",
      "HS_Individual       0.71      0.66      0.68       732\n",
      "     HS_Group       0.64      0.60      0.62       402\n",
      "  HS_Religion       0.72      0.52      0.61       157\n",
      "      HS_Race       0.80      0.60      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.75      0.74       762\n",
      "      HS_Weak       0.66      0.63      0.64       689\n",
      "  HS_Moderate       0.51      0.50      0.51       331\n",
      "    HS_Strong       0.92      0.31      0.46       114\n",
      "\n",
      "    micro avg       0.75      0.70      0.72      5556\n",
      "    macro avg       0.62      0.52      0.55      5556\n",
      " weighted avg       0.73      0.70      0.71      5556\n",
      "  samples avg       0.41      0.39      0.39      5556\n",
      "\n",
      "Training completed in 99.2702968120575 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4559, Accuracy: 0.8388, F1 Micro: 0.1453, F1 Macro: 0.0553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3569, Accuracy: 0.8744, F1 Micro: 0.4965, F1 Macro: 0.2197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2974, Accuracy: 0.889, F1 Micro: 0.6168, F1 Macro: 0.3307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2536, Accuracy: 0.8946, F1 Micro: 0.6828, F1 Macro: 0.4164\n",
      "Epoch 5/10, Train Loss: 0.2223, Accuracy: 0.8993, F1 Micro: 0.674, F1 Macro: 0.4317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.187, Accuracy: 0.904, F1 Micro: 0.7097, F1 Macro: 0.5054\n",
      "Epoch 7/10, Train Loss: 0.1632, Accuracy: 0.8989, F1 Micro: 0.6731, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1516, Accuracy: 0.9064, F1 Micro: 0.7219, F1 Macro: 0.5317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1205, Accuracy: 0.9084, F1 Micro: 0.7276, F1 Macro: 0.5315\n",
      "Epoch 10/10, Train Loss: 0.1076, Accuracy: 0.9099, F1 Micro: 0.7241, F1 Macro: 0.5449\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9084, F1 Micro: 0.7276, F1 Macro: 0.5315\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.85      0.89      0.87       992\n",
      "HS_Individual       0.69      0.71      0.70       732\n",
      "     HS_Group       0.69      0.58      0.63       402\n",
      "  HS_Religion       0.72      0.46      0.56       157\n",
      "      HS_Race       0.81      0.47      0.59       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.75      0.73       762\n",
      "      HS_Weak       0.64      0.68      0.66       689\n",
      "  HS_Moderate       0.57      0.48      0.52       331\n",
      "    HS_Strong       0.95      0.16      0.27       114\n",
      "\n",
      "    micro avg       0.75      0.71      0.73      5556\n",
      "    macro avg       0.62      0.50      0.53      5556\n",
      " weighted avg       0.73      0.71      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 97.5204906463623 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9083, F1 Micro: 0.7254, F1 Macro: 0.5519\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 808.4107796084145\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 447.2175211906433 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4541, Accuracy: 0.8362, F1 Micro: 0.1176, F1 Macro: 0.0434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3478, Accuracy: 0.8774, F1 Micro: 0.6213, F1 Macro: 0.3046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2866, Accuracy: 0.8942, F1 Micro: 0.6548, F1 Macro: 0.3493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.242, Accuracy: 0.8978, F1 Micro: 0.705, F1 Macro: 0.4812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2104, Accuracy: 0.9054, F1 Micro: 0.7119, F1 Macro: 0.5198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1824, Accuracy: 0.9095, F1 Micro: 0.7215, F1 Macro: 0.5392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1554, Accuracy: 0.9068, F1 Micro: 0.7337, F1 Macro: 0.5634\n",
      "Epoch 8/10, Train Loss: 0.1389, Accuracy: 0.9102, F1 Micro: 0.7333, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1157, Accuracy: 0.9134, F1 Micro: 0.7386, F1 Macro: 0.5816\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9097, F1 Micro: 0.7351, F1 Macro: 0.5912\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9134, F1 Micro: 0.7386, F1 Macro: 0.5816\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.70      0.70      0.70       732\n",
      "     HS_Group       0.71      0.57      0.63       402\n",
      "  HS_Religion       0.75      0.44      0.55       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.72      0.77      0.75       762\n",
      "      HS_Weak       0.68      0.67      0.67       689\n",
      "  HS_Moderate       0.61      0.44      0.51       331\n",
      "    HS_Strong       0.94      0.55      0.70       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.72      0.54      0.58      5556\n",
      " weighted avg       0.76      0.71      0.73      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 110.79405641555786 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4527, Accuracy: 0.8379, F1 Micro: 0.169, F1 Macro: 0.0534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3483, Accuracy: 0.8757, F1 Micro: 0.6257, F1 Macro: 0.2988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.29, Accuracy: 0.8933, F1 Micro: 0.6622, F1 Macro: 0.3719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2447, Accuracy: 0.8955, F1 Micro: 0.7008, F1 Macro: 0.4685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2126, Accuracy: 0.9048, F1 Micro: 0.7026, F1 Macro: 0.4982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1838, Accuracy: 0.9075, F1 Micro: 0.7209, F1 Macro: 0.5267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1567, Accuracy: 0.9062, F1 Micro: 0.7298, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1413, Accuracy: 0.9112, F1 Micro: 0.7313, F1 Macro: 0.5675\n",
      "Epoch 9/10, Train Loss: 0.1171, Accuracy: 0.913, F1 Micro: 0.7312, F1 Macro: 0.5692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1008, Accuracy: 0.9133, F1 Micro: 0.7375, F1 Macro: 0.5844\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9133, F1 Micro: 0.7375, F1 Macro: 0.5844\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.71      0.68      0.69       732\n",
      "     HS_Group       0.66      0.60      0.63       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.77      0.72      0.75       762\n",
      "      HS_Weak       0.69      0.64      0.67       689\n",
      "  HS_Moderate       0.55      0.52      0.53       331\n",
      "    HS_Strong       0.94      0.45      0.61       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.71      0.55      0.58      5556\n",
      " weighted avg       0.76      0.71      0.73      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 111.82375049591064 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4494, Accuracy: 0.8439, F1 Micro: 0.2173, F1 Macro: 0.0757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3464, Accuracy: 0.8804, F1 Micro: 0.6169, F1 Macro: 0.2991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2862, Accuracy: 0.8939, F1 Micro: 0.6655, F1 Macro: 0.3942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2419, Accuracy: 0.8989, F1 Micro: 0.6978, F1 Macro: 0.4542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2114, Accuracy: 0.9041, F1 Micro: 0.7075, F1 Macro: 0.4949\n",
      "Epoch 6/10, Train Loss: 0.1836, Accuracy: 0.906, F1 Micro: 0.7073, F1 Macro: 0.496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1592, Accuracy: 0.9065, F1 Micro: 0.7267, F1 Macro: 0.5412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1407, Accuracy: 0.9101, F1 Micro: 0.7314, F1 Macro: 0.5535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.116, Accuracy: 0.9108, F1 Micro: 0.7367, F1 Macro: 0.5744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1023, Accuracy: 0.9089, F1 Micro: 0.741, F1 Macro: 0.5826\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9089, F1 Micro: 0.741, F1 Macro: 0.5826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.68      0.71      0.69       732\n",
      "     HS_Group       0.61      0.66      0.64       402\n",
      "  HS_Religion       0.62      0.61      0.61       157\n",
      "      HS_Race       0.67      0.68      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.65      0.69      0.67       689\n",
      "  HS_Moderate       0.50      0.58      0.54       331\n",
      "    HS_Strong       0.92      0.49      0.64       114\n",
      "\n",
      "    micro avg       0.73      0.76      0.74      5556\n",
      "    macro avg       0.67      0.59      0.58      5556\n",
      " weighted avg       0.72      0.76      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 111.23493313789368 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9119, F1 Micro: 0.739, F1 Macro: 0.5829\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 538.22798620802\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 401.6336758136749 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4417, Accuracy: 0.8553, F1 Micro: 0.3898, F1 Macro: 0.1178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3247, Accuracy: 0.8849, F1 Micro: 0.6063, F1 Macro: 0.3132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2719, Accuracy: 0.8972, F1 Micro: 0.6706, F1 Macro: 0.4165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2297, Accuracy: 0.9021, F1 Micro: 0.6779, F1 Macro: 0.4627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.9073, F1 Micro: 0.7169, F1 Macro: 0.5488\n",
      "Epoch 6/10, Train Loss: 0.1739, Accuracy: 0.911, F1 Micro: 0.7156, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1428, Accuracy: 0.9105, F1 Micro: 0.7351, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1196, Accuracy: 0.91, F1 Micro: 0.738, F1 Macro: 0.5859\n",
      "Epoch 9/10, Train Loss: 0.1057, Accuracy: 0.912, F1 Micro: 0.7375, F1 Macro: 0.5952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.092, Accuracy: 0.9131, F1 Micro: 0.7409, F1 Macro: 0.5977\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9131, F1 Micro: 0.7409, F1 Macro: 0.5977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.90      0.85      0.88       992\n",
      "HS_Individual       0.66      0.76      0.71       732\n",
      "     HS_Group       0.79      0.48      0.60       402\n",
      "  HS_Religion       0.74      0.47      0.58       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.75      0.77      0.76       762\n",
      "      HS_Weak       0.62      0.74      0.68       689\n",
      "  HS_Moderate       0.69      0.39      0.50       331\n",
      "    HS_Strong       0.87      0.65      0.74       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.77      0.56      0.60      5556\n",
      " weighted avg       0.77      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 121.13119721412659 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4421, Accuracy: 0.8552, F1 Micro: 0.3702, F1 Macro: 0.1193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3294, Accuracy: 0.8831, F1 Micro: 0.5773, F1 Macro: 0.2758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2746, Accuracy: 0.898, F1 Micro: 0.6727, F1 Macro: 0.4092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2324, Accuracy: 0.9036, F1 Micro: 0.6869, F1 Macro: 0.4585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2035, Accuracy: 0.9059, F1 Micro: 0.7043, F1 Macro: 0.543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1746, Accuracy: 0.9099, F1 Micro: 0.7101, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1455, Accuracy: 0.9103, F1 Micro: 0.7356, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1214, Accuracy: 0.9114, F1 Micro: 0.7367, F1 Macro: 0.5752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1063, Accuracy: 0.9109, F1 Micro: 0.7399, F1 Macro: 0.584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0943, Accuracy: 0.9123, F1 Micro: 0.7413, F1 Macro: 0.5853\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9123, F1 Micro: 0.7413, F1 Macro: 0.5853\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.67      0.75      0.71       732\n",
      "     HS_Group       0.70      0.54      0.61       402\n",
      "  HS_Religion       0.72      0.50      0.59       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.73      0.79      0.76       762\n",
      "      HS_Weak       0.64      0.72      0.68       689\n",
      "  HS_Moderate       0.62      0.45      0.52       331\n",
      "    HS_Strong       0.89      0.57      0.70       114\n",
      "\n",
      "    micro avg       0.75      0.73      0.74      5556\n",
      "    macro avg       0.67      0.56      0.59      5556\n",
      " weighted avg       0.74      0.73      0.73      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 125.09146451950073 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4368, Accuracy: 0.8524, F1 Micro: 0.3238, F1 Macro: 0.1058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.325, Accuracy: 0.8859, F1 Micro: 0.6074, F1 Macro: 0.315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2705, Accuracy: 0.8965, F1 Micro: 0.6806, F1 Macro: 0.4159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2308, Accuracy: 0.9029, F1 Micro: 0.6968, F1 Macro: 0.4768\n",
      "Epoch 5/10, Train Loss: 0.2019, Accuracy: 0.904, F1 Micro: 0.6814, F1 Macro: 0.4999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1779, Accuracy: 0.9078, F1 Micro: 0.7103, F1 Macro: 0.5414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1477, Accuracy: 0.9115, F1 Micro: 0.7257, F1 Macro: 0.5569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1219, Accuracy: 0.9103, F1 Micro: 0.7355, F1 Macro: 0.5718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1087, Accuracy: 0.9112, F1 Micro: 0.736, F1 Macro: 0.5763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.095, Accuracy: 0.9139, F1 Micro: 0.7418, F1 Macro: 0.5968\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9139, F1 Micro: 0.7418, F1 Macro: 0.5968\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.70      0.70      0.70       732\n",
      "     HS_Group       0.70      0.57      0.63       402\n",
      "  HS_Religion       0.70      0.47      0.56       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.57      0.08      0.14        51\n",
      "     HS_Other       0.77      0.76      0.76       762\n",
      "      HS_Weak       0.66      0.69      0.68       689\n",
      "  HS_Moderate       0.60      0.49      0.54       331\n",
      "    HS_Strong       0.91      0.53      0.67       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5556\n",
      "    macro avg       0.76      0.55      0.60      5556\n",
      " weighted avg       0.77      0.72      0.73      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 122.4762315750122 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9131, F1 Micro: 0.7413, F1 Macro: 0.5933\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 717.9119534558365\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 362.0445730686188 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.8599, F1 Micro: 0.419, F1 Macro: 0.1408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3124, Accuracy: 0.8912, F1 Micro: 0.6267, F1 Macro: 0.3278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2523, Accuracy: 0.9022, F1 Micro: 0.694, F1 Macro: 0.5072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2204, Accuracy: 0.9082, F1 Micro: 0.7221, F1 Macro: 0.5261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1809, Accuracy: 0.9103, F1 Micro: 0.7309, F1 Macro: 0.5676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1576, Accuracy: 0.9134, F1 Micro: 0.7374, F1 Macro: 0.5631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1337, Accuracy: 0.9142, F1 Micro: 0.7387, F1 Macro: 0.5902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1155, Accuracy: 0.9119, F1 Micro: 0.743, F1 Macro: 0.5934\n",
      "Epoch 9/10, Train Loss: 0.1053, Accuracy: 0.9143, F1 Micro: 0.7403, F1 Macro: 0.5938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0914, Accuracy: 0.915, F1 Micro: 0.7443, F1 Macro: 0.6191\n",
      "Model 1 - Iteration 5287: Accuracy: 0.915, F1 Micro: 0.7443, F1 Macro: 0.6191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.83      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.70      0.70      0.70       732\n",
      "     HS_Group       0.72      0.56      0.63       402\n",
      "  HS_Religion       0.76      0.51      0.61       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.56      0.20      0.29        51\n",
      "     HS_Other       0.76      0.75      0.76       762\n",
      "      HS_Weak       0.67      0.67      0.67       689\n",
      "  HS_Moderate       0.65      0.46      0.54       331\n",
      "    HS_Strong       0.88      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5556\n",
      "    macro avg       0.76      0.58      0.62      5556\n",
      " weighted avg       0.77      0.72      0.74      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 134.6338231563568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4367, Accuracy: 0.862, F1 Micro: 0.4438, F1 Macro: 0.1749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3199, Accuracy: 0.8904, F1 Micro: 0.638, F1 Macro: 0.3354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2555, Accuracy: 0.8997, F1 Micro: 0.6813, F1 Macro: 0.4889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2222, Accuracy: 0.9061, F1 Micro: 0.7196, F1 Macro: 0.5135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1841, Accuracy: 0.9099, F1 Micro: 0.7323, F1 Macro: 0.5629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1614, Accuracy: 0.9123, F1 Micro: 0.7401, F1 Macro: 0.5624\n",
      "Epoch 7/10, Train Loss: 0.1344, Accuracy: 0.9128, F1 Micro: 0.7142, F1 Macro: 0.5656\n",
      "Epoch 8/10, Train Loss: 0.1208, Accuracy: 0.9142, F1 Micro: 0.7341, F1 Macro: 0.5852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1035, Accuracy: 0.915, F1 Micro: 0.7473, F1 Macro: 0.6078\n",
      "Epoch 10/10, Train Loss: 0.0876, Accuracy: 0.9156, F1 Micro: 0.7433, F1 Macro: 0.6213\n",
      "Model 2 - Iteration 5287: Accuracy: 0.915, F1 Micro: 0.7473, F1 Macro: 0.6078\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.67      0.74      0.70       732\n",
      "     HS_Group       0.76      0.55      0.64       402\n",
      "  HS_Religion       0.76      0.51      0.61       157\n",
      "      HS_Race       0.80      0.70      0.75       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.08      0.15        51\n",
      "     HS_Other       0.76      0.75      0.76       762\n",
      "      HS_Weak       0.65      0.71      0.68       689\n",
      "  HS_Moderate       0.66      0.44      0.53       331\n",
      "    HS_Strong       0.89      0.64      0.74       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.80      0.57      0.61      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 130.36027717590332 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4315, Accuracy: 0.8607, F1 Micro: 0.4148, F1 Macro: 0.1521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3132, Accuracy: 0.8906, F1 Micro: 0.6297, F1 Macro: 0.3444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2541, Accuracy: 0.8995, F1 Micro: 0.6825, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.22, Accuracy: 0.9048, F1 Micro: 0.7182, F1 Macro: 0.5071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1839, Accuracy: 0.9072, F1 Micro: 0.7359, F1 Macro: 0.5606\n",
      "Epoch 6/10, Train Loss: 0.1615, Accuracy: 0.9124, F1 Micro: 0.7292, F1 Macro: 0.5501\n",
      "Epoch 7/10, Train Loss: 0.1364, Accuracy: 0.9111, F1 Micro: 0.7097, F1 Macro: 0.5567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.118, Accuracy: 0.912, F1 Micro: 0.7431, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1046, Accuracy: 0.9177, F1 Micro: 0.7478, F1 Macro: 0.6158\n",
      "Epoch 10/10, Train Loss: 0.088, Accuracy: 0.9156, F1 Micro: 0.7448, F1 Macro: 0.6034\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9177, F1 Micro: 0.7478, F1 Macro: 0.6158\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.81      0.83      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.72      0.68      0.70       732\n",
      "     HS_Group       0.74      0.57      0.65       402\n",
      "  HS_Religion       0.84      0.45      0.58       157\n",
      "      HS_Race       0.82      0.61      0.70       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.57      0.16      0.25        51\n",
      "     HS_Other       0.78      0.75      0.77       762\n",
      "      HS_Weak       0.69      0.67      0.68       689\n",
      "  HS_Moderate       0.65      0.48      0.55       331\n",
      "    HS_Strong       0.91      0.56      0.70       114\n",
      "\n",
      "    micro avg       0.79      0.71      0.75      5556\n",
      "    macro avg       0.77      0.56      0.62      5556\n",
      " weighted avg       0.79      0.71      0.74      5556\n",
      "  samples avg       0.43      0.40      0.39      5556\n",
      "\n",
      "Training completed in 130.3137435913086 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9159, F1 Micro: 0.7465, F1 Macro: 0.6142\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 763.2054338802789\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 325.59982085227966 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4284, Accuracy: 0.8628, F1 Micro: 0.4034, F1 Macro: 0.1458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.8895, F1 Micro: 0.6111, F1 Macro: 0.3295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2532, Accuracy: 0.9002, F1 Micro: 0.7126, F1 Macro: 0.5206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2107, Accuracy: 0.9095, F1 Micro: 0.7259, F1 Macro: 0.5552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.912, F1 Micro: 0.7379, F1 Macro: 0.5507\n",
      "Epoch 6/10, Train Loss: 0.153, Accuracy: 0.9157, F1 Micro: 0.7309, F1 Macro: 0.5676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1286, Accuracy: 0.9163, F1 Micro: 0.7485, F1 Macro: 0.5988\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.9148, F1 Micro: 0.7445, F1 Macro: 0.6083\n",
      "Epoch 9/10, Train Loss: 0.0959, Accuracy: 0.9143, F1 Micro: 0.7453, F1 Macro: 0.6323\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.9166, F1 Micro: 0.748, F1 Macro: 0.6225\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9163, F1 Micro: 0.7485, F1 Macro: 0.5988\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.68      0.72      0.70       732\n",
      "     HS_Group       0.77      0.55      0.64       402\n",
      "  HS_Religion       0.87      0.43      0.57       157\n",
      "      HS_Race       0.84      0.57      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.75      0.06      0.11        51\n",
      "     HS_Other       0.74      0.77      0.76       762\n",
      "      HS_Weak       0.66      0.70      0.68       689\n",
      "  HS_Moderate       0.69      0.45      0.54       331\n",
      "    HS_Strong       0.91      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.78      0.72      0.75      5556\n",
      "    macro avg       0.72      0.55      0.60      5556\n",
      " weighted avg       0.77      0.72      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 138.1163113117218 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4295, Accuracy: 0.8563, F1 Micro: 0.3785, F1 Macro: 0.1309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3101, Accuracy: 0.8898, F1 Micro: 0.6137, F1 Macro: 0.3384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9037, F1 Micro: 0.7047, F1 Macro: 0.4795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2148, Accuracy: 0.909, F1 Micro: 0.7232, F1 Macro: 0.5461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1815, Accuracy: 0.9122, F1 Micro: 0.743, F1 Macro: 0.5549\n",
      "Epoch 6/10, Train Loss: 0.1572, Accuracy: 0.9169, F1 Micro: 0.7386, F1 Macro: 0.5769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1315, Accuracy: 0.9153, F1 Micro: 0.7439, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.9168, F1 Micro: 0.7498, F1 Macro: 0.6094\n",
      "Epoch 9/10, Train Loss: 0.0958, Accuracy: 0.9092, F1 Micro: 0.7442, F1 Macro: 0.6091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9189, F1 Micro: 0.7561, F1 Macro: 0.6303\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9189, F1 Micro: 0.7561, F1 Macro: 0.6303\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.75      0.66      0.70       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.76      0.59      0.66       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.42      0.10      0.16        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.73      0.63      0.67       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.89      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.76      0.60      0.63      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 140.53534269332886 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4249, Accuracy: 0.8652, F1 Micro: 0.4317, F1 Macro: 0.162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3061, Accuracy: 0.8889, F1 Micro: 0.6102, F1 Macro: 0.3526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2527, Accuracy: 0.9031, F1 Micro: 0.7074, F1 Macro: 0.4869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2132, Accuracy: 0.9085, F1 Micro: 0.7208, F1 Macro: 0.5321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.9125, F1 Micro: 0.7287, F1 Macro: 0.5188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1543, Accuracy: 0.9146, F1 Micro: 0.7297, F1 Macro: 0.5667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1318, Accuracy: 0.916, F1 Micro: 0.7564, F1 Macro: 0.6008\n",
      "Epoch 8/10, Train Loss: 0.1072, Accuracy: 0.9161, F1 Micro: 0.7547, F1 Macro: 0.6145\n",
      "Epoch 9/10, Train Loss: 0.0935, Accuracy: 0.9109, F1 Micro: 0.7495, F1 Macro: 0.6271\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9185, F1 Micro: 0.7558, F1 Macro: 0.6265\n",
      "Model 3 - Iteration 5812: Accuracy: 0.916, F1 Micro: 0.7564, F1 Macro: 0.6008\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.77      0.47      0.58       157\n",
      "      HS_Race       0.83      0.57      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.72      0.82      0.77       762\n",
      "      HS_Weak       0.66      0.73      0.69       689\n",
      "  HS_Moderate       0.64      0.52      0.57       331\n",
      "    HS_Strong       0.91      0.55      0.69       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.72      0.57      0.60      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 139.21271872520447 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9171, F1 Micro: 0.7537, F1 Macro: 0.61\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 816.8888242997115\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 293.24291014671326 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4237, Accuracy: 0.8733, F1 Micro: 0.5478, F1 Macro: 0.2387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2981, Accuracy: 0.8936, F1 Micro: 0.6847, F1 Macro: 0.4081\n",
      "Epoch 3/10, Train Loss: 0.253, Accuracy: 0.9034, F1 Micro: 0.6783, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2105, Accuracy: 0.9133, F1 Micro: 0.7336, F1 Macro: 0.5711\n",
      "Epoch 5/10, Train Loss: 0.1759, Accuracy: 0.914, F1 Micro: 0.7263, F1 Macro: 0.5638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9168, F1 Micro: 0.7503, F1 Macro: 0.5922\n",
      "Epoch 7/10, Train Loss: 0.1241, Accuracy: 0.9151, F1 Micro: 0.7457, F1 Macro: 0.6077\n",
      "Epoch 8/10, Train Loss: 0.1045, Accuracy: 0.9119, F1 Micro: 0.742, F1 Macro: 0.6079\n",
      "Epoch 9/10, Train Loss: 0.0959, Accuracy: 0.9173, F1 Micro: 0.7463, F1 Macro: 0.6169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9154, F1 Micro: 0.752, F1 Macro: 0.6308\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9154, F1 Micro: 0.752, F1 Macro: 0.6308\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.67      0.69       732\n",
      "     HS_Group       0.66      0.68      0.67       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.46      0.12      0.19        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.65      0.67       689\n",
      "  HS_Moderate       0.58      0.59      0.58       331\n",
      "    HS_Strong       0.89      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.75      0.60      0.63      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 144.7776017189026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.424, Accuracy: 0.8727, F1 Micro: 0.5224, F1 Macro: 0.2339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3025, Accuracy: 0.8938, F1 Micro: 0.6815, F1 Macro: 0.4197\n",
      "Epoch 3/10, Train Loss: 0.2549, Accuracy: 0.9017, F1 Micro: 0.6655, F1 Macro: 0.4174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2112, Accuracy: 0.9111, F1 Micro: 0.7223, F1 Macro: 0.5525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1745, Accuracy: 0.9139, F1 Micro: 0.7235, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1506, Accuracy: 0.9156, F1 Micro: 0.7409, F1 Macro: 0.5631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1247, Accuracy: 0.9195, F1 Micro: 0.752, F1 Macro: 0.6013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.916, F1 Micro: 0.7537, F1 Macro: 0.6137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0947, Accuracy: 0.9175, F1 Micro: 0.7539, F1 Macro: 0.6232\n",
      "Epoch 10/10, Train Loss: 0.0797, Accuracy: 0.915, F1 Micro: 0.7539, F1 Macro: 0.632\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9175, F1 Micro: 0.7539, F1 Macro: 0.6232\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.89      0.86      0.88       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.77      0.56      0.65       402\n",
      "  HS_Religion       0.84      0.48      0.61       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.76      0.76      0.76       762\n",
      "      HS_Weak       0.65      0.74      0.69       689\n",
      "  HS_Moderate       0.69      0.47      0.56       331\n",
      "    HS_Strong       0.91      0.66      0.77       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.79      0.58      0.62      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 149.99879956245422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4193, Accuracy: 0.8743, F1 Micro: 0.5116, F1 Macro: 0.2295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2999, Accuracy: 0.8948, F1 Micro: 0.6788, F1 Macro: 0.3925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2526, Accuracy: 0.9041, F1 Micro: 0.6849, F1 Macro: 0.461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2101, Accuracy: 0.911, F1 Micro: 0.7288, F1 Macro: 0.5714\n",
      "Epoch 5/10, Train Loss: 0.1761, Accuracy: 0.9142, F1 Micro: 0.7282, F1 Macro: 0.5408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1513, Accuracy: 0.9127, F1 Micro: 0.7521, F1 Macro: 0.5832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1259, Accuracy: 0.9171, F1 Micro: 0.7545, F1 Macro: 0.6159\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9125, F1 Micro: 0.7466, F1 Macro: 0.6095\n",
      "Epoch 9/10, Train Loss: 0.0933, Accuracy: 0.9161, F1 Micro: 0.7487, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0816, Accuracy: 0.9186, F1 Micro: 0.7587, F1 Macro: 0.6389\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9186, F1 Micro: 0.7587, F1 Macro: 0.6389\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.70      0.71       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.76      0.62      0.68       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.58      0.14      0.22        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.68      0.69       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.75      0.60      0.64      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 147.30928683280945 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9172, F1 Micro: 0.7549, F1 Macro: 0.631\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 533.0563229269826\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 264.77896094322205 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4177, Accuracy: 0.8734, F1 Micro: 0.5001, F1 Macro: 0.2191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.295, Accuracy: 0.8946, F1 Micro: 0.6367, F1 Macro: 0.4173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9056, F1 Micro: 0.7187, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2056, Accuracy: 0.9122, F1 Micro: 0.7393, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1771, Accuracy: 0.9156, F1 Micro: 0.7508, F1 Macro: 0.5888\n",
      "Epoch 6/10, Train Loss: 0.1443, Accuracy: 0.9133, F1 Micro: 0.748, F1 Macro: 0.5919\n",
      "Epoch 7/10, Train Loss: 0.1287, Accuracy: 0.9112, F1 Micro: 0.7451, F1 Macro: 0.6008\n",
      "Epoch 8/10, Train Loss: 0.1061, Accuracy: 0.9169, F1 Micro: 0.7485, F1 Macro: 0.6057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9189, F1 Micro: 0.7569, F1 Macro: 0.6563\n",
      "Epoch 10/10, Train Loss: 0.0843, Accuracy: 0.9195, F1 Micro: 0.7509, F1 Macro: 0.6505\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9189, F1 Micro: 0.7569, F1 Macro: 0.6563\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.84      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.74      0.58      0.65       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.77      0.65      0.71       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.78      0.75      0.77       762\n",
      "      HS_Weak       0.68      0.69      0.69       689\n",
      "  HS_Moderate       0.62      0.50      0.56       331\n",
      "    HS_Strong       0.90      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.76      5556\n",
      "    macro avg       0.76      0.61      0.66      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 150.81835889816284 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4192, Accuracy: 0.8737, F1 Micro: 0.5238, F1 Macro: 0.23\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2987, Accuracy: 0.8982, F1 Micro: 0.6715, F1 Macro: 0.4383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2497, Accuracy: 0.9053, F1 Micro: 0.7137, F1 Macro: 0.5152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2083, Accuracy: 0.9096, F1 Micro: 0.7358, F1 Macro: 0.5692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1818, Accuracy: 0.9153, F1 Micro: 0.7404, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1469, Accuracy: 0.9159, F1 Micro: 0.7545, F1 Macro: 0.592\n",
      "Epoch 7/10, Train Loss: 0.1304, Accuracy: 0.917, F1 Micro: 0.7507, F1 Macro: 0.5996\n",
      "Epoch 8/10, Train Loss: 0.1103, Accuracy: 0.9153, F1 Micro: 0.7531, F1 Macro: 0.6188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9167, F1 Micro: 0.7557, F1 Macro: 0.6372\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9187, F1 Micro: 0.7521, F1 Macro: 0.6349\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9167, F1 Micro: 0.7557, F1 Macro: 0.6372\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.70      0.71       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.72      0.63      0.67       157\n",
      "      HS_Race       0.73      0.71      0.72       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.50      0.14      0.22        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.68      0.68      0.68       689\n",
      "  HS_Moderate       0.58      0.57      0.57       331\n",
      "    HS_Strong       0.90      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.75      0.61      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 152.41562366485596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4148, Accuracy: 0.8747, F1 Micro: 0.5099, F1 Macro: 0.2221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2961, Accuracy: 0.8936, F1 Micro: 0.6424, F1 Macro: 0.402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2468, Accuracy: 0.9051, F1 Micro: 0.7143, F1 Macro: 0.4989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.9118, F1 Micro: 0.7416, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.9153, F1 Micro: 0.7457, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1487, Accuracy: 0.9111, F1 Micro: 0.7523, F1 Macro: 0.593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1292, Accuracy: 0.9165, F1 Micro: 0.7562, F1 Macro: 0.6082\n",
      "Epoch 8/10, Train Loss: 0.1068, Accuracy: 0.9182, F1 Micro: 0.7537, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9178, F1 Micro: 0.762, F1 Macro: 0.6558\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9193, F1 Micro: 0.7611, F1 Macro: 0.6684\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9178, F1 Micro: 0.762, F1 Macro: 0.6558\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.66      0.65      0.66       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.71      0.69       689\n",
      "  HS_Moderate       0.58      0.56      0.57       331\n",
      "    HS_Strong       0.87      0.69      0.77       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 153.9224820137024 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9178, F1 Micro: 0.7582, F1 Macro: 0.6498\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1110.2907155260677\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 247.60040402412415 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.41, Accuracy: 0.877, F1 Micro: 0.5851, F1 Macro: 0.2812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.294, Accuracy: 0.8978, F1 Micro: 0.6872, F1 Macro: 0.4706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2409, Accuracy: 0.9045, F1 Micro: 0.6956, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2033, Accuracy: 0.9126, F1 Micro: 0.7177, F1 Macro: 0.5537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1755, Accuracy: 0.915, F1 Micro: 0.7483, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1474, Accuracy: 0.9171, F1 Micro: 0.7563, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1242, Accuracy: 0.9178, F1 Micro: 0.7573, F1 Macro: 0.6201\n",
      "Epoch 8/10, Train Loss: 0.1044, Accuracy: 0.9195, F1 Micro: 0.7515, F1 Macro: 0.6221\n",
      "Epoch 9/10, Train Loss: 0.09, Accuracy: 0.9152, F1 Micro: 0.7472, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0796, Accuracy: 0.9182, F1 Micro: 0.7621, F1 Macro: 0.6572\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9182, F1 Micro: 0.7621, F1 Macro: 0.6572\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.73      0.67      0.70       732\n",
      "     HS_Group       0.64      0.71      0.67       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.59      0.20      0.29        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.65      0.68       689\n",
      "  HS_Moderate       0.56      0.65      0.60       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.64      0.66      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 160.14121317863464 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4114, Accuracy: 0.878, F1 Micro: 0.571, F1 Macro: 0.2633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2964, Accuracy: 0.8969, F1 Micro: 0.6907, F1 Macro: 0.4671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2431, Accuracy: 0.9058, F1 Micro: 0.6946, F1 Macro: 0.5283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2055, Accuracy: 0.9127, F1 Micro: 0.7276, F1 Macro: 0.5488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1727, Accuracy: 0.9156, F1 Micro: 0.7465, F1 Macro: 0.5846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1452, Accuracy: 0.9176, F1 Micro: 0.7608, F1 Macro: 0.6023\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9185, F1 Micro: 0.753, F1 Macro: 0.6053\n",
      "Epoch 8/10, Train Loss: 0.1033, Accuracy: 0.9179, F1 Micro: 0.7599, F1 Macro: 0.6335\n",
      "Epoch 9/10, Train Loss: 0.0918, Accuracy: 0.9171, F1 Micro: 0.7523, F1 Macro: 0.6333\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.9204, F1 Micro: 0.7605, F1 Macro: 0.6609\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9176, F1 Micro: 0.7608, F1 Macro: 0.6023\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.72      0.60      0.66       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.73      0.70      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.92      0.59      0.72       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.63      0.59      0.60      5556\n",
      " weighted avg       0.74      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.41      5556\n",
      "\n",
      "Training completed in 158.03890585899353 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.407, Accuracy: 0.8798, F1 Micro: 0.5844, F1 Macro: 0.2689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.897, F1 Micro: 0.6887, F1 Macro: 0.4376\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9046, F1 Micro: 0.6808, F1 Macro: 0.5157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2065, Accuracy: 0.9144, F1 Micro: 0.7283, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1728, Accuracy: 0.9156, F1 Micro: 0.7513, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1482, Accuracy: 0.9167, F1 Micro: 0.7549, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1226, Accuracy: 0.9203, F1 Micro: 0.7581, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1051, Accuracy: 0.919, F1 Micro: 0.7597, F1 Macro: 0.6395\n",
      "Epoch 9/10, Train Loss: 0.0893, Accuracy: 0.9137, F1 Micro: 0.7397, F1 Macro: 0.6288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9213, F1 Micro: 0.7672, F1 Macro: 0.6661\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9213, F1 Micro: 0.7672, F1 Macro: 0.6661\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.74      0.68      0.71       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.69      0.66      0.67       157\n",
      "      HS_Race       0.80      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.66      0.68       689\n",
      "  HS_Moderate       0.59      0.60      0.59       331\n",
      "    HS_Strong       0.87      0.73      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.77      0.63      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 160.3333501815796 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.919, F1 Micro: 0.7634, F1 Macro: 0.6419\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 813.6022843809436\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 221.77393579483032 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4134, Accuracy: 0.8775, F1 Micro: 0.5983, F1 Macro: 0.282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.8984, F1 Micro: 0.6912, F1 Macro: 0.464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2361, Accuracy: 0.9084, F1 Micro: 0.7065, F1 Macro: 0.4854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.202, Accuracy: 0.9148, F1 Micro: 0.7337, F1 Macro: 0.5663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9168, F1 Micro: 0.7508, F1 Macro: 0.5981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.141, Accuracy: 0.9155, F1 Micro: 0.7562, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9199, F1 Micro: 0.7588, F1 Macro: 0.6161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9181, F1 Micro: 0.7598, F1 Macro: 0.6365\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9166, F1 Micro: 0.7567, F1 Macro: 0.6504\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.9213, F1 Micro: 0.758, F1 Macro: 0.6484\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9181, F1 Micro: 0.7598, F1 Macro: 0.6365\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.90      0.86      0.88       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.80      0.67      0.73       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.77      0.61      0.64      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 167.2324984073639 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4146, Accuracy: 0.8788, F1 Micro: 0.593, F1 Macro: 0.2759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2902, Accuracy: 0.8983, F1 Micro: 0.6924, F1 Macro: 0.4568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2372, Accuracy: 0.9066, F1 Micro: 0.7032, F1 Macro: 0.4658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2008, Accuracy: 0.9137, F1 Micro: 0.7214, F1 Macro: 0.5417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1658, Accuracy: 0.9151, F1 Micro: 0.7507, F1 Macro: 0.5937\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9156, F1 Micro: 0.7499, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1186, Accuracy: 0.9192, F1 Micro: 0.7591, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1038, Accuracy: 0.9202, F1 Micro: 0.7641, F1 Macro: 0.6411\n",
      "Epoch 9/10, Train Loss: 0.0866, Accuracy: 0.9202, F1 Micro: 0.762, F1 Macro: 0.6452\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.9193, F1 Micro: 0.763, F1 Macro: 0.6507\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9202, F1 Micro: 0.7641, F1 Macro: 0.6411\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.90      0.87      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.74      0.59      0.65       402\n",
      "  HS_Religion       0.77      0.59      0.66       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.55      0.12      0.19        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.66      0.50      0.56       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.75      0.61      0.64      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 164.5831446647644 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4101, Accuracy: 0.8801, F1 Micro: 0.5912, F1 Macro: 0.2717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.8981, F1 Micro: 0.6781, F1 Macro: 0.4205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.9067, F1 Micro: 0.7038, F1 Macro: 0.4701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9145, F1 Micro: 0.7264, F1 Macro: 0.5432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9159, F1 Micro: 0.7424, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1422, Accuracy: 0.9142, F1 Micro: 0.7505, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.9202, F1 Micro: 0.7601, F1 Macro: 0.6061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.919, F1 Micro: 0.7608, F1 Macro: 0.6498\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9189, F1 Micro: 0.7607, F1 Macro: 0.6633\n",
      "Epoch 10/10, Train Loss: 0.074, Accuracy: 0.919, F1 Micro: 0.759, F1 Macro: 0.6587\n",
      "Model 3 - Iteration 7336: Accuracy: 0.919, F1 Micro: 0.7608, F1 Macro: 0.6498\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.66      0.65      0.66       402\n",
      "  HS_Religion       0.72      0.56      0.63       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.58      0.57      0.57       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.75      0.62      0.65      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 167.35977745056152 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9191, F1 Micro: 0.7616, F1 Macro: 0.6425\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 898.3232512555634\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 201.4047396183014 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4067, Accuracy: 0.8797, F1 Micro: 0.5887, F1 Macro: 0.2837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.282, Accuracy: 0.8989, F1 Micro: 0.6967, F1 Macro: 0.4833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2319, Accuracy: 0.909, F1 Micro: 0.7194, F1 Macro: 0.5238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9129, F1 Micro: 0.7463, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1678, Accuracy: 0.9167, F1 Micro: 0.7514, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9193, F1 Micro: 0.7549, F1 Macro: 0.6199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9156, F1 Micro: 0.7552, F1 Macro: 0.624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1008, Accuracy: 0.92, F1 Micro: 0.7646, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.92, F1 Micro: 0.7655, F1 Macro: 0.673\n",
      "Epoch 10/10, Train Loss: 0.0714, Accuracy: 0.919, F1 Micro: 0.76, F1 Macro: 0.668\n",
      "Model 1 - Iteration 7656: Accuracy: 0.92, F1 Micro: 0.7655, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.70      0.71      0.71       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.80      0.57      0.67       157\n",
      "      HS_Race       0.81      0.64      0.72       120\n",
      "  HS_Physical       0.59      0.14      0.22        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.68      0.69      0.69       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.11671662330627 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4092, Accuracy: 0.8762, F1 Micro: 0.518, F1 Macro: 0.239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2865, Accuracy: 0.8986, F1 Micro: 0.6972, F1 Macro: 0.4772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2355, Accuracy: 0.9077, F1 Micro: 0.7075, F1 Macro: 0.4962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1935, Accuracy: 0.9125, F1 Micro: 0.7441, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1717, Accuracy: 0.9168, F1 Micro: 0.7521, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1438, Accuracy: 0.9165, F1 Micro: 0.757, F1 Macro: 0.6112\n",
      "Epoch 7/10, Train Loss: 0.1174, Accuracy: 0.916, F1 Micro: 0.7557, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1007, Accuracy: 0.9195, F1 Micro: 0.7614, F1 Macro: 0.6419\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9157, F1 Micro: 0.7603, F1 Macro: 0.6509\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9194, F1 Micro: 0.7576, F1 Macro: 0.6535\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9195, F1 Micro: 0.7614, F1 Macro: 0.6419\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.68      0.69       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.86      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.78      0.61      0.64      5556\n",
      " weighted avg       0.78      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 169.6455488204956 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4042, Accuracy: 0.8788, F1 Micro: 0.5468, F1 Macro: 0.2514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2847, Accuracy: 0.8987, F1 Micro: 0.6958, F1 Macro: 0.4866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9071, F1 Micro: 0.7003, F1 Macro: 0.489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1917, Accuracy: 0.9133, F1 Micro: 0.7441, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1666, Accuracy: 0.9154, F1 Micro: 0.7518, F1 Macro: 0.5981\n",
      "Epoch 6/10, Train Loss: 0.14, Accuracy: 0.9162, F1 Micro: 0.7502, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1158, Accuracy: 0.9174, F1 Micro: 0.7579, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.099, Accuracy: 0.92, F1 Micro: 0.7646, F1 Macro: 0.6454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0831, Accuracy: 0.9182, F1 Micro: 0.7651, F1 Macro: 0.6639\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.9222, F1 Micro: 0.7643, F1 Macro: 0.6726\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9182, F1 Micro: 0.7651, F1 Macro: 0.6639\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.65      0.68      0.67       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.73      0.64      0.68       120\n",
      "  HS_Physical       0.63      0.17      0.26        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.56      0.60      0.58       331\n",
      "    HS_Strong       0.90      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.72      0.64      0.66      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.6482925415039 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9192, F1 Micro: 0.764, F1 Macro: 0.6596\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 346.97818276320356\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 180.85392928123474 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4048, Accuracy: 0.8807, F1 Micro: 0.5986, F1 Macro: 0.2944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9004, F1 Micro: 0.6787, F1 Macro: 0.4133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2329, Accuracy: 0.9107, F1 Micro: 0.728, F1 Macro: 0.5462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1966, Accuracy: 0.9153, F1 Micro: 0.7495, F1 Macro: 0.586\n",
      "Epoch 5/10, Train Loss: 0.164, Accuracy: 0.9184, F1 Micro: 0.7427, F1 Macro: 0.5845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1427, Accuracy: 0.9176, F1 Micro: 0.7577, F1 Macro: 0.6053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9203, F1 Micro: 0.7635, F1 Macro: 0.6289\n",
      "Epoch 8/10, Train Loss: 0.0991, Accuracy: 0.9185, F1 Micro: 0.7597, F1 Macro: 0.6421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9191, F1 Micro: 0.7662, F1 Macro: 0.6699\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9178, F1 Micro: 0.7627, F1 Macro: 0.6697\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9191, F1 Micro: 0.7662, F1 Macro: 0.6699\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.58      0.29      0.39        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.73      0.65      0.67      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 174.2592418193817 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4042, Accuracy: 0.8812, F1 Micro: 0.5986, F1 Macro: 0.2831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2819, Accuracy: 0.9013, F1 Micro: 0.6816, F1 Macro: 0.4358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2357, Accuracy: 0.9105, F1 Micro: 0.7211, F1 Macro: 0.5257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9146, F1 Micro: 0.7477, F1 Macro: 0.5778\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.917, F1 Micro: 0.7459, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1443, Accuracy: 0.9213, F1 Micro: 0.7624, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.9226, F1 Micro: 0.766, F1 Macro: 0.6391\n",
      "Epoch 8/10, Train Loss: 0.1001, Accuracy: 0.92, F1 Micro: 0.7547, F1 Macro: 0.6335\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9158, F1 Micro: 0.7627, F1 Macro: 0.6655\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9203, F1 Micro: 0.7651, F1 Macro: 0.6687\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9226, F1 Micro: 0.766, F1 Macro: 0.6391\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.78      0.57      0.66       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.86      0.63      0.73       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       1.00      0.10      0.18        51\n",
      "     HS_Other       0.78      0.76      0.77       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.73      0.48      0.58       331\n",
      "    HS_Strong       0.88      0.72      0.79       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.83      0.59      0.64      5556\n",
      " weighted avg       0.80      0.74      0.76      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 172.01993465423584 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4023, Accuracy: 0.8819, F1 Micro: 0.5793, F1 Macro: 0.279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.9014, F1 Micro: 0.679, F1 Macro: 0.437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2328, Accuracy: 0.9088, F1 Micro: 0.7166, F1 Macro: 0.527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9148, F1 Micro: 0.7431, F1 Macro: 0.5606\n",
      "Epoch 5/10, Train Loss: 0.1619, Accuracy: 0.9164, F1 Micro: 0.7372, F1 Macro: 0.5751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9188, F1 Micro: 0.7469, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9218, F1 Micro: 0.7681, F1 Macro: 0.634\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.9198, F1 Micro: 0.7617, F1 Macro: 0.6484\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9152, F1 Micro: 0.761, F1 Macro: 0.6665\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9193, F1 Micro: 0.7648, F1 Macro: 0.6752\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9218, F1 Micro: 0.7681, F1 Macro: 0.634\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.78      0.59      0.67       402\n",
      "  HS_Religion       0.74      0.54      0.62       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.71      0.48      0.57       331\n",
      "    HS_Strong       0.87      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.79      0.60      0.63      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 172.18221473693848 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9212, F1 Micro: 0.7668, F1 Macro: 0.6477\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 793.3603570542532\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 165.84304404258728 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4025, Accuracy: 0.8813, F1 Micro: 0.6061, F1 Macro: 0.3081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9023, F1 Micro: 0.698, F1 Macro: 0.498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2281, Accuracy: 0.9113, F1 Micro: 0.7329, F1 Macro: 0.5486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9159, F1 Micro: 0.7408, F1 Macro: 0.5788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.92, F1 Micro: 0.7561, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9191, F1 Micro: 0.7656, F1 Macro: 0.6209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1104, Accuracy: 0.9208, F1 Micro: 0.7672, F1 Macro: 0.6508\n",
      "Epoch 8/10, Train Loss: 0.0975, Accuracy: 0.9219, F1 Micro: 0.7663, F1 Macro: 0.6538\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.9215, F1 Micro: 0.7663, F1 Macro: 0.661\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9209, F1 Micro: 0.7642, F1 Macro: 0.6749\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9208, F1 Micro: 0.7672, F1 Macro: 0.6508\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.75      0.52      0.62       157\n",
      "      HS_Race       0.86      0.56      0.68       120\n",
      "  HS_Physical       0.62      0.11      0.19        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.71      0.50      0.58       331\n",
      "    HS_Strong       0.89      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.61      0.65      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 179.0637731552124 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.403, Accuracy: 0.8811, F1 Micro: 0.607, F1 Macro: 0.2936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2774, Accuracy: 0.9028, F1 Micro: 0.6923, F1 Macro: 0.479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2282, Accuracy: 0.9107, F1 Micro: 0.7224, F1 Macro: 0.5208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9152, F1 Micro: 0.7499, F1 Macro: 0.5898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9206, F1 Micro: 0.7574, F1 Macro: 0.6019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.92, F1 Micro: 0.7651, F1 Macro: 0.6086\n",
      "Epoch 7/10, Train Loss: 0.1139, Accuracy: 0.921, F1 Micro: 0.7591, F1 Macro: 0.639\n",
      "Epoch 8/10, Train Loss: 0.0966, Accuracy: 0.9167, F1 Micro: 0.7582, F1 Macro: 0.6443\n",
      "Epoch 9/10, Train Loss: 0.0854, Accuracy: 0.9196, F1 Micro: 0.7629, F1 Macro: 0.6495\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.9195, F1 Micro: 0.7543, F1 Macro: 0.6599\n",
      "Model 2 - Iteration 8165: Accuracy: 0.92, F1 Micro: 0.7651, F1 Macro: 0.6086\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.79      0.57      0.66       402\n",
      "  HS_Religion       0.81      0.50      0.62       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.66      0.76      0.71       689\n",
      "  HS_Moderate       0.68      0.48      0.56       331\n",
      "    HS_Strong       0.90      0.69      0.78       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.65      0.58      0.61      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 176.83658695220947 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.8817, F1 Micro: 0.6216, F1 Macro: 0.3228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2764, Accuracy: 0.9023, F1 Micro: 0.6961, F1 Macro: 0.4921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2281, Accuracy: 0.9104, F1 Micro: 0.7326, F1 Macro: 0.5424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.9162, F1 Micro: 0.7497, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1613, Accuracy: 0.9206, F1 Micro: 0.7509, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1346, Accuracy: 0.9157, F1 Micro: 0.763, F1 Macro: 0.6217\n",
      "Epoch 7/10, Train Loss: 0.1105, Accuracy: 0.9206, F1 Micro: 0.7603, F1 Macro: 0.6332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9193, F1 Micro: 0.7632, F1 Macro: 0.66\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0821, Accuracy: 0.9191, F1 Micro: 0.7682, F1 Macro: 0.652\n",
      "Epoch 10/10, Train Loss: 0.0723, Accuracy: 0.9206, F1 Micro: 0.7622, F1 Macro: 0.6529\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9191, F1 Micro: 0.7682, F1 Macro: 0.652\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.71      0.60      0.65       157\n",
      "      HS_Race       0.71      0.68      0.69       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.57      0.58       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.64      0.65      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 179.99777698516846 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.92, F1 Micro: 0.7668, F1 Macro: 0.6371\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 1057.8482508386137\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 148.65271282196045 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4025, Accuracy: 0.8809, F1 Micro: 0.6002, F1 Macro: 0.3007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.896, F1 Micro: 0.6199, F1 Macro: 0.4088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2263, Accuracy: 0.9076, F1 Micro: 0.7375, F1 Macro: 0.5785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1886, Accuracy: 0.918, F1 Micro: 0.754, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9167, F1 Micro: 0.7624, F1 Macro: 0.6042\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9178, F1 Micro: 0.7608, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1116, Accuracy: 0.9219, F1 Micro: 0.7654, F1 Macro: 0.6592\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9199, F1 Micro: 0.7654, F1 Macro: 0.6601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.9197, F1 Micro: 0.7688, F1 Macro: 0.6723\n",
      "Epoch 10/10, Train Loss: 0.0699, Accuracy: 0.9202, F1 Micro: 0.7657, F1 Macro: 0.6728\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9197, F1 Micro: 0.7688, F1 Macro: 0.6723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.70      0.62      0.66       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.75      0.35      0.48        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.67      0.73      0.70       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.77      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.42      5556\n",
      "\n",
      "Training completed in 182.91939449310303 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4031, Accuracy: 0.8822, F1 Micro: 0.6115, F1 Macro: 0.2904\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8941, F1 Micro: 0.6084, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2303, Accuracy: 0.9116, F1 Micro: 0.7379, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9176, F1 Micro: 0.7548, F1 Macro: 0.5938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.9176, F1 Micro: 0.7568, F1 Macro: 0.597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9173, F1 Micro: 0.7592, F1 Macro: 0.6164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9182, F1 Micro: 0.7675, F1 Macro: 0.6628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.9206, F1 Micro: 0.7684, F1 Macro: 0.6558\n",
      "Epoch 9/10, Train Loss: 0.0804, Accuracy: 0.9197, F1 Micro: 0.7645, F1 Macro: 0.6679\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9179, F1 Micro: 0.7662, F1 Macro: 0.6639\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9206, F1 Micro: 0.7684, F1 Macro: 0.6558\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.58      0.59      0.59       331\n",
      "    HS_Strong       0.91      0.74      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.78      0.63      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 183.1900498867035 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4007, Accuracy: 0.8824, F1 Micro: 0.5973, F1 Macro: 0.2868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2793, Accuracy: 0.8963, F1 Micro: 0.621, F1 Macro: 0.4161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.9075, F1 Micro: 0.7378, F1 Macro: 0.5687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9155, F1 Micro: 0.7489, F1 Macro: 0.5839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.9162, F1 Micro: 0.7579, F1 Macro: 0.5927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9183, F1 Micro: 0.7635, F1 Macro: 0.6178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.923, F1 Micro: 0.7695, F1 Macro: 0.6683\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9214, F1 Micro: 0.7666, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9203, F1 Micro: 0.7702, F1 Macro: 0.6773\n",
      "Epoch 10/10, Train Loss: 0.0717, Accuracy: 0.9151, F1 Micro: 0.7607, F1 Macro: 0.6599\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9203, F1 Micro: 0.7702, F1 Macro: 0.6773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       1.00      0.17      0.29        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.86      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 184.14146900177002 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9202, F1 Micro: 0.7692, F1 Macro: 0.6685\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 802.9317372014983\n",
      "Nearest checkpoint: 9218\n",
      "214Acquired samples: \n",
      "Sampling duration: 133.87687635421753 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3954, Accuracy: 0.8826, F1 Micro: 0.5981, F1 Macro: 0.286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2734, Accuracy: 0.8985, F1 Micro: 0.6378, F1 Macro: 0.4443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2221, Accuracy: 0.9089, F1 Micro: 0.7402, F1 Macro: 0.5781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9163, F1 Micro: 0.7544, F1 Macro: 0.5921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.157, Accuracy: 0.9135, F1 Micro: 0.756, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1349, Accuracy: 0.9204, F1 Micro: 0.7595, F1 Macro: 0.6278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1117, Accuracy: 0.9229, F1 Micro: 0.7608, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9236, F1 Micro: 0.7678, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9209, F1 Micro: 0.7681, F1 Macro: 0.6775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9228, F1 Micro: 0.7743, F1 Macro: 0.687\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9228, F1 Micro: 0.7743, F1 Macro: 0.687\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.75      0.59      0.66       402\n",
      "  HS_Religion       0.75      0.63      0.69       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.91      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 192.41468167304993 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3968, Accuracy: 0.8808, F1 Micro: 0.5616, F1 Macro: 0.2634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2782, Accuracy: 0.8964, F1 Micro: 0.6289, F1 Macro: 0.3988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9112, F1 Micro: 0.7347, F1 Macro: 0.5488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9184, F1 Micro: 0.76, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.919, F1 Micro: 0.7645, F1 Macro: 0.6137\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9207, F1 Micro: 0.7519, F1 Macro: 0.5998\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9202, F1 Micro: 0.7642, F1 Macro: 0.6278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9227, F1 Micro: 0.7683, F1 Macro: 0.656\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9212, F1 Micro: 0.7674, F1 Macro: 0.6707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.922, F1 Micro: 0.774, F1 Macro: 0.6809\n",
      "Model 2 - Iteration 8616: Accuracy: 0.922, F1 Micro: 0.774, F1 Macro: 0.6809\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.54      0.29      0.38        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.92      0.81      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 187.3826642036438 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.393, Accuracy: 0.8831, F1 Micro: 0.5997, F1 Macro: 0.2841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.8995, F1 Micro: 0.6512, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.9106, F1 Micro: 0.7368, F1 Macro: 0.5638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9173, F1 Micro: 0.7571, F1 Macro: 0.5917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1578, Accuracy: 0.9146, F1 Micro: 0.7577, F1 Macro: 0.6006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.137, Accuracy: 0.9223, F1 Micro: 0.7641, F1 Macro: 0.6083\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9224, F1 Micro: 0.7583, F1 Macro: 0.6166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9213, F1 Micro: 0.7691, F1 Macro: 0.6617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0782, Accuracy: 0.9209, F1 Micro: 0.7692, F1 Macro: 0.6761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9227, F1 Micro: 0.7709, F1 Macro: 0.6793\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9227, F1 Micro: 0.7709, F1 Macro: 0.6793\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.73      0.61      0.67       402\n",
      "  HS_Religion       0.74      0.65      0.69       157\n",
      "      HS_Race       0.82      0.63      0.71       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.74      0.33      0.46        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.52      0.57       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 190.2457537651062 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9225, F1 Micro: 0.7731, F1 Macro: 0.6824\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 670.5077494810505\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 119.70758557319641 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3907, Accuracy: 0.8842, F1 Micro: 0.6178, F1 Macro: 0.306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2692, Accuracy: 0.9039, F1 Micro: 0.6878, F1 Macro: 0.467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.913, F1 Micro: 0.742, F1 Macro: 0.5592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9176, F1 Micro: 0.743, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1578, Accuracy: 0.9192, F1 Micro: 0.7635, F1 Macro: 0.6167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.92, F1 Micro: 0.7689, F1 Macro: 0.6414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9192, F1 Micro: 0.7695, F1 Macro: 0.6718\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9184, F1 Micro: 0.7653, F1 Macro: 0.6559\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9195, F1 Micro: 0.7662, F1 Macro: 0.6728\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9227, F1 Micro: 0.7581, F1 Macro: 0.6721\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9192, F1 Micro: 0.7695, F1 Macro: 0.6718\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.66      0.66      0.66       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.74      0.71      0.72       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.58      0.57      0.58       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 189.1229133605957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3936, Accuracy: 0.8821, F1 Micro: 0.6132, F1 Macro: 0.2891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9028, F1 Micro: 0.6852, F1 Macro: 0.4387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9115, F1 Micro: 0.7425, F1 Macro: 0.5644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9187, F1 Micro: 0.7478, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.922, F1 Micro: 0.7718, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.9218, F1 Micro: 0.7731, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9209, F1 Micro: 0.7693, F1 Macro: 0.6516\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9235, F1 Micro: 0.7713, F1 Macro: 0.6524\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9223, F1 Micro: 0.7705, F1 Macro: 0.6715\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.921, F1 Micro: 0.7725, F1 Macro: 0.6803\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9218, F1 Micro: 0.7731, F1 Macro: 0.6416\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.81      0.57      0.67       157\n",
      "      HS_Race       0.84      0.65      0.73       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.83      0.10      0.18        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.92      0.67      0.77       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.80      0.61      0.64      5556\n",
      " weighted avg       0.78      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 187.2305998802185 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.39, Accuracy: 0.8832, F1 Micro: 0.5996, F1 Macro: 0.2944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2706, Accuracy: 0.9038, F1 Micro: 0.6945, F1 Macro: 0.4661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2207, Accuracy: 0.914, F1 Micro: 0.7447, F1 Macro: 0.5585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.918, F1 Micro: 0.7485, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.922, F1 Micro: 0.7697, F1 Macro: 0.6177\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9156, F1 Micro: 0.7634, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9196, F1 Micro: 0.7704, F1 Macro: 0.6672\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9157, F1 Micro: 0.7624, F1 Macro: 0.6645\n",
      "Epoch 9/10, Train Loss: 0.0797, Accuracy: 0.922, F1 Micro: 0.7685, F1 Macro: 0.6662\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9233, F1 Micro: 0.762, F1 Macro: 0.6585\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9196, F1 Micro: 0.7704, F1 Macro: 0.6672\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.63      0.73      0.67       402\n",
      "  HS_Religion       0.70      0.65      0.68       157\n",
      "      HS_Race       0.72      0.66      0.69       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.58      0.29      0.39        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.53      0.66      0.59       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 186.93594694137573 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9202, F1 Micro: 0.771, F1 Macro: 0.6602\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 547.6457789665644\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 107.55425786972046 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3933, Accuracy: 0.8834, F1 Micro: 0.6299, F1 Macro: 0.3262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9049, F1 Micro: 0.6906, F1 Macro: 0.4698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9151, F1 Micro: 0.7366, F1 Macro: 0.5703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1877, Accuracy: 0.9158, F1 Micro: 0.7581, F1 Macro: 0.5909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1525, Accuracy: 0.9213, F1 Micro: 0.7733, F1 Macro: 0.6295\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6322\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9179, F1 Micro: 0.7672, F1 Macro: 0.6545\n",
      "Epoch 8/10, Train Loss: 0.0967, Accuracy: 0.9195, F1 Micro: 0.7693, F1 Macro: 0.6546\n",
      "Epoch 9/10, Train Loss: 0.0792, Accuracy: 0.9249, F1 Micro: 0.7719, F1 Macro: 0.67\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9219, F1 Micro: 0.769, F1 Macro: 0.6789\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9213, F1 Micro: 0.7733, F1 Macro: 0.6295\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.75      0.61      0.67       402\n",
      "  HS_Religion       0.80      0.53      0.64       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.08      0.15        51\n",
      "     HS_Other       0.73      0.85      0.78       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.90      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.73      0.61      0.63      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.30220794677734 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3953, Accuracy: 0.8828, F1 Micro: 0.6217, F1 Macro: 0.2983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2717, Accuracy: 0.904, F1 Micro: 0.7025, F1 Macro: 0.4725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2264, Accuracy: 0.9147, F1 Micro: 0.7372, F1 Macro: 0.5707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9145, F1 Micro: 0.7591, F1 Macro: 0.5927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1548, Accuracy: 0.921, F1 Micro: 0.7697, F1 Macro: 0.6165\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9214, F1 Micro: 0.7647, F1 Macro: 0.6146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9184, F1 Micro: 0.7698, F1 Macro: 0.6583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9194, F1 Micro: 0.772, F1 Macro: 0.6641\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9206, F1 Micro: 0.769, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9213, F1 Micro: 0.7661, F1 Macro: 0.6659\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9194, F1 Micro: 0.772, F1 Macro: 0.6641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.65      0.76      0.70       120\n",
      "  HS_Physical       1.00      0.12      0.22        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.59      0.61      0.60       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.74      0.66      0.66      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 193.01899790763855 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3915, Accuracy: 0.8839, F1 Micro: 0.6309, F1 Macro: 0.3298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.9028, F1 Micro: 0.6866, F1 Macro: 0.4599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2244, Accuracy: 0.913, F1 Micro: 0.7239, F1 Macro: 0.5474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9142, F1 Micro: 0.7557, F1 Macro: 0.5798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.6038\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9226, F1 Micro: 0.7614, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9211, F1 Micro: 0.764, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.9234, F1 Micro: 0.7704, F1 Macro: 0.6659\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9236, F1 Micro: 0.7683, F1 Macro: 0.6673\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9214, F1 Micro: 0.7684, F1 Macro: 0.6851\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9234, F1 Micro: 0.7704, F1 Macro: 0.6659\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.76      0.61      0.67       157\n",
      "      HS_Race       0.75      0.70      0.72       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.71      0.24      0.35        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.79      0.63      0.67      5556\n",
      " weighted avg       0.80      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 191.62510228157043 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9214, F1 Micro: 0.7719, F1 Macro: 0.6532\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 901.6363081503894\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 96.90174770355225 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.392, Accuracy: 0.8844, F1 Micro: 0.6123, F1 Macro: 0.3158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.9042, F1 Micro: 0.6818, F1 Macro: 0.4992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9149, F1 Micro: 0.7407, F1 Macro: 0.563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1856, Accuracy: 0.92, F1 Micro: 0.7553, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9209, F1 Micro: 0.7669, F1 Macro: 0.6353\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.9193, F1 Micro: 0.7653, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9236, F1 Micro: 0.7735, F1 Macro: 0.6576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9243, F1 Micro: 0.7749, F1 Macro: 0.6822\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9172, F1 Micro: 0.7683, F1 Macro: 0.6713\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9204, F1 Micro: 0.7665, F1 Macro: 0.6932\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9243, F1 Micro: 0.7749, F1 Macro: 0.6822\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.82      0.68      0.75       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.84849166870117 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3922, Accuracy: 0.8841, F1 Micro: 0.6058, F1 Macro: 0.2877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2737, Accuracy: 0.9021, F1 Micro: 0.6637, F1 Macro: 0.477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2265, Accuracy: 0.9137, F1 Micro: 0.7454, F1 Macro: 0.5607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9177, F1 Micro: 0.7504, F1 Macro: 0.596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9214, F1 Micro: 0.7691, F1 Macro: 0.6175\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.9157, F1 Micro: 0.7605, F1 Macro: 0.6321\n",
      "Epoch 7/10, Train Loss: 0.1117, Accuracy: 0.924, F1 Micro: 0.7668, F1 Macro: 0.651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9231, F1 Micro: 0.7767, F1 Macro: 0.6705\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9237, F1 Micro: 0.7752, F1 Macro: 0.6577\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9221, F1 Micro: 0.7686, F1 Macro: 0.6916\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9231, F1 Micro: 0.7767, F1 Macro: 0.6705\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       1.00      0.12      0.22        72\n",
      "    HS_Gender       0.57      0.24      0.33        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.53      0.58       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 192.88882398605347 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3904, Accuracy: 0.8857, F1 Micro: 0.6106, F1 Macro: 0.3052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.272, Accuracy: 0.9023, F1 Micro: 0.6698, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9137, F1 Micro: 0.7361, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9182, F1 Micro: 0.7512, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.9202, F1 Micro: 0.7668, F1 Macro: 0.6357\n",
      "Epoch 6/10, Train Loss: 0.1305, Accuracy: 0.9159, F1 Micro: 0.7585, F1 Macro: 0.6344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9237, F1 Micro: 0.7688, F1 Macro: 0.6567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9225, F1 Micro: 0.7706, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9218, F1 Micro: 0.7734, F1 Macro: 0.6776\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9203, F1 Micro: 0.7714, F1 Macro: 0.6896\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9218, F1 Micro: 0.7734, F1 Macro: 0.6776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       1.00      0.12      0.22        72\n",
      "    HS_Gender       0.74      0.27      0.40        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.7705638408661 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9231, F1 Micro: 0.775, F1 Macro: 0.6768\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 574.4747028641393\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 83.4475245475769 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3896, Accuracy: 0.8823, F1 Micro: 0.6089, F1 Macro: 0.3022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9036, F1 Micro: 0.7046, F1 Macro: 0.5153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2157, Accuracy: 0.9151, F1 Micro: 0.7386, F1 Macro: 0.5659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9191, F1 Micro: 0.758, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1563, Accuracy: 0.92, F1 Micro: 0.7616, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.922, F1 Micro: 0.7689, F1 Macro: 0.6548\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9233, F1 Micro: 0.7657, F1 Macro: 0.6576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.922, F1 Micro: 0.776, F1 Macro: 0.6804\n",
      "Epoch 9/10, Train Loss: 0.0811, Accuracy: 0.9187, F1 Micro: 0.769, F1 Macro: 0.6816\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.9238, F1 Micro: 0.7749, F1 Macro: 0.7023\n",
      "Model 1 - Iteration 9218: Accuracy: 0.922, F1 Micro: 0.776, F1 Macro: 0.6804\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.69      0.79      0.74       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.67      0.76      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.66433024406433 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3897, Accuracy: 0.8829, F1 Micro: 0.6178, F1 Macro: 0.3096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2693, Accuracy: 0.9027, F1 Micro: 0.7112, F1 Macro: 0.4957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.9147, F1 Micro: 0.7403, F1 Macro: 0.5711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9194, F1 Micro: 0.7637, F1 Macro: 0.6146\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.922, F1 Micro: 0.76, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.9216, F1 Micro: 0.7645, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.9232, F1 Micro: 0.7689, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9226, F1 Micro: 0.7752, F1 Macro: 0.6821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9211, F1 Micro: 0.7753, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9221, F1 Micro: 0.7761, F1 Macro: 0.6939\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9221, F1 Micro: 0.7761, F1 Macro: 0.6939\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.70      0.75      0.73       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       1.00      0.19      0.33        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.62      0.59      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 200.80626916885376 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3863, Accuracy: 0.8845, F1 Micro: 0.6196, F1 Macro: 0.3156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.9044, F1 Micro: 0.713, F1 Macro: 0.5121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2163, Accuracy: 0.9154, F1 Micro: 0.7405, F1 Macro: 0.5652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9189, F1 Micro: 0.7563, F1 Macro: 0.5991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.155, Accuracy: 0.9231, F1 Micro: 0.7694, F1 Macro: 0.6175\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9223, F1 Micro: 0.7674, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9224, F1 Micro: 0.7728, F1 Macro: 0.6577\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9224, F1 Micro: 0.7713, F1 Macro: 0.6819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9218, F1 Micro: 0.7742, F1 Macro: 0.6752\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.923, F1 Micro: 0.77, F1 Macro: 0.6865\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9218, F1 Micro: 0.7742, F1 Macro: 0.6752\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.71      0.72      0.72       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.71      0.29      0.42        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.61      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.61656713485718 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.922, F1 Micro: 0.7754, F1 Macro: 0.6832\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 602.860281013398\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 83.90968680381775 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.39, Accuracy: 0.8814, F1 Micro: 0.5494, F1 Macro: 0.2662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.9063, F1 Micro: 0.7113, F1 Macro: 0.5243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.9145, F1 Micro: 0.7435, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1832, Accuracy: 0.9183, F1 Micro: 0.7538, F1 Macro: 0.5996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9228, F1 Micro: 0.7726, F1 Macro: 0.6387\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9199, F1 Micro: 0.7659, F1 Macro: 0.6332\n",
      "Epoch 7/10, Train Loss: 0.108, Accuracy: 0.9198, F1 Micro: 0.7696, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.924, F1 Micro: 0.7742, F1 Macro: 0.6758\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9233, F1 Micro: 0.7688, F1 Macro: 0.6867\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9188, F1 Micro: 0.7706, F1 Macro: 0.6895\n",
      "Model 1 - Iteration 9418: Accuracy: 0.924, F1 Micro: 0.7742, F1 Macro: 0.6758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.84      0.63      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.63      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.95121264457703 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3909, Accuracy: 0.8757, F1 Micro: 0.4877, F1 Macro: 0.2237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2706, Accuracy: 0.9052, F1 Micro: 0.711, F1 Macro: 0.5239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.9112, F1 Micro: 0.7438, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9187, F1 Micro: 0.7578, F1 Macro: 0.6079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9214, F1 Micro: 0.7721, F1 Macro: 0.6267\n",
      "Epoch 6/10, Train Loss: 0.1279, Accuracy: 0.9223, F1 Micro: 0.7591, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.106, Accuracy: 0.9227, F1 Micro: 0.7729, F1 Macro: 0.6698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9242, F1 Micro: 0.7757, F1 Macro: 0.6669\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9242, F1 Micro: 0.7708, F1 Macro: 0.6885\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9175, F1 Micro: 0.7646, F1 Macro: 0.6681\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9242, F1 Micro: 0.7757, F1 Macro: 0.6669\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.82      0.59      0.68       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 200.0370831489563 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3875, Accuracy: 0.8791, F1 Micro: 0.5231, F1 Macro: 0.2472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.9064, F1 Micro: 0.7055, F1 Macro: 0.5217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2234, Accuracy: 0.9145, F1 Micro: 0.7459, F1 Macro: 0.5817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9185, F1 Micro: 0.7509, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1505, Accuracy: 0.9205, F1 Micro: 0.7676, F1 Macro: 0.6267\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9217, F1 Micro: 0.767, F1 Macro: 0.6322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9219, F1 Micro: 0.7716, F1 Macro: 0.6542\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9225, F1 Micro: 0.7617, F1 Macro: 0.6463\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.924, F1 Micro: 0.7691, F1 Macro: 0.6866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9203, F1 Micro: 0.7717, F1 Macro: 0.6917\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9203, F1 Micro: 0.7717, F1 Macro: 0.6917\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.64      0.71      0.67       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.54      0.41      0.47        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.55      0.65      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 199.82705974578857 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9228, F1 Micro: 0.7739, F1 Macro: 0.6781\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 714.8160522349349\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 71.05345010757446 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3868, Accuracy: 0.8869, F1 Micro: 0.6314, F1 Macro: 0.3351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.9068, F1 Micro: 0.7111, F1 Macro: 0.5262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9151, F1 Micro: 0.745, F1 Macro: 0.5716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.177, Accuracy: 0.917, F1 Micro: 0.7644, F1 Macro: 0.6123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.9209, F1 Micro: 0.7717, F1 Macro: 0.6422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1246, Accuracy: 0.9241, F1 Micro: 0.7737, F1 Macro: 0.6508\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9247, F1 Micro: 0.7671, F1 Macro: 0.6661\n",
      "Epoch 8/10, Train Loss: 0.0882, Accuracy: 0.922, F1 Micro: 0.7727, F1 Macro: 0.6831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9213, F1 Micro: 0.7751, F1 Macro: 0.6976\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9236, F1 Micro: 0.7724, F1 Macro: 0.7008\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9213, F1 Micro: 0.7751, F1 Macro: 0.6976\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.86      0.93      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.69      0.61      0.65       402\n",
      "  HS_Religion       0.72      0.63      0.67       157\n",
      "      HS_Race       0.80      0.73      0.77       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.61      0.45      0.52        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.89      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 204.61251521110535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3878, Accuracy: 0.8865, F1 Micro: 0.6148, F1 Macro: 0.308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.9076, F1 Micro: 0.7103, F1 Macro: 0.5173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2174, Accuracy: 0.9147, F1 Micro: 0.7451, F1 Macro: 0.5551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9177, F1 Micro: 0.766, F1 Macro: 0.6157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9217, F1 Micro: 0.7713, F1 Macro: 0.6344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9232, F1 Micro: 0.7722, F1 Macro: 0.6498\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9254, F1 Micro: 0.7698, F1 Macro: 0.6602\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9209, F1 Micro: 0.7675, F1 Macro: 0.6678\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9178, F1 Micro: 0.7682, F1 Macro: 0.6762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9203, F1 Micro: 0.7722, F1 Macro: 0.6931\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9203, F1 Micro: 0.7722, F1 Macro: 0.6931\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.66      0.67      0.67       402\n",
      "  HS_Religion       0.67      0.65      0.66       157\n",
      "      HS_Race       0.66      0.79      0.72       120\n",
      "  HS_Physical       1.00      0.21      0.34        72\n",
      "    HS_Gender       0.59      0.47      0.52        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.59      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.69      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 204.40751123428345 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8872, F1 Micro: 0.6259, F1 Macro: 0.3283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9081, F1 Micro: 0.708, F1 Macro: 0.5109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9139, F1 Micro: 0.7414, F1 Macro: 0.5469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9164, F1 Micro: 0.7608, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9204, F1 Micro: 0.7695, F1 Macro: 0.6301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.9249, F1 Micro: 0.7755, F1 Macro: 0.6629\n",
      "Epoch 7/10, Train Loss: 0.1019, Accuracy: 0.9213, F1 Micro: 0.7624, F1 Macro: 0.6601\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.917, F1 Micro: 0.7648, F1 Macro: 0.6801\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9226, F1 Micro: 0.7737, F1 Macro: 0.6948\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9218, F1 Micro: 0.7732, F1 Macro: 0.7062\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9249, F1 Micro: 0.7755, F1 Macro: 0.6629\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.77      0.61      0.68       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.68      0.50      0.57       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.80      0.62      0.66      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 202.31859850883484 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9222, F1 Micro: 0.7743, F1 Macro: 0.6845\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 396.2647762816917\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 59.254430532455444 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3832, Accuracy: 0.8865, F1 Micro: 0.6306, F1 Macro: 0.3285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.9047, F1 Micro: 0.7168, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9145, F1 Micro: 0.7317, F1 Macro: 0.5587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9197, F1 Micro: 0.7627, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.924, F1 Micro: 0.7769, F1 Macro: 0.6431\n",
      "Epoch 6/10, Train Loss: 0.1266, Accuracy: 0.9227, F1 Micro: 0.774, F1 Macro: 0.6592\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9236, F1 Micro: 0.7754, F1 Macro: 0.6733\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.923, F1 Micro: 0.776, F1 Macro: 0.6844\n",
      "Epoch 9/10, Train Loss: 0.0785, Accuracy: 0.9226, F1 Micro: 0.7745, F1 Macro: 0.6933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9254, F1 Micro: 0.7791, F1 Macro: 0.7015\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9254, F1 Micro: 0.7791, F1 Macro: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.76      0.70      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.74      0.77      0.75       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.71      0.47      0.56        51\n",
      "     HS_Other       0.79      0.80      0.80       762\n",
      "      HS_Weak       0.75      0.67      0.71       689\n",
      "  HS_Moderate       0.62      0.62      0.62       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.11145043373108 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3845, Accuracy: 0.8858, F1 Micro: 0.6389, F1 Macro: 0.3336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2655, Accuracy: 0.9064, F1 Micro: 0.7137, F1 Macro: 0.5033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.9143, F1 Micro: 0.7289, F1 Macro: 0.5367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9189, F1 Micro: 0.7623, F1 Macro: 0.5995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9243, F1 Micro: 0.7709, F1 Macro: 0.6234\n",
      "Epoch 6/10, Train Loss: 0.1245, Accuracy: 0.9244, F1 Micro: 0.7688, F1 Macro: 0.65\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9236, F1 Micro: 0.7674, F1 Macro: 0.658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9221, F1 Micro: 0.776, F1 Macro: 0.6766\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9227, F1 Micro: 0.7676, F1 Macro: 0.6795\n",
      "Epoch 10/10, Train Loss: 0.0692, Accuracy: 0.9238, F1 Micro: 0.7718, F1 Macro: 0.6945\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9221, F1 Micro: 0.776, F1 Macro: 0.6766\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.74      0.60      0.67       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.67      0.78      0.72       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 204.71077752113342 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.381, Accuracy: 0.8866, F1 Micro: 0.6427, F1 Macro: 0.3423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2634, Accuracy: 0.9069, F1 Micro: 0.7157, F1 Macro: 0.5147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9127, F1 Micro: 0.7158, F1 Macro: 0.5311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9179, F1 Micro: 0.7592, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9201, F1 Micro: 0.7668, F1 Macro: 0.6314\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9212, F1 Micro: 0.7628, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9198, F1 Micro: 0.772, F1 Macro: 0.6739\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9179, F1 Micro: 0.7704, F1 Macro: 0.6787\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.922, F1 Micro: 0.7636, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9252, F1 Micro: 0.7764, F1 Macro: 0.7042\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9252, F1 Micro: 0.7764, F1 Macro: 0.7042\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.77      0.74      0.76       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 206.3140046596527 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9242, F1 Micro: 0.7772, F1 Macro: 0.6941\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 471.3900205797974\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 46.168866872787476 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3819, Accuracy: 0.8856, F1 Micro: 0.5853, F1 Macro: 0.3145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2545, Accuracy: 0.907, F1 Micro: 0.703, F1 Macro: 0.5119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.9156, F1 Micro: 0.7393, F1 Macro: 0.5822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1753, Accuracy: 0.9196, F1 Micro: 0.7515, F1 Macro: 0.5984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.149, Accuracy: 0.9196, F1 Micro: 0.7723, F1 Macro: 0.6426\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9199, F1 Micro: 0.7712, F1 Macro: 0.6558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9223, F1 Micro: 0.7729, F1 Macro: 0.667\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9147, F1 Micro: 0.7639, F1 Macro: 0.6768\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9231, F1 Micro: 0.7729, F1 Macro: 0.6982\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9223, F1 Micro: 0.7724, F1 Macro: 0.6972\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9223, F1 Micro: 0.7729, F1 Macro: 0.667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.70      0.62      0.66       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.71      0.24      0.35        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 209.16259765625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3824, Accuracy: 0.8856, F1 Micro: 0.5863, F1 Macro: 0.3088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2567, Accuracy: 0.9066, F1 Micro: 0.7066, F1 Macro: 0.4981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.21, Accuracy: 0.9168, F1 Micro: 0.7534, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.9216, F1 Micro: 0.7576, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9225, F1 Micro: 0.7743, F1 Macro: 0.6425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1252, Accuracy: 0.9216, F1 Micro: 0.7772, F1 Macro: 0.6527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9243, F1 Micro: 0.7781, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9234, F1 Micro: 0.7786, F1 Macro: 0.6887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9227, F1 Micro: 0.7787, F1 Macro: 0.6924\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9237, F1 Micro: 0.7739, F1 Macro: 0.7018\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9227, F1 Micro: 0.7787, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.61      0.66       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.84      0.63      0.72       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.75      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 214.37927412986755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.8851, F1 Micro: 0.5847, F1 Macro: 0.3028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.9075, F1 Micro: 0.7044, F1 Macro: 0.5049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2109, Accuracy: 0.9169, F1 Micro: 0.7503, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9206, F1 Micro: 0.7509, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.9216, F1 Micro: 0.7702, F1 Macro: 0.6457\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9189, F1 Micro: 0.7696, F1 Macro: 0.67\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.9214, F1 Micro: 0.7738, F1 Macro: 0.6757\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9203, F1 Micro: 0.7676, F1 Macro: 0.6738\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9202, F1 Micro: 0.7673, F1 Macro: 0.6955\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9224, F1 Micro: 0.7696, F1 Macro: 0.6973\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9214, F1 Micro: 0.7738, F1 Macro: 0.6757\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.67      0.69      0.68       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.67      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 209.29213070869446 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9221, F1 Micro: 0.7751, F1 Macro: 0.6784\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 260.9642256176082\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 35.23283100128174 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.8877, F1 Micro: 0.6139, F1 Macro: 0.3303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2557, Accuracy: 0.9059, F1 Micro: 0.7154, F1 Macro: 0.5028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9121, F1 Micro: 0.7511, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.919, F1 Micro: 0.7588, F1 Macro: 0.6093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.9236, F1 Micro: 0.7732, F1 Macro: 0.6367\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9218, F1 Micro: 0.7732, F1 Macro: 0.6629\n",
      "Epoch 7/10, Train Loss: 0.1015, Accuracy: 0.92, F1 Micro: 0.7716, F1 Macro: 0.6854\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9221, F1 Micro: 0.7713, F1 Macro: 0.6922\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9224, F1 Micro: 0.7706, F1 Macro: 0.6879\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9219, F1 Micro: 0.7712, F1 Macro: 0.7003\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9236, F1 Micro: 0.7732, F1 Macro: 0.6367\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.74      0.57      0.65       157\n",
      "      HS_Race       0.82      0.67      0.73       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.90      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.75      0.61      0.64      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 209.09343719482422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3823, Accuracy: 0.8885, F1 Micro: 0.6406, F1 Macro: 0.3303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2595, Accuracy: 0.905, F1 Micro: 0.7177, F1 Macro: 0.5027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2097, Accuracy: 0.9134, F1 Micro: 0.7517, F1 Macro: 0.5869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.9211, F1 Micro: 0.7651, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9214, F1 Micro: 0.7711, F1 Macro: 0.6351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6666\n",
      "Epoch 7/10, Train Loss: 0.1003, Accuracy: 0.9228, F1 Micro: 0.7727, F1 Macro: 0.6838\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9229, F1 Micro: 0.771, F1 Macro: 0.6721\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9224, F1 Micro: 0.7733, F1 Macro: 0.6899\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9209, F1 Micro: 0.7681, F1 Macro: 0.6805\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.74      0.66      0.70       402\n",
      "  HS_Religion       0.78      0.51      0.62       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.66      0.57      0.62       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.63      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 211.03083634376526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3776, Accuracy: 0.8895, F1 Micro: 0.6299, F1 Macro: 0.3514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.9066, F1 Micro: 0.7195, F1 Macro: 0.5026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9152, F1 Micro: 0.7499, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9191, F1 Micro: 0.7588, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9209, F1 Micro: 0.7713, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9223, F1 Micro: 0.7725, F1 Macro: 0.6611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9227, F1 Micro: 0.773, F1 Macro: 0.6872\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9186, F1 Micro: 0.7708, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6959\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9215, F1 Micro: 0.7658, F1 Macro: 0.692\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6959\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.72      0.62      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.56      0.43      0.49        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 214.09077191352844 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.923, F1 Micro: 0.7741, F1 Macro: 0.6664\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -28.990798737777354\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.57442617416382 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3716, Accuracy: 0.8883, F1 Micro: 0.617, F1 Macro: 0.3397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.9069, F1 Micro: 0.7087, F1 Macro: 0.4904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9152, F1 Micro: 0.7492, F1 Macro: 0.5679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9179, F1 Micro: 0.767, F1 Macro: 0.6179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.9206, F1 Micro: 0.7697, F1 Macro: 0.6397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.921, F1 Micro: 0.7755, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.9267, F1 Micro: 0.7818, F1 Macro: 0.6989\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9221, F1 Micro: 0.7741, F1 Macro: 0.6826\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.918, F1 Micro: 0.77, F1 Macro: 0.6901\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9269, F1 Micro: 0.7783, F1 Macro: 0.7082\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9267, F1 Micro: 0.7818, F1 Macro: 0.6989\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.76      0.72      0.74       732\n",
      "     HS_Group       0.72      0.69      0.70       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.83      0.70      0.76       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.64      0.62      0.63       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.43      0.42      0.42      5556\n",
      "\n",
      "Training completed in 218.27890133857727 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3741, Accuracy: 0.8868, F1 Micro: 0.6151, F1 Macro: 0.3197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.908, F1 Micro: 0.7028, F1 Macro: 0.4994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9157, F1 Micro: 0.7527, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9176, F1 Micro: 0.7666, F1 Macro: 0.6134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9209, F1 Micro: 0.7704, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.12, Accuracy: 0.9246, F1 Micro: 0.7829, F1 Macro: 0.6721\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9222, F1 Micro: 0.7764, F1 Macro: 0.6795\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9235, F1 Micro: 0.7765, F1 Macro: 0.6867\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.9217, F1 Micro: 0.7754, F1 Macro: 0.6933\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.9168, F1 Micro: 0.7701, F1 Macro: 0.6916\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9246, F1 Micro: 0.7829, F1 Macro: 0.6721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.75      0.75      0.75       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.69      0.18      0.28        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.65      0.61      0.63       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.77      0.66      0.67      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 215.52513194084167 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.37, Accuracy: 0.8876, F1 Micro: 0.6276, F1 Macro: 0.357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2508, Accuracy: 0.9077, F1 Micro: 0.7163, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9154, F1 Micro: 0.7459, F1 Macro: 0.5661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1718, Accuracy: 0.9203, F1 Micro: 0.7629, F1 Macro: 0.5943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1423, Accuracy: 0.9204, F1 Micro: 0.7663, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1204, Accuracy: 0.9207, F1 Micro: 0.7676, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9238, F1 Micro: 0.7754, F1 Macro: 0.696\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9179, F1 Micro: 0.7662, F1 Macro: 0.6784\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9231, F1 Micro: 0.7721, F1 Macro: 0.6933\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9222, F1 Micro: 0.7732, F1 Macro: 0.7033\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9238, F1 Micro: 0.7754, F1 Macro: 0.696\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.77      0.67      0.72       732\n",
      "     HS_Group       0.65      0.74      0.69       402\n",
      "  HS_Religion       0.71      0.67      0.69       157\n",
      "      HS_Race       0.76      0.70      0.73       120\n",
      "  HS_Physical       0.64      0.22      0.33        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.75      0.66      0.71       689\n",
      "  HS_Moderate       0.56      0.66      0.61       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.74      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 218.42003059387207 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.925, F1 Micro: 0.7801, F1 Macro: 0.689\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold -18.14997587270133\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 6.797137498855591 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3715, Accuracy: 0.8879, F1 Micro: 0.6105, F1 Macro: 0.3099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2469, Accuracy: 0.9051, F1 Micro: 0.7239, F1 Macro: 0.5482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9185, F1 Micro: 0.752, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9209, F1 Micro: 0.7611, F1 Macro: 0.6185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9238, F1 Micro: 0.7679, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9246, F1 Micro: 0.7741, F1 Macro: 0.6551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9252, F1 Micro: 0.7765, F1 Macro: 0.6864\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9218, F1 Micro: 0.774, F1 Macro: 0.6932\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9243, F1 Micro: 0.7745, F1 Macro: 0.6898\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9205, F1 Micro: 0.7726, F1 Macro: 0.6923\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9252, F1 Micro: 0.7765, F1 Macro: 0.6864\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.75      0.72      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.82      0.69      0.75       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.80      0.80      0.80       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 219.14422750473022 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3741, Accuracy: 0.8891, F1 Micro: 0.6211, F1 Macro: 0.3083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2489, Accuracy: 0.9062, F1 Micro: 0.7216, F1 Macro: 0.5361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9167, F1 Micro: 0.7461, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9187, F1 Micro: 0.7593, F1 Macro: 0.6142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1423, Accuracy: 0.9237, F1 Micro: 0.769, F1 Macro: 0.6438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1193, Accuracy: 0.9232, F1 Micro: 0.7759, F1 Macro: 0.6596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9252, F1 Micro: 0.7785, F1 Macro: 0.6934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9253, F1 Micro: 0.7788, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.9252, F1 Micro: 0.7795, F1 Macro: 0.6991\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9223, F1 Micro: 0.775, F1 Macro: 0.7022\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9252, F1 Micro: 0.7795, F1 Macro: 0.6991\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.81      0.59      0.68       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 222.23438167572021 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3692, Accuracy: 0.8887, F1 Micro: 0.6134, F1 Macro: 0.3182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2469, Accuracy: 0.9074, F1 Micro: 0.7249, F1 Macro: 0.5395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.9164, F1 Micro: 0.7487, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1667, Accuracy: 0.9207, F1 Micro: 0.764, F1 Macro: 0.618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9233, F1 Micro: 0.7668, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9211, F1 Micro: 0.7747, F1 Macro: 0.6601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6791\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9237, F1 Micro: 0.7716, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9234, F1 Micro: 0.7741, F1 Macro: 0.6955\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9207, F1 Micro: 0.7717, F1 Macro: 0.7008\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9233, F1 Micro: 0.7757, F1 Macro: 0.6791\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.71      0.60      0.65       157\n",
      "      HS_Race       0.85      0.62      0.71       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.81      0.33      0.47        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 219.4649441242218 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9246, F1 Micro: 0.7772, F1 Macro: 0.6882\n",
      "Total sampling time: 5789.37 seconds\n",
      "Total runtime: 20056.94004178047 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7611336,
     "sourceId": 12090808,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20088.759461,
   "end_time": "2025-06-08T13:34:52.403459",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-08T08:00:03.643998",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d552caf881f4c32bf1e6438347b5cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f01c18716844e0fbba8e8be56ea72b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11ed9aac5e28488e86aa4a2db9606923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "124f853a9c2446df9680eeecaf066dfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "128fa68a7bd446baba4b6ce8099df658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ae8676140544b3d8752ccb3a7f52e8d",
       "placeholder": "​",
       "style": "IPY_MODEL_96249bfb870b4d849f315a4620d55f6a",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "129df9c340f5441bbedcc68afbe6fe26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ae8676140544b3d8752ccb3a7f52e8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1cc8da33648d4005ab054a27d6880b97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e37741ec77d847f7aed31305b314d376",
        "IPY_MODEL_f5d6c470d78c412082bd211b0053df57",
        "IPY_MODEL_d08d21c8bf4a408baa7c3205c9a59ab0"
       ],
       "layout": "IPY_MODEL_f9df87a52bb040c494c63a47386449eb",
       "tabbable": null,
       "tooltip": null
      }
     },
     "21c85a2f946d4d3da26bb0f44802c56d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "268f5ffef2654e749f1318e4222143f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30b24b3aeef84b18b418050f04311c67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "34e27660b2d24f5194e892243b23634b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_268f5ffef2654e749f1318e4222143f4",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_11ed9aac5e28488e86aa4a2db9606923",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "3541f84deb6a4891829ad18e3ab73777": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36325a7c734d47d4a52ce838a035c1e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_527467ee060c40c1bfb5c4f04a128108",
        "IPY_MODEL_48f2df6e0a31451da0d86ca02632bdab",
        "IPY_MODEL_7f37c08ba5684b57b2bdb0d41b6a1e8e"
       ],
       "layout": "IPY_MODEL_9db0b1589cfc49abafbb4560563b117c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "43d823191ae44bfcbeabfff7e2fc4fd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "48808dc65920491392a4b1bf32b3def1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48f2df6e0a31451da0d86ca02632bdab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_129df9c340f5441bbedcc68afbe6fe26",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5ae02993820a44dd8f53f071c07f3946",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "527467ee060c40c1bfb5c4f04a128108": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_da67d3713a5c47fea09f537a59770116",
       "placeholder": "​",
       "style": "IPY_MODEL_43d823191ae44bfcbeabfff7e2fc4fd3",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "52b1fa37deca435f9e72d5c01aa53ae1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "54270f69d2294b44b7e28ab54ca0ae0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8cf7471126674c07a5598020cba29115",
       "placeholder": "​",
       "style": "IPY_MODEL_30b24b3aeef84b18b418050f04311c67",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 12.9kB/s]"
      }
     },
     "5a78c35e1ef446e0ad2d58c07aa8016f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f01c18716844e0fbba8e8be56ea72b2",
       "placeholder": "​",
       "style": "IPY_MODEL_b3a1f7a483ce4468b6522a8aa5feabba",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "5ae02993820a44dd8f53f071c07f3946": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5aff3cb9294d424491912257864bb3b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6f69505c6372425383bd31dce886ab05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a6d3edd442364e2f8f0b22171c86ff77",
       "placeholder": "​",
       "style": "IPY_MODEL_124f853a9c2446df9680eeecaf066dfa",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/229k [00:00&lt;00:00, 5.68MB/s]"
      }
     },
     "7266a37d0faf40b8bf63ed08cd08c0ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f37c08ba5684b57b2bdb0d41b6a1e8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8fa2d3a8b2674800ae3d2b71d2f951b4",
       "placeholder": "​",
       "style": "IPY_MODEL_93722a8cd31140e9980d81433f2f040b",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 184B/s]"
      }
     },
     "7faecb6763be42fa91038f87ce5eda2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83b31a93e155425d8fbc2a8a05eae8b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8cdfcf27346348fbad31ab832c7e5024": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a78c35e1ef446e0ad2d58c07aa8016f",
        "IPY_MODEL_34e27660b2d24f5194e892243b23634b",
        "IPY_MODEL_54270f69d2294b44b7e28ab54ca0ae0f"
       ],
       "layout": "IPY_MODEL_b31798e17ed4441ba19c36a356c726f0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8cf7471126674c07a5598020cba29115": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8fa2d3a8b2674800ae3d2b71d2f951b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93722a8cd31140e9980d81433f2f040b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96249bfb870b4d849f315a4620d55f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9db0b1589cfc49abafbb4560563b117c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9de7645fa11d4c759935a95bc1cfc003": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a67d67b980ef416ba62385af6659e217": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d8649d25616848d68ae5f2ca4cee05ba",
       "placeholder": "​",
       "style": "IPY_MODEL_0d552caf881f4c32bf1e6438347b5cf2",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "a6d3edd442364e2f8f0b22171c86ff77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8ac16458df44dcd8ee587df0912e7bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a67d67b980ef416ba62385af6659e217",
        "IPY_MODEL_f25dac5f80234b87aa76302d189d1f2d",
        "IPY_MODEL_6f69505c6372425383bd31dce886ab05"
       ],
       "layout": "IPY_MODEL_83b31a93e155425d8fbc2a8a05eae8b7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b0f19c20a0e2412b9dd60ffb8f2d4a81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b29c9d953d2d473bbf400c402284d2af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b31798e17ed4441ba19c36a356c726f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3a1f7a483ce4468b6522a8aa5feabba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b62d76a407184f07a1b0110bd5406535": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9179a927da2438aa153526a92408d21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9daa7b2fae6491082569bd477f6b4dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_128fa68a7bd446baba4b6ce8099df658",
        "IPY_MODEL_fad322959df9480caa66b794060bf06e",
        "IPY_MODEL_e9829cbae8e64392953f21bdcd45a063"
       ],
       "layout": "IPY_MODEL_b29c9d953d2d473bbf400c402284d2af",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d08d21c8bf4a408baa7c3205c9a59ab0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b62d76a407184f07a1b0110bd5406535",
       "placeholder": "​",
       "style": "IPY_MODEL_f818d5a08b4e417487226f8113c2f88c",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/1.53k [00:00&lt;00:00, 173kB/s]"
      }
     },
     "d8649d25616848d68ae5f2ca4cee05ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da67d3713a5c47fea09f537a59770116": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e37741ec77d847f7aed31305b314d376": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3541f84deb6a4891829ad18e3ab73777",
       "placeholder": "​",
       "style": "IPY_MODEL_52b1fa37deca435f9e72d5c01aa53ae1",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "e9829cbae8e64392953f21bdcd45a063": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_48808dc65920491392a4b1bf32b3def1",
       "placeholder": "​",
       "style": "IPY_MODEL_b0f19c20a0e2412b9dd60ffb8f2d4a81",
       "tabbable": null,
       "tooltip": null,
       "value": " 498M/498M [00:02&lt;00:00, 218MB/s]"
      }
     },
     "f25dac5f80234b87aa76302d189d1f2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7faecb6763be42fa91038f87ce5eda2a",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_21c85a2f946d4d3da26bb0f44802c56d",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "f5d6c470d78c412082bd211b0053df57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b9179a927da2438aa153526a92408d21",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5aff3cb9294d424491912257864bb3b0",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "f818d5a08b4e417487226f8113c2f88c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9df87a52bb040c494c63a47386449eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fad322959df9480caa66b794060bf06e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7266a37d0faf40b8bf63ed08cd08c0ea",
       "max": 497787752,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9de7645fa11d4c759935a95bc1cfc003",
       "tabbable": null,
       "tooltip": null,
       "value": 497787752
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
