{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5016ebf",
   "metadata": {
    "papermill": {
     "duration": 0.011788,
     "end_time": "2025-06-25T13:42:25.893905",
     "exception": false,
     "start_time": "2025-06-25T13:42:25.882117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac02fc9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:25.916750Z",
     "iopub.status.busy": "2025-06-25T13:42:25.916470Z",
     "iopub.status.idle": "2025-06-25T13:42:49.682641Z",
     "shell.execute_reply": "2025-06-25T13:42:49.681956Z"
    },
    "papermill": {
     "duration": 23.779345,
     "end_time": "2025-06-25T13:42:49.684382",
     "exception": false,
     "start_time": "2025-06-25T13:42:25.905037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242067ba",
   "metadata": {
    "papermill": {
     "duration": 0.010696,
     "end_time": "2025-06-25T13:42:49.706633",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.695937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e64975c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:49.729173Z",
     "iopub.status.busy": "2025-06-25T13:42:49.728669Z",
     "iopub.status.idle": "2025-06-25T13:42:49.732171Z",
     "shell.execute_reply": "2025-06-25T13:42:49.731389Z"
    },
    "papermill": {
     "duration": 0.015975,
     "end_time": "2025-06-25T13:42:49.733405",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.717430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d3b4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:49.755845Z",
     "iopub.status.busy": "2025-06-25T13:42:49.755638Z",
     "iopub.status.idle": "2025-06-25T13:42:49.759352Z",
     "shell.execute_reply": "2025-06-25T13:42:49.758579Z"
    },
    "papermill": {
     "duration": 0.016172,
     "end_time": "2025-06-25T13:42:49.760564",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.744392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddf6145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:49.782900Z",
     "iopub.status.busy": "2025-06-25T13:42:49.782670Z",
     "iopub.status.idle": "2025-06-25T13:42:49.791630Z",
     "shell.execute_reply": "2025-06-25T13:42:49.790849Z"
    },
    "papermill": {
     "duration": 0.0213,
     "end_time": "2025-06-25T13:42:49.792823",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.771523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6dadd",
   "metadata": {
    "papermill": {
     "duration": 0.010574,
     "end_time": "2025-06-25T13:42:49.814177",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.803603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd0e823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:49.836630Z",
     "iopub.status.busy": "2025-06-25T13:42:49.836402Z",
     "iopub.status.idle": "2025-06-25T13:42:49.887493Z",
     "shell.execute_reply": "2025-06-25T13:42:49.886201Z"
    },
    "papermill": {
     "duration": 0.064381,
     "end_time": "2025-06-25T13:42:49.889332",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.824951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-mc-kfold'\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "sequence_length = 80\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f061c6",
   "metadata": {
    "papermill": {
     "duration": 0.010649,
     "end_time": "2025-06-25T13:42:49.911558",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.900909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e166a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:49.934578Z",
     "iopub.status.busy": "2025-06-25T13:42:49.934306Z",
     "iopub.status.idle": "2025-06-25T13:42:50.079085Z",
     "shell.execute_reply": "2025-06-25T13:42:50.078198Z"
    },
    "papermill": {
     "duration": 0.157689,
     "end_time": "2025-06-25T13:42:50.080279",
     "exception": false,
     "start_time": "2025-06-25T13:42:49.922590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efca40df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.104506Z",
     "iopub.status.busy": "2025-06-25T13:42:50.104253Z",
     "iopub.status.idle": "2025-06-25T13:42:50.116080Z",
     "shell.execute_reply": "2025-06-25T13:42:50.115450Z"
    },
    "papermill": {
     "duration": 0.025299,
     "end_time": "2025-06-25T13:42:50.117375",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.092076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12281a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.143068Z",
     "iopub.status.busy": "2025-06-25T13:42:50.142847Z",
     "iopub.status.idle": "2025-06-25T13:42:50.148496Z",
     "shell.execute_reply": "2025-06-25T13:42:50.147670Z"
    },
    "papermill": {
     "duration": 0.020428,
     "end_time": "2025-06-25T13:42:50.149796",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.129368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fe454b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.173480Z",
     "iopub.status.busy": "2025-06-25T13:42:50.173255Z",
     "iopub.status.idle": "2025-06-25T13:42:50.181656Z",
     "shell.execute_reply": "2025-06-25T13:42:50.180911Z"
    },
    "papermill": {
     "duration": 0.021832,
     "end_time": "2025-06-25T13:42:50.182949",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.161117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d170f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.208209Z",
     "iopub.status.busy": "2025-06-25T13:42:50.207979Z",
     "iopub.status.idle": "2025-06-25T13:42:50.220613Z",
     "shell.execute_reply": "2025-06-25T13:42:50.219976Z"
    },
    "papermill": {
     "duration": 0.027246,
     "end_time": "2025-06-25T13:42:50.221799",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.194553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54dfaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.248840Z",
     "iopub.status.busy": "2025-06-25T13:42:50.248607Z",
     "iopub.status.idle": "2025-06-25T13:42:50.251930Z",
     "shell.execute_reply": "2025-06-25T13:42:50.251156Z"
    },
    "papermill": {
     "duration": 0.019373,
     "end_time": "2025-06-25T13:42:50.253194",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.233821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde18bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.276497Z",
     "iopub.status.busy": "2025-06-25T13:42:50.276300Z",
     "iopub.status.idle": "2025-06-25T13:42:50.627974Z",
     "shell.execute_reply": "2025-06-25T13:42:50.627319Z"
    },
    "papermill": {
     "duration": 0.364793,
     "end_time": "2025-06-25T13:42:50.629377",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.264584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "label_columns = data.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8adde",
   "metadata": {
    "papermill": {
     "duration": 0.011333,
     "end_time": "2025-06-25T13:42:50.652958",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.641625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2908e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:50.677788Z",
     "iopub.status.busy": "2025-06-25T13:42:50.677530Z",
     "iopub.status.idle": "2025-06-25T13:42:51.655359Z",
     "shell.execute_reply": "2025-06-25T13:42:51.654436Z"
    },
    "papermill": {
     "duration": 0.992057,
     "end_time": "2025-06-25T13:42:51.656940",
     "exception": false,
     "start_time": "2025-06-25T13:42:50.664883",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76aa0047973647be85df8c6e20ce3b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3593be42fde042828e96a116ee9f1bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327717b8391c4de8806c86f3b23bea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823dabc5dd9a4f149b78e13b1f799633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e244afa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.682655Z",
     "iopub.status.busy": "2025-06-25T13:42:51.682396Z",
     "iopub.status.idle": "2025-06-25T13:42:51.686846Z",
     "shell.execute_reply": "2025-06-25T13:42:51.686038Z"
    },
    "papermill": {
     "duration": 0.018277,
     "end_time": "2025-06-25T13:42:51.688074",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.669797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=sequence_length, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412184aa",
   "metadata": {
    "papermill": {
     "duration": 0.011797,
     "end_time": "2025-06-25T13:42:51.711753",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.699956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de9d1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.735865Z",
     "iopub.status.busy": "2025-06-25T13:42:51.735636Z",
     "iopub.status.idle": "2025-06-25T13:42:51.740523Z",
     "shell.execute_reply": "2025-06-25T13:42:51.739740Z"
    },
    "papermill": {
     "duration": 0.018286,
     "end_time": "2025-06-25T13:42:51.741728",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.723442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3676dd41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.766092Z",
     "iopub.status.busy": "2025-06-25T13:42:51.765886Z",
     "iopub.status.idle": "2025-06-25T13:42:51.778190Z",
     "shell.execute_reply": "2025-06-25T13:42:51.777570Z"
    },
    "papermill": {
     "duration": 0.025755,
     "end_time": "2025-06-25T13:42:51.779390",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.753635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36607e",
   "metadata": {
    "papermill": {
     "duration": 0.021655,
     "end_time": "2025-06-25T13:42:51.812996",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.791341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2605393f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.839117Z",
     "iopub.status.busy": "2025-06-25T13:42:51.838865Z",
     "iopub.status.idle": "2025-06-25T13:42:51.844416Z",
     "shell.execute_reply": "2025-06-25T13:42:51.843587Z"
    },
    "papermill": {
     "duration": 0.020067,
     "end_time": "2025-06-25T13:42:51.845608",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.825541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4bcd1",
   "metadata": {
    "papermill": {
     "duration": 0.012291,
     "end_time": "2025-06-25T13:42:51.870278",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.857987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e766b882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.898194Z",
     "iopub.status.busy": "2025-06-25T13:42:51.897948Z",
     "iopub.status.idle": "2025-06-25T13:42:51.909022Z",
     "shell.execute_reply": "2025-06-25T13:42:51.908431Z"
    },
    "papermill": {
     "duration": 0.026872,
     "end_time": "2025-06-25T13:42:51.910203",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.883331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def monte_carlo_dropout_sampling(model, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, mc_passes=3, n_samples=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    dataset = HateSpeechDataset(X_pool, np.zeros((len(X_pool), 12)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    confidences = []\n",
    "    for data in dataloader:\n",
    "        # Collect multiple predictions to calculate uncertainty\n",
    "        batch_probs = []\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "        for _ in range(mc_passes):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()  # Shape: (batch_size, num_classes)\n",
    "            batch_probs.append(probs)\n",
    "\n",
    "        # Stack the probabilities from multiple MC passes\n",
    "        batch_probs = np.stack(batch_probs, axis=0)  # Shape: (mc_passes, batch_size, num_classes)\n",
    "\n",
    "        # Calculate mean probability and uncertainty for each sample in the batch\n",
    "        mean_probs = np.mean(batch_probs, axis=0)  # Shape: (batch_size, num_classes)\n",
    "        uncertainties = np.mean(np.var(batch_probs, axis=0), axis=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Append the uncertainties to the confidences list\n",
    "        confidences.extend(uncertainties)\n",
    "    \n",
    "    uncertainties = np.array(confidences)\n",
    "    sorted_unc = np.argsort(confidences)\n",
    "    sorted_unc = sorted_unc[::-1]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        threshold = np.percentile(confidences, 90)\n",
    "        items_greater_than_average = uncertainties[confidences >= threshold]\n",
    "        num_of_candidates = len(items_greater_than_average)\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "        \n",
    "        if num_of_candidates <= n_samples and n_samples < nearest_cp - current_train_size:\n",
    "            most_uncertain_indices = sorted_unc[:n_samples]\n",
    "        elif num_of_candidates > n_samples and num_of_candidates < nearest_cp - current_train_size:\n",
    "            most_uncertain_indices = sorted_unc[:max(n_samples, min(math.ceil(0.1*len(sorted_unc)), num_of_candidates))]\n",
    "        else:\n",
    "            most_uncertain_indices = sorted_unc[:nearest_cp - current_train_size]\n",
    "    \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend([remaining_indices[i] for i in most_uncertain_indices])\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train_fold[i] for i in temp],\n",
    "                'HS': [y_train_fold[i][0] for i in temp],\n",
    "                'Abusive': [y_train_fold[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train_fold[i][2] for i in temp],\n",
    "                'HS_Group': [y_train_fold[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train_fold[i][4] for i in temp],\n",
    "                'HS_Race': [y_train_fold[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train_fold[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train_fold[i][7] for i in temp],\n",
    "                'HS_Other': [y_train_fold[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train_fold[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train_fold[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train_fold[i][11] for i in temp],\n",
    "            })\n",
    "    \n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "        end_time = time.time() \n",
    "        duration = end_time - start_time\n",
    "    \n",
    "        sampling_dur.append(duration)\n",
    "        for i in most_uncertain_indices:\n",
    "            new_samples.append(remaining_indices[i])\n",
    "            \n",
    "        print(\"Nearest checkpoint:\", nearest_cp)\n",
    "        print(\"Threshold:\", threshold)\n",
    "        print(\"Samples above threshold:\", num_of_candidates)\n",
    "        print(\"Acquired samples:\", len(most_uncertain_indices))\n",
    "        print(f\"Sampling duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7b2c3",
   "metadata": {
    "papermill": {
     "duration": 0.011744,
     "end_time": "2025-06-25T13:42:51.933664",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.921920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a1828e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:42:51.957855Z",
     "iopub.status.busy": "2025-06-25T13:42:51.957651Z",
     "iopub.status.idle": "2025-06-25T22:54:09.094880Z",
     "shell.execute_reply": "2025-06-25T22:54:09.093872Z"
    },
    "papermill": {
     "duration": 33077.170086,
     "end_time": "2025-06-25T22:54:09.115483",
     "exception": false,
     "start_time": "2025-06-25T13:42:51.945397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 658 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b4a7b615f24ba790261bc8e2891383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6398, Accuracy: 0.7977, F1 Micro: 0.3894, F1 Macro: 0.1173\n",
      "Epoch 2/10, Train Loss: 0.4791, Accuracy: 0.8349, F1 Micro: 0.1631, F1 Macro: 0.0493\n",
      "Epoch 3/10, Train Loss: 0.4083, Accuracy: 0.8329, F1 Micro: 0.1055, F1 Macro: 0.0371\n",
      "Epoch 4/10, Train Loss: 0.3819, Accuracy: 0.8329, F1 Micro: 0.0977, F1 Macro: 0.0354\n",
      "Epoch 5/10, Train Loss: 0.3775, Accuracy: 0.8368, F1 Micro: 0.1577, F1 Macro: 0.0517\n",
      "Epoch 6/10, Train Loss: 0.3646, Accuracy: 0.8432, F1 Micro: 0.2313, F1 Macro: 0.0778\n",
      "Epoch 7/10, Train Loss: 0.3564, Accuracy: 0.8514, F1 Micro: 0.3095, F1 Macro: 0.1032\n",
      "Epoch 8/10, Train Loss: 0.3305, Accuracy: 0.861, F1 Micro: 0.3998, F1 Macro: 0.1449\n",
      "Epoch 9/10, Train Loss: 0.3159, Accuracy: 0.8714, F1 Micro: 0.4936, F1 Macro: 0.2158\n",
      "Epoch 10/10, Train Loss: 0.2956, Accuracy: 0.8752, F1 Micro: 0.5329, F1 Macro: 0.2447\n",
      "Best result for 658 samples: F1 Micro: 0.5329\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.62      0.71      1141\n",
      "      Abusive       0.80      0.75      0.77      1012\n",
      "HS_Individual       0.67      0.40      0.50       737\n",
      "     HS_Group       0.00      0.00      0.00       404\n",
      "  HS_Religion       0.00      0.00      0.00       164\n",
      "      HS_Race       0.00      0.00      0.00       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.72      0.36      0.48       779\n",
      "      HS_Weak       0.66      0.36      0.46       686\n",
      "  HS_Moderate       0.00      0.00      0.00       356\n",
      "    HS_Strong       0.00      0.00      0.00        99\n",
      "\n",
      "    micro avg       0.76      0.41      0.53      5608\n",
      "    macro avg       0.31      0.21      0.24      5608\n",
      " weighted avg       0.58      0.41      0.47      5608\n",
      "  samples avg       0.37      0.26      0.28      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0015653452603146432\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 132.04086351394653 seconds\n",
      "\n",
      "Fold 1 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1646 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5681, Accuracy: 0.8133, F1 Micro: 0.4068, F1 Macro: 0.1045\n",
      "Epoch 2/10, Train Loss: 0.4459, Accuracy: 0.8305, F1 Micro: 0.3974, F1 Macro: 0.1085\n",
      "Epoch 3/10, Train Loss: 0.414, Accuracy: 0.8459, F1 Micro: 0.4143, F1 Macro: 0.1236\n",
      "Epoch 4/10, Train Loss: 0.3952, Accuracy: 0.8635, F1 Micro: 0.5191, F1 Macro: 0.2379\n",
      "Epoch 5/10, Train Loss: 0.362, Accuracy: 0.8846, F1 Micro: 0.6156, F1 Macro: 0.347\n",
      "Epoch 6/10, Train Loss: 0.3214, Accuracy: 0.8947, F1 Micro: 0.6508, F1 Macro: 0.3995\n",
      "Epoch 7/10, Train Loss: 0.2899, Accuracy: 0.8984, F1 Micro: 0.6812, F1 Macro: 0.442\n",
      "Epoch 8/10, Train Loss: 0.2547, Accuracy: 0.9022, F1 Micro: 0.6833, F1 Macro: 0.4618\n",
      "Epoch 9/10, Train Loss: 0.2332, Accuracy: 0.9042, F1 Micro: 0.6865, F1 Macro: 0.4929\n",
      "Epoch 10/10, Train Loss: 0.208, Accuracy: 0.9041, F1 Micro: 0.7137, F1 Macro: 0.5303\n",
      "Best result for 1646 samples: F1 Micro: 0.7137\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1141\n",
      "      Abusive       0.82      0.85      0.83      1012\n",
      "HS_Individual       0.69      0.69      0.69       737\n",
      "     HS_Group       0.67      0.54      0.60       404\n",
      "  HS_Religion       0.75      0.35      0.48       164\n",
      "      HS_Race       0.78      0.58      0.66       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.74      0.73      0.73       779\n",
      "      HS_Weak       0.65      0.68      0.66       686\n",
      "  HS_Moderate       0.58      0.42      0.48       356\n",
      "    HS_Strong       0.84      0.26      0.40        99\n",
      "\n",
      "    micro avg       0.74      0.69      0.71      5608\n",
      "    macro avg       0.61      0.49      0.53      5608\n",
      " weighted avg       0.72      0.69      0.70      5608\n",
      "  samples avg       0.42      0.39      0.38      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.003566955029964447\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 122.49140334129333 seconds\n",
      "\n",
      "Fold 1 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.8232, F1 Micro: 0.3681, F1 Macro: 0.0994\n",
      "Epoch 2/10, Train Loss: 0.43, Accuracy: 0.8374, F1 Micro: 0.3831, F1 Macro: 0.1126\n",
      "Epoch 3/10, Train Loss: 0.3981, Accuracy: 0.8585, F1 Micro: 0.4999, F1 Macro: 0.2242\n",
      "Epoch 4/10, Train Loss: 0.3684, Accuracy: 0.893, F1 Micro: 0.6549, F1 Macro: 0.4103\n",
      "Epoch 5/10, Train Loss: 0.3188, Accuracy: 0.9017, F1 Micro: 0.7042, F1 Macro: 0.4974\n",
      "Epoch 6/10, Train Loss: 0.2871, Accuracy: 0.907, F1 Micro: 0.7038, F1 Macro: 0.5022\n",
      "Epoch 7/10, Train Loss: 0.2417, Accuracy: 0.908, F1 Micro: 0.7307, F1 Macro: 0.551\n",
      "Epoch 8/10, Train Loss: 0.2121, Accuracy: 0.9119, F1 Micro: 0.7326, F1 Macro: 0.5598\n",
      "Epoch 9/10, Train Loss: 0.1826, Accuracy: 0.9129, F1 Micro: 0.7263, F1 Macro: 0.5596\n",
      "Epoch 10/10, Train Loss: 0.1641, Accuracy: 0.9136, F1 Micro: 0.7224, F1 Macro: 0.5503\n",
      "Best result for 2535 samples: F1 Micro: 0.7326\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1141\n",
      "      Abusive       0.87      0.82      0.84      1012\n",
      "HS_Individual       0.75      0.66      0.70       737\n",
      "     HS_Group       0.67      0.64      0.65       404\n",
      "  HS_Religion       0.81      0.40      0.54       164\n",
      "      HS_Race       0.80      0.59      0.68       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.75      0.76      0.75       779\n",
      "      HS_Weak       0.71      0.63      0.67       686\n",
      "  HS_Moderate       0.60      0.53      0.56       356\n",
      "    HS_Strong       0.76      0.35      0.48        99\n",
      "\n",
      "    micro avg       0.78      0.69      0.73      5608\n",
      "    macro avg       0.63      0.52      0.56      5608\n",
      " weighted avg       0.76      0.69      0.72      5608\n",
      "  samples avg       0.41      0.39      0.38      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0033630066784098747\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 109.54185819625854 seconds\n",
      "\n",
      "Fold 1 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3335 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5063, Accuracy: 0.8243, F1 Micro: 0.3909, F1 Macro: 0.1055\n",
      "Epoch 2/10, Train Loss: 0.4214, Accuracy: 0.8422, F1 Micro: 0.4313, F1 Macro: 0.1725\n",
      "Epoch 3/10, Train Loss: 0.3845, Accuracy: 0.8821, F1 Micro: 0.6236, F1 Macro: 0.3529\n",
      "Epoch 4/10, Train Loss: 0.3405, Accuracy: 0.9017, F1 Micro: 0.7179, F1 Macro: 0.5275\n",
      "Epoch 5/10, Train Loss: 0.2978, Accuracy: 0.9097, F1 Micro: 0.7019, F1 Macro: 0.5324\n",
      "Epoch 6/10, Train Loss: 0.2624, Accuracy: 0.9104, F1 Micro: 0.746, F1 Macro: 0.5633\n",
      "Epoch 7/10, Train Loss: 0.2242, Accuracy: 0.9109, F1 Micro: 0.7473, F1 Macro: 0.5669\n",
      "Epoch 8/10, Train Loss: 0.1884, Accuracy: 0.9179, F1 Micro: 0.7485, F1 Macro: 0.589\n",
      "Epoch 9/10, Train Loss: 0.1564, Accuracy: 0.9185, F1 Micro: 0.7562, F1 Macro: 0.6002\n",
      "Epoch 10/10, Train Loss: 0.1383, Accuracy: 0.9163, F1 Micro: 0.7489, F1 Macro: 0.5941\n",
      "Best result for 3335 samples: F1 Micro: 0.7562\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1141\n",
      "      Abusive       0.88      0.83      0.85      1012\n",
      "HS_Individual       0.75      0.72      0.73       737\n",
      "     HS_Group       0.69      0.64      0.66       404\n",
      "  HS_Religion       0.72      0.57      0.64       164\n",
      "      HS_Race       0.77      0.69      0.73       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.78      0.78      0.78       779\n",
      "      HS_Weak       0.71      0.68      0.69       686\n",
      "  HS_Moderate       0.63      0.56      0.60       356\n",
      "    HS_Strong       0.77      0.59      0.67        99\n",
      "\n",
      "    micro avg       0.79      0.73      0.76      5608\n",
      "    macro avg       0.63      0.57      0.60      5608\n",
      " weighted avg       0.77      0.73      0.75      5608\n",
      "  samples avg       0.42      0.40      0.39      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.003966906387358909\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 98.5007975101471 seconds\n",
      "\n",
      "Fold 1 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4055 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4976, Accuracy: 0.8228, F1 Micro: 0.4018, F1 Macro: 0.1069\n",
      "Epoch 2/10, Train Loss: 0.4147, Accuracy: 0.8591, F1 Micro: 0.5616, F1 Macro: 0.2829\n",
      "Epoch 3/10, Train Loss: 0.3724, Accuracy: 0.8928, F1 Micro: 0.6546, F1 Macro: 0.4166\n",
      "Epoch 4/10, Train Loss: 0.3319, Accuracy: 0.9081, F1 Micro: 0.7173, F1 Macro: 0.5273\n",
      "Epoch 5/10, Train Loss: 0.2942, Accuracy: 0.915, F1 Micro: 0.7319, F1 Macro: 0.556\n",
      "Epoch 6/10, Train Loss: 0.2474, Accuracy: 0.9176, F1 Micro: 0.7517, F1 Macro: 0.5913\n",
      "Epoch 7/10, Train Loss: 0.2113, Accuracy: 0.9169, F1 Micro: 0.7604, F1 Macro: 0.5975\n",
      "Epoch 8/10, Train Loss: 0.1758, Accuracy: 0.9165, F1 Micro: 0.759, F1 Macro: 0.6001\n",
      "Epoch 9/10, Train Loss: 0.1581, Accuracy: 0.9187, F1 Micro: 0.7505, F1 Macro: 0.5963\n",
      "Epoch 10/10, Train Loss: 0.1298, Accuracy: 0.9194, F1 Micro: 0.7605, F1 Macro: 0.6208\n",
      "Best result for 4055 samples: F1 Micro: 0.7605\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1141\n",
      "      Abusive       0.88      0.85      0.86      1012\n",
      "HS_Individual       0.74      0.73      0.73       737\n",
      "     HS_Group       0.70      0.62      0.66       404\n",
      "  HS_Religion       0.74      0.58      0.65       164\n",
      "      HS_Race       0.78      0.71      0.75       119\n",
      "  HS_Physical       1.00      0.04      0.07        53\n",
      "    HS_Gender       0.75      0.05      0.10        58\n",
      "     HS_Other       0.81      0.77      0.79       779\n",
      "      HS_Weak       0.69      0.70      0.70       686\n",
      "  HS_Moderate       0.63      0.56      0.59       356\n",
      "    HS_Strong       0.82      0.63      0.71        99\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5608\n",
      "    macro avg       0.78      0.59      0.62      5608\n",
      " weighted avg       0.79      0.74      0.75      5608\n",
      "  samples avg       0.42      0.41      0.40      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.002550102327950299\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 88.52957844734192 seconds\n",
      "\n",
      "Fold 1 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4703 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4883, Accuracy: 0.8252, F1 Micro: 0.3989, F1 Macro: 0.1079\n",
      "Epoch 2/10, Train Loss: 0.4047, Accuracy: 0.8631, F1 Micro: 0.5206, F1 Macro: 0.2591\n",
      "Epoch 3/10, Train Loss: 0.3606, Accuracy: 0.9028, F1 Micro: 0.6937, F1 Macro: 0.5093\n",
      "Epoch 4/10, Train Loss: 0.3126, Accuracy: 0.9138, F1 Micro: 0.72, F1 Macro: 0.5479\n",
      "Epoch 5/10, Train Loss: 0.2707, Accuracy: 0.9169, F1 Micro: 0.7355, F1 Macro: 0.58\n",
      "Epoch 6/10, Train Loss: 0.2356, Accuracy: 0.9191, F1 Micro: 0.7612, F1 Macro: 0.6018\n",
      "Epoch 7/10, Train Loss: 0.1907, Accuracy: 0.9186, F1 Micro: 0.768, F1 Macro: 0.6095\n",
      "Epoch 8/10, Train Loss: 0.161, Accuracy: 0.9207, F1 Micro: 0.7566, F1 Macro: 0.6039\n",
      "Epoch 9/10, Train Loss: 0.1319, Accuracy: 0.9208, F1 Micro: 0.7676, F1 Macro: 0.6146\n",
      "Epoch 10/10, Train Loss: 0.1159, Accuracy: 0.9187, F1 Micro: 0.7659, F1 Macro: 0.6243\n",
      "Best result for 4703 samples: F1 Micro: 0.768\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1141\n",
      "      Abusive       0.85      0.88      0.86      1012\n",
      "HS_Individual       0.73      0.76      0.74       737\n",
      "     HS_Group       0.68      0.67      0.68       404\n",
      "  HS_Religion       0.73      0.62      0.67       164\n",
      "      HS_Race       0.71      0.76      0.73       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.77      0.81      0.79       779\n",
      "      HS_Weak       0.68      0.74      0.71       686\n",
      "  HS_Moderate       0.61      0.60      0.61       356\n",
      "    HS_Strong       0.76      0.59      0.66        99\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5608\n",
      "    macro avg       0.61      0.61      0.61      5608\n",
      " weighted avg       0.75      0.77      0.76      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0017726403311826336\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 79.65074801445007 seconds\n",
      "\n",
      "Fold 1 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5287 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.481, Accuracy: 0.8327, F1 Micro: 0.4124, F1 Macro: 0.1238\n",
      "Epoch 2/10, Train Loss: 0.3918, Accuracy: 0.8885, F1 Micro: 0.6385, F1 Macro: 0.3669\n",
      "Epoch 3/10, Train Loss: 0.3381, Accuracy: 0.9058, F1 Micro: 0.7041, F1 Macro: 0.5218\n",
      "Epoch 4/10, Train Loss: 0.2922, Accuracy: 0.9153, F1 Micro: 0.7506, F1 Macro: 0.5688\n",
      "Epoch 5/10, Train Loss: 0.247, Accuracy: 0.918, F1 Micro: 0.7342, F1 Macro: 0.5791\n",
      "Epoch 6/10, Train Loss: 0.2109, Accuracy: 0.9187, F1 Micro: 0.7669, F1 Macro: 0.6152\n",
      "Epoch 7/10, Train Loss: 0.1701, Accuracy: 0.9207, F1 Micro: 0.7683, F1 Macro: 0.615\n",
      "Epoch 8/10, Train Loss: 0.1463, Accuracy: 0.9242, F1 Micro: 0.7742, F1 Macro: 0.6322\n",
      "Epoch 9/10, Train Loss: 0.1321, Accuracy: 0.924, F1 Micro: 0.769, F1 Macro: 0.629\n",
      "Epoch 10/10, Train Loss: 0.1049, Accuracy: 0.9241, F1 Micro: 0.775, F1 Macro: 0.629\n",
      "Best result for 5287 samples: F1 Micro: 0.775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1141\n",
      "      Abusive       0.88      0.88      0.88      1012\n",
      "HS_Individual       0.76      0.74      0.75       737\n",
      "     HS_Group       0.73      0.62      0.67       404\n",
      "  HS_Religion       0.75      0.59      0.66       164\n",
      "      HS_Race       0.78      0.67      0.72       119\n",
      "  HS_Physical       1.00      0.02      0.04        53\n",
      "    HS_Gender       0.75      0.05      0.10        58\n",
      "     HS_Other       0.80      0.79      0.80       779\n",
      "      HS_Weak       0.71      0.71      0.71       686\n",
      "  HS_Moderate       0.67      0.58      0.62       356\n",
      "    HS_Strong       0.80      0.69      0.74        99\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5608\n",
      "    macro avg       0.79      0.60      0.63      5608\n",
      " weighted avg       0.80      0.75      0.77      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0010601570364087824\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 71.86459922790527 seconds\n",
      "\n",
      "Fold 1 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5812 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4754, Accuracy: 0.8383, F1 Micro: 0.4443, F1 Macro: 0.1625\n",
      "Epoch 2/10, Train Loss: 0.3881, Accuracy: 0.895, F1 Micro: 0.6555, F1 Macro: 0.4399\n",
      "Epoch 3/10, Train Loss: 0.3292, Accuracy: 0.9091, F1 Micro: 0.6982, F1 Macro: 0.522\n",
      "Epoch 4/10, Train Loss: 0.2793, Accuracy: 0.9171, F1 Micro: 0.7589, F1 Macro: 0.585\n",
      "Epoch 5/10, Train Loss: 0.2404, Accuracy: 0.9211, F1 Micro: 0.7686, F1 Macro: 0.6156\n",
      "Epoch 6/10, Train Loss: 0.2015, Accuracy: 0.9202, F1 Micro: 0.7699, F1 Macro: 0.6294\n",
      "Epoch 7/10, Train Loss: 0.1719, Accuracy: 0.9178, F1 Micro: 0.7692, F1 Macro: 0.6356\n",
      "Epoch 8/10, Train Loss: 0.1439, Accuracy: 0.9247, F1 Micro: 0.7775, F1 Macro: 0.6398\n",
      "Epoch 9/10, Train Loss: 0.1214, Accuracy: 0.9236, F1 Micro: 0.7752, F1 Macro: 0.6487\n",
      "Epoch 10/10, Train Loss: 0.106, Accuracy: 0.9249, F1 Micro: 0.7819, F1 Macro: 0.6733\n",
      "Best result for 5812 samples: F1 Micro: 0.7819\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1141\n",
      "      Abusive       0.90      0.86      0.88      1012\n",
      "HS_Individual       0.74      0.76      0.75       737\n",
      "     HS_Group       0.72      0.67      0.69       404\n",
      "  HS_Religion       0.70      0.66      0.68       164\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.70      0.13      0.22        53\n",
      "    HS_Gender       0.71      0.17      0.28        58\n",
      "     HS_Other       0.80      0.80      0.80       779\n",
      "      HS_Weak       0.72      0.74      0.73       686\n",
      "  HS_Moderate       0.66      0.62      0.64       356\n",
      "    HS_Strong       0.79      0.79      0.79        99\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5608\n",
      "    macro avg       0.75      0.65      0.67      5608\n",
      " weighted avg       0.79      0.77      0.78      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.000848142744507641\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 64.88138318061829 seconds\n",
      "\n",
      "Fold 1 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6285 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4693, Accuracy: 0.8477, F1 Micro: 0.4658, F1 Macro: 0.192\n",
      "Epoch 2/10, Train Loss: 0.3711, Accuracy: 0.899, F1 Micro: 0.6735, F1 Macro: 0.4635\n",
      "Epoch 3/10, Train Loss: 0.316, Accuracy: 0.915, F1 Micro: 0.7313, F1 Macro: 0.5666\n",
      "Epoch 4/10, Train Loss: 0.265, Accuracy: 0.9213, F1 Micro: 0.7522, F1 Macro: 0.599\n",
      "Epoch 5/10, Train Loss: 0.2236, Accuracy: 0.9218, F1 Micro: 0.7704, F1 Macro: 0.6258\n",
      "Epoch 6/10, Train Loss: 0.1903, Accuracy: 0.9257, F1 Micro: 0.7845, F1 Macro: 0.6444\n",
      "Epoch 7/10, Train Loss: 0.1557, Accuracy: 0.9253, F1 Micro: 0.7807, F1 Macro: 0.6468\n",
      "Epoch 8/10, Train Loss: 0.1313, Accuracy: 0.9258, F1 Micro: 0.7762, F1 Macro: 0.6578\n",
      "Epoch 9/10, Train Loss: 0.1147, Accuracy: 0.9258, F1 Micro: 0.779, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0999, Accuracy: 0.9255, F1 Micro: 0.7799, F1 Macro: 0.678\n",
      "Best result for 6285 samples: F1 Micro: 0.7845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1141\n",
      "      Abusive       0.87      0.90      0.88      1012\n",
      "HS_Individual       0.75      0.76      0.76       737\n",
      "     HS_Group       0.71      0.67      0.69       404\n",
      "  HS_Religion       0.82      0.58      0.68       164\n",
      "      HS_Race       0.74      0.73      0.74       119\n",
      "  HS_Physical       1.00      0.08      0.14        53\n",
      "    HS_Gender       0.67      0.03      0.07        58\n",
      "     HS_Other       0.80      0.82      0.81       779\n",
      "      HS_Weak       0.72      0.74      0.73       686\n",
      "  HS_Moderate       0.66      0.60      0.63       356\n",
      "    HS_Strong       0.77      0.73      0.75        99\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5608\n",
      "    macro avg       0.78      0.63      0.64      5608\n",
      " weighted avg       0.79      0.78      0.78      5608\n",
      "  samples avg       0.44      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0006012403173372148\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 58.59895348548889 seconds\n",
      "\n",
      "Fold 1 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6584 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4648, Accuracy: 0.8518, F1 Micro: 0.4635, F1 Macro: 0.1985\n",
      "Epoch 2/10, Train Loss: 0.3663, Accuracy: 0.9022, F1 Micro: 0.6898, F1 Macro: 0.4667\n",
      "Epoch 3/10, Train Loss: 0.3115, Accuracy: 0.9147, F1 Micro: 0.7476, F1 Macro: 0.5814\n",
      "Epoch 4/10, Train Loss: 0.258, Accuracy: 0.9214, F1 Micro: 0.7604, F1 Macro: 0.5983\n",
      "Epoch 5/10, Train Loss: 0.2202, Accuracy: 0.9229, F1 Micro: 0.761, F1 Macro: 0.6088\n",
      "Epoch 6/10, Train Loss: 0.1837, Accuracy: 0.9199, F1 Micro: 0.774, F1 Macro: 0.6263\n",
      "Epoch 7/10, Train Loss: 0.1554, Accuracy: 0.9261, F1 Micro: 0.7748, F1 Macro: 0.6354\n",
      "Epoch 8/10, Train Loss: 0.1309, Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6493\n",
      "Epoch 9/10, Train Loss: 0.1146, Accuracy: 0.9267, F1 Micro: 0.7849, F1 Macro: 0.6623\n",
      "Epoch 10/10, Train Loss: 0.0969, Accuracy: 0.9265, F1 Micro: 0.7838, F1 Macro: 0.6852\n",
      "Best result for 6584 samples: F1 Micro: 0.7849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1141\n",
      "      Abusive       0.88      0.90      0.89      1012\n",
      "HS_Individual       0.78      0.73      0.75       737\n",
      "     HS_Group       0.70      0.69      0.70       404\n",
      "  HS_Religion       0.77      0.60      0.68       164\n",
      "      HS_Race       0.75      0.74      0.74       119\n",
      "  HS_Physical       1.00      0.06      0.11        53\n",
      "    HS_Gender       0.82      0.16      0.26        58\n",
      "     HS_Other       0.79      0.82      0.80       779\n",
      "      HS_Weak       0.75      0.69      0.72       686\n",
      "  HS_Moderate       0.64      0.64      0.64       356\n",
      "    HS_Strong       0.85      0.75      0.80        99\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5608\n",
      "    macro avg       0.80      0.63      0.66      5608\n",
      " weighted avg       0.80      0.77      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0004240544221829623\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.60328531265259 seconds\n",
      "\n",
      "Fold 1 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6980 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4568, Accuracy: 0.8549, F1 Micro: 0.4905, F1 Macro: 0.2188\n",
      "Epoch 2/10, Train Loss: 0.3573, Accuracy: 0.8973, F1 Micro: 0.6255, F1 Macro: 0.4079\n",
      "Epoch 3/10, Train Loss: 0.2959, Accuracy: 0.917, F1 Micro: 0.7384, F1 Macro: 0.5841\n",
      "Epoch 4/10, Train Loss: 0.2467, Accuracy: 0.9213, F1 Micro: 0.7478, F1 Macro: 0.602\n",
      "Epoch 5/10, Train Loss: 0.2108, Accuracy: 0.9254, F1 Micro: 0.7769, F1 Macro: 0.6321\n",
      "Epoch 6/10, Train Loss: 0.1758, Accuracy: 0.9247, F1 Micro: 0.7784, F1 Macro: 0.6611\n",
      "Epoch 7/10, Train Loss: 0.1519, Accuracy: 0.9255, F1 Micro: 0.7808, F1 Macro: 0.6648\n",
      "Epoch 8/10, Train Loss: 0.1273, Accuracy: 0.9275, F1 Micro: 0.7857, F1 Macro: 0.6769\n",
      "Epoch 9/10, Train Loss: 0.1076, Accuracy: 0.9257, F1 Micro: 0.7845, F1 Macro: 0.6962\n",
      "Epoch 10/10, Train Loss: 0.0959, Accuracy: 0.9227, F1 Micro: 0.7835, F1 Macro: 0.7001\n",
      "Best result for 6980 samples: F1 Micro: 0.7857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1141\n",
      "      Abusive       0.89      0.90      0.89      1012\n",
      "HS_Individual       0.76      0.74      0.75       737\n",
      "     HS_Group       0.74      0.65      0.69       404\n",
      "  HS_Religion       0.79      0.63      0.70       164\n",
      "      HS_Race       0.77      0.74      0.75       119\n",
      "  HS_Physical       0.83      0.09      0.17        53\n",
      "    HS_Gender       0.78      0.24      0.37        58\n",
      "     HS_Other       0.82      0.79      0.81       779\n",
      "      HS_Weak       0.74      0.72      0.73       686\n",
      "  HS_Moderate       0.68      0.57      0.62       356\n",
      "    HS_Strong       0.78      0.79      0.78        99\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5608\n",
      "    macro avg       0.79      0.64      0.68      5608\n",
      " weighted avg       0.81      0.76      0.78      5608\n",
      "  samples avg       0.45      0.43      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0002994250098709017\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 49.077475786209106 seconds\n",
      "\n",
      "Fold 1 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4544, Accuracy: 0.8583, F1 Micro: 0.5122, F1 Macro: 0.223\n",
      "Epoch 2/10, Train Loss: 0.3467, Accuracy: 0.9043, F1 Micro: 0.7223, F1 Macro: 0.5323\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9167, F1 Micro: 0.751, F1 Macro: 0.5686\n",
      "Epoch 4/10, Train Loss: 0.2453, Accuracy: 0.9228, F1 Micro: 0.7725, F1 Macro: 0.6199\n",
      "Epoch 5/10, Train Loss: 0.2048, Accuracy: 0.9259, F1 Micro: 0.7825, F1 Macro: 0.6423\n",
      "Epoch 6/10, Train Loss: 0.1782, Accuracy: 0.9237, F1 Micro: 0.7818, F1 Macro: 0.6373\n",
      "Epoch 7/10, Train Loss: 0.1445, Accuracy: 0.9257, F1 Micro: 0.7801, F1 Macro: 0.6502\n",
      "Epoch 8/10, Train Loss: 0.1282, Accuracy: 0.927, F1 Micro: 0.7813, F1 Macro: 0.6548\n",
      "Epoch 9/10, Train Loss: 0.1132, Accuracy: 0.9232, F1 Micro: 0.7859, F1 Macro: 0.6852\n",
      "Epoch 10/10, Train Loss: 0.0932, Accuracy: 0.9277, F1 Micro: 0.7839, F1 Macro: 0.7036\n",
      "Best result for 7336 samples: F1 Micro: 0.7859\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.91      0.85      1141\n",
      "      Abusive       0.87      0.91      0.89      1012\n",
      "HS_Individual       0.70      0.83      0.76       737\n",
      "     HS_Group       0.72      0.63      0.68       404\n",
      "  HS_Religion       0.72      0.66      0.69       164\n",
      "      HS_Race       0.76      0.75      0.75       119\n",
      "  HS_Physical       0.69      0.21      0.32        53\n",
      "    HS_Gender       0.86      0.21      0.33        58\n",
      "     HS_Other       0.76      0.87      0.81       779\n",
      "      HS_Weak       0.68      0.80      0.74       686\n",
      "  HS_Moderate       0.66      0.55      0.60       356\n",
      "    HS_Strong       0.79      0.81      0.80        99\n",
      "\n",
      "    micro avg       0.76      0.81      0.79      5608\n",
      "    macro avg       0.75      0.68      0.69      5608\n",
      " weighted avg       0.76      0.81      0.78      5608\n",
      "  samples avg       0.46      0.46      0.45      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00033491447684355104\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.63830351829529 seconds\n",
      "\n",
      "Fold 1 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7656 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4479, Accuracy: 0.8717, F1 Micro: 0.5744, F1 Macro: 0.2869\n",
      "Epoch 2/10, Train Loss: 0.339, Accuracy: 0.9077, F1 Micro: 0.7163, F1 Macro: 0.5107\n",
      "Epoch 3/10, Train Loss: 0.2764, Accuracy: 0.9181, F1 Micro: 0.7535, F1 Macro: 0.5874\n",
      "Epoch 4/10, Train Loss: 0.2368, Accuracy: 0.922, F1 Micro: 0.7709, F1 Macro: 0.614\n",
      "Epoch 5/10, Train Loss: 0.2005, Accuracy: 0.9227, F1 Micro: 0.7703, F1 Macro: 0.6346\n",
      "Epoch 6/10, Train Loss: 0.1729, Accuracy: 0.9282, F1 Micro: 0.7802, F1 Macro: 0.6546\n",
      "Epoch 7/10, Train Loss: 0.1451, Accuracy: 0.9259, F1 Micro: 0.7849, F1 Macro: 0.6636\n",
      "Epoch 8/10, Train Loss: 0.1219, Accuracy: 0.9266, F1 Micro: 0.7818, F1 Macro: 0.6778\n",
      "Epoch 9/10, Train Loss: 0.1026, Accuracy: 0.9262, F1 Micro: 0.7844, F1 Macro: 0.6797\n",
      "Epoch 10/10, Train Loss: 0.0904, Accuracy: 0.9269, F1 Micro: 0.7826, F1 Macro: 0.6983\n",
      "Best result for 7656 samples: F1 Micro: 0.7849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1141\n",
      "      Abusive       0.89      0.90      0.89      1012\n",
      "HS_Individual       0.72      0.78      0.75       737\n",
      "     HS_Group       0.75      0.62      0.68       404\n",
      "  HS_Religion       0.76      0.63      0.69       164\n",
      "      HS_Race       0.75      0.75      0.75       119\n",
      "  HS_Physical       0.46      0.11      0.18        53\n",
      "    HS_Gender       0.73      0.14      0.23        58\n",
      "     HS_Other       0.81      0.81      0.81       779\n",
      "      HS_Weak       0.70      0.76      0.73       686\n",
      "  HS_Moderate       0.70      0.53      0.60       356\n",
      "    HS_Strong       0.78      0.79      0.78        99\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5608\n",
      "    macro avg       0.74      0.64      0.66      5608\n",
      " weighted avg       0.79      0.78      0.78      5608\n",
      "  samples avg       0.44      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0001558861287776383\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 40.11461114883423 seconds\n",
      "\n",
      "Fold 1 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7901 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4449, Accuracy: 0.8782, F1 Micro: 0.556, F1 Macro: 0.2842\n",
      "Epoch 2/10, Train Loss: 0.327, Accuracy: 0.9062, F1 Micro: 0.6909, F1 Macro: 0.5184\n",
      "Epoch 3/10, Train Loss: 0.265, Accuracy: 0.9147, F1 Micro: 0.7139, F1 Macro: 0.5602\n",
      "Epoch 4/10, Train Loss: 0.2299, Accuracy: 0.9236, F1 Micro: 0.7694, F1 Macro: 0.6175\n",
      "Epoch 5/10, Train Loss: 0.1906, Accuracy: 0.925, F1 Micro: 0.7769, F1 Macro: 0.6394\n",
      "Epoch 6/10, Train Loss: 0.1605, Accuracy: 0.9266, F1 Micro: 0.7838, F1 Macro: 0.6649\n",
      "Epoch 7/10, Train Loss: 0.1404, Accuracy: 0.9258, F1 Micro: 0.7851, F1 Macro: 0.6646\n",
      "Epoch 8/10, Train Loss: 0.1206, Accuracy: 0.9233, F1 Micro: 0.7851, F1 Macro: 0.6876\n",
      "Epoch 9/10, Train Loss: 0.1064, Accuracy: 0.9283, F1 Micro: 0.7859, F1 Macro: 0.6974\n",
      "Epoch 10/10, Train Loss: 0.0878, Accuracy: 0.9252, F1 Micro: 0.7839, F1 Macro: 0.6952\n",
      "Best result for 7901 samples: F1 Micro: 0.7859\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1141\n",
      "      Abusive       0.89      0.90      0.90      1012\n",
      "HS_Individual       0.78      0.71      0.75       737\n",
      "     HS_Group       0.73      0.67      0.70       404\n",
      "  HS_Religion       0.74      0.62      0.68       164\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.65      0.21      0.31        53\n",
      "    HS_Gender       0.73      0.33      0.45        58\n",
      "     HS_Other       0.84      0.78      0.81       779\n",
      "      HS_Weak       0.77      0.69      0.73       686\n",
      "  HS_Moderate       0.66      0.60      0.63       356\n",
      "    HS_Strong       0.81      0.80      0.80        99\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5608\n",
      "    macro avg       0.77      0.66      0.70      5608\n",
      " weighted avg       0.81      0.76      0.78      5608\n",
      "  samples avg       0.45      0.43      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 6.171087006805465e-05\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.88251852989197 seconds\n",
      "\n",
      "Fold 1 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4388, Accuracy: 0.883, F1 Micro: 0.6017, F1 Macro: 0.3041\n",
      "Epoch 2/10, Train Loss: 0.3165, Accuracy: 0.9105, F1 Micro: 0.7282, F1 Macro: 0.5359\n",
      "Epoch 3/10, Train Loss: 0.2628, Accuracy: 0.9183, F1 Micro: 0.7461, F1 Macro: 0.5512\n",
      "Epoch 4/10, Train Loss: 0.218, Accuracy: 0.9225, F1 Micro: 0.7528, F1 Macro: 0.5935\n",
      "Epoch 5/10, Train Loss: 0.1847, Accuracy: 0.9267, F1 Micro: 0.7802, F1 Macro: 0.6362\n",
      "Epoch 6/10, Train Loss: 0.1577, Accuracy: 0.9238, F1 Micro: 0.7822, F1 Macro: 0.6411\n",
      "Epoch 7/10, Train Loss: 0.1331, Accuracy: 0.9265, F1 Micro: 0.7777, F1 Macro: 0.6573\n",
      "Epoch 8/10, Train Loss: 0.1147, Accuracy: 0.9279, F1 Micro: 0.7882, F1 Macro: 0.6785\n",
      "Epoch 9/10, Train Loss: 0.1003, Accuracy: 0.9268, F1 Micro: 0.7828, F1 Macro: 0.6946\n",
      "Epoch 10/10, Train Loss: 0.0855, Accuracy: 0.926, F1 Micro: 0.7865, F1 Macro: 0.7054\n",
      "Best result for 8165 samples: F1 Micro: 0.7882\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1141\n",
      "      Abusive       0.90      0.90      0.90      1012\n",
      "HS_Individual       0.77      0.72      0.74       737\n",
      "     HS_Group       0.70      0.70      0.70       404\n",
      "  HS_Religion       0.73      0.68      0.70       164\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.71      0.09      0.17        53\n",
      "    HS_Gender       0.76      0.22      0.35        58\n",
      "     HS_Other       0.83      0.79      0.81       779\n",
      "      HS_Weak       0.75      0.70      0.72       686\n",
      "  HS_Moderate       0.63      0.63      0.63       356\n",
      "    HS_Strong       0.80      0.78      0.79        99\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5608\n",
      "    macro avg       0.77      0.65      0.68      5608\n",
      " weighted avg       0.80      0.77      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.721321234275818e-05\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 33.294456243515015 seconds\n",
      "\n",
      "Fold 1 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8402 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4308, Accuracy: 0.8854, F1 Micro: 0.6447, F1 Macro: 0.3482\n",
      "Epoch 2/10, Train Loss: 0.3105, Accuracy: 0.9106, F1 Micro: 0.7277, F1 Macro: 0.5276\n",
      "Epoch 3/10, Train Loss: 0.255, Accuracy: 0.9206, F1 Micro: 0.7547, F1 Macro: 0.5901\n",
      "Epoch 4/10, Train Loss: 0.2103, Accuracy: 0.9225, F1 Micro: 0.7711, F1 Macro: 0.6209\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9267, F1 Micro: 0.7741, F1 Macro: 0.623\n",
      "Epoch 6/10, Train Loss: 0.1542, Accuracy: 0.9267, F1 Micro: 0.7772, F1 Macro: 0.6593\n",
      "Epoch 7/10, Train Loss: 0.1294, Accuracy: 0.9257, F1 Micro: 0.7798, F1 Macro: 0.6491\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.9275, F1 Micro: 0.7817, F1 Macro: 0.6625\n",
      "Epoch 9/10, Train Loss: 0.0958, Accuracy: 0.9294, F1 Micro: 0.782, F1 Macro: 0.6707\n",
      "Epoch 10/10, Train Loss: 0.0818, Accuracy: 0.9285, F1 Micro: 0.786, F1 Macro: 0.6995\n",
      "Best result for 8402 samples: F1 Micro: 0.786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1141\n",
      "      Abusive       0.92      0.89      0.90      1012\n",
      "HS_Individual       0.75      0.77      0.76       737\n",
      "     HS_Group       0.78      0.57      0.66       404\n",
      "  HS_Religion       0.78      0.63      0.70       164\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.68      0.25      0.36        53\n",
      "    HS_Gender       0.83      0.34      0.49        58\n",
      "     HS_Other       0.84      0.77      0.80       779\n",
      "      HS_Weak       0.72      0.74      0.73       686\n",
      "  HS_Moderate       0.70      0.50      0.58       356\n",
      "    HS_Strong       0.82      0.78      0.80        99\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5608\n",
      "    macro avg       0.79      0.65      0.70      5608\n",
      " weighted avg       0.82      0.76      0.78      5608\n",
      "  samples avg       0.45      0.43      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 6.4528096118010565e-06\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 30.276130437850952 seconds\n",
      "\n",
      "Fold 1 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8616 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4242, Accuracy: 0.8834, F1 Micro: 0.5715, F1 Macro: 0.2888\n",
      "Epoch 2/10, Train Loss: 0.3005, Accuracy: 0.911, F1 Micro: 0.7189, F1 Macro: 0.5164\n",
      "Epoch 3/10, Train Loss: 0.2491, Accuracy: 0.9198, F1 Micro: 0.7503, F1 Macro: 0.5759\n",
      "Epoch 4/10, Train Loss: 0.2124, Accuracy: 0.9236, F1 Micro: 0.7816, F1 Macro: 0.6256\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.9237, F1 Micro: 0.7792, F1 Macro: 0.6515\n",
      "Epoch 6/10, Train Loss: 0.1505, Accuracy: 0.9249, F1 Micro: 0.7841, F1 Macro: 0.6559\n",
      "Epoch 7/10, Train Loss: 0.1246, Accuracy: 0.9264, F1 Micro: 0.7838, F1 Macro: 0.6726\n",
      "Epoch 8/10, Train Loss: 0.109, Accuracy: 0.924, F1 Micro: 0.7831, F1 Macro: 0.6842\n",
      "Epoch 9/10, Train Loss: 0.0927, Accuracy: 0.9262, F1 Micro: 0.7812, F1 Macro: 0.6807\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9272, F1 Micro: 0.7875, F1 Macro: 0.7036\n",
      "Best result for 8616 samples: F1 Micro: 0.7875\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1141\n",
      "      Abusive       0.92      0.90      0.91      1012\n",
      "HS_Individual       0.73      0.77      0.75       737\n",
      "     HS_Group       0.74      0.60      0.67       404\n",
      "  HS_Religion       0.74      0.64      0.69       164\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.50      0.30      0.38        53\n",
      "    HS_Gender       0.71      0.38      0.49        58\n",
      "     HS_Other       0.83      0.81      0.82       779\n",
      "      HS_Weak       0.70      0.75      0.72       686\n",
      "  HS_Moderate       0.69      0.54      0.60       356\n",
      "    HS_Strong       0.83      0.78      0.80        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.75      0.67      0.70      5608\n",
      " weighted avg       0.80      0.78      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 5.091926777822664e-06\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.203811168670654 seconds\n",
      "\n",
      "Fold 1 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8816 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4224, Accuracy: 0.8871, F1 Micro: 0.6057, F1 Macro: 0.303\n",
      "Epoch 2/10, Train Loss: 0.2968, Accuracy: 0.9109, F1 Micro: 0.7228, F1 Macro: 0.5481\n",
      "Epoch 3/10, Train Loss: 0.2434, Accuracy: 0.9202, F1 Micro: 0.764, F1 Macro: 0.6022\n",
      "Epoch 4/10, Train Loss: 0.2016, Accuracy: 0.9258, F1 Micro: 0.7732, F1 Macro: 0.6194\n",
      "Epoch 5/10, Train Loss: 0.1749, Accuracy: 0.9231, F1 Micro: 0.7801, F1 Macro: 0.6395\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.9263, F1 Micro: 0.7796, F1 Macro: 0.6554\n",
      "Epoch 7/10, Train Loss: 0.1257, Accuracy: 0.9255, F1 Micro: 0.7825, F1 Macro: 0.6665\n",
      "Epoch 8/10, Train Loss: 0.1061, Accuracy: 0.9267, F1 Micro: 0.7832, F1 Macro: 0.6693\n",
      "Epoch 9/10, Train Loss: 0.0948, Accuracy: 0.9276, F1 Micro: 0.7832, F1 Macro: 0.6945\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9289, F1 Micro: 0.7864, F1 Macro: 0.6975\n",
      "Best result for 8816 samples: F1 Micro: 0.7864\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.83      0.86      1141\n",
      "      Abusive       0.90      0.90      0.90      1012\n",
      "HS_Individual       0.76      0.76      0.76       737\n",
      "     HS_Group       0.76      0.57      0.65       404\n",
      "  HS_Religion       0.74      0.64      0.69       164\n",
      "      HS_Race       0.81      0.73      0.77       119\n",
      "  HS_Physical       0.65      0.28      0.39        53\n",
      "    HS_Gender       0.76      0.28      0.41        58\n",
      "     HS_Other       0.84      0.76      0.80       779\n",
      "      HS_Weak       0.74      0.74      0.74       686\n",
      "  HS_Moderate       0.72      0.52      0.60       356\n",
      "    HS_Strong       0.84      0.78      0.81        99\n",
      "\n",
      "    micro avg       0.82      0.75      0.79      5608\n",
      "    macro avg       0.78      0.65      0.70      5608\n",
      " weighted avg       0.82      0.75      0.78      5608\n",
      "  samples avg       0.45      0.43      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 5.0391372496960685e-06\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.67730474472046 seconds\n",
      "\n",
      "Fold 1 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4214, Accuracy: 0.8879, F1 Micro: 0.6153, F1 Macro: 0.3049\n",
      "Epoch 2/10, Train Loss: 0.2956, Accuracy: 0.9115, F1 Micro: 0.7195, F1 Macro: 0.5267\n",
      "Epoch 3/10, Train Loss: 0.2392, Accuracy: 0.9204, F1 Micro: 0.7473, F1 Macro: 0.5901\n",
      "Epoch 4/10, Train Loss: 0.202, Accuracy: 0.9254, F1 Micro: 0.7699, F1 Macro: 0.6047\n",
      "Epoch 5/10, Train Loss: 0.1715, Accuracy: 0.9258, F1 Micro: 0.7858, F1 Macro: 0.6395\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.9265, F1 Micro: 0.7738, F1 Macro: 0.6604\n",
      "Epoch 7/10, Train Loss: 0.1205, Accuracy: 0.9267, F1 Micro: 0.7862, F1 Macro: 0.6628\n",
      "Epoch 8/10, Train Loss: 0.105, Accuracy: 0.9272, F1 Micro: 0.7757, F1 Macro: 0.6789\n",
      "Epoch 9/10, Train Loss: 0.0908, Accuracy: 0.9272, F1 Micro: 0.7845, F1 Macro: 0.6818\n",
      "Epoch 10/10, Train Loss: 0.079, Accuracy: 0.9235, F1 Micro: 0.7876, F1 Macro: 0.7102\n",
      "Best result for 9016 samples: F1 Micro: 0.7876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.91      0.86      1141\n",
      "      Abusive       0.87      0.93      0.90      1012\n",
      "HS_Individual       0.70      0.80      0.75       737\n",
      "     HS_Group       0.71      0.68      0.69       404\n",
      "  HS_Religion       0.73      0.65      0.69       164\n",
      "      HS_Race       0.74      0.78      0.76       119\n",
      "  HS_Physical       0.47      0.34      0.40        53\n",
      "    HS_Gender       0.54      0.50      0.52        58\n",
      "     HS_Other       0.78      0.84      0.81       779\n",
      "      HS_Weak       0.68      0.79      0.73       686\n",
      "  HS_Moderate       0.64      0.63      0.64       356\n",
      "    HS_Strong       0.85      0.74      0.79        99\n",
      "\n",
      "    micro avg       0.76      0.82      0.79      5608\n",
      "    macro avg       0.71      0.71      0.71      5608\n",
      " weighted avg       0.76      0.82      0.79      5608\n",
      "  samples avg       0.45      0.46      0.44      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 4.802627427125118e-06\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.655061721801758 seconds\n",
      "\n",
      "Fold 1 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9216 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4144, Accuracy: 0.8865, F1 Micro: 0.6061, F1 Macro: 0.3137\n",
      "Epoch 2/10, Train Loss: 0.289, Accuracy: 0.9111, F1 Micro: 0.7306, F1 Macro: 0.5274\n",
      "Epoch 3/10, Train Loss: 0.2333, Accuracy: 0.9211, F1 Micro: 0.7642, F1 Macro: 0.5842\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9234, F1 Micro: 0.7737, F1 Macro: 0.6131\n",
      "Epoch 5/10, Train Loss: 0.1678, Accuracy: 0.9262, F1 Micro: 0.7751, F1 Macro: 0.6339\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.923, F1 Micro: 0.7847, F1 Macro: 0.6459\n",
      "Epoch 7/10, Train Loss: 0.118, Accuracy: 0.925, F1 Micro: 0.7835, F1 Macro: 0.6665\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9258, F1 Micro: 0.7833, F1 Macro: 0.6949\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9255, F1 Micro: 0.7802, F1 Macro: 0.6888\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9289, F1 Micro: 0.7842, F1 Macro: 0.7042\n",
      "Best result for 9216 samples: F1 Micro: 0.7847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.91      0.85      1141\n",
      "      Abusive       0.88      0.92      0.90      1012\n",
      "HS_Individual       0.74      0.77      0.75       737\n",
      "     HS_Group       0.66      0.74      0.70       404\n",
      "  HS_Religion       0.65      0.67      0.66       164\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.67      0.04      0.07        53\n",
      "    HS_Gender       1.00      0.07      0.13        58\n",
      "     HS_Other       0.75      0.87      0.81       779\n",
      "      HS_Weak       0.71      0.73      0.72       686\n",
      "  HS_Moderate       0.63      0.63      0.63       356\n",
      "    HS_Strong       0.76      0.79      0.78        99\n",
      "\n",
      "    micro avg       0.76      0.81      0.78      5608\n",
      "    macro avg       0.75      0.66      0.65      5608\n",
      " weighted avg       0.76      0.81      0.78      5608\n",
      "  samples avg       0.46      0.46      0.44      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.038325874309522e-05\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.294084548950195 seconds\n",
      "\n",
      "Fold 1 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4108, Accuracy: 0.8846, F1 Micro: 0.5693, F1 Macro: 0.2904\n",
      "Epoch 2/10, Train Loss: 0.2869, Accuracy: 0.9082, F1 Micro: 0.7328, F1 Macro: 0.5256\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9203, F1 Micro: 0.7657, F1 Macro: 0.6023\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9253, F1 Micro: 0.7713, F1 Macro: 0.6027\n",
      "Epoch 5/10, Train Loss: 0.1676, Accuracy: 0.9251, F1 Micro: 0.7826, F1 Macro: 0.6382\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9239, F1 Micro: 0.787, F1 Macro: 0.6765\n",
      "Epoch 7/10, Train Loss: 0.1184, Accuracy: 0.9274, F1 Micro: 0.7863, F1 Macro: 0.6757\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9277, F1 Micro: 0.7899, F1 Macro: 0.681\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9267, F1 Micro: 0.7844, F1 Macro: 0.6948\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.9244, F1 Micro: 0.7819, F1 Macro: 0.7052\n",
      "Best result for 9218 samples: F1 Micro: 0.7899\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1141\n",
      "      Abusive       0.89      0.92      0.90      1012\n",
      "HS_Individual       0.75      0.76      0.75       737\n",
      "     HS_Group       0.72      0.65      0.68       404\n",
      "  HS_Religion       0.77      0.62      0.69       164\n",
      "      HS_Race       0.77      0.72      0.75       119\n",
      "  HS_Physical       0.50      0.11      0.18        53\n",
      "    HS_Gender       0.75      0.26      0.38        58\n",
      "     HS_Other       0.80      0.83      0.81       779\n",
      "      HS_Weak       0.72      0.73      0.73       686\n",
      "  HS_Moderate       0.68      0.59      0.63       356\n",
      "    HS_Strong       0.80      0.78      0.79        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.75      0.65      0.68      5608\n",
      " weighted avg       0.79      0.78      0.78      5608\n",
      "  samples avg       0.46      0.45      0.44      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 7.349584120674992e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 18.94397735595703 seconds\n",
      "\n",
      "Fold 1 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4109, Accuracy: 0.8851, F1 Micro: 0.5849, F1 Macro: 0.2815\n",
      "Epoch 2/10, Train Loss: 0.2833, Accuracy: 0.9116, F1 Micro: 0.7164, F1 Macro: 0.5338\n",
      "Epoch 3/10, Train Loss: 0.2342, Accuracy: 0.9184, F1 Micro: 0.7617, F1 Macro: 0.5791\n",
      "Epoch 4/10, Train Loss: 0.1924, Accuracy: 0.9244, F1 Micro: 0.7655, F1 Macro: 0.6127\n",
      "Epoch 5/10, Train Loss: 0.162, Accuracy: 0.9258, F1 Micro: 0.7744, F1 Macro: 0.632\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9251, F1 Micro: 0.7816, F1 Macro: 0.6592\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9192, F1 Micro: 0.7799, F1 Macro: 0.6562\n",
      "Epoch 8/10, Train Loss: 0.0975, Accuracy: 0.9274, F1 Micro: 0.7796, F1 Macro: 0.6681\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9263, F1 Micro: 0.7879, F1 Macro: 0.7008\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9225, F1 Micro: 0.786, F1 Macro: 0.6903\n",
      "Best result for 9418 samples: F1 Micro: 0.7879\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1141\n",
      "      Abusive       0.88      0.92      0.90      1012\n",
      "HS_Individual       0.74      0.77      0.75       737\n",
      "     HS_Group       0.72      0.65      0.68       404\n",
      "  HS_Religion       0.70      0.70      0.70       164\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.54      0.25      0.34        53\n",
      "    HS_Gender       0.71      0.34      0.47        58\n",
      "     HS_Other       0.81      0.80      0.80       779\n",
      "      HS_Weak       0.71      0.75      0.73       686\n",
      "  HS_Moderate       0.66      0.58      0.62       356\n",
      "    HS_Strong       0.80      0.79      0.80        99\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5608\n",
      "    macro avg       0.74      0.68      0.70      5608\n",
      " weighted avg       0.78      0.79      0.78      5608\n",
      "  samples avg       0.45      0.45      0.44      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.032995002489769e-06\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.440335035324097 seconds\n",
      "\n",
      "Fold 1 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9618 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4042, Accuracy: 0.8899, F1 Micro: 0.6449, F1 Macro: 0.3467\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.9096, F1 Micro: 0.7312, F1 Macro: 0.5522\n",
      "Epoch 3/10, Train Loss: 0.2269, Accuracy: 0.9202, F1 Micro: 0.7631, F1 Macro: 0.6036\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.9239, F1 Micro: 0.7744, F1 Macro: 0.6177\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9266, F1 Micro: 0.7847, F1 Macro: 0.6421\n",
      "Epoch 6/10, Train Loss: 0.138, Accuracy: 0.9258, F1 Micro: 0.7885, F1 Macro: 0.6578\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.926, F1 Micro: 0.7825, F1 Macro: 0.6853\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.9253, F1 Micro: 0.7676, F1 Macro: 0.6372\n",
      "Epoch 9/10, Train Loss: 0.0844, Accuracy: 0.9267, F1 Micro: 0.7884, F1 Macro: 0.6997\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.9267, F1 Micro: 0.786, F1 Macro: 0.7014\n",
      "Best result for 9618 samples: F1 Micro: 0.7885\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1141\n",
      "      Abusive       0.90      0.91      0.90      1012\n",
      "HS_Individual       0.70      0.83      0.76       737\n",
      "     HS_Group       0.76      0.59      0.66       404\n",
      "  HS_Religion       0.74      0.64      0.69       164\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.60      0.11      0.19        53\n",
      "    HS_Gender       0.83      0.09      0.16        58\n",
      "     HS_Other       0.80      0.83      0.81       779\n",
      "      HS_Weak       0.67      0.83      0.74       686\n",
      "  HS_Moderate       0.70      0.51      0.59       356\n",
      "    HS_Strong       0.80      0.71      0.75        99\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5608\n",
      "    macro avg       0.76      0.64      0.66      5608\n",
      " weighted avg       0.78      0.80      0.78      5608\n",
      "  samples avg       0.45      0.45      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 9.344671161670703e-06\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.839950561523438 seconds\n",
      "\n",
      "Fold 1 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9818 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.8888, F1 Micro: 0.622, F1 Macro: 0.3258\n",
      "Epoch 2/10, Train Loss: 0.2742, Accuracy: 0.9111, F1 Micro: 0.7179, F1 Macro: 0.5227\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9152, F1 Micro: 0.7588, F1 Macro: 0.587\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9212, F1 Micro: 0.7735, F1 Macro: 0.6072\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9258, F1 Micro: 0.7786, F1 Macro: 0.6317\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9265, F1 Micro: 0.7818, F1 Macro: 0.66\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.927, F1 Micro: 0.7845, F1 Macro: 0.675\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9274, F1 Micro: 0.7863, F1 Macro: 0.6833\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.9265, F1 Micro: 0.7868, F1 Macro: 0.6968\n",
      "Epoch 10/10, Train Loss: 0.0723, Accuracy: 0.929, F1 Micro: 0.784, F1 Macro: 0.6964\n",
      "Best result for 9818 samples: F1 Micro: 0.7868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1141\n",
      "      Abusive       0.87      0.94      0.90      1012\n",
      "HS_Individual       0.77      0.72      0.74       737\n",
      "     HS_Group       0.68      0.71      0.69       404\n",
      "  HS_Religion       0.74      0.65      0.69       164\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.67      0.19      0.29        53\n",
      "    HS_Gender       0.86      0.31      0.46        58\n",
      "     HS_Other       0.81      0.81      0.81       779\n",
      "      HS_Weak       0.74      0.68      0.71       686\n",
      "  HS_Moderate       0.63      0.64      0.64       356\n",
      "    HS_Strong       0.82      0.79      0.80        99\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5608\n",
      "    macro avg       0.77      0.67      0.70      5608\n",
      " weighted avg       0.79      0.78      0.78      5608\n",
      "  samples avg       0.45      0.45      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 2.7470101031212833e-06\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.148795366287231 seconds\n",
      "\n",
      "Fold 1 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10018 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3955, Accuracy: 0.8892, F1 Micro: 0.6342, F1 Macro: 0.3184\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9117, F1 Micro: 0.7238, F1 Macro: 0.5135\n",
      "Epoch 3/10, Train Loss: 0.2173, Accuracy: 0.9157, F1 Micro: 0.7601, F1 Macro: 0.5817\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6145\n",
      "Epoch 5/10, Train Loss: 0.1528, Accuracy: 0.9281, F1 Micro: 0.788, F1 Macro: 0.6551\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9283, F1 Micro: 0.7887, F1 Macro: 0.6659\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9263, F1 Micro: 0.7822, F1 Macro: 0.653\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9235, F1 Micro: 0.7835, F1 Macro: 0.6914\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.9266, F1 Micro: 0.778, F1 Macro: 0.6883\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.927, F1 Micro: 0.792, F1 Macro: 0.7144\n",
      "Best result for 10018 samples: F1 Micro: 0.792\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1141\n",
      "      Abusive       0.89      0.91      0.90      1012\n",
      "HS_Individual       0.73      0.79      0.76       737\n",
      "     HS_Group       0.72      0.67      0.70       404\n",
      "  HS_Religion       0.71      0.66      0.68       164\n",
      "      HS_Race       0.75      0.71      0.73       119\n",
      "  HS_Physical       0.63      0.32      0.42        53\n",
      "    HS_Gender       0.69      0.43      0.53        58\n",
      "     HS_Other       0.80      0.83      0.82       779\n",
      "      HS_Weak       0.70      0.76      0.73       686\n",
      "  HS_Moderate       0.66      0.60      0.63       356\n",
      "    HS_Strong       0.84      0.78      0.81        99\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5608\n",
      "    macro avg       0.75      0.70      0.71      5608\n",
      " weighted avg       0.78      0.80      0.79      5608\n",
      "  samples avg       0.45      0.45      0.44      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.7506410358691946e-06\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.159729719161987 seconds\n",
      "\n",
      "Fold 1 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.391, Accuracy: 0.8778, F1 Micro: 0.515, F1 Macro: 0.2484\n",
      "Epoch 2/10, Train Loss: 0.2642, Accuracy: 0.9104, F1 Micro: 0.7387, F1 Macro: 0.5502\n",
      "Epoch 3/10, Train Loss: 0.2107, Accuracy: 0.9214, F1 Micro: 0.7618, F1 Macro: 0.5895\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9237, F1 Micro: 0.7619, F1 Macro: 0.6066\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9255, F1 Micro: 0.7762, F1 Macro: 0.6437\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9249, F1 Micro: 0.7791, F1 Macro: 0.6514\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9212, F1 Micro: 0.7827, F1 Macro: 0.6664\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9287, F1 Micro: 0.7847, F1 Macro: 0.6861\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9278, F1 Micro: 0.7889, F1 Macro: 0.6947\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9285, F1 Micro: 0.7907, F1 Macro: 0.7054\n",
      "Best result for 10218 samples: F1 Micro: 0.7907\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1141\n",
      "      Abusive       0.91      0.90      0.90      1012\n",
      "HS_Individual       0.75      0.77      0.76       737\n",
      "     HS_Group       0.73      0.65      0.69       404\n",
      "  HS_Religion       0.73      0.70      0.71       164\n",
      "      HS_Race       0.72      0.77      0.74       119\n",
      "  HS_Physical       0.68      0.25      0.36        53\n",
      "    HS_Gender       0.72      0.36      0.48        58\n",
      "     HS_Other       0.84      0.78      0.81       779\n",
      "      HS_Weak       0.71      0.74      0.73       686\n",
      "  HS_Moderate       0.68      0.58      0.62       356\n",
      "    HS_Strong       0.82      0.76      0.79        99\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5608\n",
      "    macro avg       0.76      0.68      0.71      5608\n",
      " weighted avg       0.80      0.78      0.79      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.6734988548705595e-06\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.3450281620025635 seconds\n",
      "\n",
      "Fold 1 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8907, F1 Micro: 0.6389, F1 Macro: 0.3219\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.911, F1 Micro: 0.7237, F1 Macro: 0.5474\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9209, F1 Micro: 0.7505, F1 Macro: 0.5945\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9249, F1 Micro: 0.7761, F1 Macro: 0.616\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9268, F1 Micro: 0.7821, F1 Macro: 0.6352\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.9274, F1 Micro: 0.784, F1 Macro: 0.6659\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.92, F1 Micro: 0.7808, F1 Macro: 0.6773\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9267, F1 Micro: 0.7883, F1 Macro: 0.7001\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9279, F1 Micro: 0.7807, F1 Macro: 0.6847\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9293, F1 Micro: 0.7911, F1 Macro: 0.7131\n",
      "Best result for 10418 samples: F1 Micro: 0.7911\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1141\n",
      "      Abusive       0.90      0.92      0.91      1012\n",
      "HS_Individual       0.78      0.72      0.75       737\n",
      "     HS_Group       0.72      0.70      0.71       404\n",
      "  HS_Religion       0.71      0.67      0.69       164\n",
      "      HS_Race       0.77      0.71      0.74       119\n",
      "  HS_Physical       0.59      0.30      0.40        53\n",
      "    HS_Gender       0.77      0.40      0.52        58\n",
      "     HS_Other       0.84      0.77      0.80       779\n",
      "      HS_Weak       0.76      0.69      0.73       686\n",
      "  HS_Moderate       0.67      0.65      0.66       356\n",
      "    HS_Strong       0.80      0.79      0.80        99\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5608\n",
      "    macro avg       0.77      0.68      0.71      5608\n",
      " weighted avg       0.81      0.77      0.79      5608\n",
      "  samples avg       0.46      0.44      0.43      5608\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.4757225926587127e-06\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.8836684226989746 seconds\n",
      "\n",
      "Fold 1 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8878, F1 Micro: 0.6039, F1 Macro: 0.2911\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9124, F1 Micro: 0.7397, F1 Macro: 0.5452\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.922, F1 Micro: 0.7705, F1 Macro: 0.5909\n",
      "Epoch 4/10, Train Loss: 0.1684, Accuracy: 0.9255, F1 Micro: 0.7802, F1 Macro: 0.6171\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.9272, F1 Micro: 0.7852, F1 Macro: 0.6391\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9249, F1 Micro: 0.7827, F1 Macro: 0.6518\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9263, F1 Micro: 0.7861, F1 Macro: 0.6606\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9282, F1 Micro: 0.7912, F1 Macro: 0.6943\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9288, F1 Micro: 0.7894, F1 Macro: 0.6999\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9282, F1 Micro: 0.7912, F1 Macro: 0.7008\n",
      "Best result for 10535 samples: F1 Micro: 0.7912\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1141\n",
      "      Abusive       0.91      0.89      0.90      1012\n",
      "HS_Individual       0.77      0.76      0.76       737\n",
      "     HS_Group       0.71      0.69      0.70       404\n",
      "  HS_Religion       0.72      0.68      0.70       164\n",
      "      HS_Race       0.69      0.81      0.74       119\n",
      "  HS_Physical       0.65      0.21      0.31        53\n",
      "    HS_Gender       0.79      0.26      0.39        58\n",
      "     HS_Other       0.81      0.81      0.81       779\n",
      "      HS_Weak       0.73      0.73      0.73       686\n",
      "  HS_Moderate       0.64      0.62      0.63       356\n",
      "    HS_Strong       0.79      0.78      0.79        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.76      0.67      0.69      5608\n",
      " weighted avg       0.80      0.78      0.79      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 6613.00 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 658 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6071, Accuracy: 0.8373, F1 Micro: 0.2811, F1 Macro: 0.0967\n",
      "Epoch 2/10, Train Loss: 0.4721, Accuracy: 0.8453, F1 Micro: 0.2409, F1 Macro: 0.0795\n",
      "Epoch 3/10, Train Loss: 0.4136, Accuracy: 0.8385, F1 Micro: 0.1122, F1 Macro: 0.0411\n",
      "Epoch 4/10, Train Loss: 0.3858, Accuracy: 0.8491, F1 Micro: 0.2595, F1 Macro: 0.0869\n",
      "Epoch 5/10, Train Loss: 0.3744, Accuracy: 0.8514, F1 Micro: 0.2787, F1 Macro: 0.092\n",
      "Epoch 6/10, Train Loss: 0.3634, Accuracy: 0.8587, F1 Micro: 0.3797, F1 Macro: 0.1286\n",
      "Epoch 7/10, Train Loss: 0.3489, Accuracy: 0.8708, F1 Micro: 0.5093, F1 Macro: 0.2237\n",
      "Epoch 8/10, Train Loss: 0.3042, Accuracy: 0.8762, F1 Micro: 0.5769, F1 Macro: 0.2684\n",
      "Epoch 9/10, Train Loss: 0.2947, Accuracy: 0.8795, F1 Micro: 0.581, F1 Macro: 0.2943\n",
      "Epoch 10/10, Train Loss: 0.2572, Accuracy: 0.8806, F1 Micro: 0.6047, F1 Macro: 0.3163\n",
      "Best result for 658 samples: F1 Micro: 0.6047\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.76      0.75      1094\n",
      "      Abusive       0.80      0.73      0.76      1072\n",
      "HS_Individual       0.60      0.57      0.59       689\n",
      "     HS_Group       0.65      0.28      0.40       405\n",
      "  HS_Religion       0.00      0.00      0.00       124\n",
      "      HS_Race       0.00      0.00      0.00       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.64      0.59      0.62       754\n",
      "      HS_Weak       0.58      0.53      0.56       664\n",
      "  HS_Moderate       0.50      0.07      0.12       346\n",
      "    HS_Strong       0.00      0.00      0.00        84\n",
      "\n",
      "    micro avg       0.69      0.54      0.60      5476\n",
      "    macro avg       0.38      0.29      0.32      5476\n",
      " weighted avg       0.62      0.54      0.56      5476\n",
      "  samples avg       0.37      0.32      0.31      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0019405035534873602\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 133.88139939308167 seconds\n",
      "\n",
      "Fold 2 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1646 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5449, Accuracy: 0.8296, F1 Micro: 0.4172, F1 Macro: 0.1095\n",
      "Epoch 2/10, Train Loss: 0.4344, Accuracy: 0.831, F1 Micro: 0.3956, F1 Macro: 0.1064\n",
      "Epoch 3/10, Train Loss: 0.4191, Accuracy: 0.8464, F1 Micro: 0.424, F1 Macro: 0.1244\n",
      "Epoch 4/10, Train Loss: 0.3965, Accuracy: 0.865, F1 Micro: 0.4644, F1 Macro: 0.1902\n",
      "Epoch 5/10, Train Loss: 0.364, Accuracy: 0.884, F1 Micro: 0.6287, F1 Macro: 0.3315\n",
      "Epoch 6/10, Train Loss: 0.3172, Accuracy: 0.8924, F1 Micro: 0.6577, F1 Macro: 0.3998\n",
      "Epoch 7/10, Train Loss: 0.2845, Accuracy: 0.8952, F1 Micro: 0.6796, F1 Macro: 0.5035\n",
      "Epoch 8/10, Train Loss: 0.2484, Accuracy: 0.8961, F1 Micro: 0.6942, F1 Macro: 0.5048\n",
      "Epoch 9/10, Train Loss: 0.2206, Accuracy: 0.9003, F1 Micro: 0.6878, F1 Macro: 0.509\n",
      "Epoch 10/10, Train Loss: 0.1828, Accuracy: 0.8999, F1 Micro: 0.6989, F1 Macro: 0.5187\n",
      "Best result for 1646 samples: F1 Micro: 0.6989\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.82      0.80      1094\n",
      "      Abusive       0.85      0.83      0.84      1072\n",
      "HS_Individual       0.60      0.75      0.67       689\n",
      "     HS_Group       0.74      0.42      0.53       405\n",
      "  HS_Religion       0.68      0.38      0.49       124\n",
      "      HS_Race       0.85      0.46      0.59       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.68      0.73      0.70       754\n",
      "      HS_Weak       0.60      0.73      0.66       664\n",
      "  HS_Moderate       0.65      0.27      0.39       346\n",
      "    HS_Strong       0.83      0.42      0.56        84\n",
      "\n",
      "    micro avg       0.71      0.68      0.70      5476\n",
      "    macro avg       0.60      0.48      0.52      5476\n",
      " weighted avg       0.71      0.68      0.68      5476\n",
      "  samples avg       0.41      0.39      0.38      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0044293467886745925\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 121.08189487457275 seconds\n",
      "\n",
      "Fold 2 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.512, Accuracy: 0.8319, F1 Micro: 0.3735, F1 Macro: 0.1011\n",
      "Epoch 2/10, Train Loss: 0.4227, Accuracy: 0.8308, F1 Micro: 0.3996, F1 Macro: 0.1082\n",
      "Epoch 3/10, Train Loss: 0.4041, Accuracy: 0.8602, F1 Micro: 0.5071, F1 Macro: 0.2226\n",
      "Epoch 4/10, Train Loss: 0.3615, Accuracy: 0.8903, F1 Micro: 0.625, F1 Macro: 0.4223\n",
      "Epoch 5/10, Train Loss: 0.3219, Accuracy: 0.8982, F1 Micro: 0.6984, F1 Macro: 0.5276\n",
      "Epoch 6/10, Train Loss: 0.282, Accuracy: 0.9039, F1 Micro: 0.711, F1 Macro: 0.5444\n",
      "Epoch 7/10, Train Loss: 0.2392, Accuracy: 0.9049, F1 Micro: 0.6844, F1 Macro: 0.5091\n",
      "Epoch 8/10, Train Loss: 0.2068, Accuracy: 0.9065, F1 Micro: 0.7184, F1 Macro: 0.5491\n",
      "Epoch 9/10, Train Loss: 0.1746, Accuracy: 0.9085, F1 Micro: 0.7084, F1 Macro: 0.5494\n",
      "Epoch 10/10, Train Loss: 0.1495, Accuracy: 0.9079, F1 Micro: 0.7233, F1 Macro: 0.5678\n",
      "Best result for 2535 samples: F1 Micro: 0.7233\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.82      0.81      1094\n",
      "      Abusive       0.87      0.86      0.86      1072\n",
      "HS_Individual       0.66      0.71      0.68       689\n",
      "     HS_Group       0.68      0.56      0.61       405\n",
      "  HS_Religion       0.70      0.58      0.63       124\n",
      "      HS_Race       0.82      0.64      0.72       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.70      0.72      0.71       754\n",
      "      HS_Weak       0.64      0.69      0.67       664\n",
      "  HS_Moderate       0.60      0.44      0.51       346\n",
      "    HS_Strong       0.80      0.49      0.61        84\n",
      "\n",
      "    micro avg       0.74      0.71      0.72      5476\n",
      "    macro avg       0.61      0.54      0.57      5476\n",
      " weighted avg       0.72      0.71      0.71      5476\n",
      "  samples avg       0.43      0.41      0.40      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0049167967867106204\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 109.5303885936737 seconds\n",
      "\n",
      "Fold 2 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3335 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4941, Accuracy: 0.8282, F1 Micro: 0.4047, F1 Macro: 0.1075\n",
      "Epoch 2/10, Train Loss: 0.4163, Accuracy: 0.8459, F1 Micro: 0.4114, F1 Macro: 0.1224\n",
      "Epoch 3/10, Train Loss: 0.3815, Accuracy: 0.8851, F1 Micro: 0.5746, F1 Macro: 0.3279\n",
      "Epoch 4/10, Train Loss: 0.336, Accuracy: 0.8962, F1 Micro: 0.6986, F1 Macro: 0.503\n",
      "Epoch 5/10, Train Loss: 0.2939, Accuracy: 0.9076, F1 Micro: 0.7208, F1 Macro: 0.5334\n",
      "Epoch 6/10, Train Loss: 0.255, Accuracy: 0.9092, F1 Micro: 0.7374, F1 Macro: 0.5576\n",
      "Epoch 7/10, Train Loss: 0.2075, Accuracy: 0.9135, F1 Micro: 0.7322, F1 Macro: 0.5578\n",
      "Epoch 8/10, Train Loss: 0.1734, Accuracy: 0.9124, F1 Micro: 0.7374, F1 Macro: 0.5754\n",
      "Epoch 9/10, Train Loss: 0.1462, Accuracy: 0.9153, F1 Micro: 0.7374, F1 Macro: 0.5753\n",
      "Epoch 10/10, Train Loss: 0.1294, Accuracy: 0.9128, F1 Micro: 0.7441, F1 Macro: 0.5962\n",
      "Best result for 3335 samples: F1 Micro: 0.7441\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1094\n",
      "      Abusive       0.85      0.90      0.87      1072\n",
      "HS_Individual       0.64      0.79      0.71       689\n",
      "     HS_Group       0.75      0.53      0.62       405\n",
      "  HS_Religion       0.68      0.71      0.70       124\n",
      "      HS_Race       0.81      0.67      0.73       125\n",
      "  HS_Physical       0.67      0.03      0.06        61\n",
      "    HS_Gender       1.00      0.03      0.07        58\n",
      "     HS_Other       0.73      0.73      0.73       754\n",
      "      HS_Weak       0.63      0.78      0.69       664\n",
      "  HS_Moderate       0.65      0.45      0.53       346\n",
      "    HS_Strong       0.83      0.48      0.61        84\n",
      "\n",
      "    micro avg       0.74      0.75      0.74      5476\n",
      "    macro avg       0.75      0.58      0.60      5476\n",
      " weighted avg       0.75      0.75      0.73      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0038287477567791957\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 98.6142840385437 seconds\n",
      "\n",
      "Fold 2 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4055 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4867, Accuracy: 0.8289, F1 Micro: 0.4065, F1 Macro: 0.1082\n",
      "Epoch 2/10, Train Loss: 0.4156, Accuracy: 0.8628, F1 Micro: 0.5143, F1 Macro: 0.2175\n",
      "Epoch 3/10, Train Loss: 0.3689, Accuracy: 0.8964, F1 Micro: 0.6431, F1 Macro: 0.3754\n",
      "Epoch 4/10, Train Loss: 0.3236, Accuracy: 0.9061, F1 Micro: 0.7032, F1 Macro: 0.4882\n",
      "Epoch 5/10, Train Loss: 0.274, Accuracy: 0.9125, F1 Micro: 0.7308, F1 Macro: 0.5547\n",
      "Epoch 6/10, Train Loss: 0.2308, Accuracy: 0.9147, F1 Micro: 0.7498, F1 Macro: 0.5798\n",
      "Epoch 7/10, Train Loss: 0.191, Accuracy: 0.9133, F1 Micro: 0.7458, F1 Macro: 0.5716\n",
      "Epoch 8/10, Train Loss: 0.1641, Accuracy: 0.9162, F1 Micro: 0.7372, F1 Macro: 0.5841\n",
      "Epoch 9/10, Train Loss: 0.1387, Accuracy: 0.9154, F1 Micro: 0.7527, F1 Macro: 0.6061\n",
      "Epoch 10/10, Train Loss: 0.1149, Accuracy: 0.9167, F1 Micro: 0.7553, F1 Macro: 0.6006\n",
      "Best result for 4055 samples: F1 Micro: 0.7553\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1094\n",
      "      Abusive       0.87      0.87      0.87      1072\n",
      "HS_Individual       0.67      0.78      0.72       689\n",
      "     HS_Group       0.75      0.61      0.67       405\n",
      "  HS_Religion       0.74      0.64      0.68       124\n",
      "      HS_Race       0.87      0.59      0.70       125\n",
      "  HS_Physical       1.00      0.02      0.03        61\n",
      "    HS_Gender       0.50      0.03      0.06        58\n",
      "     HS_Other       0.72      0.81      0.76       754\n",
      "      HS_Weak       0.65      0.77      0.70       664\n",
      "  HS_Moderate       0.66      0.51      0.58       346\n",
      "    HS_Strong       0.81      0.45      0.58        84\n",
      "\n",
      "    micro avg       0.75      0.76      0.76      5476\n",
      "    macro avg       0.75      0.58      0.60      5476\n",
      " weighted avg       0.76      0.76      0.75      5476\n",
      "  samples avg       0.43      0.43      0.41      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0022643763339146978\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 88.51601696014404 seconds\n",
      "\n",
      "Fold 2 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4703 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4782, Accuracy: 0.8369, F1 Micro: 0.4058, F1 Macro: 0.1108\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.8793, F1 Micro: 0.6085, F1 Macro: 0.3435\n",
      "Epoch 3/10, Train Loss: 0.3471, Accuracy: 0.899, F1 Micro: 0.7016, F1 Macro: 0.4944\n",
      "Epoch 4/10, Train Loss: 0.3033, Accuracy: 0.9095, F1 Micro: 0.7099, F1 Macro: 0.528\n",
      "Epoch 5/10, Train Loss: 0.2572, Accuracy: 0.9126, F1 Micro: 0.731, F1 Macro: 0.5565\n",
      "Epoch 6/10, Train Loss: 0.2178, Accuracy: 0.9184, F1 Micro: 0.7555, F1 Macro: 0.6065\n",
      "Epoch 7/10, Train Loss: 0.18, Accuracy: 0.9178, F1 Micro: 0.7524, F1 Macro: 0.6047\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 0.919, F1 Micro: 0.7533, F1 Macro: 0.6118\n",
      "Epoch 9/10, Train Loss: 0.1298, Accuracy: 0.9178, F1 Micro: 0.7511, F1 Macro: 0.6187\n",
      "Epoch 10/10, Train Loss: 0.1076, Accuracy: 0.9185, F1 Micro: 0.7591, F1 Macro: 0.6298\n",
      "Best result for 4703 samples: F1 Micro: 0.7591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1094\n",
      "      Abusive       0.89      0.86      0.88      1072\n",
      "HS_Individual       0.69      0.76      0.72       689\n",
      "     HS_Group       0.72      0.63      0.67       405\n",
      "  HS_Religion       0.70      0.65      0.68       124\n",
      "      HS_Race       0.85      0.65      0.74       125\n",
      "  HS_Physical       1.00      0.03      0.06        61\n",
      "    HS_Gender       0.38      0.09      0.14        58\n",
      "     HS_Other       0.74      0.78      0.76       754\n",
      "      HS_Weak       0.66      0.74      0.70       664\n",
      "  HS_Moderate       0.64      0.57      0.60       346\n",
      "    HS_Strong       0.82      0.73      0.77        84\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5476\n",
      "    macro avg       0.74      0.61      0.63      5476\n",
      " weighted avg       0.76      0.76      0.75      5476\n",
      "  samples avg       0.43      0.43      0.42      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0012783651705831319\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 79.78836846351624 seconds\n",
      "\n",
      "Fold 2 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5287 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.8384, F1 Micro: 0.4105, F1 Macro: 0.1153\n",
      "Epoch 2/10, Train Loss: 0.3925, Accuracy: 0.8854, F1 Micro: 0.654, F1 Macro: 0.3957\n",
      "Epoch 3/10, Train Loss: 0.331, Accuracy: 0.9072, F1 Micro: 0.7082, F1 Macro: 0.5144\n",
      "Epoch 4/10, Train Loss: 0.29, Accuracy: 0.9122, F1 Micro: 0.7224, F1 Macro: 0.5405\n",
      "Epoch 5/10, Train Loss: 0.2447, Accuracy: 0.9167, F1 Micro: 0.7338, F1 Macro: 0.5922\n",
      "Epoch 6/10, Train Loss: 0.1988, Accuracy: 0.9173, F1 Micro: 0.7495, F1 Macro: 0.6044\n",
      "Epoch 7/10, Train Loss: 0.1661, Accuracy: 0.919, F1 Micro: 0.7589, F1 Macro: 0.6333\n",
      "Epoch 8/10, Train Loss: 0.1373, Accuracy: 0.9193, F1 Micro: 0.7593, F1 Macro: 0.6258\n",
      "Epoch 9/10, Train Loss: 0.1182, Accuracy: 0.9214, F1 Micro: 0.7647, F1 Macro: 0.6502\n",
      "Epoch 10/10, Train Loss: 0.1064, Accuracy: 0.9208, F1 Micro: 0.7599, F1 Macro: 0.6677\n",
      "Best result for 5287 samples: F1 Micro: 0.7647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1094\n",
      "      Abusive       0.89      0.88      0.88      1072\n",
      "HS_Individual       0.71      0.74      0.72       689\n",
      "     HS_Group       0.73      0.61      0.66       405\n",
      "  HS_Religion       0.73      0.66      0.69       124\n",
      "      HS_Race       0.84      0.73      0.78       125\n",
      "  HS_Physical       0.80      0.07      0.12        61\n",
      "    HS_Gender       0.48      0.17      0.25        58\n",
      "     HS_Other       0.77      0.76      0.77       754\n",
      "      HS_Weak       0.69      0.74      0.71       664\n",
      "  HS_Moderate       0.65      0.54      0.59       346\n",
      "    HS_Strong       0.82      0.75      0.78        84\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5476\n",
      "    macro avg       0.74      0.62      0.65      5476\n",
      " weighted avg       0.77      0.75      0.76      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0009537667501717819\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 71.95371079444885 seconds\n",
      "\n",
      "Fold 2 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5812 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4677, Accuracy: 0.839, F1 Micro: 0.4373, F1 Macro: 0.1455\n",
      "Epoch 2/10, Train Loss: 0.3779, Accuracy: 0.896, F1 Micro: 0.6658, F1 Macro: 0.433\n",
      "Epoch 3/10, Train Loss: 0.3209, Accuracy: 0.9085, F1 Micro: 0.7243, F1 Macro: 0.5566\n",
      "Epoch 4/10, Train Loss: 0.2703, Accuracy: 0.9164, F1 Micro: 0.741, F1 Macro: 0.583\n",
      "Epoch 5/10, Train Loss: 0.2255, Accuracy: 0.9133, F1 Micro: 0.7571, F1 Macro: 0.6089\n",
      "Epoch 6/10, Train Loss: 0.2, Accuracy: 0.9173, F1 Micro: 0.7646, F1 Macro: 0.645\n",
      "Epoch 7/10, Train Loss: 0.1648, Accuracy: 0.9221, F1 Micro: 0.7613, F1 Macro: 0.6403\n",
      "Epoch 8/10, Train Loss: 0.1373, Accuracy: 0.9195, F1 Micro: 0.7645, F1 Macro: 0.6707\n",
      "Epoch 9/10, Train Loss: 0.1164, Accuracy: 0.9198, F1 Micro: 0.7678, F1 Macro: 0.6702\n",
      "Epoch 10/10, Train Loss: 0.0995, Accuracy: 0.9204, F1 Micro: 0.7682, F1 Macro: 0.6686\n",
      "Best result for 5812 samples: F1 Micro: 0.7682\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1094\n",
      "      Abusive       0.88      0.90      0.89      1072\n",
      "HS_Individual       0.67      0.79      0.73       689\n",
      "     HS_Group       0.73      0.62      0.67       405\n",
      "  HS_Religion       0.68      0.69      0.69       124\n",
      "      HS_Race       0.81      0.73      0.76       125\n",
      "  HS_Physical       0.69      0.15      0.24        61\n",
      "    HS_Gender       0.59      0.28      0.38        58\n",
      "     HS_Other       0.76      0.77      0.77       754\n",
      "      HS_Weak       0.66      0.77      0.71       664\n",
      "  HS_Moderate       0.65      0.53      0.59       346\n",
      "    HS_Strong       0.80      0.73      0.76        84\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5476\n",
      "    macro avg       0.73      0.65      0.67      5476\n",
      " weighted avg       0.76      0.78      0.76      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0007562323473393918\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 65.0185375213623 seconds\n",
      "\n",
      "Fold 2 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6285 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.464, Accuracy: 0.855, F1 Micro: 0.5035, F1 Macro: 0.2201\n",
      "Epoch 2/10, Train Loss: 0.3648, Accuracy: 0.8999, F1 Micro: 0.6969, F1 Macro: 0.5012\n",
      "Epoch 3/10, Train Loss: 0.3051, Accuracy: 0.9097, F1 Micro: 0.7041, F1 Macro: 0.5443\n",
      "Epoch 4/10, Train Loss: 0.26, Accuracy: 0.9191, F1 Micro: 0.7524, F1 Macro: 0.5993\n",
      "Epoch 5/10, Train Loss: 0.2153, Accuracy: 0.9213, F1 Micro: 0.7637, F1 Macro: 0.6406\n",
      "Epoch 6/10, Train Loss: 0.1797, Accuracy: 0.9182, F1 Micro: 0.7673, F1 Macro: 0.6492\n",
      "Epoch 7/10, Train Loss: 0.152, Accuracy: 0.9185, F1 Micro: 0.7639, F1 Macro: 0.6591\n",
      "Epoch 8/10, Train Loss: 0.13, Accuracy: 0.9241, F1 Micro: 0.7713, F1 Macro: 0.6702\n",
      "Epoch 9/10, Train Loss: 0.1118, Accuracy: 0.9207, F1 Micro: 0.7656, F1 Macro: 0.6745\n",
      "Epoch 10/10, Train Loss: 0.1, Accuracy: 0.92, F1 Micro: 0.7555, F1 Macro: 0.6786\n",
      "Best result for 6285 samples: F1 Micro: 0.7713\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1094\n",
      "      Abusive       0.90      0.88      0.89      1072\n",
      "HS_Individual       0.72      0.75      0.74       689\n",
      "     HS_Group       0.73      0.60      0.66       405\n",
      "  HS_Religion       0.73      0.65      0.69       124\n",
      "      HS_Race       0.82      0.75      0.79       125\n",
      "  HS_Physical       0.50      0.13      0.21        61\n",
      "    HS_Gender       0.61      0.29      0.40        58\n",
      "     HS_Other       0.78      0.75      0.76       754\n",
      "      HS_Weak       0.72      0.75      0.73       664\n",
      "  HS_Moderate       0.67      0.51      0.58       346\n",
      "    HS_Strong       0.80      0.73      0.76        84\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5476\n",
      "    macro avg       0.73      0.64      0.67      5476\n",
      " weighted avg       0.78      0.75      0.77      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.000534932519076392\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 58.79240131378174 seconds\n",
      "\n",
      "Fold 2 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6584 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4588, Accuracy: 0.8551, F1 Micro: 0.4378, F1 Macro: 0.168\n",
      "Epoch 2/10, Train Loss: 0.3606, Accuracy: 0.9018, F1 Micro: 0.6701, F1 Macro: 0.4581\n",
      "Epoch 3/10, Train Loss: 0.2974, Accuracy: 0.9109, F1 Micro: 0.7336, F1 Macro: 0.5714\n",
      "Epoch 4/10, Train Loss: 0.2542, Accuracy: 0.9201, F1 Micro: 0.7532, F1 Macro: 0.606\n",
      "Epoch 5/10, Train Loss: 0.209, Accuracy: 0.9213, F1 Micro: 0.7623, F1 Macro: 0.6195\n",
      "Epoch 6/10, Train Loss: 0.1765, Accuracy: 0.9225, F1 Micro: 0.7602, F1 Macro: 0.6426\n",
      "Epoch 7/10, Train Loss: 0.1462, Accuracy: 0.9219, F1 Micro: 0.7623, F1 Macro: 0.6476\n",
      "Epoch 8/10, Train Loss: 0.1247, Accuracy: 0.9213, F1 Micro: 0.766, F1 Macro: 0.663\n",
      "Epoch 9/10, Train Loss: 0.1076, Accuracy: 0.9208, F1 Micro: 0.7685, F1 Macro: 0.6882\n",
      "Epoch 10/10, Train Loss: 0.0959, Accuracy: 0.9211, F1 Micro: 0.7695, F1 Macro: 0.6903\n",
      "Best result for 6584 samples: F1 Micro: 0.7695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.84      1094\n",
      "      Abusive       0.87      0.91      0.89      1072\n",
      "HS_Individual       0.70      0.74      0.72       689\n",
      "     HS_Group       0.69      0.67      0.68       405\n",
      "  HS_Religion       0.68      0.67      0.67       124\n",
      "      HS_Race       0.80      0.75      0.78       125\n",
      "  HS_Physical       0.60      0.20      0.30        61\n",
      "    HS_Gender       0.66      0.47      0.55        58\n",
      "     HS_Other       0.76      0.78      0.77       754\n",
      "      HS_Weak       0.70      0.73      0.71       664\n",
      "  HS_Moderate       0.62      0.60      0.61       346\n",
      "    HS_Strong       0.78      0.77      0.78        84\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5476\n",
      "    macro avg       0.72      0.68      0.69      5476\n",
      " weighted avg       0.76      0.78      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0004642473941203207\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.34031128883362 seconds\n",
      "\n",
      "Fold 2 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6980 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4518, Accuracy: 0.8709, F1 Micro: 0.5506, F1 Macro: 0.2593\n",
      "Epoch 2/10, Train Loss: 0.3465, Accuracy: 0.9016, F1 Micro: 0.6554, F1 Macro: 0.4493\n",
      "Epoch 3/10, Train Loss: 0.2832, Accuracy: 0.9142, F1 Micro: 0.7277, F1 Macro: 0.5527\n",
      "Epoch 4/10, Train Loss: 0.2412, Accuracy: 0.919, F1 Micro: 0.7497, F1 Macro: 0.578\n",
      "Epoch 5/10, Train Loss: 0.2043, Accuracy: 0.9223, F1 Micro: 0.7579, F1 Macro: 0.6236\n",
      "Epoch 6/10, Train Loss: 0.1701, Accuracy: 0.9231, F1 Micro: 0.7699, F1 Macro: 0.6657\n",
      "Epoch 7/10, Train Loss: 0.1428, Accuracy: 0.921, F1 Micro: 0.7727, F1 Macro: 0.6736\n",
      "Epoch 8/10, Train Loss: 0.1181, Accuracy: 0.9222, F1 Micro: 0.7737, F1 Macro: 0.6735\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.9215, F1 Micro: 0.7681, F1 Macro: 0.6731\n",
      "Epoch 10/10, Train Loss: 0.0925, Accuracy: 0.9233, F1 Micro: 0.7655, F1 Macro: 0.6802\n",
      "Best result for 6980 samples: F1 Micro: 0.7737\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1094\n",
      "      Abusive       0.87      0.90      0.89      1072\n",
      "HS_Individual       0.69      0.79      0.73       689\n",
      "     HS_Group       0.73      0.62      0.67       405\n",
      "  HS_Religion       0.74      0.64      0.68       124\n",
      "      HS_Race       0.81      0.78      0.79       125\n",
      "  HS_Physical       0.64      0.11      0.19        61\n",
      "    HS_Gender       0.62      0.28      0.38        58\n",
      "     HS_Other       0.74      0.81      0.77       754\n",
      "      HS_Weak       0.69      0.77      0.73       664\n",
      "  HS_Moderate       0.66      0.55      0.60       346\n",
      "    HS_Strong       0.80      0.80      0.80        84\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5476\n",
      "    macro avg       0.73      0.66      0.67      5476\n",
      " weighted avg       0.76      0.78      0.77      5476\n",
      "  samples avg       0.44      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0002766952791716903\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 49.17172646522522 seconds\n",
      "\n",
      "Fold 2 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4486, Accuracy: 0.874, F1 Micro: 0.5642, F1 Macro: 0.26\n",
      "Epoch 2/10, Train Loss: 0.3389, Accuracy: 0.9055, F1 Micro: 0.6898, F1 Macro: 0.4795\n",
      "Epoch 3/10, Train Loss: 0.2757, Accuracy: 0.9148, F1 Micro: 0.739, F1 Macro: 0.5789\n",
      "Epoch 4/10, Train Loss: 0.2307, Accuracy: 0.921, F1 Micro: 0.7584, F1 Macro: 0.615\n",
      "Epoch 5/10, Train Loss: 0.1961, Accuracy: 0.923, F1 Micro: 0.767, F1 Macro: 0.6372\n",
      "Epoch 6/10, Train Loss: 0.1621, Accuracy: 0.9219, F1 Micro: 0.7695, F1 Macro: 0.6491\n",
      "Epoch 7/10, Train Loss: 0.1325, Accuracy: 0.9225, F1 Micro: 0.7653, F1 Macro: 0.6551\n",
      "Epoch 8/10, Train Loss: 0.1126, Accuracy: 0.9238, F1 Micro: 0.7695, F1 Macro: 0.6753\n",
      "Epoch 9/10, Train Loss: 0.0965, Accuracy: 0.9222, F1 Micro: 0.7572, F1 Macro: 0.6763\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9222, F1 Micro: 0.7715, F1 Macro: 0.7031\n",
      "Best result for 7336 samples: F1 Micro: 0.7715\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.88      0.91      0.90      1072\n",
      "HS_Individual       0.70      0.74      0.72       689\n",
      "     HS_Group       0.69      0.64      0.66       405\n",
      "  HS_Religion       0.71      0.68      0.69       124\n",
      "      HS_Race       0.78      0.82      0.80       125\n",
      "  HS_Physical       0.58      0.23      0.33        61\n",
      "    HS_Gender       0.70      0.53      0.61        58\n",
      "     HS_Other       0.77      0.74      0.75       754\n",
      "      HS_Weak       0.70      0.72      0.71       664\n",
      "  HS_Moderate       0.65      0.59      0.62       346\n",
      "    HS_Strong       0.80      0.81      0.80        84\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5476\n",
      "    macro avg       0.73      0.69      0.70      5476\n",
      " weighted avg       0.77      0.77      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0001305245677940548\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.4009428024292 seconds\n",
      "\n",
      "Fold 2 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7656 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4382, Accuracy: 0.8816, F1 Micro: 0.5853, F1 Macro: 0.2878\n",
      "Epoch 2/10, Train Loss: 0.3242, Accuracy: 0.9036, F1 Micro: 0.7066, F1 Macro: 0.5339\n",
      "Epoch 3/10, Train Loss: 0.263, Accuracy: 0.9145, F1 Micro: 0.7385, F1 Macro: 0.5576\n",
      "Epoch 4/10, Train Loss: 0.2226, Accuracy: 0.9196, F1 Micro: 0.7536, F1 Macro: 0.602\n",
      "Epoch 5/10, Train Loss: 0.1884, Accuracy: 0.922, F1 Micro: 0.77, F1 Macro: 0.6418\n",
      "Epoch 6/10, Train Loss: 0.1579, Accuracy: 0.924, F1 Micro: 0.7613, F1 Macro: 0.6538\n",
      "Epoch 7/10, Train Loss: 0.137, Accuracy: 0.9234, F1 Micro: 0.7678, F1 Macro: 0.672\n",
      "Epoch 8/10, Train Loss: 0.1109, Accuracy: 0.9225, F1 Micro: 0.7722, F1 Macro: 0.647\n",
      "Epoch 9/10, Train Loss: 0.0939, Accuracy: 0.9213, F1 Micro: 0.7643, F1 Macro: 0.6739\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.9239, F1 Micro: 0.7778, F1 Macro: 0.6891\n",
      "Best result for 7656 samples: F1 Micro: 0.7778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.90      0.89      0.90      1072\n",
      "HS_Individual       0.69      0.78      0.73       689\n",
      "     HS_Group       0.73      0.64      0.68       405\n",
      "  HS_Religion       0.74      0.65      0.69       124\n",
      "      HS_Race       0.82      0.75      0.79       125\n",
      "  HS_Physical       0.83      0.16      0.27        61\n",
      "    HS_Gender       0.66      0.36      0.47        58\n",
      "     HS_Other       0.75      0.81      0.78       754\n",
      "      HS_Weak       0.68      0.77      0.72       664\n",
      "  HS_Moderate       0.66      0.58      0.62       346\n",
      "    HS_Strong       0.78      0.79      0.78        84\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5476\n",
      "    macro avg       0.76      0.67      0.69      5476\n",
      " weighted avg       0.77      0.78      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 4.801309041795329e-05\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 40.36178231239319 seconds\n",
      "\n",
      "Fold 2 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7901 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4325, Accuracy: 0.8839, F1 Micro: 0.609, F1 Macro: 0.3023\n",
      "Epoch 2/10, Train Loss: 0.3175, Accuracy: 0.905, F1 Micro: 0.6867, F1 Macro: 0.4834\n",
      "Epoch 3/10, Train Loss: 0.2579, Accuracy: 0.9125, F1 Micro: 0.7111, F1 Macro: 0.548\n",
      "Epoch 4/10, Train Loss: 0.2163, Accuracy: 0.9206, F1 Micro: 0.7564, F1 Macro: 0.5943\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.9226, F1 Micro: 0.769, F1 Macro: 0.6237\n",
      "Epoch 6/10, Train Loss: 0.1509, Accuracy: 0.9227, F1 Micro: 0.7707, F1 Macro: 0.6625\n",
      "Epoch 7/10, Train Loss: 0.1274, Accuracy: 0.9227, F1 Micro: 0.7726, F1 Macro: 0.6601\n",
      "Epoch 8/10, Train Loss: 0.1094, Accuracy: 0.9217, F1 Micro: 0.7708, F1 Macro: 0.6721\n",
      "Epoch 9/10, Train Loss: 0.0919, Accuracy: 0.924, F1 Micro: 0.7702, F1 Macro: 0.6884\n",
      "Epoch 10/10, Train Loss: 0.0818, Accuracy: 0.9235, F1 Micro: 0.7626, F1 Macro: 0.6873\n",
      "Best result for 7901 samples: F1 Micro: 0.7726\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.88      0.90      0.89      1072\n",
      "HS_Individual       0.70      0.77      0.74       689\n",
      "     HS_Group       0.73      0.63      0.68       405\n",
      "  HS_Religion       0.75      0.65      0.69       124\n",
      "      HS_Race       0.86      0.63      0.73       125\n",
      "  HS_Physical       0.50      0.16      0.25        61\n",
      "    HS_Gender       0.50      0.14      0.22        58\n",
      "     HS_Other       0.75      0.79      0.77       754\n",
      "      HS_Weak       0.69      0.76      0.72       664\n",
      "  HS_Moderate       0.65      0.55      0.59       346\n",
      "    HS_Strong       0.80      0.80      0.80        84\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5476\n",
      "    macro avg       0.72      0.64      0.66      5476\n",
      " weighted avg       0.77      0.77      0.77      5476\n",
      "  samples avg       0.44      0.44      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 6.429191271308817e-05\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.72638773918152 seconds\n",
      "\n",
      "Fold 2 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4318, Accuracy: 0.8845, F1 Micro: 0.6, F1 Macro: 0.2957\n",
      "Epoch 2/10, Train Loss: 0.3077, Accuracy: 0.907, F1 Micro: 0.7174, F1 Macro: 0.5423\n",
      "Epoch 3/10, Train Loss: 0.2516, Accuracy: 0.915, F1 Micro: 0.7395, F1 Macro: 0.5796\n",
      "Epoch 4/10, Train Loss: 0.2107, Accuracy: 0.9196, F1 Micro: 0.7633, F1 Macro: 0.6212\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.9216, F1 Micro: 0.7565, F1 Macro: 0.6001\n",
      "Epoch 6/10, Train Loss: 0.1487, Accuracy: 0.9229, F1 Micro: 0.7757, F1 Macro: 0.6521\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.9237, F1 Micro: 0.7685, F1 Macro: 0.6641\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.9243, F1 Micro: 0.7757, F1 Macro: 0.6712\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9215, F1 Micro: 0.763, F1 Macro: 0.6735\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9227, F1 Micro: 0.7733, F1 Macro: 0.7036\n",
      "Best result for 8165 samples: F1 Micro: 0.7757\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1094\n",
      "      Abusive       0.92      0.88      0.90      1072\n",
      "HS_Individual       0.69      0.79      0.74       689\n",
      "     HS_Group       0.76      0.61      0.67       405\n",
      "  HS_Religion       0.75      0.65      0.70       124\n",
      "      HS_Race       0.81      0.74      0.77       125\n",
      "  HS_Physical       0.55      0.10      0.17        61\n",
      "    HS_Gender       0.62      0.28      0.38        58\n",
      "     HS_Other       0.75      0.80      0.77       754\n",
      "      HS_Weak       0.69      0.77      0.73       664\n",
      "  HS_Moderate       0.69      0.53      0.60       346\n",
      "    HS_Strong       0.79      0.77      0.78        84\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5476\n",
      "    macro avg       0.74      0.65      0.67      5476\n",
      " weighted avg       0.78      0.77      0.77      5476\n",
      "  samples avg       0.44      0.44      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.1788556841784155e-05\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 32.96320295333862 seconds\n",
      "\n",
      "Fold 2 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8402 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4253, Accuracy: 0.8853, F1 Micro: 0.5876, F1 Macro: 0.2938\n",
      "Epoch 2/10, Train Loss: 0.3055, Accuracy: 0.9049, F1 Micro: 0.7138, F1 Macro: 0.5009\n",
      "Epoch 3/10, Train Loss: 0.2467, Accuracy: 0.916, F1 Micro: 0.7232, F1 Macro: 0.5538\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.922, F1 Micro: 0.7592, F1 Macro: 0.609\n",
      "Epoch 5/10, Train Loss: 0.171, Accuracy: 0.9223, F1 Micro: 0.7705, F1 Macro: 0.6384\n",
      "Epoch 6/10, Train Loss: 0.1423, Accuracy: 0.9241, F1 Micro: 0.7718, F1 Macro: 0.6417\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.9217, F1 Micro: 0.7536, F1 Macro: 0.6508\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9246, F1 Micro: 0.7799, F1 Macro: 0.6861\n",
      "Epoch 9/10, Train Loss: 0.0887, Accuracy: 0.9232, F1 Micro: 0.777, F1 Macro: 0.6772\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.9235, F1 Micro: 0.7671, F1 Macro: 0.6939\n",
      "Best result for 8402 samples: F1 Micro: 0.7799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.89      0.91      0.90      1072\n",
      "HS_Individual       0.70      0.80      0.74       689\n",
      "     HS_Group       0.74      0.63      0.68       405\n",
      "  HS_Religion       0.70      0.66      0.68       124\n",
      "      HS_Race       0.86      0.71      0.78       125\n",
      "  HS_Physical       0.58      0.18      0.28        61\n",
      "    HS_Gender       0.62      0.31      0.41        58\n",
      "     HS_Other       0.76      0.78      0.77       754\n",
      "      HS_Weak       0.69      0.79      0.73       664\n",
      "  HS_Moderate       0.66      0.57      0.61       346\n",
      "    HS_Strong       0.82      0.77      0.80        84\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5476\n",
      "    macro avg       0.74      0.67      0.69      5476\n",
      " weighted avg       0.77      0.79      0.78      5476\n",
      "  samples avg       0.45      0.45      0.44      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.4946393750724376e-05\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 30.077919244766235 seconds\n",
      "\n",
      "Fold 2 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8616 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4192, Accuracy: 0.8878, F1 Micro: 0.5903, F1 Macro: 0.2901\n",
      "Epoch 2/10, Train Loss: 0.2948, Accuracy: 0.9042, F1 Micro: 0.697, F1 Macro: 0.4609\n",
      "Epoch 3/10, Train Loss: 0.2385, Accuracy: 0.915, F1 Micro: 0.7501, F1 Macro: 0.5767\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9222, F1 Micro: 0.7606, F1 Macro: 0.618\n",
      "Epoch 5/10, Train Loss: 0.1688, Accuracy: 0.9222, F1 Micro: 0.7532, F1 Macro: 0.6227\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.9237, F1 Micro: 0.7666, F1 Macro: 0.6325\n",
      "Epoch 7/10, Train Loss: 0.117, Accuracy: 0.9236, F1 Micro: 0.7726, F1 Macro: 0.6642\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9221, F1 Micro: 0.7655, F1 Macro: 0.6738\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9237, F1 Micro: 0.7766, F1 Macro: 0.6963\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9197, F1 Micro: 0.7716, F1 Macro: 0.6893\n",
      "Best result for 8616 samples: F1 Micro: 0.7766\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1094\n",
      "      Abusive       0.90      0.90      0.90      1072\n",
      "HS_Individual       0.70      0.77      0.73       689\n",
      "     HS_Group       0.71      0.63      0.67       405\n",
      "  HS_Religion       0.72      0.69      0.71       124\n",
      "      HS_Race       0.81      0.74      0.77       125\n",
      "  HS_Physical       0.42      0.25      0.31        61\n",
      "    HS_Gender       0.67      0.45      0.54        58\n",
      "     HS_Other       0.77      0.78      0.77       754\n",
      "      HS_Weak       0.69      0.76      0.72       664\n",
      "  HS_Moderate       0.63      0.55      0.59       346\n",
      "    HS_Strong       0.78      0.82      0.80        84\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5476\n",
      "    macro avg       0.72      0.68      0.70      5476\n",
      " weighted avg       0.77      0.78      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 8.794377754384186e-06\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.115222692489624 seconds\n",
      "\n",
      "Fold 2 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8816 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4186, Accuracy: 0.8853, F1 Micro: 0.5809, F1 Macro: 0.2821\n",
      "Epoch 2/10, Train Loss: 0.2908, Accuracy: 0.907, F1 Micro: 0.6926, F1 Macro: 0.4839\n",
      "Epoch 3/10, Train Loss: 0.2377, Accuracy: 0.9157, F1 Micro: 0.7337, F1 Macro: 0.5657\n",
      "Epoch 4/10, Train Loss: 0.204, Accuracy: 0.921, F1 Micro: 0.7649, F1 Macro: 0.6038\n",
      "Epoch 5/10, Train Loss: 0.1705, Accuracy: 0.9221, F1 Micro: 0.7645, F1 Macro: 0.6362\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9249, F1 Micro: 0.771, F1 Macro: 0.6571\n",
      "Epoch 7/10, Train Loss: 0.1152, Accuracy: 0.9221, F1 Micro: 0.7618, F1 Macro: 0.6694\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.9231, F1 Micro: 0.7725, F1 Macro: 0.6834\n",
      "Epoch 9/10, Train Loss: 0.0834, Accuracy: 0.9225, F1 Micro: 0.7631, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.924, F1 Micro: 0.777, F1 Macro: 0.6907\n",
      "Best result for 8816 samples: F1 Micro: 0.777\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.91      0.90      0.90      1072\n",
      "HS_Individual       0.72      0.73      0.72       689\n",
      "     HS_Group       0.68      0.69      0.68       405\n",
      "  HS_Religion       0.70      0.71      0.70       124\n",
      "      HS_Race       0.83      0.69      0.75       125\n",
      "  HS_Physical       0.77      0.16      0.27        61\n",
      "    HS_Gender       0.68      0.36      0.47        58\n",
      "     HS_Other       0.76      0.82      0.79       754\n",
      "      HS_Weak       0.71      0.70      0.71       664\n",
      "  HS_Moderate       0.61      0.64      0.63       346\n",
      "    HS_Strong       0.77      0.87      0.82        84\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5476\n",
      "    macro avg       0.75      0.68      0.69      5476\n",
      " weighted avg       0.77      0.78      0.77      5476\n",
      "  samples avg       0.44      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 3.8014255551388492e-06\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.500444650650024 seconds\n",
      "\n",
      "Fold 2 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4127, Accuracy: 0.8897, F1 Micro: 0.6248, F1 Macro: 0.3199\n",
      "Epoch 2/10, Train Loss: 0.2864, Accuracy: 0.9068, F1 Micro: 0.6921, F1 Macro: 0.5052\n",
      "Epoch 3/10, Train Loss: 0.2294, Accuracy: 0.915, F1 Micro: 0.7536, F1 Macro: 0.5887\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9216, F1 Micro: 0.7553, F1 Macro: 0.6007\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9239, F1 Micro: 0.7638, F1 Macro: 0.6572\n",
      "Epoch 6/10, Train Loss: 0.137, Accuracy: 0.923, F1 Micro: 0.7734, F1 Macro: 0.6539\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9241, F1 Micro: 0.7712, F1 Macro: 0.6786\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.923, F1 Micro: 0.7593, F1 Macro: 0.6701\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9234, F1 Micro: 0.7759, F1 Macro: 0.69\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9227, F1 Micro: 0.766, F1 Macro: 0.6905\n",
      "Best result for 9016 samples: F1 Micro: 0.7759\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1094\n",
      "      Abusive       0.90      0.90      0.90      1072\n",
      "HS_Individual       0.70      0.74      0.72       689\n",
      "     HS_Group       0.70      0.70      0.70       405\n",
      "  HS_Religion       0.72      0.71      0.71       124\n",
      "      HS_Race       0.84      0.75      0.79       125\n",
      "  HS_Physical       1.00      0.16      0.28        61\n",
      "    HS_Gender       0.66      0.33      0.44        58\n",
      "     HS_Other       0.75      0.80      0.77       754\n",
      "      HS_Weak       0.70      0.72      0.71       664\n",
      "  HS_Moderate       0.63      0.63      0.63       346\n",
      "    HS_Strong       0.78      0.79      0.78        84\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5476\n",
      "    macro avg       0.77      0.67      0.69      5476\n",
      " weighted avg       0.77      0.78      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 5.184574274608168e-06\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.78122639656067 seconds\n",
      "\n",
      "Fold 2 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9216 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4085, Accuracy: 0.889, F1 Micro: 0.6274, F1 Macro: 0.3172\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.9068, F1 Micro: 0.6913, F1 Macro: 0.4916\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.9185, F1 Micro: 0.752, F1 Macro: 0.5909\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9212, F1 Micro: 0.7604, F1 Macro: 0.6125\n",
      "Epoch 5/10, Train Loss: 0.166, Accuracy: 0.9206, F1 Micro: 0.7465, F1 Macro: 0.6087\n",
      "Epoch 6/10, Train Loss: 0.1361, Accuracy: 0.9216, F1 Micro: 0.7677, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.923, F1 Micro: 0.7675, F1 Macro: 0.6691\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.9239, F1 Micro: 0.769, F1 Macro: 0.6756\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.921, F1 Micro: 0.7662, F1 Macro: 0.6801\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9235, F1 Micro: 0.7726, F1 Macro: 0.703\n",
      "Best result for 9216 samples: F1 Micro: 0.7726\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1094\n",
      "      Abusive       0.91      0.89      0.90      1072\n",
      "HS_Individual       0.71      0.76      0.73       689\n",
      "     HS_Group       0.72      0.61      0.66       405\n",
      "  HS_Religion       0.72      0.65      0.68       124\n",
      "      HS_Race       0.80      0.78      0.79       125\n",
      "  HS_Physical       0.57      0.33      0.42        61\n",
      "    HS_Gender       0.73      0.47      0.57        58\n",
      "     HS_Other       0.78      0.74      0.76       754\n",
      "      HS_Weak       0.70      0.74      0.72       664\n",
      "  HS_Moderate       0.64      0.55      0.59       346\n",
      "    HS_Strong       0.76      0.79      0.77        84\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5476\n",
      "    macro avg       0.74      0.68      0.70      5476\n",
      " weighted avg       0.78      0.77      0.77      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.904752727772575e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.296575784683228 seconds\n",
      "\n",
      "Fold 2 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4076, Accuracy: 0.8898, F1 Micro: 0.6417, F1 Macro: 0.3401\n",
      "Epoch 2/10, Train Loss: 0.2821, Accuracy: 0.9083, F1 Micro: 0.7122, F1 Macro: 0.507\n",
      "Epoch 3/10, Train Loss: 0.2297, Accuracy: 0.9161, F1 Micro: 0.7299, F1 Macro: 0.5555\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9207, F1 Micro: 0.7567, F1 Macro: 0.6118\n",
      "Epoch 5/10, Train Loss: 0.1636, Accuracy: 0.9234, F1 Micro: 0.7693, F1 Macro: 0.6565\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.921, F1 Micro: 0.7569, F1 Macro: 0.64\n",
      "Epoch 7/10, Train Loss: 0.1129, Accuracy: 0.9239, F1 Micro: 0.7724, F1 Macro: 0.6781\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9218, F1 Micro: 0.7649, F1 Macro: 0.6871\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9231, F1 Micro: 0.7671, F1 Macro: 0.6789\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9235, F1 Micro: 0.7767, F1 Macro: 0.7038\n",
      "Best result for 9218 samples: F1 Micro: 0.7767\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1094\n",
      "      Abusive       0.88      0.92      0.90      1072\n",
      "HS_Individual       0.73      0.72      0.72       689\n",
      "     HS_Group       0.66      0.71      0.68       405\n",
      "  HS_Religion       0.70      0.65      0.68       124\n",
      "      HS_Race       0.78      0.77      0.77       125\n",
      "  HS_Physical       0.74      0.23      0.35        61\n",
      "    HS_Gender       0.71      0.52      0.60        58\n",
      "     HS_Other       0.75      0.80      0.78       754\n",
      "      HS_Weak       0.73      0.70      0.71       664\n",
      "  HS_Moderate       0.59      0.64      0.61       346\n",
      "    HS_Strong       0.78      0.80      0.79        84\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5476\n",
      "    macro avg       0.74      0.69      0.70      5476\n",
      " weighted avg       0.77      0.78      0.77      5476\n",
      "  samples avg       0.45      0.45      0.44      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 3.043686820092262e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 18.966331481933594 seconds\n",
      "\n",
      "Fold 2 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4023, Accuracy: 0.8895, F1 Micro: 0.6248, F1 Macro: 0.3266\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.9062, F1 Micro: 0.6853, F1 Macro: 0.4805\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.9138, F1 Micro: 0.7553, F1 Macro: 0.5986\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9206, F1 Micro: 0.762, F1 Macro: 0.6096\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9228, F1 Micro: 0.7715, F1 Macro: 0.6604\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9233, F1 Micro: 0.7743, F1 Macro: 0.6631\n",
      "Epoch 7/10, Train Loss: 0.11, Accuracy: 0.9199, F1 Micro: 0.7742, F1 Macro: 0.6587\n",
      "Epoch 8/10, Train Loss: 0.0924, Accuracy: 0.9248, F1 Micro: 0.777, F1 Macro: 0.6669\n",
      "Epoch 9/10, Train Loss: 0.0804, Accuracy: 0.9214, F1 Micro: 0.7698, F1 Macro: 0.6837\n",
      "Epoch 10/10, Train Loss: 0.0663, Accuracy: 0.9236, F1 Micro: 0.7751, F1 Macro: 0.6993\n",
      "Best result for 9418 samples: F1 Micro: 0.777\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1094\n",
      "      Abusive       0.91      0.89      0.90      1072\n",
      "HS_Individual       0.72      0.76      0.74       689\n",
      "     HS_Group       0.71      0.66      0.68       405\n",
      "  HS_Religion       0.75      0.67      0.71       124\n",
      "      HS_Race       0.83      0.72      0.77       125\n",
      "  HS_Physical       0.80      0.13      0.23        61\n",
      "    HS_Gender       0.67      0.14      0.23        58\n",
      "     HS_Other       0.75      0.80      0.77       754\n",
      "      HS_Weak       0.72      0.74      0.73       664\n",
      "  HS_Moderate       0.64      0.55      0.60       346\n",
      "    HS_Strong       0.79      0.82      0.81        84\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5476\n",
      "    macro avg       0.76      0.64      0.67      5476\n",
      " weighted avg       0.78      0.77      0.77      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.3907860344916115e-06\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.4566707611084 seconds\n",
      "\n",
      "Fold 2 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9618 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8898, F1 Micro: 0.6079, F1 Macro: 0.3058\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.9075, F1 Micro: 0.7134, F1 Macro: 0.5133\n",
      "Epoch 3/10, Train Loss: 0.2135, Accuracy: 0.9174, F1 Micro: 0.7541, F1 Macro: 0.5786\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9231, F1 Micro: 0.7635, F1 Macro: 0.6107\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9242, F1 Micro: 0.775, F1 Macro: 0.6484\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9218, F1 Micro: 0.767, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.9231, F1 Micro: 0.7773, F1 Macro: 0.6769\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.923, F1 Micro: 0.7748, F1 Macro: 0.6853\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6986\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9231, F1 Micro: 0.7666, F1 Macro: 0.687\n",
      "Best result for 9618 samples: F1 Micro: 0.7794\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1094\n",
      "      Abusive       0.90      0.90      0.90      1072\n",
      "HS_Individual       0.69      0.79      0.74       689\n",
      "     HS_Group       0.70      0.63      0.67       405\n",
      "  HS_Religion       0.70      0.74      0.72       124\n",
      "      HS_Race       0.82      0.74      0.78       125\n",
      "  HS_Physical       0.61      0.23      0.33        61\n",
      "    HS_Gender       0.69      0.38      0.49        58\n",
      "     HS_Other       0.76      0.80      0.78       754\n",
      "      HS_Weak       0.69      0.77      0.72       664\n",
      "  HS_Moderate       0.64      0.58      0.61       346\n",
      "    HS_Strong       0.81      0.80      0.80        84\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5476\n",
      "    macro avg       0.74      0.69      0.70      5476\n",
      " weighted avg       0.77      0.79      0.78      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 2.711811748667969e-06\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.74655032157898 seconds\n",
      "\n",
      "Fold 2 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9818 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.891, F1 Micro: 0.6465, F1 Macro: 0.3389\n",
      "Epoch 2/10, Train Loss: 0.2644, Accuracy: 0.9092, F1 Micro: 0.7117, F1 Macro: 0.5039\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9189, F1 Micro: 0.756, F1 Macro: 0.5905\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9215, F1 Micro: 0.7639, F1 Macro: 0.6091\n",
      "Epoch 5/10, Train Loss: 0.1535, Accuracy: 0.9176, F1 Micro: 0.7678, F1 Macro: 0.637\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.922, F1 Micro: 0.7545, F1 Macro: 0.6351\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9227, F1 Micro: 0.7689, F1 Macro: 0.6607\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9225, F1 Micro: 0.77, F1 Macro: 0.6829\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9238, F1 Micro: 0.773, F1 Macro: 0.6825\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9233, F1 Micro: 0.7715, F1 Macro: 0.6988\n",
      "Best result for 9818 samples: F1 Micro: 0.773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1094\n",
      "      Abusive       0.91      0.90      0.90      1072\n",
      "HS_Individual       0.70      0.75      0.73       689\n",
      "     HS_Group       0.71      0.60      0.65       405\n",
      "  HS_Religion       0.74      0.66      0.70       124\n",
      "      HS_Race       0.85      0.70      0.77       125\n",
      "  HS_Physical       0.75      0.20      0.31        61\n",
      "    HS_Gender       0.65      0.34      0.45        58\n",
      "     HS_Other       0.78      0.78      0.78       754\n",
      "      HS_Weak       0.69      0.73      0.71       664\n",
      "  HS_Moderate       0.65      0.54      0.59       346\n",
      "    HS_Strong       0.81      0.71      0.76        84\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5476\n",
      "    macro avg       0.76      0.65      0.68      5476\n",
      " weighted avg       0.78      0.76      0.77      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 2.247102338515105e-06\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.000322341918945 seconds\n",
      "\n",
      "Fold 2 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10018 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3894, Accuracy: 0.8867, F1 Micro: 0.6493, F1 Macro: 0.3408\n",
      "Epoch 2/10, Train Loss: 0.2622, Accuracy: 0.9076, F1 Micro: 0.6949, F1 Macro: 0.4814\n",
      "Epoch 3/10, Train Loss: 0.2113, Accuracy: 0.9167, F1 Micro: 0.7423, F1 Macro: 0.5823\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.9199, F1 Micro: 0.7547, F1 Macro: 0.5806\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.924, F1 Micro: 0.7674, F1 Macro: 0.6449\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.922, F1 Micro: 0.7757, F1 Macro: 0.6668\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9236, F1 Micro: 0.761, F1 Macro: 0.6619\n",
      "Epoch 8/10, Train Loss: 0.0878, Accuracy: 0.9244, F1 Micro: 0.7708, F1 Macro: 0.684\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9249, F1 Micro: 0.7665, F1 Macro: 0.6755\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9242, F1 Micro: 0.7738, F1 Macro: 0.6969\n",
      "Best result for 10018 samples: F1 Micro: 0.7757\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.84      1094\n",
      "      Abusive       0.89      0.91      0.90      1072\n",
      "HS_Individual       0.69      0.80      0.74       689\n",
      "     HS_Group       0.72      0.65      0.69       405\n",
      "  HS_Religion       0.72      0.63      0.67       124\n",
      "      HS_Race       0.82      0.76      0.79       125\n",
      "  HS_Physical       0.47      0.15      0.23        61\n",
      "    HS_Gender       0.55      0.21      0.30        58\n",
      "     HS_Other       0.73      0.81      0.77       754\n",
      "      HS_Weak       0.67      0.78      0.72       664\n",
      "  HS_Moderate       0.66      0.56      0.60       346\n",
      "    HS_Strong       0.80      0.71      0.75        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5476\n",
      "    macro avg       0.71      0.66      0.67      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 6.2842998886480934e-06\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.110292911529541 seconds\n",
      "\n",
      "Fold 2 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3821, Accuracy: 0.8885, F1 Micro: 0.5941, F1 Macro: 0.2976\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.9038, F1 Micro: 0.7106, F1 Macro: 0.4841\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.9153, F1 Micro: 0.7199, F1 Macro: 0.5516\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9196, F1 Micro: 0.7585, F1 Macro: 0.6021\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.9224, F1 Micro: 0.7647, F1 Macro: 0.6465\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9237, F1 Micro: 0.7767, F1 Macro: 0.6522\n",
      "Epoch 7/10, Train Loss: 0.0993, Accuracy: 0.9245, F1 Micro: 0.7765, F1 Macro: 0.6817\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.922, F1 Micro: 0.7731, F1 Macro: 0.6885\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9228, F1 Micro: 0.7774, F1 Macro: 0.699\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9227, F1 Micro: 0.7635, F1 Macro: 0.6926\n",
      "Best result for 10218 samples: F1 Micro: 0.7774\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1094\n",
      "      Abusive       0.90      0.91      0.90      1072\n",
      "HS_Individual       0.67      0.82      0.74       689\n",
      "     HS_Group       0.75      0.58      0.66       405\n",
      "  HS_Religion       0.67      0.74      0.70       124\n",
      "      HS_Race       0.81      0.72      0.76       125\n",
      "  HS_Physical       0.59      0.26      0.36        61\n",
      "    HS_Gender       0.64      0.43      0.52        58\n",
      "     HS_Other       0.75      0.80      0.77       754\n",
      "      HS_Weak       0.66      0.80      0.73       664\n",
      "  HS_Moderate       0.70      0.52      0.60       346\n",
      "    HS_Strong       0.79      0.82      0.81        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5476\n",
      "    macro avg       0.73      0.69      0.70      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.8595824485601044e-06\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.388010501861572 seconds\n",
      "\n",
      "Fold 2 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3744, Accuracy: 0.8871, F1 Micro: 0.5918, F1 Macro: 0.2911\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9099, F1 Micro: 0.7148, F1 Macro: 0.5117\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9169, F1 Micro: 0.7519, F1 Macro: 0.5828\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9202, F1 Micro: 0.767, F1 Macro: 0.6169\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9235, F1 Micro: 0.779, F1 Macro: 0.661\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9223, F1 Micro: 0.7704, F1 Macro: 0.6483\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9244, F1 Micro: 0.7728, F1 Macro: 0.6808\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.9221, F1 Micro: 0.7708, F1 Macro: 0.6813\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9233, F1 Micro: 0.7756, F1 Macro: 0.6889\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9232, F1 Micro: 0.765, F1 Macro: 0.6885\n",
      "Best result for 10418 samples: F1 Micro: 0.779\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.91      0.85      1094\n",
      "      Abusive       0.90      0.89      0.90      1072\n",
      "HS_Individual       0.71      0.79      0.75       689\n",
      "     HS_Group       0.69      0.69      0.69       405\n",
      "  HS_Religion       0.68      0.69      0.68       124\n",
      "      HS_Race       0.75      0.78      0.77       125\n",
      "  HS_Physical       0.67      0.07      0.12        61\n",
      "    HS_Gender       0.67      0.17      0.27        58\n",
      "     HS_Other       0.74      0.82      0.78       754\n",
      "      HS_Weak       0.70      0.75      0.72       664\n",
      "  HS_Moderate       0.64      0.58      0.61       346\n",
      "    HS_Strong       0.76      0.85      0.80        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5476\n",
      "    macro avg       0.73      0.66      0.66      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 6.762006069038762e-06\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.8883626461029053 seconds\n",
      "\n",
      "Fold 2 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3763, Accuracy: 0.8905, F1 Micro: 0.6177, F1 Macro: 0.3477\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9097, F1 Micro: 0.7192, F1 Macro: 0.5096\n",
      "Epoch 3/10, Train Loss: 0.2008, Accuracy: 0.9181, F1 Micro: 0.7531, F1 Macro: 0.591\n",
      "Epoch 4/10, Train Loss: 0.1656, Accuracy: 0.9211, F1 Micro: 0.7646, F1 Macro: 0.6244\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9219, F1 Micro: 0.7663, F1 Macro: 0.6567\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9237, F1 Micro: 0.7718, F1 Macro: 0.6758\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9233, F1 Micro: 0.7617, F1 Macro: 0.6715\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9226, F1 Micro: 0.774, F1 Macro: 0.6889\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9205, F1 Micro: 0.7753, F1 Macro: 0.6928\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9244, F1 Micro: 0.7668, F1 Macro: 0.683\n",
      "Best result for 10535 samples: F1 Micro: 0.7753\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.84      1094\n",
      "      Abusive       0.89      0.92      0.90      1072\n",
      "HS_Individual       0.65      0.81      0.72       689\n",
      "     HS_Group       0.71      0.63      0.67       405\n",
      "  HS_Religion       0.74      0.72      0.73       124\n",
      "      HS_Race       0.81      0.78      0.80       125\n",
      "  HS_Physical       0.58      0.18      0.28        61\n",
      "    HS_Gender       0.66      0.36      0.47        58\n",
      "     HS_Other       0.72      0.85      0.78       754\n",
      "      HS_Weak       0.64      0.79      0.71       664\n",
      "  HS_Moderate       0.65      0.53      0.58       346\n",
      "    HS_Strong       0.79      0.89      0.84        84\n",
      "\n",
      "    micro avg       0.75      0.81      0.78      5476\n",
      "    macro avg       0.72      0.70      0.69      5476\n",
      " weighted avg       0.75      0.81      0.77      5476\n",
      "  samples avg       0.45      0.46      0.44      5476\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 6618.78 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 658 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5882, Accuracy: 0.8199, F1 Micro: 0.3218, F1 Macro: 0.1158\n",
      "Epoch 2/10, Train Loss: 0.4613, Accuracy: 0.8328, F1 Micro: 0.1359, F1 Macro: 0.0419\n",
      "Epoch 3/10, Train Loss: 0.4093, Accuracy: 0.8371, F1 Micro: 0.1625, F1 Macro: 0.0606\n",
      "Epoch 4/10, Train Loss: 0.3943, Accuracy: 0.8407, F1 Micro: 0.2002, F1 Macro: 0.0725\n",
      "Epoch 5/10, Train Loss: 0.3792, Accuracy: 0.851, F1 Micro: 0.3263, F1 Macro: 0.1049\n",
      "Epoch 6/10, Train Loss: 0.362, Accuracy: 0.8581, F1 Micro: 0.3855, F1 Macro: 0.1378\n",
      "Epoch 7/10, Train Loss: 0.3352, Accuracy: 0.8715, F1 Micro: 0.5616, F1 Macro: 0.2467\n",
      "Epoch 8/10, Train Loss: 0.31, Accuracy: 0.8726, F1 Micro: 0.5138, F1 Macro: 0.2394\n",
      "Epoch 9/10, Train Loss: 0.2875, Accuracy: 0.8785, F1 Micro: 0.6054, F1 Macro: 0.3035\n",
      "Epoch 10/10, Train Loss: 0.2688, Accuracy: 0.8795, F1 Micro: 0.6353, F1 Macro: 0.3581\n",
      "Best result for 658 samples: F1 Micro: 0.6353\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.82      0.78      1142\n",
      "      Abusive       0.78      0.81      0.79      1026\n",
      "HS_Individual       0.60      0.64      0.62       723\n",
      "     HS_Group       0.57      0.38      0.46       419\n",
      "  HS_Religion       0.00      0.00      0.00       177\n",
      "      HS_Race       1.00      0.02      0.03       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.61      0.71      0.66       746\n",
      "      HS_Weak       0.59      0.57      0.58       685\n",
      "  HS_Moderate       0.58      0.20      0.30       352\n",
      "    HS_Strong       1.00      0.04      0.07       105\n",
      "\n",
      "    micro avg       0.67      0.60      0.64      5634\n",
      "    macro avg       0.54      0.35      0.36      5634\n",
      " weighted avg       0.64      0.60      0.59      5634\n",
      "  samples avg       0.38      0.34      0.33      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0019392259884625672\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 133.9344494342804 seconds\n",
      "\n",
      "Fold 3 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1646 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.53, Accuracy: 0.8325, F1 Micro: 0.3371, F1 Macro: 0.0934\n",
      "Epoch 2/10, Train Loss: 0.4302, Accuracy: 0.8365, F1 Micro: 0.3977, F1 Macro: 0.1096\n",
      "Epoch 3/10, Train Loss: 0.4087, Accuracy: 0.8469, F1 Micro: 0.4411, F1 Macro: 0.1394\n",
      "Epoch 4/10, Train Loss: 0.3852, Accuracy: 0.869, F1 Micro: 0.5313, F1 Macro: 0.2542\n",
      "Epoch 5/10, Train Loss: 0.3544, Accuracy: 0.8879, F1 Micro: 0.63, F1 Macro: 0.3685\n",
      "Epoch 6/10, Train Loss: 0.3047, Accuracy: 0.8925, F1 Micro: 0.6808, F1 Macro: 0.4575\n",
      "Epoch 7/10, Train Loss: 0.2714, Accuracy: 0.8952, F1 Micro: 0.6811, F1 Macro: 0.4889\n",
      "Epoch 8/10, Train Loss: 0.2355, Accuracy: 0.8999, F1 Micro: 0.7003, F1 Macro: 0.5036\n",
      "Epoch 9/10, Train Loss: 0.2126, Accuracy: 0.9003, F1 Micro: 0.7061, F1 Macro: 0.4818\n",
      "Epoch 10/10, Train Loss: 0.1861, Accuracy: 0.9022, F1 Micro: 0.7083, F1 Macro: 0.5176\n",
      "Best result for 1646 samples: F1 Micro: 0.7083\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1142\n",
      "      Abusive       0.87      0.80      0.84      1026\n",
      "HS_Individual       0.68      0.70      0.69       723\n",
      "     HS_Group       0.66      0.58      0.62       419\n",
      "  HS_Religion       0.81      0.27      0.41       177\n",
      "      HS_Race       0.86      0.46      0.60       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.67      0.75      0.71       746\n",
      "      HS_Weak       0.66      0.67      0.67       685\n",
      "  HS_Moderate       0.55      0.47      0.51       352\n",
      "    HS_Strong       0.88      0.22      0.35       105\n",
      "\n",
      "    micro avg       0.74      0.68      0.71      5634\n",
      "    macro avg       0.62      0.48      0.52      5634\n",
      " weighted avg       0.73      0.68      0.69      5634\n",
      "  samples avg       0.39      0.37      0.36      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.004147285688668487\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 120.8562741279602 seconds\n",
      "\n",
      "Fold 3 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.503, Accuracy: 0.8225, F1 Micro: 0.407, F1 Macro: 0.1076\n",
      "Epoch 2/10, Train Loss: 0.4167, Accuracy: 0.8347, F1 Micro: 0.4312, F1 Macro: 0.148\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8659, F1 Micro: 0.5262, F1 Macro: 0.2642\n",
      "Epoch 4/10, Train Loss: 0.3558, Accuracy: 0.8931, F1 Micro: 0.6364, F1 Macro: 0.4178\n",
      "Epoch 5/10, Train Loss: 0.3093, Accuracy: 0.8991, F1 Micro: 0.7144, F1 Macro: 0.4995\n",
      "Epoch 6/10, Train Loss: 0.2737, Accuracy: 0.9053, F1 Micro: 0.7048, F1 Macro: 0.4965\n",
      "Epoch 7/10, Train Loss: 0.2371, Accuracy: 0.9078, F1 Micro: 0.7232, F1 Macro: 0.5308\n",
      "Epoch 8/10, Train Loss: 0.1999, Accuracy: 0.9076, F1 Micro: 0.7232, F1 Macro: 0.559\n",
      "Epoch 9/10, Train Loss: 0.1812, Accuracy: 0.9083, F1 Micro: 0.7271, F1 Macro: 0.5547\n",
      "Epoch 10/10, Train Loss: 0.1452, Accuracy: 0.9072, F1 Micro: 0.7375, F1 Macro: 0.5615\n",
      "Best result for 2535 samples: F1 Micro: 0.7375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1142\n",
      "      Abusive       0.86      0.89      0.87      1026\n",
      "HS_Individual       0.64      0.79      0.71       723\n",
      "     HS_Group       0.70      0.56      0.63       419\n",
      "  HS_Religion       0.76      0.44      0.56       177\n",
      "      HS_Race       0.83      0.63      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.68      0.82      0.74       746\n",
      "      HS_Weak       0.62      0.76      0.68       685\n",
      "  HS_Moderate       0.61      0.49      0.54       352\n",
      "    HS_Strong       0.82      0.30      0.44       105\n",
      "\n",
      "    micro avg       0.73      0.75      0.74      5634\n",
      "    macro avg       0.61      0.55      0.56      5634\n",
      " weighted avg       0.72      0.75      0.72      5634\n",
      "  samples avg       0.43      0.42      0.41      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.004568855417892342\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 109.28515720367432 seconds\n",
      "\n",
      "Fold 3 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3335 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4795, Accuracy: 0.8281, F1 Micro: 0.4032, F1 Macro: 0.1085\n",
      "Epoch 2/10, Train Loss: 0.4108, Accuracy: 0.8505, F1 Micro: 0.4426, F1 Macro: 0.1435\n",
      "Epoch 3/10, Train Loss: 0.3752, Accuracy: 0.8868, F1 Micro: 0.5983, F1 Macro: 0.356\n",
      "Epoch 4/10, Train Loss: 0.3295, Accuracy: 0.9021, F1 Micro: 0.7108, F1 Macro: 0.5027\n",
      "Epoch 5/10, Train Loss: 0.286, Accuracy: 0.9097, F1 Micro: 0.7213, F1 Macro: 0.5248\n",
      "Epoch 6/10, Train Loss: 0.2422, Accuracy: 0.9083, F1 Micro: 0.7454, F1 Macro: 0.5721\n",
      "Epoch 7/10, Train Loss: 0.2049, Accuracy: 0.9138, F1 Micro: 0.738, F1 Macro: 0.5688\n",
      "Epoch 8/10, Train Loss: 0.1747, Accuracy: 0.9107, F1 Micro: 0.7471, F1 Macro: 0.5772\n",
      "Epoch 9/10, Train Loss: 0.1471, Accuracy: 0.9118, F1 Micro: 0.7486, F1 Macro: 0.5998\n",
      "Epoch 10/10, Train Loss: 0.1265, Accuracy: 0.9151, F1 Micro: 0.7547, F1 Macro: 0.6122\n",
      "Best result for 3335 samples: F1 Micro: 0.7547\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1142\n",
      "      Abusive       0.88      0.88      0.88      1026\n",
      "HS_Individual       0.69      0.75      0.72       723\n",
      "     HS_Group       0.71      0.64      0.67       419\n",
      "  HS_Religion       0.78      0.59      0.67       177\n",
      "      HS_Race       0.87      0.66      0.75       119\n",
      "  HS_Physical       1.00      0.01      0.02        80\n",
      "    HS_Gender       0.64      0.12      0.20        60\n",
      "     HS_Other       0.73      0.79      0.76       746\n",
      "      HS_Weak       0.67      0.72      0.69       685\n",
      "  HS_Moderate       0.62      0.56      0.59       352\n",
      "    HS_Strong       0.81      0.42      0.55       105\n",
      "\n",
      "    micro avg       0.76      0.75      0.75      5634\n",
      "    macro avg       0.77      0.58      0.61      5634\n",
      " weighted avg       0.77      0.75      0.75      5634\n",
      "  samples avg       0.43      0.42      0.41      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.00409501157701016\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 98.32071304321289 seconds\n",
      "\n",
      "Fold 3 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4055 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4769, Accuracy: 0.824, F1 Micro: 0.414, F1 Macro: 0.1093\n",
      "Epoch 2/10, Train Loss: 0.4047, Accuracy: 0.8632, F1 Micro: 0.5027, F1 Macro: 0.24\n",
      "Epoch 3/10, Train Loss: 0.3615, Accuracy: 0.8996, F1 Micro: 0.6689, F1 Macro: 0.4148\n",
      "Epoch 4/10, Train Loss: 0.3178, Accuracy: 0.908, F1 Micro: 0.6998, F1 Macro: 0.4965\n",
      "Epoch 5/10, Train Loss: 0.2721, Accuracy: 0.9123, F1 Micro: 0.7427, F1 Macro: 0.5673\n",
      "Epoch 6/10, Train Loss: 0.2322, Accuracy: 0.9157, F1 Micro: 0.7536, F1 Macro: 0.5887\n",
      "Epoch 7/10, Train Loss: 0.1931, Accuracy: 0.9156, F1 Micro: 0.7483, F1 Macro: 0.5728\n",
      "Epoch 8/10, Train Loss: 0.1687, Accuracy: 0.9182, F1 Micro: 0.7593, F1 Macro: 0.6081\n",
      "Epoch 9/10, Train Loss: 0.1401, Accuracy: 0.9153, F1 Micro: 0.7589, F1 Macro: 0.6115\n",
      "Epoch 10/10, Train Loss: 0.1217, Accuracy: 0.9153, F1 Micro: 0.761, F1 Macro: 0.6141\n",
      "Best result for 4055 samples: F1 Micro: 0.761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1142\n",
      "      Abusive       0.85      0.93      0.89      1026\n",
      "HS_Individual       0.69      0.74      0.72       723\n",
      "     HS_Group       0.70      0.67      0.69       419\n",
      "  HS_Religion       0.75      0.58      0.65       177\n",
      "      HS_Race       0.79      0.66      0.72       119\n",
      "  HS_Physical       0.67      0.03      0.05        80\n",
      "    HS_Gender       1.00      0.07      0.12        60\n",
      "     HS_Other       0.72      0.82      0.77       746\n",
      "      HS_Weak       0.66      0.72      0.69       685\n",
      "  HS_Moderate       0.61      0.60      0.61       352\n",
      "    HS_Strong       0.79      0.50      0.62       105\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5634\n",
      "    macro avg       0.75      0.60      0.61      5634\n",
      " weighted avg       0.75      0.77      0.75      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0025710938032716523\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 88.2697081565857 seconds\n",
      "\n",
      "Fold 3 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4703 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4711, Accuracy: 0.8343, F1 Micro: 0.4053, F1 Macro: 0.1111\n",
      "Epoch 2/10, Train Loss: 0.3996, Accuracy: 0.8846, F1 Micro: 0.6022, F1 Macro: 0.3439\n",
      "Epoch 3/10, Train Loss: 0.3454, Accuracy: 0.9045, F1 Micro: 0.7154, F1 Macro: 0.5209\n",
      "Epoch 4/10, Train Loss: 0.3029, Accuracy: 0.9106, F1 Micro: 0.7418, F1 Macro: 0.561\n",
      "Epoch 5/10, Train Loss: 0.2563, Accuracy: 0.9117, F1 Micro: 0.7527, F1 Macro: 0.5806\n",
      "Epoch 6/10, Train Loss: 0.2154, Accuracy: 0.9197, F1 Micro: 0.7629, F1 Macro: 0.6016\n",
      "Epoch 7/10, Train Loss: 0.1803, Accuracy: 0.9196, F1 Micro: 0.7622, F1 Macro: 0.6122\n",
      "Epoch 8/10, Train Loss: 0.1547, Accuracy: 0.9213, F1 Micro: 0.7724, F1 Macro: 0.631\n",
      "Epoch 9/10, Train Loss: 0.1289, Accuracy: 0.9135, F1 Micro: 0.7606, F1 Macro: 0.6335\n",
      "Epoch 10/10, Train Loss: 0.115, Accuracy: 0.9194, F1 Micro: 0.7637, F1 Macro: 0.6451\n",
      "Best result for 4703 samples: F1 Micro: 0.7724\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.89      0.89      0.89      1026\n",
      "HS_Individual       0.73      0.76      0.74       723\n",
      "     HS_Group       0.74      0.68      0.71       419\n",
      "  HS_Religion       0.73      0.61      0.66       177\n",
      "      HS_Race       0.85      0.67      0.75       119\n",
      "  HS_Physical       1.00      0.03      0.05        80\n",
      "    HS_Gender       1.00      0.08      0.15        60\n",
      "     HS_Other       0.74      0.80      0.77       746\n",
      "      HS_Weak       0.70      0.73      0.72       685\n",
      "  HS_Moderate       0.64      0.62      0.63       352\n",
      "    HS_Strong       0.80      0.53      0.64       105\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5634\n",
      "    macro avg       0.81      0.61      0.63      5634\n",
      " weighted avg       0.79      0.76      0.76      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0013533673365600436\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 79.07465648651123 seconds\n",
      "\n",
      "Fold 3 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5287 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4629, Accuracy: 0.837, F1 Micro: 0.4243, F1 Macro: 0.1288\n",
      "Epoch 2/10, Train Loss: 0.3878, Accuracy: 0.8929, F1 Micro: 0.6343, F1 Macro: 0.4188\n",
      "Epoch 3/10, Train Loss: 0.3323, Accuracy: 0.9103, F1 Micro: 0.727, F1 Macro: 0.5435\n",
      "Epoch 4/10, Train Loss: 0.2835, Accuracy: 0.9165, F1 Micro: 0.7481, F1 Macro: 0.5765\n",
      "Epoch 5/10, Train Loss: 0.2375, Accuracy: 0.9189, F1 Micro: 0.7587, F1 Macro: 0.602\n",
      "Epoch 6/10, Train Loss: 0.1985, Accuracy: 0.9179, F1 Micro: 0.7586, F1 Macro: 0.6126\n",
      "Epoch 7/10, Train Loss: 0.1772, Accuracy: 0.9203, F1 Micro: 0.7703, F1 Macro: 0.6283\n",
      "Epoch 8/10, Train Loss: 0.1388, Accuracy: 0.9191, F1 Micro: 0.7678, F1 Macro: 0.646\n",
      "Epoch 9/10, Train Loss: 0.1238, Accuracy: 0.9225, F1 Micro: 0.768, F1 Macro: 0.6321\n",
      "Epoch 10/10, Train Loss: 0.1074, Accuracy: 0.9182, F1 Micro: 0.7687, F1 Macro: 0.6469\n",
      "Best result for 5287 samples: F1 Micro: 0.7703\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.91      0.85      0.88      1026\n",
      "HS_Individual       0.70      0.79      0.74       723\n",
      "     HS_Group       0.75      0.63      0.69       419\n",
      "  HS_Religion       0.77      0.63      0.70       177\n",
      "      HS_Race       0.84      0.70      0.76       119\n",
      "  HS_Physical       0.67      0.03      0.05        80\n",
      "    HS_Gender       1.00      0.03      0.06        60\n",
      "     HS_Other       0.73      0.82      0.77       746\n",
      "      HS_Weak       0.68      0.76      0.72       685\n",
      "  HS_Moderate       0.67      0.57      0.61       352\n",
      "    HS_Strong       0.77      0.64      0.70       105\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5634\n",
      "    macro avg       0.78      0.61      0.63      5634\n",
      " weighted avg       0.78      0.76      0.76      5634\n",
      "  samples avg       0.43      0.42      0.41      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0008962064108345658\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 71.91703653335571 seconds\n",
      "\n",
      "Fold 3 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5812 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4607, Accuracy: 0.8413, F1 Micro: 0.4342, F1 Macro: 0.1473\n",
      "Epoch 2/10, Train Loss: 0.372, Accuracy: 0.9016, F1 Micro: 0.6874, F1 Macro: 0.4871\n",
      "Epoch 3/10, Train Loss: 0.3173, Accuracy: 0.9131, F1 Micro: 0.7387, F1 Macro: 0.5724\n",
      "Epoch 4/10, Train Loss: 0.2677, Accuracy: 0.9191, F1 Micro: 0.759, F1 Macro: 0.6002\n",
      "Epoch 5/10, Train Loss: 0.231, Accuracy: 0.9213, F1 Micro: 0.7698, F1 Macro: 0.619\n",
      "Epoch 6/10, Train Loss: 0.1954, Accuracy: 0.9211, F1 Micro: 0.773, F1 Macro: 0.6196\n",
      "Epoch 7/10, Train Loss: 0.165, Accuracy: 0.9237, F1 Micro: 0.7777, F1 Macro: 0.6435\n",
      "Epoch 8/10, Train Loss: 0.1339, Accuracy: 0.9239, F1 Micro: 0.7741, F1 Macro: 0.6444\n",
      "Epoch 9/10, Train Loss: 0.1135, Accuracy: 0.923, F1 Micro: 0.7785, F1 Macro: 0.6566\n",
      "Epoch 10/10, Train Loss: 0.1047, Accuracy: 0.923, F1 Micro: 0.78, F1 Macro: 0.671\n",
      "Best result for 5812 samples: F1 Micro: 0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.87      0.93      0.90      1026\n",
      "HS_Individual       0.77      0.70      0.73       723\n",
      "     HS_Group       0.68      0.78      0.72       419\n",
      "  HS_Religion       0.72      0.68      0.70       177\n",
      "      HS_Race       0.82      0.76      0.79       119\n",
      "  HS_Physical       0.73      0.10      0.18        80\n",
      "    HS_Gender       0.61      0.18      0.28        60\n",
      "     HS_Other       0.75      0.80      0.78       746\n",
      "      HS_Weak       0.74      0.67      0.71       685\n",
      "  HS_Moderate       0.60      0.73      0.66       352\n",
      "    HS_Strong       0.84      0.69      0.75       105\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5634\n",
      "    macro avg       0.75      0.66      0.67      5634\n",
      " weighted avg       0.78      0.78      0.77      5634\n",
      "  samples avg       0.45      0.45      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0008874394814483824\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 65.01222586631775 seconds\n",
      "\n",
      "Fold 3 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6285 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4553, Accuracy: 0.8566, F1 Micro: 0.5103, F1 Macro: 0.22\n",
      "Epoch 2/10, Train Loss: 0.3622, Accuracy: 0.9043, F1 Micro: 0.6876, F1 Macro: 0.5004\n",
      "Epoch 3/10, Train Loss: 0.3036, Accuracy: 0.9149, F1 Micro: 0.7456, F1 Macro: 0.5714\n",
      "Epoch 4/10, Train Loss: 0.2586, Accuracy: 0.9208, F1 Micro: 0.7679, F1 Macro: 0.6205\n",
      "Epoch 5/10, Train Loss: 0.2105, Accuracy: 0.9191, F1 Micro: 0.7704, F1 Macro: 0.6228\n",
      "Epoch 6/10, Train Loss: 0.1833, Accuracy: 0.9222, F1 Micro: 0.7713, F1 Macro: 0.6341\n",
      "Epoch 7/10, Train Loss: 0.1544, Accuracy: 0.9225, F1 Micro: 0.7772, F1 Macro: 0.6522\n",
      "Epoch 8/10, Train Loss: 0.1308, Accuracy: 0.924, F1 Micro: 0.7767, F1 Macro: 0.6616\n",
      "Epoch 9/10, Train Loss: 0.1087, Accuracy: 0.9242, F1 Micro: 0.7824, F1 Macro: 0.6722\n",
      "Epoch 10/10, Train Loss: 0.0931, Accuracy: 0.9268, F1 Micro: 0.7819, F1 Macro: 0.6798\n",
      "Best result for 6285 samples: F1 Micro: 0.7824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.90      0.91      0.90      1026\n",
      "HS_Individual       0.75      0.74      0.74       723\n",
      "     HS_Group       0.70      0.73      0.71       419\n",
      "  HS_Religion       0.77      0.62      0.69       177\n",
      "      HS_Race       0.80      0.73      0.76       119\n",
      "  HS_Physical       0.80      0.15      0.25        80\n",
      "    HS_Gender       0.67      0.17      0.27        60\n",
      "     HS_Other       0.75      0.82      0.78       746\n",
      "      HS_Weak       0.73      0.72      0.72       685\n",
      "  HS_Moderate       0.63      0.67      0.65       352\n",
      "    HS_Strong       0.83      0.65      0.73       105\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5634\n",
      "    macro avg       0.76      0.65      0.67      5634\n",
      " weighted avg       0.78      0.78      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0005562087288126349\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 58.679831743240356 seconds\n",
      "\n",
      "Fold 3 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6584 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4532, Accuracy: 0.8616, F1 Micro: 0.5178, F1 Macro: 0.2384\n",
      "Epoch 2/10, Train Loss: 0.3578, Accuracy: 0.9035, F1 Micro: 0.6717, F1 Macro: 0.4964\n",
      "Epoch 3/10, Train Loss: 0.2966, Accuracy: 0.9176, F1 Micro: 0.7549, F1 Macro: 0.5868\n",
      "Epoch 4/10, Train Loss: 0.2537, Accuracy: 0.9206, F1 Micro: 0.7668, F1 Macro: 0.597\n",
      "Epoch 5/10, Train Loss: 0.2093, Accuracy: 0.9232, F1 Micro: 0.7765, F1 Macro: 0.6308\n",
      "Epoch 6/10, Train Loss: 0.1762, Accuracy: 0.9228, F1 Micro: 0.7746, F1 Macro: 0.6542\n",
      "Epoch 7/10, Train Loss: 0.1492, Accuracy: 0.9241, F1 Micro: 0.7772, F1 Macro: 0.644\n",
      "Epoch 8/10, Train Loss: 0.1256, Accuracy: 0.9231, F1 Micro: 0.7734, F1 Macro: 0.6659\n",
      "Epoch 9/10, Train Loss: 0.1091, Accuracy: 0.9234, F1 Micro: 0.7758, F1 Macro: 0.6822\n",
      "Epoch 10/10, Train Loss: 0.0917, Accuracy: 0.9222, F1 Micro: 0.7784, F1 Macro: 0.6967\n",
      "Best result for 6584 samples: F1 Micro: 0.7784\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.89      0.91      0.90      1026\n",
      "HS_Individual       0.70      0.76      0.73       723\n",
      "     HS_Group       0.71      0.68      0.70       419\n",
      "  HS_Religion       0.73      0.67      0.70       177\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.73      0.28      0.40        80\n",
      "    HS_Gender       0.69      0.33      0.45        60\n",
      "     HS_Other       0.78      0.79      0.79       746\n",
      "      HS_Weak       0.68      0.74      0.71       685\n",
      "  HS_Moderate       0.66      0.62      0.64       352\n",
      "    HS_Strong       0.84      0.66      0.74       105\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5634\n",
      "    macro avg       0.75      0.67      0.70      5634\n",
      " weighted avg       0.77      0.78      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00044310325756669044\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.72168183326721 seconds\n",
      "\n",
      "Fold 3 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6980 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4445, Accuracy: 0.8734, F1 Micro: 0.5951, F1 Macro: 0.303\n",
      "Epoch 2/10, Train Loss: 0.3438, Accuracy: 0.9045, F1 Micro: 0.7238, F1 Macro: 0.521\n",
      "Epoch 3/10, Train Loss: 0.2878, Accuracy: 0.9187, F1 Micro: 0.752, F1 Macro: 0.5814\n",
      "Epoch 4/10, Train Loss: 0.2417, Accuracy: 0.9205, F1 Micro: 0.7563, F1 Macro: 0.6035\n",
      "Epoch 5/10, Train Loss: 0.2055, Accuracy: 0.9186, F1 Micro: 0.7731, F1 Macro: 0.6288\n",
      "Epoch 6/10, Train Loss: 0.1696, Accuracy: 0.9244, F1 Micro: 0.7796, F1 Macro: 0.6518\n",
      "Epoch 7/10, Train Loss: 0.1462, Accuracy: 0.9222, F1 Micro: 0.7717, F1 Macro: 0.6451\n",
      "Epoch 8/10, Train Loss: 0.1227, Accuracy: 0.9188, F1 Micro: 0.7725, F1 Macro: 0.6524\n",
      "Epoch 9/10, Train Loss: 0.1027, Accuracy: 0.9236, F1 Micro: 0.7758, F1 Macro: 0.6707\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.9241, F1 Micro: 0.7817, F1 Macro: 0.6957\n",
      "Best result for 6980 samples: F1 Micro: 0.7817\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.88      0.93      0.91      1026\n",
      "HS_Individual       0.74      0.72      0.73       723\n",
      "     HS_Group       0.69      0.72      0.70       419\n",
      "  HS_Religion       0.75      0.66      0.70       177\n",
      "      HS_Race       0.79      0.75      0.77       119\n",
      "  HS_Physical       0.89      0.21      0.34        80\n",
      "    HS_Gender       0.73      0.32      0.44        60\n",
      "     HS_Other       0.76      0.80      0.78       746\n",
      "      HS_Weak       0.73      0.69      0.71       685\n",
      "  HS_Moderate       0.64      0.66      0.65       352\n",
      "    HS_Strong       0.85      0.69      0.76       105\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5634\n",
      "    macro avg       0.78      0.67      0.70      5634\n",
      " weighted avg       0.79      0.78      0.78      5634\n",
      "  samples avg       0.46      0.45      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00026592764770612116\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 49.408400535583496 seconds\n",
      "\n",
      "Fold 3 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4386, Accuracy: 0.88, F1 Micro: 0.6001, F1 Macro: 0.3441\n",
      "Epoch 2/10, Train Loss: 0.3347, Accuracy: 0.9097, F1 Micro: 0.7324, F1 Macro: 0.544\n",
      "Epoch 3/10, Train Loss: 0.2761, Accuracy: 0.9131, F1 Micro: 0.7496, F1 Macro: 0.5592\n",
      "Epoch 4/10, Train Loss: 0.2333, Accuracy: 0.9225, F1 Micro: 0.7652, F1 Macro: 0.6094\n",
      "Epoch 5/10, Train Loss: 0.1943, Accuracy: 0.9227, F1 Micro: 0.7785, F1 Macro: 0.637\n",
      "Epoch 6/10, Train Loss: 0.1663, Accuracy: 0.9232, F1 Micro: 0.7798, F1 Macro: 0.6508\n",
      "Epoch 7/10, Train Loss: 0.1362, Accuracy: 0.9253, F1 Micro: 0.7854, F1 Macro: 0.6779\n",
      "Epoch 8/10, Train Loss: 0.1177, Accuracy: 0.9239, F1 Micro: 0.7787, F1 Macro: 0.6548\n",
      "Epoch 9/10, Train Loss: 0.1048, Accuracy: 0.9247, F1 Micro: 0.7799, F1 Macro: 0.6936\n",
      "Epoch 10/10, Train Loss: 0.0894, Accuracy: 0.9172, F1 Micro: 0.7716, F1 Macro: 0.6921\n",
      "Best result for 7336 samples: F1 Micro: 0.7854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.91      0.91      0.91      1026\n",
      "HS_Individual       0.75      0.76      0.75       723\n",
      "     HS_Group       0.71      0.71      0.71       419\n",
      "  HS_Religion       0.74      0.64      0.69       177\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.63      0.15      0.24        80\n",
      "    HS_Gender       0.61      0.18      0.28        60\n",
      "     HS_Other       0.77      0.80      0.78       746\n",
      "      HS_Weak       0.73      0.74      0.73       685\n",
      "  HS_Moderate       0.65      0.65      0.65       352\n",
      "    HS_Strong       0.84      0.70      0.76       105\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5634\n",
      "    macro avg       0.74      0.66      0.68      5634\n",
      " weighted avg       0.79      0.78      0.78      5634\n",
      "  samples avg       0.45      0.45      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00021128209482412815\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.36754846572876 seconds\n",
      "\n",
      "Fold 3 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7656 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4348, Accuracy: 0.8823, F1 Micro: 0.594, F1 Macro: 0.3388\n",
      "Epoch 2/10, Train Loss: 0.3224, Accuracy: 0.9096, F1 Micro: 0.7053, F1 Macro: 0.5392\n",
      "Epoch 3/10, Train Loss: 0.2697, Accuracy: 0.9153, F1 Micro: 0.757, F1 Macro: 0.5965\n",
      "Epoch 4/10, Train Loss: 0.227, Accuracy: 0.9191, F1 Micro: 0.769, F1 Macro: 0.6121\n",
      "Epoch 5/10, Train Loss: 0.1928, Accuracy: 0.9174, F1 Micro: 0.7727, F1 Macro: 0.634\n",
      "Epoch 6/10, Train Loss: 0.1662, Accuracy: 0.9192, F1 Micro: 0.7767, F1 Macro: 0.6569\n",
      "Epoch 7/10, Train Loss: 0.1373, Accuracy: 0.922, F1 Micro: 0.7771, F1 Macro: 0.6637\n",
      "Epoch 8/10, Train Loss: 0.1157, Accuracy: 0.9227, F1 Micro: 0.7737, F1 Macro: 0.6664\n",
      "Epoch 9/10, Train Loss: 0.1013, Accuracy: 0.9254, F1 Micro: 0.7773, F1 Macro: 0.6608\n",
      "Epoch 10/10, Train Loss: 0.0864, Accuracy: 0.9231, F1 Micro: 0.7808, F1 Macro: 0.687\n",
      "Best result for 7656 samples: F1 Micro: 0.7808\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.89      0.93      0.91      1026\n",
      "HS_Individual       0.73      0.73      0.73       723\n",
      "     HS_Group       0.68      0.71      0.70       419\n",
      "  HS_Religion       0.72      0.66      0.69       177\n",
      "      HS_Race       0.84      0.65      0.73       119\n",
      "  HS_Physical       0.85      0.21      0.34        80\n",
      "    HS_Gender       0.71      0.28      0.40        60\n",
      "     HS_Other       0.75      0.82      0.78       746\n",
      "      HS_Weak       0.72      0.71      0.71       685\n",
      "  HS_Moderate       0.63      0.67      0.65       352\n",
      "    HS_Strong       0.83      0.67      0.74       105\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5634\n",
      "    macro avg       0.77      0.66      0.69      5634\n",
      " weighted avg       0.78      0.78      0.78      5634\n",
      "  samples avg       0.45      0.45      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00010114923788933088\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 40.47818446159363 seconds\n",
      "\n",
      "Fold 3 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7901 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4289, Accuracy: 0.8827, F1 Micro: 0.5798, F1 Macro: 0.3102\n",
      "Epoch 2/10, Train Loss: 0.3197, Accuracy: 0.9093, F1 Micro: 0.7183, F1 Macro: 0.5212\n",
      "Epoch 3/10, Train Loss: 0.2647, Accuracy: 0.9193, F1 Micro: 0.7565, F1 Macro: 0.5846\n",
      "Epoch 4/10, Train Loss: 0.2207, Accuracy: 0.9223, F1 Micro: 0.7666, F1 Macro: 0.6021\n",
      "Epoch 5/10, Train Loss: 0.188, Accuracy: 0.9183, F1 Micro: 0.7741, F1 Macro: 0.6302\n",
      "Epoch 6/10, Train Loss: 0.1573, Accuracy: 0.922, F1 Micro: 0.7762, F1 Macro: 0.6506\n",
      "Epoch 7/10, Train Loss: 0.1291, Accuracy: 0.9225, F1 Micro: 0.778, F1 Macro: 0.6655\n",
      "Epoch 8/10, Train Loss: 0.1148, Accuracy: 0.9227, F1 Micro: 0.7786, F1 Macro: 0.6592\n",
      "Epoch 9/10, Train Loss: 0.0954, Accuracy: 0.9242, F1 Micro: 0.7807, F1 Macro: 0.6932\n",
      "Epoch 10/10, Train Loss: 0.0813, Accuracy: 0.9226, F1 Micro: 0.7756, F1 Macro: 0.6752\n",
      "Best result for 7901 samples: F1 Micro: 0.7807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.91      0.91      0.91      1026\n",
      "HS_Individual       0.73      0.74      0.73       723\n",
      "     HS_Group       0.72      0.67      0.69       419\n",
      "  HS_Religion       0.75      0.64      0.69       177\n",
      "      HS_Race       0.84      0.71      0.77       119\n",
      "  HS_Physical       0.77      0.25      0.38        80\n",
      "    HS_Gender       0.69      0.30      0.42        60\n",
      "     HS_Other       0.77      0.80      0.79       746\n",
      "      HS_Weak       0.71      0.72      0.71       685\n",
      "  HS_Moderate       0.64      0.60      0.62       352\n",
      "    HS_Strong       0.80      0.70      0.74       105\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5634\n",
      "    macro avg       0.76      0.66      0.69      5634\n",
      " weighted avg       0.79      0.77      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 4.248175537213689e-05\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.91034150123596 seconds\n",
      "\n",
      "Fold 3 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4223, Accuracy: 0.8836, F1 Micro: 0.5761, F1 Macro: 0.2833\n",
      "Epoch 2/10, Train Loss: 0.3098, Accuracy: 0.9086, F1 Micro: 0.7074, F1 Macro: 0.5066\n",
      "Epoch 3/10, Train Loss: 0.2576, Accuracy: 0.9173, F1 Micro: 0.7575, F1 Macro: 0.5788\n",
      "Epoch 4/10, Train Loss: 0.2144, Accuracy: 0.9208, F1 Micro: 0.7607, F1 Macro: 0.594\n",
      "Epoch 5/10, Train Loss: 0.1823, Accuracy: 0.9248, F1 Micro: 0.7834, F1 Macro: 0.6386\n",
      "Epoch 6/10, Train Loss: 0.1537, Accuracy: 0.9168, F1 Micro: 0.771, F1 Macro: 0.6501\n",
      "Epoch 7/10, Train Loss: 0.1317, Accuracy: 0.9221, F1 Micro: 0.7775, F1 Macro: 0.667\n",
      "Epoch 8/10, Train Loss: 0.1089, Accuracy: 0.9231, F1 Micro: 0.7785, F1 Macro: 0.6824\n",
      "Epoch 9/10, Train Loss: 0.0928, Accuracy: 0.9204, F1 Micro: 0.7746, F1 Macro: 0.6858\n",
      "Epoch 10/10, Train Loss: 0.0843, Accuracy: 0.9188, F1 Micro: 0.7753, F1 Macro: 0.6973\n",
      "Best result for 8165 samples: F1 Micro: 0.7834\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.89      0.91      0.90      1026\n",
      "HS_Individual       0.74      0.76      0.75       723\n",
      "     HS_Group       0.73      0.71      0.72       419\n",
      "  HS_Religion       0.77      0.64      0.70       177\n",
      "      HS_Race       0.82      0.71      0.76       119\n",
      "  HS_Physical       1.00      0.03      0.05        80\n",
      "    HS_Gender       0.50      0.02      0.03        60\n",
      "     HS_Other       0.75      0.81      0.78       746\n",
      "      HS_Weak       0.72      0.74      0.73       685\n",
      "  HS_Moderate       0.67      0.62      0.65       352\n",
      "    HS_Strong       0.79      0.69      0.73       105\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5634\n",
      "    macro avg       0.77      0.63      0.64      5634\n",
      " weighted avg       0.79      0.78      0.77      5634\n",
      "  samples avg       0.45      0.45      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 0.0001119049666158389\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 33.29889154434204 seconds\n",
      "\n",
      "Fold 3 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8402 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4165, Accuracy: 0.8876, F1 Micro: 0.6242, F1 Macro: 0.3416\n",
      "Epoch 2/10, Train Loss: 0.3063, Accuracy: 0.9073, F1 Micro: 0.7435, F1 Macro: 0.5655\n",
      "Epoch 3/10, Train Loss: 0.2494, Accuracy: 0.9171, F1 Micro: 0.7543, F1 Macro: 0.5869\n",
      "Epoch 4/10, Train Loss: 0.2138, Accuracy: 0.9228, F1 Micro: 0.7695, F1 Macro: 0.6128\n",
      "Epoch 5/10, Train Loss: 0.1775, Accuracy: 0.9224, F1 Micro: 0.7731, F1 Macro: 0.6176\n",
      "Epoch 6/10, Train Loss: 0.1505, Accuracy: 0.9258, F1 Micro: 0.7775, F1 Macro: 0.6513\n",
      "Epoch 7/10, Train Loss: 0.1235, Accuracy: 0.9238, F1 Micro: 0.7777, F1 Macro: 0.6579\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.9233, F1 Micro: 0.7768, F1 Macro: 0.6758\n",
      "Epoch 9/10, Train Loss: 0.0935, Accuracy: 0.924, F1 Micro: 0.7803, F1 Macro: 0.6933\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9265, F1 Micro: 0.7853, F1 Macro: 0.7101\n",
      "Best result for 8402 samples: F1 Micro: 0.7853\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1142\n",
      "      Abusive       0.91      0.92      0.91      1026\n",
      "HS_Individual       0.75      0.71      0.73       723\n",
      "     HS_Group       0.72      0.71      0.71       419\n",
      "  HS_Religion       0.76      0.67      0.71       177\n",
      "      HS_Race       0.84      0.73      0.78       119\n",
      "  HS_Physical       0.81      0.31      0.45        80\n",
      "    HS_Gender       0.69      0.33      0.45        60\n",
      "     HS_Other       0.78      0.79      0.79       746\n",
      "      HS_Weak       0.73      0.69      0.71       685\n",
      "  HS_Moderate       0.66      0.64      0.65       352\n",
      "    HS_Strong       0.82      0.71      0.77       105\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5634\n",
      "    macro avg       0.78      0.67      0.71      5634\n",
      " weighted avg       0.80      0.77      0.78      5634\n",
      "  samples avg       0.46      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 1.3355513692658855e-05\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 30.243266105651855 seconds\n",
      "\n",
      "Fold 3 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8616 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4118, Accuracy: 0.8887, F1 Micro: 0.6396, F1 Macro: 0.3423\n",
      "Epoch 2/10, Train Loss: 0.2942, Accuracy: 0.9107, F1 Micro: 0.7379, F1 Macro: 0.5475\n",
      "Epoch 3/10, Train Loss: 0.2441, Accuracy: 0.917, F1 Micro: 0.7512, F1 Macro: 0.5916\n",
      "Epoch 4/10, Train Loss: 0.2051, Accuracy: 0.9169, F1 Micro: 0.7695, F1 Macro: 0.6144\n",
      "Epoch 5/10, Train Loss: 0.1738, Accuracy: 0.9242, F1 Micro: 0.7771, F1 Macro: 0.6455\n",
      "Epoch 6/10, Train Loss: 0.1421, Accuracy: 0.9249, F1 Micro: 0.7793, F1 Macro: 0.6509\n",
      "Epoch 7/10, Train Loss: 0.1239, Accuracy: 0.9227, F1 Micro: 0.7805, F1 Macro: 0.6553\n",
      "Epoch 8/10, Train Loss: 0.1045, Accuracy: 0.9233, F1 Micro: 0.7783, F1 Macro: 0.6668\n",
      "Epoch 9/10, Train Loss: 0.0881, Accuracy: 0.924, F1 Micro: 0.7777, F1 Macro: 0.6702\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9252, F1 Micro: 0.7814, F1 Macro: 0.7016\n",
      "Best result for 8616 samples: F1 Micro: 0.7814\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1142\n",
      "      Abusive       0.91      0.92      0.91      1026\n",
      "HS_Individual       0.77      0.69      0.73       723\n",
      "     HS_Group       0.69      0.73      0.71       419\n",
      "  HS_Religion       0.74      0.62      0.67       177\n",
      "      HS_Race       0.83      0.76      0.79       119\n",
      "  HS_Physical       0.80      0.25      0.38        80\n",
      "    HS_Gender       0.68      0.35      0.46        60\n",
      "     HS_Other       0.78      0.78      0.78       746\n",
      "      HS_Weak       0.75      0.67      0.71       685\n",
      "  HS_Moderate       0.63      0.66      0.64       352\n",
      "    HS_Strong       0.80      0.75      0.77       105\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5634\n",
      "    macro avg       0.77      0.67      0.70      5634\n",
      " weighted avg       0.80      0.76      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 7.987450044311118e-06\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.122127056121826 seconds\n",
      "\n",
      "Fold 3 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8816 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4068, Accuracy: 0.8897, F1 Micro: 0.6352, F1 Macro: 0.3514\n",
      "Epoch 2/10, Train Loss: 0.2905, Accuracy: 0.9125, F1 Micro: 0.7277, F1 Macro: 0.5534\n",
      "Epoch 3/10, Train Loss: 0.2355, Accuracy: 0.9202, F1 Micro: 0.7632, F1 Macro: 0.5976\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.9225, F1 Micro: 0.7636, F1 Macro: 0.6137\n",
      "Epoch 5/10, Train Loss: 0.1672, Accuracy: 0.9217, F1 Micro: 0.7778, F1 Macro: 0.6271\n",
      "Epoch 6/10, Train Loss: 0.1386, Accuracy: 0.9249, F1 Micro: 0.779, F1 Macro: 0.6534\n",
      "Epoch 7/10, Train Loss: 0.1206, Accuracy: 0.924, F1 Micro: 0.7779, F1 Macro: 0.6653\n",
      "Epoch 8/10, Train Loss: 0.1003, Accuracy: 0.9165, F1 Micro: 0.7706, F1 Macro: 0.6647\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9224, F1 Micro: 0.7717, F1 Macro: 0.6814\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.924, F1 Micro: 0.7836, F1 Macro: 0.6863\n",
      "Best result for 8816 samples: F1 Micro: 0.7836\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1142\n",
      "      Abusive       0.90      0.92      0.91      1026\n",
      "HS_Individual       0.71      0.77      0.74       723\n",
      "     HS_Group       0.73      0.68      0.70       419\n",
      "  HS_Religion       0.70      0.67      0.68       177\n",
      "      HS_Race       0.82      0.74      0.78       119\n",
      "  HS_Physical       0.78      0.17      0.29        80\n",
      "    HS_Gender       0.65      0.25      0.36        60\n",
      "     HS_Other       0.75      0.83      0.79       746\n",
      "      HS_Weak       0.70      0.75      0.72       685\n",
      "  HS_Moderate       0.67      0.61      0.64       352\n",
      "    HS_Strong       0.82      0.72      0.77       105\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5634\n",
      "    macro avg       0.76      0.67      0.69      5634\n",
      " weighted avg       0.78      0.79      0.78      5634\n",
      "  samples avg       0.45      0.45      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 6.677965666312958e-06\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.547956228256226 seconds\n",
      "\n",
      "Fold 3 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8878, F1 Micro: 0.6184, F1 Macro: 0.3521\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.9086, F1 Micro: 0.7074, F1 Macro: 0.4647\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.9184, F1 Micro: 0.7688, F1 Macro: 0.6043\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9238, F1 Micro: 0.7774, F1 Macro: 0.6191\n",
      "Epoch 5/10, Train Loss: 0.1665, Accuracy: 0.9215, F1 Micro: 0.774, F1 Macro: 0.6353\n",
      "Epoch 6/10, Train Loss: 0.1411, Accuracy: 0.9244, F1 Micro: 0.7801, F1 Macro: 0.6556\n",
      "Epoch 7/10, Train Loss: 0.1175, Accuracy: 0.9216, F1 Micro: 0.7728, F1 Macro: 0.6522\n",
      "Epoch 8/10, Train Loss: 0.101, Accuracy: 0.9265, F1 Micro: 0.7835, F1 Macro: 0.6888\n",
      "Epoch 9/10, Train Loss: 0.0854, Accuracy: 0.9225, F1 Micro: 0.7746, F1 Macro: 0.6822\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.9247, F1 Micro: 0.7705, F1 Macro: 0.6762\n",
      "Best result for 9016 samples: F1 Micro: 0.7835\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1142\n",
      "      Abusive       0.90      0.91      0.91      1026\n",
      "HS_Individual       0.73      0.77      0.75       723\n",
      "     HS_Group       0.79      0.62      0.69       419\n",
      "  HS_Religion       0.78      0.63      0.69       177\n",
      "      HS_Race       0.86      0.71      0.77       119\n",
      "  HS_Physical       0.76      0.20      0.32        80\n",
      "    HS_Gender       0.81      0.28      0.42        60\n",
      "     HS_Other       0.80      0.77      0.78       746\n",
      "      HS_Weak       0.71      0.74      0.72       685\n",
      "  HS_Moderate       0.71      0.55      0.62       352\n",
      "    HS_Strong       0.83      0.65      0.73       105\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5634\n",
      "    macro avg       0.79      0.64      0.69      5634\n",
      " weighted avg       0.81      0.76      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 7.4645192398747905e-06\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.74571681022644 seconds\n",
      "\n",
      "Fold 3 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9216 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3968, Accuracy: 0.8907, F1 Micro: 0.6543, F1 Macro: 0.3935\n",
      "Epoch 2/10, Train Loss: 0.2798, Accuracy: 0.911, F1 Micro: 0.7328, F1 Macro: 0.5598\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9183, F1 Micro: 0.7616, F1 Macro: 0.5956\n",
      "Epoch 4/10, Train Loss: 0.1985, Accuracy: 0.9227, F1 Micro: 0.7678, F1 Macro: 0.6038\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.9217, F1 Micro: 0.7753, F1 Macro: 0.6238\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9233, F1 Micro: 0.7804, F1 Macro: 0.6615\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.924, F1 Micro: 0.7775, F1 Macro: 0.6651\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.9249, F1 Micro: 0.7713, F1 Macro: 0.6445\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9271, F1 Micro: 0.7816, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9166, F1 Micro: 0.7671, F1 Macro: 0.6803\n",
      "Best result for 9216 samples: F1 Micro: 0.7816\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1142\n",
      "      Abusive       0.91      0.91      0.91      1026\n",
      "HS_Individual       0.76      0.71      0.73       723\n",
      "     HS_Group       0.75      0.65      0.70       419\n",
      "  HS_Religion       0.77      0.63      0.69       177\n",
      "      HS_Race       0.87      0.63      0.73       119\n",
      "  HS_Physical       0.81      0.26      0.40        80\n",
      "    HS_Gender       0.71      0.25      0.37        60\n",
      "     HS_Other       0.82      0.76      0.79       746\n",
      "      HS_Weak       0.74      0.70      0.72       685\n",
      "  HS_Moderate       0.69      0.57      0.62       352\n",
      "    HS_Strong       0.86      0.70      0.77       105\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      5634\n",
      "    macro avg       0.80      0.63      0.69      5634\n",
      " weighted avg       0.82      0.75      0.78      5634\n",
      "  samples avg       0.45      0.43      0.42      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 3.644132038971293e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.348145008087158 seconds\n",
      "\n",
      "Fold 3 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3993, Accuracy: 0.8909, F1 Micro: 0.6575, F1 Macro: 0.3856\n",
      "Epoch 2/10, Train Loss: 0.2777, Accuracy: 0.9071, F1 Micro: 0.7333, F1 Macro: 0.5477\n",
      "Epoch 3/10, Train Loss: 0.232, Accuracy: 0.918, F1 Micro: 0.7667, F1 Macro: 0.6085\n",
      "Epoch 4/10, Train Loss: 0.1911, Accuracy: 0.9204, F1 Micro: 0.7654, F1 Macro: 0.6056\n",
      "Epoch 5/10, Train Loss: 0.1623, Accuracy: 0.9238, F1 Micro: 0.7768, F1 Macro: 0.6263\n",
      "Epoch 6/10, Train Loss: 0.1358, Accuracy: 0.9199, F1 Micro: 0.7755, F1 Macro: 0.6589\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9211, F1 Micro: 0.771, F1 Macro: 0.6469\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9219, F1 Micro: 0.7766, F1 Macro: 0.6663\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9216, F1 Micro: 0.7756, F1 Macro: 0.6759\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9224, F1 Micro: 0.7786, F1 Macro: 0.7118\n",
      "Best result for 9218 samples: F1 Micro: 0.7786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1142\n",
      "      Abusive       0.90      0.93      0.92      1026\n",
      "HS_Individual       0.72      0.72      0.72       723\n",
      "     HS_Group       0.70      0.70      0.70       419\n",
      "  HS_Religion       0.70      0.70      0.70       177\n",
      "      HS_Race       0.81      0.79      0.80       119\n",
      "  HS_Physical       0.78      0.36      0.50        80\n",
      "    HS_Gender       0.69      0.42      0.52        60\n",
      "     HS_Other       0.78      0.78      0.78       746\n",
      "      HS_Weak       0.69      0.71      0.70       685\n",
      "  HS_Moderate       0.61      0.66      0.64       352\n",
      "    HS_Strong       0.83      0.64      0.72       105\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5634\n",
      "    macro avg       0.75      0.69      0.71      5634\n",
      " weighted avg       0.78      0.78      0.78      5634\n",
      "  samples avg       0.45      0.45      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 3.1246050639310865e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.138036251068115 seconds\n",
      "\n",
      "Fold 3 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3952, Accuracy: 0.8913, F1 Micro: 0.6548, F1 Macro: 0.3645\n",
      "Epoch 2/10, Train Loss: 0.2784, Accuracy: 0.9107, F1 Micro: 0.7331, F1 Macro: 0.5264\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9169, F1 Micro: 0.7618, F1 Macro: 0.6005\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9226, F1 Micro: 0.7655, F1 Macro: 0.6001\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.9216, F1 Micro: 0.7774, F1 Macro: 0.6421\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.9204, F1 Micro: 0.7762, F1 Macro: 0.6553\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.925, F1 Micro: 0.777, F1 Macro: 0.6572\n",
      "Epoch 8/10, Train Loss: 0.0969, Accuracy: 0.9259, F1 Micro: 0.7828, F1 Macro: 0.6809\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9243, F1 Micro: 0.7802, F1 Macro: 0.6861\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9258, F1 Micro: 0.7813, F1 Macro: 0.7088\n",
      "Best result for 9418 samples: F1 Micro: 0.7828\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1142\n",
      "      Abusive       0.91      0.90      0.91      1026\n",
      "HS_Individual       0.72      0.76      0.74       723\n",
      "     HS_Group       0.78      0.63      0.70       419\n",
      "  HS_Religion       0.80      0.60      0.68       177\n",
      "      HS_Race       0.86      0.70      0.77       119\n",
      "  HS_Physical       0.88      0.17      0.29        80\n",
      "    HS_Gender       0.72      0.22      0.33        60\n",
      "     HS_Other       0.78      0.81      0.80       746\n",
      "      HS_Weak       0.70      0.74      0.72       685\n",
      "  HS_Moderate       0.73      0.55      0.63       352\n",
      "    HS_Strong       0.83      0.69      0.75       105\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5634\n",
      "    macro avg       0.80      0.63      0.68      5634\n",
      " weighted avg       0.80      0.76      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.479229664866579e-06\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.295434951782227 seconds\n",
      "\n",
      "Fold 3 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9618 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3867, Accuracy: 0.8843, F1 Micro: 0.571, F1 Macro: 0.3181\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.9106, F1 Micro: 0.7284, F1 Macro: 0.5245\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9187, F1 Micro: 0.7508, F1 Macro: 0.581\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9237, F1 Micro: 0.7767, F1 Macro: 0.6196\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9261, F1 Micro: 0.7749, F1 Macro: 0.6282\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.9237, F1 Micro: 0.7678, F1 Macro: 0.6269\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9157, F1 Micro: 0.769, F1 Macro: 0.6665\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9244, F1 Micro: 0.7817, F1 Macro: 0.6915\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.9216, F1 Micro: 0.7742, F1 Macro: 0.694\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9234, F1 Micro: 0.7763, F1 Macro: 0.6859\n",
      "Best result for 9618 samples: F1 Micro: 0.7817\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1142\n",
      "      Abusive       0.89      0.93      0.91      1026\n",
      "HS_Individual       0.77      0.71      0.74       723\n",
      "     HS_Group       0.68      0.72      0.70       419\n",
      "  HS_Religion       0.76      0.60      0.67       177\n",
      "      HS_Race       0.79      0.70      0.74       119\n",
      "  HS_Physical       0.70      0.24      0.36        80\n",
      "    HS_Gender       0.69      0.30      0.42        60\n",
      "     HS_Other       0.75      0.80      0.78       746\n",
      "      HS_Weak       0.75      0.69      0.72       685\n",
      "  HS_Moderate       0.63      0.68      0.65       352\n",
      "    HS_Strong       0.83      0.70      0.76       105\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5634\n",
      "    macro avg       0.76      0.66      0.69      5634\n",
      " weighted avg       0.79      0.77      0.78      5634\n",
      "  samples avg       0.44      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.388988600112498e-06\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.581920862197876 seconds\n",
      "\n",
      "Fold 3 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9818 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3856, Accuracy: 0.8909, F1 Micro: 0.6466, F1 Macro: 0.3675\n",
      "Epoch 2/10, Train Loss: 0.2644, Accuracy: 0.909, F1 Micro: 0.6986, F1 Macro: 0.5189\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.9186, F1 Micro: 0.7461, F1 Macro: 0.5859\n",
      "Epoch 4/10, Train Loss: 0.1858, Accuracy: 0.9227, F1 Micro: 0.7591, F1 Macro: 0.5936\n",
      "Epoch 5/10, Train Loss: 0.1515, Accuracy: 0.9191, F1 Micro: 0.7736, F1 Macro: 0.6241\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9237, F1 Micro: 0.779, F1 Macro: 0.6571\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.923, F1 Micro: 0.7773, F1 Macro: 0.66\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.9228, F1 Micro: 0.775, F1 Macro: 0.6661\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9218, F1 Micro: 0.7779, F1 Macro: 0.6875\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9243, F1 Micro: 0.7759, F1 Macro: 0.6902\n",
      "Best result for 9818 samples: F1 Micro: 0.779\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.88      0.93      0.90      1026\n",
      "HS_Individual       0.72      0.74      0.73       723\n",
      "     HS_Group       0.73      0.66      0.69       419\n",
      "  HS_Religion       0.78      0.59      0.67       177\n",
      "      HS_Race       0.84      0.66      0.74       119\n",
      "  HS_Physical       0.75      0.11      0.20        80\n",
      "    HS_Gender       0.67      0.13      0.22        60\n",
      "     HS_Other       0.77      0.80      0.79       746\n",
      "      HS_Weak       0.70      0.73      0.71       685\n",
      "  HS_Moderate       0.69      0.60      0.64       352\n",
      "    HS_Strong       0.79      0.68      0.73       105\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5634\n",
      "    macro avg       0.76      0.62      0.66      5634\n",
      " weighted avg       0.79      0.77      0.77      5634\n",
      "  samples avg       0.46      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 8.340360909642183e-06\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.804278373718262 seconds\n",
      "\n",
      "Fold 3 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10018 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3758, Accuracy: 0.8911, F1 Micro: 0.6543, F1 Macro: 0.3489\n",
      "Epoch 2/10, Train Loss: 0.2629, Accuracy: 0.9127, F1 Micro: 0.74, F1 Macro: 0.572\n",
      "Epoch 3/10, Train Loss: 0.2135, Accuracy: 0.9146, F1 Micro: 0.7588, F1 Macro: 0.5954\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9216, F1 Micro: 0.7693, F1 Macro: 0.6076\n",
      "Epoch 5/10, Train Loss: 0.1467, Accuracy: 0.9221, F1 Micro: 0.7736, F1 Macro: 0.6197\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9248, F1 Micro: 0.7804, F1 Macro: 0.6581\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9218, F1 Micro: 0.7809, F1 Macro: 0.6904\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9249, F1 Micro: 0.7794, F1 Macro: 0.6697\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9259, F1 Micro: 0.7774, F1 Macro: 0.6854\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9264, F1 Micro: 0.7791, F1 Macro: 0.7022\n",
      "Best result for 10018 samples: F1 Micro: 0.7809\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1142\n",
      "      Abusive       0.88      0.94      0.91      1026\n",
      "HS_Individual       0.72      0.74      0.73       723\n",
      "     HS_Group       0.68      0.76      0.72       419\n",
      "  HS_Religion       0.74      0.68      0.71       177\n",
      "      HS_Race       0.79      0.74      0.76       119\n",
      "  HS_Physical       0.76      0.20      0.32        80\n",
      "    HS_Gender       0.63      0.28      0.39        60\n",
      "     HS_Other       0.74      0.81      0.77       746\n",
      "      HS_Weak       0.71      0.71      0.71       685\n",
      "  HS_Moderate       0.63      0.72      0.67       352\n",
      "    HS_Strong       0.80      0.70      0.74       105\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5634\n",
      "    macro avg       0.74      0.68      0.69      5634\n",
      " weighted avg       0.76      0.80      0.78      5634\n",
      "  samples avg       0.46      0.46      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.8722961764724475e-06\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.143306255340576 seconds\n",
      "\n",
      "Fold 3 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3734, Accuracy: 0.8898, F1 Micro: 0.6275, F1 Macro: 0.3362\n",
      "Epoch 2/10, Train Loss: 0.2512, Accuracy: 0.9118, F1 Micro: 0.7433, F1 Macro: 0.562\n",
      "Epoch 3/10, Train Loss: 0.2086, Accuracy: 0.9187, F1 Micro: 0.7579, F1 Macro: 0.6015\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.9228, F1 Micro: 0.7723, F1 Macro: 0.6192\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.9195, F1 Micro: 0.7719, F1 Macro: 0.629\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9254, F1 Micro: 0.7836, F1 Macro: 0.6516\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9228, F1 Micro: 0.7812, F1 Macro: 0.684\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.9248, F1 Micro: 0.7858, F1 Macro: 0.6973\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9255, F1 Micro: 0.7786, F1 Macro: 0.6837\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9239, F1 Micro: 0.7796, F1 Macro: 0.697\n",
      "Best result for 10218 samples: F1 Micro: 0.7858\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.89      0.93      0.91      1026\n",
      "HS_Individual       0.74      0.75      0.74       723\n",
      "     HS_Group       0.70      0.73      0.71       419\n",
      "  HS_Religion       0.74      0.62      0.67       177\n",
      "      HS_Race       0.83      0.68      0.75       119\n",
      "  HS_Physical       0.85      0.21      0.34        80\n",
      "    HS_Gender       0.72      0.35      0.47        60\n",
      "     HS_Other       0.77      0.81      0.79       746\n",
      "      HS_Weak       0.71      0.72      0.72       685\n",
      "  HS_Moderate       0.64      0.69      0.66       352\n",
      "    HS_Strong       0.81      0.69      0.74       105\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5634\n",
      "    macro avg       0.77      0.67      0.70      5634\n",
      " weighted avg       0.78      0.79      0.78      5634\n",
      "  samples avg       0.45      0.45      0.44      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 2.6228233309666415e-06\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.303308963775635 seconds\n",
      "\n",
      "Fold 3 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3705, Accuracy: 0.8902, F1 Micro: 0.6444, F1 Macro: 0.3318\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.9092, F1 Micro: 0.7247, F1 Macro: 0.5337\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9189, F1 Micro: 0.7486, F1 Macro: 0.5754\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9225, F1 Micro: 0.7721, F1 Macro: 0.6076\n",
      "Epoch 5/10, Train Loss: 0.1406, Accuracy: 0.9246, F1 Micro: 0.7712, F1 Macro: 0.6183\n",
      "Epoch 6/10, Train Loss: 0.12, Accuracy: 0.9244, F1 Micro: 0.7787, F1 Macro: 0.6524\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9225, F1 Micro: 0.7776, F1 Macro: 0.6654\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9221, F1 Micro: 0.7767, F1 Macro: 0.6765\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9237, F1 Micro: 0.7775, F1 Macro: 0.6968\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9254, F1 Micro: 0.7749, F1 Macro: 0.6782\n",
      "Best result for 10418 samples: F1 Micro: 0.7787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.90      0.91      0.90      1026\n",
      "HS_Individual       0.74      0.76      0.75       723\n",
      "     HS_Group       0.76      0.63      0.69       419\n",
      "  HS_Religion       0.75      0.53      0.62       177\n",
      "      HS_Race       0.82      0.68      0.74       119\n",
      "  HS_Physical       0.78      0.09      0.16        80\n",
      "    HS_Gender       0.75      0.15      0.25        60\n",
      "     HS_Other       0.76      0.79      0.78       746\n",
      "      HS_Weak       0.72      0.74      0.73       685\n",
      "  HS_Moderate       0.72      0.55      0.62       352\n",
      "    HS_Strong       0.78      0.69      0.73       105\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5634\n",
      "    macro avg       0.78      0.62      0.65      5634\n",
      " weighted avg       0.79      0.76      0.77      5634\n",
      "  samples avg       0.46      0.44      0.43      5634\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 5.9337108723411816e-06\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.839700937271118 seconds\n",
      "\n",
      "Fold 3 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3712, Accuracy: 0.8873, F1 Micro: 0.6012, F1 Macro: 0.3171\n",
      "Epoch 2/10, Train Loss: 0.2485, Accuracy: 0.9096, F1 Micro: 0.7134, F1 Macro: 0.4795\n",
      "Epoch 3/10, Train Loss: 0.2014, Accuracy: 0.9181, F1 Micro: 0.756, F1 Macro: 0.5852\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9232, F1 Micro: 0.7696, F1 Macro: 0.6094\n",
      "Epoch 5/10, Train Loss: 0.1395, Accuracy: 0.9175, F1 Micro: 0.7732, F1 Macro: 0.6529\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9222, F1 Micro: 0.773, F1 Macro: 0.6565\n",
      "Epoch 7/10, Train Loss: 0.0969, Accuracy: 0.9204, F1 Micro: 0.7706, F1 Macro: 0.6725\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.9241, F1 Micro: 0.7807, F1 Macro: 0.6993\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9202, F1 Micro: 0.776, F1 Macro: 0.6952\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9243, F1 Micro: 0.7801, F1 Macro: 0.7\n",
      "Best result for 10535 samples: F1 Micro: 0.7807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1142\n",
      "      Abusive       0.93      0.88      0.90      1026\n",
      "HS_Individual       0.74      0.73      0.73       723\n",
      "     HS_Group       0.70      0.70      0.70       419\n",
      "  HS_Religion       0.74      0.66      0.69       177\n",
      "      HS_Race       0.88      0.66      0.76       119\n",
      "  HS_Physical       0.76      0.24      0.36        80\n",
      "    HS_Gender       0.72      0.35      0.47        60\n",
      "     HS_Other       0.76      0.82      0.79       746\n",
      "      HS_Weak       0.71      0.71      0.71       685\n",
      "  HS_Moderate       0.64      0.65      0.64       352\n",
      "    HS_Strong       0.82      0.72      0.77       105\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5634\n",
      "    macro avg       0.77      0.67      0.70      5634\n",
      " weighted avg       0.79      0.77      0.78      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 6601.63 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 658 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5939, Accuracy: 0.8336, F1 Micro: 0.1027, F1 Macro: 0.041\n",
      "Epoch 2/10, Train Loss: 0.4543, Accuracy: 0.8308, F1 Micro: 0.0278, F1 Macro: 0.0116\n",
      "Epoch 3/10, Train Loss: 0.3982, Accuracy: 0.8346, F1 Micro: 0.0749, F1 Macro: 0.028\n",
      "Epoch 4/10, Train Loss: 0.3658, Accuracy: 0.8368, F1 Micro: 0.1066, F1 Macro: 0.0373\n",
      "Epoch 5/10, Train Loss: 0.354, Accuracy: 0.8477, F1 Micro: 0.2387, F1 Macro: 0.0822\n",
      "Epoch 6/10, Train Loss: 0.3471, Accuracy: 0.858, F1 Micro: 0.3466, F1 Macro: 0.1245\n",
      "Epoch 7/10, Train Loss: 0.3256, Accuracy: 0.8668, F1 Micro: 0.4318, F1 Macro: 0.1709\n",
      "Epoch 8/10, Train Loss: 0.3059, Accuracy: 0.8763, F1 Micro: 0.5264, F1 Macro: 0.231\n",
      "Epoch 9/10, Train Loss: 0.2677, Accuracy: 0.8784, F1 Micro: 0.5723, F1 Macro: 0.2659\n",
      "Epoch 10/10, Train Loss: 0.262, Accuracy: 0.881, F1 Micro: 0.5895, F1 Macro: 0.2937\n",
      "Best result for 658 samples: F1 Micro: 0.5895\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.76      0.78      1107\n",
      "      Abusive       0.82      0.73      0.78      1030\n",
      "HS_Individual       0.62      0.49      0.55       729\n",
      "     HS_Group       0.61      0.13      0.21       378\n",
      "  HS_Religion       0.00      0.00      0.00       167\n",
      "      HS_Race       0.00      0.00      0.00        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.65      0.60      0.62       744\n",
      "      HS_Weak       0.59      0.42      0.49       690\n",
      "  HS_Moderate       0.51      0.06      0.10       338\n",
      "    HS_Strong       0.00      0.00      0.00        79\n",
      "\n",
      "    micro avg       0.72      0.50      0.59      5499\n",
      "    macro avg       0.38      0.27      0.29      5499\n",
      " weighted avg       0.63      0.50      0.54      5499\n",
      "  samples avg       0.38      0.29      0.30      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.001688623218797147\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 133.69935011863708 seconds\n",
      "\n",
      "Fold 4 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1646 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.8314, F1 Micro: 0.3579, F1 Macro: 0.0968\n",
      "Epoch 2/10, Train Loss: 0.432, Accuracy: 0.8411, F1 Micro: 0.3673, F1 Macro: 0.1042\n",
      "Epoch 3/10, Train Loss: 0.4053, Accuracy: 0.8532, F1 Micro: 0.4433, F1 Macro: 0.1593\n",
      "Epoch 4/10, Train Loss: 0.3822, Accuracy: 0.8694, F1 Micro: 0.4998, F1 Macro: 0.2225\n",
      "Epoch 5/10, Train Loss: 0.3443, Accuracy: 0.8859, F1 Micro: 0.6312, F1 Macro: 0.3712\n",
      "Epoch 6/10, Train Loss: 0.3089, Accuracy: 0.8953, F1 Micro: 0.6535, F1 Macro: 0.3979\n",
      "Epoch 7/10, Train Loss: 0.277, Accuracy: 0.8976, F1 Micro: 0.6778, F1 Macro: 0.4278\n",
      "Epoch 8/10, Train Loss: 0.2482, Accuracy: 0.9005, F1 Micro: 0.6937, F1 Macro: 0.4518\n",
      "Epoch 9/10, Train Loss: 0.2235, Accuracy: 0.904, F1 Micro: 0.7135, F1 Macro: 0.5345\n",
      "Epoch 10/10, Train Loss: 0.1911, Accuracy: 0.9056, F1 Micro: 0.7069, F1 Macro: 0.5248\n",
      "Best result for 1646 samples: F1 Micro: 0.7135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.83      1107\n",
      "      Abusive       0.84      0.87      0.85      1030\n",
      "HS_Individual       0.65      0.71      0.68       729\n",
      "     HS_Group       0.68      0.56      0.61       378\n",
      "  HS_Religion       0.77      0.26      0.39       167\n",
      "      HS_Race       0.69      0.51      0.59        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.68      0.75      0.71       744\n",
      "      HS_Weak       0.63      0.67      0.65       690\n",
      "  HS_Moderate       0.59      0.47      0.52       338\n",
      "    HS_Strong       0.75      0.48      0.58        79\n",
      "\n",
      "    micro avg       0.73      0.70      0.71      5499\n",
      "    macro avg       0.59      0.51      0.53      5499\n",
      " weighted avg       0.71      0.70      0.70      5499\n",
      "  samples avg       0.41      0.39      0.38      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.003010955872014165\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 121.18889164924622 seconds\n",
      "\n",
      "Fold 4 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5069, Accuracy: 0.8257, F1 Micro: 0.3736, F1 Macro: 0.1005\n",
      "Epoch 2/10, Train Loss: 0.4194, Accuracy: 0.845, F1 Micro: 0.4007, F1 Macro: 0.1342\n",
      "Epoch 3/10, Train Loss: 0.3976, Accuracy: 0.8656, F1 Micro: 0.5162, F1 Macro: 0.2338\n",
      "Epoch 4/10, Train Loss: 0.3635, Accuracy: 0.8834, F1 Micro: 0.6493, F1 Macro: 0.3849\n",
      "Epoch 5/10, Train Loss: 0.3248, Accuracy: 0.9024, F1 Micro: 0.6779, F1 Macro: 0.4636\n",
      "Epoch 6/10, Train Loss: 0.2797, Accuracy: 0.9069, F1 Micro: 0.6918, F1 Macro: 0.5014\n",
      "Epoch 7/10, Train Loss: 0.2504, Accuracy: 0.9064, F1 Micro: 0.7342, F1 Macro: 0.545\n",
      "Epoch 8/10, Train Loss: 0.2191, Accuracy: 0.9105, F1 Micro: 0.7045, F1 Macro: 0.5182\n",
      "Epoch 9/10, Train Loss: 0.1872, Accuracy: 0.9134, F1 Micro: 0.7305, F1 Macro: 0.5354\n",
      "Epoch 10/10, Train Loss: 0.1617, Accuracy: 0.9142, F1 Micro: 0.7378, F1 Macro: 0.5601\n",
      "Best result for 2535 samples: F1 Micro: 0.7378\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.84      1107\n",
      "      Abusive       0.87      0.87      0.87      1030\n",
      "HS_Individual       0.73      0.68      0.71       729\n",
      "     HS_Group       0.71      0.60      0.65       378\n",
      "  HS_Religion       0.86      0.37      0.52       167\n",
      "      HS_Race       0.78      0.43      0.55        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.70      0.78      0.74       744\n",
      "      HS_Weak       0.70      0.67      0.69       690\n",
      "  HS_Moderate       0.63      0.54      0.58       338\n",
      "    HS_Strong       0.71      0.49      0.58        79\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5499\n",
      "    macro avg       0.63      0.52      0.56      5499\n",
      " weighted avg       0.75      0.71      0.72      5499\n",
      "  samples avg       0.42      0.39      0.39      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.004351374786347153\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 109.13618087768555 seconds\n",
      "\n",
      "Fold 4 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3335 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4915, Accuracy: 0.8341, F1 Micro: 0.3799, F1 Macro: 0.1048\n",
      "Epoch 2/10, Train Loss: 0.4134, Accuracy: 0.8567, F1 Micro: 0.471, F1 Macro: 0.1824\n",
      "Epoch 3/10, Train Loss: 0.3747, Accuracy: 0.8846, F1 Micro: 0.6407, F1 Macro: 0.3751\n",
      "Epoch 4/10, Train Loss: 0.3363, Accuracy: 0.9021, F1 Micro: 0.6984, F1 Macro: 0.4621\n",
      "Epoch 5/10, Train Loss: 0.292, Accuracy: 0.9108, F1 Micro: 0.7328, F1 Macro: 0.5465\n",
      "Epoch 6/10, Train Loss: 0.2489, Accuracy: 0.908, F1 Micro: 0.7402, F1 Macro: 0.559\n",
      "Epoch 7/10, Train Loss: 0.2197, Accuracy: 0.9186, F1 Micro: 0.746, F1 Macro: 0.5661\n",
      "Epoch 8/10, Train Loss: 0.1843, Accuracy: 0.919, F1 Micro: 0.7565, F1 Macro: 0.5943\n",
      "Epoch 9/10, Train Loss: 0.1556, Accuracy: 0.9191, F1 Micro: 0.7592, F1 Macro: 0.589\n",
      "Epoch 10/10, Train Loss: 0.1365, Accuracy: 0.9201, F1 Micro: 0.7529, F1 Macro: 0.5953\n",
      "Best result for 3335 samples: F1 Micro: 0.7592\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1107\n",
      "      Abusive       0.87      0.89      0.88      1030\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.72      0.63      0.67       378\n",
      "  HS_Religion       0.80      0.47      0.59       167\n",
      "      HS_Race       0.71      0.56      0.62        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.71      0.83      0.76       744\n",
      "      HS_Weak       0.70      0.72      0.71       690\n",
      "  HS_Moderate       0.64      0.56      0.60       338\n",
      "    HS_Strong       0.76      0.56      0.64        79\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5499\n",
      "    macro avg       0.62      0.57      0.59      5499\n",
      " weighted avg       0.75      0.75      0.75      5499\n",
      "  samples avg       0.43      0.41      0.41      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.00312100446317345\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 98.2226140499115 seconds\n",
      "\n",
      "Fold 4 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4055 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4795, Accuracy: 0.8355, F1 Micro: 0.3905, F1 Macro: 0.1085\n",
      "Epoch 2/10, Train Loss: 0.4001, Accuracy: 0.8692, F1 Micro: 0.5302, F1 Macro: 0.2424\n",
      "Epoch 3/10, Train Loss: 0.3604, Accuracy: 0.8986, F1 Micro: 0.6675, F1 Macro: 0.4088\n",
      "Epoch 4/10, Train Loss: 0.3224, Accuracy: 0.911, F1 Micro: 0.7147, F1 Macro: 0.5192\n",
      "Epoch 5/10, Train Loss: 0.2778, Accuracy: 0.9168, F1 Micro: 0.7405, F1 Macro: 0.5539\n",
      "Epoch 6/10, Train Loss: 0.2358, Accuracy: 0.918, F1 Micro: 0.7371, F1 Macro: 0.558\n",
      "Epoch 7/10, Train Loss: 0.2009, Accuracy: 0.9218, F1 Micro: 0.7598, F1 Macro: 0.5989\n",
      "Epoch 8/10, Train Loss: 0.1607, Accuracy: 0.9231, F1 Micro: 0.768, F1 Macro: 0.6078\n",
      "Epoch 9/10, Train Loss: 0.1459, Accuracy: 0.9216, F1 Micro: 0.7639, F1 Macro: 0.6069\n",
      "Epoch 10/10, Train Loss: 0.1215, Accuracy: 0.9202, F1 Micro: 0.7636, F1 Macro: 0.6068\n",
      "Best result for 4055 samples: F1 Micro: 0.768\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1107\n",
      "      Abusive       0.89      0.86      0.87      1030\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.75      0.63      0.69       378\n",
      "  HS_Religion       0.79      0.54      0.65       167\n",
      "      HS_Race       0.75      0.66      0.70        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.76      0.79      0.78       744\n",
      "      HS_Weak       0.71      0.74      0.73       690\n",
      "  HS_Moderate       0.66      0.56      0.60       338\n",
      "    HS_Strong       0.79      0.58      0.67        79\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5499\n",
      "    macro avg       0.64      0.58      0.61      5499\n",
      " weighted avg       0.77      0.75      0.76      5499\n",
      "  samples avg       0.42      0.40      0.40      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0020710814511403445\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 88.34049153327942 seconds\n",
      "\n",
      "Fold 4 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4703 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4735, Accuracy: 0.8379, F1 Micro: 0.3871, F1 Macro: 0.1144\n",
      "Epoch 2/10, Train Loss: 0.3946, Accuracy: 0.881, F1 Micro: 0.5867, F1 Macro: 0.3335\n",
      "Epoch 3/10, Train Loss: 0.3536, Accuracy: 0.9018, F1 Micro: 0.6857, F1 Macro: 0.4467\n",
      "Epoch 4/10, Train Loss: 0.3054, Accuracy: 0.9162, F1 Micro: 0.7357, F1 Macro: 0.5638\n",
      "Epoch 5/10, Train Loss: 0.2644, Accuracy: 0.919, F1 Micro: 0.7561, F1 Macro: 0.5834\n",
      "Epoch 6/10, Train Loss: 0.2197, Accuracy: 0.9204, F1 Micro: 0.7584, F1 Macro: 0.5991\n",
      "Epoch 7/10, Train Loss: 0.1845, Accuracy: 0.9206, F1 Micro: 0.7661, F1 Macro: 0.6066\n",
      "Epoch 8/10, Train Loss: 0.162, Accuracy: 0.9145, F1 Micro: 0.7592, F1 Macro: 0.5915\n",
      "Epoch 9/10, Train Loss: 0.1352, Accuracy: 0.9238, F1 Micro: 0.7657, F1 Macro: 0.6074\n",
      "Epoch 10/10, Train Loss: 0.121, Accuracy: 0.9242, F1 Micro: 0.7716, F1 Macro: 0.6123\n",
      "Best result for 4703 samples: F1 Micro: 0.7716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1107\n",
      "      Abusive       0.89      0.87      0.88      1030\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.75      0.61      0.67       378\n",
      "  HS_Religion       0.78      0.52      0.63       167\n",
      "      HS_Race       0.71      0.60      0.65        88\n",
      "  HS_Physical       0.67      0.03      0.05        74\n",
      "    HS_Gender       1.00      0.03      0.05        75\n",
      "     HS_Other       0.76      0.82      0.79       744\n",
      "      HS_Weak       0.72      0.74      0.73       690\n",
      "  HS_Moderate       0.69      0.55      0.61       338\n",
      "    HS_Strong       0.71      0.63      0.67        79\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5499\n",
      "    macro avg       0.77      0.59      0.61      5499\n",
      " weighted avg       0.79      0.75      0.76      5499\n",
      "  samples avg       0.42      0.41      0.40      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0015481895185075704\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 79.65426301956177 seconds\n",
      "\n",
      "Fold 4 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5287 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4706, Accuracy: 0.8448, F1 Micro: 0.4436, F1 Macro: 0.1582\n",
      "Epoch 2/10, Train Loss: 0.3882, Accuracy: 0.8902, F1 Micro: 0.6228, F1 Macro: 0.371\n",
      "Epoch 3/10, Train Loss: 0.3346, Accuracy: 0.9106, F1 Micro: 0.7184, F1 Macro: 0.5257\n",
      "Epoch 4/10, Train Loss: 0.2907, Accuracy: 0.9129, F1 Micro: 0.7404, F1 Macro: 0.55\n",
      "Epoch 5/10, Train Loss: 0.2505, Accuracy: 0.9207, F1 Micro: 0.7564, F1 Macro: 0.5996\n",
      "Epoch 6/10, Train Loss: 0.2095, Accuracy: 0.9229, F1 Micro: 0.7698, F1 Macro: 0.5993\n",
      "Epoch 7/10, Train Loss: 0.174, Accuracy: 0.9224, F1 Micro: 0.7745, F1 Macro: 0.6174\n",
      "Epoch 8/10, Train Loss: 0.1472, Accuracy: 0.9236, F1 Micro: 0.773, F1 Macro: 0.6158\n",
      "Epoch 9/10, Train Loss: 0.1279, Accuracy: 0.9251, F1 Micro: 0.7706, F1 Macro: 0.6308\n",
      "Epoch 10/10, Train Loss: 0.1107, Accuracy: 0.9225, F1 Micro: 0.7692, F1 Macro: 0.6216\n",
      "Best result for 5287 samples: F1 Micro: 0.7745\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1107\n",
      "      Abusive       0.86      0.91      0.88      1030\n",
      "HS_Individual       0.69      0.82      0.75       729\n",
      "     HS_Group       0.80      0.58      0.67       378\n",
      "  HS_Religion       0.78      0.58      0.67       167\n",
      "      HS_Race       0.81      0.58      0.68        88\n",
      "  HS_Physical       1.00      0.03      0.05        74\n",
      "    HS_Gender       1.00      0.01      0.03        75\n",
      "     HS_Other       0.74      0.83      0.78       744\n",
      "      HS_Weak       0.66      0.82      0.73       690\n",
      "  HS_Moderate       0.74      0.52      0.61       338\n",
      "    HS_Strong       0.79      0.62      0.70        79\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5499\n",
      "    macro avg       0.81      0.60      0.62      5499\n",
      " weighted avg       0.78      0.78      0.76      5499\n",
      "  samples avg       0.43      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0012090791249647738\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 71.80477356910706 seconds\n",
      "\n",
      "Fold 4 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5812 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4603, Accuracy: 0.8478, F1 Micro: 0.4834, F1 Macro: 0.1882\n",
      "Epoch 2/10, Train Loss: 0.3763, Accuracy: 0.8957, F1 Micro: 0.6249, F1 Macro: 0.4091\n",
      "Epoch 3/10, Train Loss: 0.3185, Accuracy: 0.9146, F1 Micro: 0.7429, F1 Macro: 0.5769\n",
      "Epoch 4/10, Train Loss: 0.2727, Accuracy: 0.9202, F1 Micro: 0.7554, F1 Macro: 0.5968\n",
      "Epoch 5/10, Train Loss: 0.228, Accuracy: 0.9237, F1 Micro: 0.7719, F1 Macro: 0.611\n",
      "Epoch 6/10, Train Loss: 0.19, Accuracy: 0.9172, F1 Micro: 0.7692, F1 Macro: 0.6128\n",
      "Epoch 7/10, Train Loss: 0.1619, Accuracy: 0.9242, F1 Micro: 0.7639, F1 Macro: 0.6149\n",
      "Epoch 8/10, Train Loss: 0.1357, Accuracy: 0.9222, F1 Micro: 0.7699, F1 Macro: 0.626\n",
      "Epoch 9/10, Train Loss: 0.1185, Accuracy: 0.925, F1 Micro: 0.7772, F1 Macro: 0.6596\n",
      "Epoch 10/10, Train Loss: 0.1058, Accuracy: 0.9236, F1 Micro: 0.7776, F1 Macro: 0.654\n",
      "Best result for 5812 samples: F1 Micro: 0.7776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1107\n",
      "      Abusive       0.87      0.92      0.89      1030\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.67      0.72      0.69       378\n",
      "  HS_Religion       0.74      0.66      0.70       167\n",
      "      HS_Race       0.73      0.75      0.74        88\n",
      "  HS_Physical       0.71      0.07      0.12        74\n",
      "    HS_Gender       0.85      0.15      0.25        75\n",
      "     HS_Other       0.76      0.81      0.79       744\n",
      "      HS_Weak       0.72      0.73      0.73       690\n",
      "  HS_Moderate       0.61      0.67      0.63       338\n",
      "    HS_Strong       0.75      0.66      0.70        79\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5499\n",
      "    macro avg       0.75      0.65      0.65      5499\n",
      " weighted avg       0.77      0.78      0.77      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.00106588585767895\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 64.79168510437012 seconds\n",
      "\n",
      "Fold 4 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6285 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4553, Accuracy: 0.861, F1 Micro: 0.4934, F1 Macro: 0.2021\n",
      "Epoch 2/10, Train Loss: 0.358, Accuracy: 0.9025, F1 Micro: 0.6732, F1 Macro: 0.4673\n",
      "Epoch 3/10, Train Loss: 0.307, Accuracy: 0.9176, F1 Micro: 0.733, F1 Macro: 0.5468\n",
      "Epoch 4/10, Train Loss: 0.2554, Accuracy: 0.9234, F1 Micro: 0.7654, F1 Macro: 0.5975\n",
      "Epoch 5/10, Train Loss: 0.2185, Accuracy: 0.926, F1 Micro: 0.7685, F1 Macro: 0.6047\n",
      "Epoch 6/10, Train Loss: 0.1823, Accuracy: 0.9279, F1 Micro: 0.7779, F1 Macro: 0.613\n",
      "Epoch 7/10, Train Loss: 0.156, Accuracy: 0.9272, F1 Micro: 0.7768, F1 Macro: 0.625\n",
      "Epoch 8/10, Train Loss: 0.1353, Accuracy: 0.9216, F1 Micro: 0.7698, F1 Macro: 0.6228\n",
      "Epoch 9/10, Train Loss: 0.1224, Accuracy: 0.9284, F1 Micro: 0.7839, F1 Macro: 0.6586\n",
      "Epoch 10/10, Train Loss: 0.0971, Accuracy: 0.9274, F1 Micro: 0.7813, F1 Macro: 0.6699\n",
      "Best result for 6285 samples: F1 Micro: 0.7839\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1107\n",
      "      Abusive       0.91      0.89      0.90      1030\n",
      "HS_Individual       0.76      0.76      0.76       729\n",
      "     HS_Group       0.77      0.65      0.70       378\n",
      "  HS_Religion       0.78      0.61      0.68       167\n",
      "      HS_Race       0.75      0.66      0.70        88\n",
      "  HS_Physical       0.46      0.08      0.14        74\n",
      "    HS_Gender       0.88      0.19      0.31        75\n",
      "     HS_Other       0.79      0.78      0.79       744\n",
      "      HS_Weak       0.73      0.74      0.74       690\n",
      "  HS_Moderate       0.71      0.57      0.63       338\n",
      "    HS_Strong       0.73      0.66      0.69        79\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5499\n",
      "    macro avg       0.76      0.62      0.66      5499\n",
      " weighted avg       0.80      0.76      0.78      5499\n",
      "  samples avg       0.44      0.42      0.41      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0005647436715662479\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 58.76395654678345 seconds\n",
      "\n",
      "Fold 4 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6584 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4542, Accuracy: 0.8567, F1 Micro: 0.5401, F1 Macro: 0.2278\n",
      "Epoch 2/10, Train Loss: 0.3579, Accuracy: 0.904, F1 Micro: 0.7079, F1 Macro: 0.4918\n",
      "Epoch 3/10, Train Loss: 0.3004, Accuracy: 0.9151, F1 Micro: 0.7491, F1 Macro: 0.5561\n",
      "Epoch 4/10, Train Loss: 0.2596, Accuracy: 0.9215, F1 Micro: 0.7647, F1 Macro: 0.5877\n",
      "Epoch 5/10, Train Loss: 0.2209, Accuracy: 0.9264, F1 Micro: 0.7761, F1 Macro: 0.618\n",
      "Epoch 6/10, Train Loss: 0.1812, Accuracy: 0.9251, F1 Micro: 0.7784, F1 Macro: 0.6255\n",
      "Epoch 7/10, Train Loss: 0.1556, Accuracy: 0.9281, F1 Micro: 0.789, F1 Macro: 0.6449\n",
      "Epoch 8/10, Train Loss: 0.1276, Accuracy: 0.9282, F1 Micro: 0.788, F1 Macro: 0.6617\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9281, F1 Micro: 0.7778, F1 Macro: 0.633\n",
      "Epoch 10/10, Train Loss: 0.0998, Accuracy: 0.9289, F1 Micro: 0.7834, F1 Macro: 0.6766\n",
      "Best result for 6584 samples: F1 Micro: 0.789\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.87      1107\n",
      "      Abusive       0.90      0.90      0.90      1030\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.75      0.69      0.72       378\n",
      "  HS_Religion       0.78      0.60      0.68       167\n",
      "      HS_Race       0.76      0.67      0.71        88\n",
      "  HS_Physical       0.75      0.04      0.08        74\n",
      "    HS_Gender       0.86      0.08      0.15        75\n",
      "     HS_Other       0.75      0.85      0.79       744\n",
      "      HS_Weak       0.72      0.77      0.75       690\n",
      "  HS_Moderate       0.68      0.62      0.65       338\n",
      "    HS_Strong       0.74      0.66      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5499\n",
      "    macro avg       0.77      0.63      0.64      5499\n",
      " weighted avg       0.79      0.79      0.78      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0004866108065471053\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.27981758117676 seconds\n",
      "\n",
      "Fold 4 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6980 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4475, Accuracy: 0.8673, F1 Micro: 0.5277, F1 Macro: 0.2247\n",
      "Epoch 2/10, Train Loss: 0.3511, Accuracy: 0.9056, F1 Micro: 0.7007, F1 Macro: 0.4791\n",
      "Epoch 3/10, Train Loss: 0.2871, Accuracy: 0.9116, F1 Micro: 0.7437, F1 Macro: 0.5537\n",
      "Epoch 4/10, Train Loss: 0.2451, Accuracy: 0.9205, F1 Micro: 0.7728, F1 Macro: 0.6137\n",
      "Epoch 5/10, Train Loss: 0.2102, Accuracy: 0.9244, F1 Micro: 0.7757, F1 Macro: 0.6115\n",
      "Epoch 6/10, Train Loss: 0.1755, Accuracy: 0.926, F1 Micro: 0.7744, F1 Macro: 0.6258\n",
      "Epoch 7/10, Train Loss: 0.1454, Accuracy: 0.9242, F1 Micro: 0.7794, F1 Macro: 0.638\n",
      "Epoch 8/10, Train Loss: 0.1229, Accuracy: 0.928, F1 Micro: 0.7857, F1 Macro: 0.6705\n",
      "Epoch 9/10, Train Loss: 0.1112, Accuracy: 0.9237, F1 Micro: 0.7786, F1 Macro: 0.6613\n",
      "Epoch 10/10, Train Loss: 0.0981, Accuracy: 0.9256, F1 Micro: 0.7819, F1 Macro: 0.6706\n",
      "Best result for 6980 samples: F1 Micro: 0.7857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1107\n",
      "      Abusive       0.88      0.92      0.90      1030\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.73      0.68      0.71       378\n",
      "  HS_Religion       0.78      0.63      0.70       167\n",
      "      HS_Race       0.79      0.59      0.68        88\n",
      "  HS_Physical       0.48      0.14      0.21        74\n",
      "    HS_Gender       0.90      0.24      0.38        75\n",
      "     HS_Other       0.81      0.79      0.80       744\n",
      "      HS_Weak       0.72      0.74      0.73       690\n",
      "  HS_Moderate       0.66      0.62      0.64       338\n",
      "    HS_Strong       0.84      0.59      0.70        79\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5499\n",
      "    macro avg       0.77      0.63      0.67      5499\n",
      " weighted avg       0.79      0.77      0.78      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00030303628882393213\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 48.94879198074341 seconds\n",
      "\n",
      "Fold 4 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4425, Accuracy: 0.8726, F1 Micro: 0.5879, F1 Macro: 0.2688\n",
      "Epoch 2/10, Train Loss: 0.3326, Accuracy: 0.9076, F1 Micro: 0.7244, F1 Macro: 0.512\n",
      "Epoch 3/10, Train Loss: 0.2769, Accuracy: 0.9194, F1 Micro: 0.7607, F1 Macro: 0.5901\n",
      "Epoch 4/10, Train Loss: 0.2368, Accuracy: 0.9232, F1 Micro: 0.7762, F1 Macro: 0.6128\n",
      "Epoch 5/10, Train Loss: 0.1988, Accuracy: 0.9235, F1 Micro: 0.7647, F1 Macro: 0.6015\n",
      "Epoch 6/10, Train Loss: 0.1686, Accuracy: 0.9278, F1 Micro: 0.7779, F1 Macro: 0.624\n",
      "Epoch 7/10, Train Loss: 0.1438, Accuracy: 0.9272, F1 Micro: 0.7824, F1 Macro: 0.6383\n",
      "Epoch 8/10, Train Loss: 0.1246, Accuracy: 0.927, F1 Micro: 0.7812, F1 Macro: 0.6452\n",
      "Epoch 9/10, Train Loss: 0.0996, Accuracy: 0.9279, F1 Micro: 0.7873, F1 Macro: 0.6774\n",
      "Epoch 10/10, Train Loss: 0.0929, Accuracy: 0.9256, F1 Micro: 0.785, F1 Macro: 0.6795\n",
      "Best result for 7336 samples: F1 Micro: 0.7873\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.70      0.69      0.70       378\n",
      "  HS_Religion       0.76      0.64      0.69       167\n",
      "      HS_Race       0.71      0.68      0.69        88\n",
      "  HS_Physical       0.67      0.11      0.19        74\n",
      "    HS_Gender       0.92      0.29      0.44        75\n",
      "     HS_Other       0.78      0.82      0.80       744\n",
      "      HS_Weak       0.74      0.72      0.73       690\n",
      "  HS_Moderate       0.64      0.65      0.65       338\n",
      "    HS_Strong       0.81      0.65      0.72        79\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5499\n",
      "    macro avg       0.77      0.65      0.68      5499\n",
      " weighted avg       0.79      0.78      0.78      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00023481629032175993\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.46724557876587 seconds\n",
      "\n",
      "Fold 4 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7656 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.437, Accuracy: 0.8814, F1 Micro: 0.6021, F1 Macro: 0.2891\n",
      "Epoch 2/10, Train Loss: 0.326, Accuracy: 0.906, F1 Micro: 0.6926, F1 Macro: 0.4439\n",
      "Epoch 3/10, Train Loss: 0.2648, Accuracy: 0.9196, F1 Micro: 0.7443, F1 Macro: 0.5741\n",
      "Epoch 4/10, Train Loss: 0.2299, Accuracy: 0.9224, F1 Micro: 0.7597, F1 Macro: 0.5927\n",
      "Epoch 5/10, Train Loss: 0.1952, Accuracy: 0.9226, F1 Micro: 0.7763, F1 Macro: 0.6224\n",
      "Epoch 6/10, Train Loss: 0.1617, Accuracy: 0.9275, F1 Micro: 0.7835, F1 Macro: 0.639\n",
      "Epoch 7/10, Train Loss: 0.1365, Accuracy: 0.9246, F1 Micro: 0.7791, F1 Macro: 0.6511\n",
      "Epoch 8/10, Train Loss: 0.1146, Accuracy: 0.9277, F1 Micro: 0.7859, F1 Macro: 0.68\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.9282, F1 Micro: 0.7808, F1 Macro: 0.6675\n",
      "Epoch 10/10, Train Loss: 0.0884, Accuracy: 0.9283, F1 Micro: 0.7835, F1 Macro: 0.6898\n",
      "Best result for 7656 samples: F1 Micro: 0.7859\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1107\n",
      "      Abusive       0.91      0.92      0.91      1030\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.71      0.68      0.69       378\n",
      "  HS_Religion       0.76      0.65      0.70       167\n",
      "      HS_Race       0.73      0.66      0.69        88\n",
      "  HS_Physical       0.52      0.15      0.23        74\n",
      "    HS_Gender       0.92      0.31      0.46        75\n",
      "     HS_Other       0.78      0.81      0.79       744\n",
      "      HS_Weak       0.72      0.73      0.73       690\n",
      "  HS_Moderate       0.64      0.63      0.64       338\n",
      "    HS_Strong       0.80      0.62      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5499\n",
      "    macro avg       0.76      0.65      0.68      5499\n",
      " weighted avg       0.79      0.78      0.78      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00010090177675010652\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 40.22536849975586 seconds\n",
      "\n",
      "Fold 4 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7901 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4288, Accuracy: 0.8837, F1 Micro: 0.614, F1 Macro: 0.3189\n",
      "Epoch 2/10, Train Loss: 0.3184, Accuracy: 0.9097, F1 Micro: 0.7314, F1 Macro: 0.5395\n",
      "Epoch 3/10, Train Loss: 0.2653, Accuracy: 0.9174, F1 Micro: 0.7609, F1 Macro: 0.5967\n",
      "Epoch 4/10, Train Loss: 0.2175, Accuracy: 0.9227, F1 Micro: 0.7686, F1 Macro: 0.6083\n",
      "Epoch 5/10, Train Loss: 0.1897, Accuracy: 0.924, F1 Micro: 0.7786, F1 Macro: 0.6292\n",
      "Epoch 6/10, Train Loss: 0.1589, Accuracy: 0.9255, F1 Micro: 0.7822, F1 Macro: 0.6264\n",
      "Epoch 7/10, Train Loss: 0.1326, Accuracy: 0.9279, F1 Micro: 0.7863, F1 Macro: 0.6634\n",
      "Epoch 8/10, Train Loss: 0.1138, Accuracy: 0.9258, F1 Micro: 0.7735, F1 Macro: 0.6667\n",
      "Epoch 9/10, Train Loss: 0.1034, Accuracy: 0.9279, F1 Micro: 0.7797, F1 Macro: 0.6766\n",
      "Epoch 10/10, Train Loss: 0.0852, Accuracy: 0.9268, F1 Micro: 0.7869, F1 Macro: 0.6809\n",
      "Best result for 7901 samples: F1 Micro: 0.7869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.73      0.79      0.76       729\n",
      "     HS_Group       0.72      0.64      0.68       378\n",
      "  HS_Religion       0.81      0.60      0.69       167\n",
      "      HS_Race       0.76      0.58      0.66        88\n",
      "  HS_Physical       0.69      0.15      0.24        74\n",
      "    HS_Gender       0.84      0.36      0.50        75\n",
      "     HS_Other       0.76      0.84      0.80       744\n",
      "      HS_Weak       0.70      0.78      0.73       690\n",
      "  HS_Moderate       0.66      0.59      0.62       338\n",
      "    HS_Strong       0.82      0.63      0.71        79\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5499\n",
      "    macro avg       0.77      0.65      0.68      5499\n",
      " weighted avg       0.78      0.79      0.78      5499\n",
      "  samples avg       0.44      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 4.085354594280949e-05\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.787134647369385 seconds\n",
      "\n",
      "Fold 4 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.425, Accuracy: 0.8866, F1 Micro: 0.6111, F1 Macro: 0.3176\n",
      "Epoch 2/10, Train Loss: 0.313, Accuracy: 0.911, F1 Micro: 0.7129, F1 Macro: 0.488\n",
      "Epoch 3/10, Train Loss: 0.255, Accuracy: 0.9195, F1 Micro: 0.7632, F1 Macro: 0.5781\n",
      "Epoch 4/10, Train Loss: 0.2203, Accuracy: 0.9261, F1 Micro: 0.7754, F1 Macro: 0.6099\n",
      "Epoch 5/10, Train Loss: 0.1844, Accuracy: 0.927, F1 Micro: 0.7804, F1 Macro: 0.6297\n",
      "Epoch 6/10, Train Loss: 0.158, Accuracy: 0.9246, F1 Micro: 0.7812, F1 Macro: 0.6106\n",
      "Epoch 7/10, Train Loss: 0.1286, Accuracy: 0.9261, F1 Micro: 0.7709, F1 Macro: 0.6093\n",
      "Epoch 8/10, Train Loss: 0.1149, Accuracy: 0.9252, F1 Micro: 0.7888, F1 Macro: 0.6839\n",
      "Epoch 9/10, Train Loss: 0.0981, Accuracy: 0.9295, F1 Micro: 0.779, F1 Macro: 0.673\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9286, F1 Micro: 0.788, F1 Macro: 0.6958\n",
      "Best result for 8165 samples: F1 Micro: 0.7888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.92      0.86      1107\n",
      "      Abusive       0.89      0.93      0.91      1030\n",
      "HS_Individual       0.68      0.85      0.76       729\n",
      "     HS_Group       0.76      0.64      0.70       378\n",
      "  HS_Religion       0.77      0.63      0.69       167\n",
      "      HS_Race       0.75      0.65      0.70        88\n",
      "  HS_Physical       0.65      0.20      0.31        74\n",
      "    HS_Gender       0.84      0.28      0.42        75\n",
      "     HS_Other       0.73      0.87      0.80       744\n",
      "      HS_Weak       0.67      0.83      0.74       690\n",
      "  HS_Moderate       0.71      0.58      0.64       338\n",
      "    HS_Strong       0.74      0.66      0.70        79\n",
      "\n",
      "    micro avg       0.76      0.82      0.79      5499\n",
      "    macro avg       0.75      0.67      0.68      5499\n",
      " weighted avg       0.76      0.82      0.78      5499\n",
      "  samples avg       0.45      0.45      0.44      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 4.8155283366213505e-05\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 33.16045618057251 seconds\n",
      "\n",
      "Fold 4 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8402 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4205, Accuracy: 0.889, F1 Micro: 0.6259, F1 Macro: 0.3398\n",
      "Epoch 2/10, Train Loss: 0.3021, Accuracy: 0.9099, F1 Micro: 0.7272, F1 Macro: 0.5104\n",
      "Epoch 3/10, Train Loss: 0.2508, Accuracy: 0.92, F1 Micro: 0.7373, F1 Macro: 0.5702\n",
      "Epoch 4/10, Train Loss: 0.2092, Accuracy: 0.9236, F1 Micro: 0.7536, F1 Macro: 0.5873\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.928, F1 Micro: 0.7788, F1 Macro: 0.6249\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.927, F1 Micro: 0.7864, F1 Macro: 0.6449\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9273, F1 Micro: 0.7694, F1 Macro: 0.6284\n",
      "Epoch 8/10, Train Loss: 0.1082, Accuracy: 0.9276, F1 Micro: 0.7857, F1 Macro: 0.6688\n",
      "Epoch 9/10, Train Loss: 0.0939, Accuracy: 0.9252, F1 Micro: 0.7754, F1 Macro: 0.6641\n",
      "Epoch 10/10, Train Loss: 0.0825, Accuracy: 0.9302, F1 Micro: 0.7905, F1 Macro: 0.6984\n",
      "Best result for 8402 samples: F1 Micro: 0.7905\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1107\n",
      "      Abusive       0.92      0.92      0.92      1030\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.72      0.70      0.71       378\n",
      "  HS_Religion       0.72      0.71      0.72       167\n",
      "      HS_Race       0.75      0.64      0.69        88\n",
      "  HS_Physical       0.70      0.19      0.30        74\n",
      "    HS_Gender       0.94      0.40      0.56        75\n",
      "     HS_Other       0.80      0.80      0.80       744\n",
      "      HS_Weak       0.75      0.71      0.73       690\n",
      "  HS_Moderate       0.64      0.63      0.64       338\n",
      "    HS_Strong       0.79      0.66      0.72        79\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5499\n",
      "    macro avg       0.78      0.66      0.70      5499\n",
      " weighted avg       0.81      0.77      0.79      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 8.814506327325943e-06\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 30.117881059646606 seconds\n",
      "\n",
      "Fold 4 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8616 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4175, Accuracy: 0.8884, F1 Micro: 0.6166, F1 Macro: 0.2967\n",
      "Epoch 2/10, Train Loss: 0.2954, Accuracy: 0.9135, F1 Micro: 0.718, F1 Macro: 0.5133\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.9196, F1 Micro: 0.7374, F1 Macro: 0.5636\n",
      "Epoch 4/10, Train Loss: 0.2036, Accuracy: 0.9243, F1 Micro: 0.759, F1 Macro: 0.5765\n",
      "Epoch 5/10, Train Loss: 0.1757, Accuracy: 0.9234, F1 Micro: 0.7795, F1 Macro: 0.6156\n",
      "Epoch 6/10, Train Loss: 0.1487, Accuracy: 0.9297, F1 Micro: 0.7824, F1 Macro: 0.6417\n",
      "Epoch 7/10, Train Loss: 0.1249, Accuracy: 0.9292, F1 Micro: 0.7873, F1 Macro: 0.6678\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.929, F1 Micro: 0.7819, F1 Macro: 0.6652\n",
      "Epoch 9/10, Train Loss: 0.0914, Accuracy: 0.9271, F1 Micro: 0.783, F1 Macro: 0.67\n",
      "Epoch 10/10, Train Loss: 0.079, Accuracy: 0.9301, F1 Micro: 0.792, F1 Macro: 0.6972\n",
      "Best result for 8616 samples: F1 Micro: 0.792\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1107\n",
      "      Abusive       0.90      0.93      0.92      1030\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.70      0.71      0.70       378\n",
      "  HS_Religion       0.77      0.63      0.70       167\n",
      "      HS_Race       0.70      0.75      0.73        88\n",
      "  HS_Physical       0.58      0.20      0.30        74\n",
      "    HS_Gender       0.90      0.37      0.53        75\n",
      "     HS_Other       0.80      0.82      0.81       744\n",
      "      HS_Weak       0.75      0.70      0.73       690\n",
      "  HS_Moderate       0.64      0.66      0.65       338\n",
      "    HS_Strong       0.76      0.65      0.70        79\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5499\n",
      "    macro avg       0.76      0.67      0.70      5499\n",
      " weighted avg       0.80      0.78      0.79      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 8.668636837683154e-06\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.48751211166382 seconds\n",
      "\n",
      "Fold 4 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8816 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4127, Accuracy: 0.8908, F1 Micro: 0.6188, F1 Macro: 0.3253\n",
      "Epoch 2/10, Train Loss: 0.2937, Accuracy: 0.9119, F1 Micro: 0.7303, F1 Macro: 0.5332\n",
      "Epoch 3/10, Train Loss: 0.2388, Accuracy: 0.9222, F1 Micro: 0.7532, F1 Macro: 0.5875\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.9222, F1 Micro: 0.7669, F1 Macro: 0.5743\n",
      "Epoch 5/10, Train Loss: 0.1725, Accuracy: 0.9278, F1 Micro: 0.776, F1 Macro: 0.6064\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9271, F1 Micro: 0.7838, F1 Macro: 0.6527\n",
      "Epoch 7/10, Train Loss: 0.1181, Accuracy: 0.9276, F1 Micro: 0.7815, F1 Macro: 0.6463\n",
      "Epoch 8/10, Train Loss: 0.1019, Accuracy: 0.9273, F1 Micro: 0.7831, F1 Macro: 0.6717\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.9284, F1 Micro: 0.7789, F1 Macro: 0.6707\n",
      "Epoch 10/10, Train Loss: 0.0774, Accuracy: 0.9277, F1 Micro: 0.7822, F1 Macro: 0.6795\n",
      "Best result for 8816 samples: F1 Micro: 0.7838\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.71      0.67      0.69       378\n",
      "  HS_Religion       0.75      0.67      0.71       167\n",
      "      HS_Race       0.75      0.62      0.68        88\n",
      "  HS_Physical       0.62      0.07      0.12        74\n",
      "    HS_Gender       0.92      0.16      0.27        75\n",
      "     HS_Other       0.79      0.79      0.79       744\n",
      "      HS_Weak       0.72      0.73      0.72       690\n",
      "  HS_Moderate       0.67      0.59      0.63       338\n",
      "    HS_Strong       0.78      0.62      0.69        79\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5499\n",
      "    macro avg       0.77      0.62      0.65      5499\n",
      " weighted avg       0.79      0.78      0.77      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.6884040926233867e-05\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.506067514419556 seconds\n",
      "\n",
      "Fold 4 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4031, Accuracy: 0.8903, F1 Micro: 0.6609, F1 Macro: 0.3729\n",
      "Epoch 2/10, Train Loss: 0.2894, Accuracy: 0.9119, F1 Micro: 0.7092, F1 Macro: 0.4817\n",
      "Epoch 3/10, Train Loss: 0.2312, Accuracy: 0.9226, F1 Micro: 0.7668, F1 Macro: 0.5902\n",
      "Epoch 4/10, Train Loss: 0.1982, Accuracy: 0.9267, F1 Micro: 0.7731, F1 Macro: 0.5987\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.9248, F1 Micro: 0.7808, F1 Macro: 0.6232\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9264, F1 Micro: 0.7815, F1 Macro: 0.6299\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.927, F1 Micro: 0.7796, F1 Macro: 0.6466\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.9288, F1 Micro: 0.7824, F1 Macro: 0.681\n",
      "Epoch 9/10, Train Loss: 0.0847, Accuracy: 0.9248, F1 Micro: 0.7815, F1 Macro: 0.6713\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9287, F1 Micro: 0.787, F1 Macro: 0.6889\n",
      "Best result for 9016 samples: F1 Micro: 0.787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1107\n",
      "      Abusive       0.91      0.92      0.92      1030\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.73      0.66      0.69       378\n",
      "  HS_Religion       0.78      0.65      0.71       167\n",
      "      HS_Race       0.78      0.52      0.63        88\n",
      "  HS_Physical       0.65      0.20      0.31        74\n",
      "    HS_Gender       0.87      0.35      0.50        75\n",
      "     HS_Other       0.78      0.81      0.80       744\n",
      "      HS_Weak       0.73      0.72      0.73       690\n",
      "  HS_Moderate       0.68      0.60      0.64       338\n",
      "    HS_Strong       0.79      0.72      0.75        79\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5499\n",
      "    macro avg       0.78      0.65      0.69      5499\n",
      " weighted avg       0.80      0.77      0.78      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 3.997958174295491e-06\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.72405505180359 seconds\n",
      "\n",
      "Fold 4 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9216 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4033, Accuracy: 0.889, F1 Micro: 0.5984, F1 Macro: 0.3031\n",
      "Epoch 2/10, Train Loss: 0.2839, Accuracy: 0.9138, F1 Micro: 0.7316, F1 Macro: 0.5341\n",
      "Epoch 3/10, Train Loss: 0.227, Accuracy: 0.9223, F1 Micro: 0.7571, F1 Macro: 0.5769\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.926, F1 Micro: 0.7622, F1 Macro: 0.5988\n",
      "Epoch 5/10, Train Loss: 0.1614, Accuracy: 0.9267, F1 Micro: 0.7772, F1 Macro: 0.6383\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9235, F1 Micro: 0.7803, F1 Macro: 0.6326\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9237, F1 Micro: 0.7793, F1 Macro: 0.6545\n",
      "Epoch 8/10, Train Loss: 0.0959, Accuracy: 0.9253, F1 Micro: 0.785, F1 Macro: 0.6797\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9242, F1 Micro: 0.7604, F1 Macro: 0.6514\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9235, F1 Micro: 0.7835, F1 Macro: 0.6987\n",
      "Best result for 9216 samples: F1 Micro: 0.785\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1107\n",
      "      Abusive       0.90      0.93      0.91      1030\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.69      0.69      0.69       378\n",
      "  HS_Religion       0.72      0.68      0.70       167\n",
      "      HS_Race       0.73      0.68      0.71        88\n",
      "  HS_Physical       0.61      0.15      0.24        74\n",
      "    HS_Gender       0.95      0.28      0.43        75\n",
      "     HS_Other       0.74      0.84      0.79       744\n",
      "      HS_Weak       0.71      0.76      0.73       690\n",
      "  HS_Moderate       0.64      0.63      0.63       338\n",
      "    HS_Strong       0.72      0.71      0.71        79\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5499\n",
      "    macro avg       0.75      0.67      0.68      5499\n",
      " weighted avg       0.77      0.80      0.78      5499\n",
      "  samples avg       0.45      0.44      0.44      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 8.568321936763824e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.185876607894897 seconds\n",
      "\n",
      "Fold 4 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3984, Accuracy: 0.8906, F1 Micro: 0.605, F1 Macro: 0.3151\n",
      "Epoch 2/10, Train Loss: 0.2781, Accuracy: 0.9144, F1 Micro: 0.726, F1 Macro: 0.523\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9209, F1 Micro: 0.7492, F1 Macro: 0.5626\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9266, F1 Micro: 0.7785, F1 Macro: 0.6142\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.9258, F1 Micro: 0.7802, F1 Macro: 0.6148\n",
      "Epoch 6/10, Train Loss: 0.1414, Accuracy: 0.927, F1 Micro: 0.7875, F1 Macro: 0.6442\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9288, F1 Micro: 0.7869, F1 Macro: 0.6725\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9279, F1 Micro: 0.7895, F1 Macro: 0.6766\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9287, F1 Micro: 0.7862, F1 Macro: 0.6624\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9259, F1 Micro: 0.7786, F1 Macro: 0.6518\n",
      "Best result for 9218 samples: F1 Micro: 0.7895\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1107\n",
      "      Abusive       0.91      0.93      0.92      1030\n",
      "HS_Individual       0.73      0.79      0.76       729\n",
      "     HS_Group       0.75      0.65      0.70       378\n",
      "  HS_Religion       0.78      0.63      0.70       167\n",
      "      HS_Race       0.73      0.65      0.69        88\n",
      "  HS_Physical       0.61      0.19      0.29        74\n",
      "    HS_Gender       0.85      0.23      0.36        75\n",
      "     HS_Other       0.76      0.85      0.80       744\n",
      "      HS_Weak       0.70      0.77      0.73       690\n",
      "  HS_Moderate       0.68      0.58      0.62       338\n",
      "    HS_Strong       0.74      0.66      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5499\n",
      "    macro avg       0.76      0.65      0.68      5499\n",
      " weighted avg       0.78      0.79      0.78      5499\n",
      "  samples avg       0.44      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 5.78752378714853e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.13653254508972 seconds\n",
      "\n",
      "Fold 4 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3972, Accuracy: 0.8908, F1 Micro: 0.6112, F1 Macro: 0.3333\n",
      "Epoch 2/10, Train Loss: 0.2769, Accuracy: 0.9153, F1 Micro: 0.7318, F1 Macro: 0.519\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.9231, F1 Micro: 0.7686, F1 Macro: 0.5982\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9261, F1 Micro: 0.7744, F1 Macro: 0.611\n",
      "Epoch 5/10, Train Loss: 0.1601, Accuracy: 0.9275, F1 Micro: 0.7794, F1 Macro: 0.6222\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9267, F1 Micro: 0.7638, F1 Macro: 0.6335\n",
      "Epoch 7/10, Train Loss: 0.1157, Accuracy: 0.9255, F1 Micro: 0.7787, F1 Macro: 0.6757\n",
      "Epoch 8/10, Train Loss: 0.0962, Accuracy: 0.9275, F1 Micro: 0.7888, F1 Macro: 0.6944\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.9262, F1 Micro: 0.7849, F1 Macro: 0.6758\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9244, F1 Micro: 0.7808, F1 Macro: 0.675\n",
      "Best result for 9418 samples: F1 Micro: 0.7888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1107\n",
      "      Abusive       0.89      0.93      0.91      1030\n",
      "HS_Individual       0.75      0.76      0.75       729\n",
      "     HS_Group       0.69      0.70      0.70       378\n",
      "  HS_Religion       0.77      0.66      0.71       167\n",
      "      HS_Race       0.70      0.69      0.70        88\n",
      "  HS_Physical       0.51      0.26      0.34        74\n",
      "    HS_Gender       0.74      0.35      0.47        75\n",
      "     HS_Other       0.77      0.82      0.80       744\n",
      "      HS_Weak       0.73      0.74      0.73       690\n",
      "  HS_Moderate       0.63      0.65      0.64       338\n",
      "    HS_Strong       0.78      0.66      0.71        79\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5499\n",
      "    macro avg       0.73      0.68      0.69      5499\n",
      " weighted avg       0.78      0.79      0.79      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 5.0613266466825735e-06\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.291650533676147 seconds\n",
      "\n",
      "Fold 4 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9618 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3913, Accuracy: 0.8925, F1 Micro: 0.6276, F1 Macro: 0.3305\n",
      "Epoch 2/10, Train Loss: 0.2706, Accuracy: 0.9122, F1 Micro: 0.7224, F1 Macro: 0.5035\n",
      "Epoch 3/10, Train Loss: 0.2162, Accuracy: 0.9219, F1 Micro: 0.7603, F1 Macro: 0.5906\n",
      "Epoch 4/10, Train Loss: 0.1835, Accuracy: 0.9262, F1 Micro: 0.7678, F1 Macro: 0.6036\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9297, F1 Micro: 0.7804, F1 Macro: 0.6435\n",
      "Epoch 6/10, Train Loss: 0.131, Accuracy: 0.9281, F1 Micro: 0.7822, F1 Macro: 0.6564\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9309, F1 Micro: 0.7889, F1 Macro: 0.6836\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9283, F1 Micro: 0.7864, F1 Macro: 0.677\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9304, F1 Micro: 0.785, F1 Macro: 0.6913\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9276, F1 Micro: 0.7819, F1 Macro: 0.6794\n",
      "Best result for 9618 samples: F1 Micro: 0.7889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1107\n",
      "      Abusive       0.93      0.88      0.91      1030\n",
      "HS_Individual       0.78      0.74      0.76       729\n",
      "     HS_Group       0.76      0.65      0.70       378\n",
      "  HS_Religion       0.81      0.66      0.73       167\n",
      "      HS_Race       0.75      0.68      0.71        88\n",
      "  HS_Physical       0.53      0.12      0.20        74\n",
      "    HS_Gender       0.89      0.32      0.47        75\n",
      "     HS_Other       0.81      0.78      0.79       744\n",
      "      HS_Weak       0.75      0.73      0.74       690\n",
      "  HS_Moderate       0.69      0.59      0.63       338\n",
      "    HS_Strong       0.80      0.62      0.70        79\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5499\n",
      "    macro avg       0.78      0.63      0.68      5499\n",
      " weighted avg       0.82      0.76      0.78      5499\n",
      "  samples avg       0.44      0.41      0.41      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 5.221537048782919e-06\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.511491298675537 seconds\n",
      "\n",
      "Fold 4 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9818 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3874, Accuracy: 0.8946, F1 Micro: 0.6491, F1 Macro: 0.3487\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.9144, F1 Micro: 0.7349, F1 Macro: 0.5513\n",
      "Epoch 3/10, Train Loss: 0.2146, Accuracy: 0.9217, F1 Micro: 0.7507, F1 Macro: 0.5891\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9264, F1 Micro: 0.7692, F1 Macro: 0.6007\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9277, F1 Micro: 0.7693, F1 Macro: 0.6209\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9264, F1 Micro: 0.7861, F1 Macro: 0.6433\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9302, F1 Micro: 0.7856, F1 Macro: 0.6762\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9289, F1 Micro: 0.7855, F1 Macro: 0.6717\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.9281, F1 Micro: 0.7825, F1 Macro: 0.6838\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9289, F1 Micro: 0.7867, F1 Macro: 0.6924\n",
      "Best result for 9818 samples: F1 Micro: 0.7867\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1107\n",
      "      Abusive       0.90      0.94      0.92      1030\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.76      0.64      0.70       378\n",
      "  HS_Religion       0.77      0.68      0.72       167\n",
      "      HS_Race       0.75      0.52      0.62        88\n",
      "  HS_Physical       0.55      0.24      0.34        74\n",
      "    HS_Gender       0.88      0.39      0.54        75\n",
      "     HS_Other       0.80      0.78      0.79       744\n",
      "      HS_Weak       0.72      0.72      0.72       690\n",
      "  HS_Moderate       0.69      0.61      0.65       338\n",
      "    HS_Strong       0.82      0.65      0.72        79\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5499\n",
      "    macro avg       0.77      0.65      0.69      5499\n",
      " weighted avg       0.80      0.77      0.78      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.9458022052276637e-06\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.87756061553955 seconds\n",
      "\n",
      "Fold 4 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10018 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.384, Accuracy: 0.893, F1 Micro: 0.6587, F1 Macro: 0.3684\n",
      "Epoch 2/10, Train Loss: 0.2617, Accuracy: 0.9152, F1 Micro: 0.7393, F1 Macro: 0.5386\n",
      "Epoch 3/10, Train Loss: 0.2127, Accuracy: 0.9217, F1 Micro: 0.7613, F1 Macro: 0.5929\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.9251, F1 Micro: 0.7548, F1 Macro: 0.5988\n",
      "Epoch 5/10, Train Loss: 0.151, Accuracy: 0.9272, F1 Micro: 0.7725, F1 Macro: 0.6226\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9244, F1 Micro: 0.7789, F1 Macro: 0.6287\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9297, F1 Micro: 0.7842, F1 Macro: 0.6721\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9267, F1 Micro: 0.7834, F1 Macro: 0.6719\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.924, F1 Micro: 0.7827, F1 Macro: 0.6892\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9273, F1 Micro: 0.7823, F1 Macro: 0.6889\n",
      "Best result for 10018 samples: F1 Micro: 0.7842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1107\n",
      "      Abusive       0.91      0.92      0.91      1030\n",
      "HS_Individual       0.78      0.71      0.75       729\n",
      "     HS_Group       0.77      0.65      0.70       378\n",
      "  HS_Religion       0.82      0.58      0.68       167\n",
      "      HS_Race       0.80      0.59      0.68        88\n",
      "  HS_Physical       0.61      0.15      0.24        74\n",
      "    HS_Gender       0.86      0.25      0.39        75\n",
      "     HS_Other       0.79      0.76      0.78       744\n",
      "      HS_Weak       0.76      0.69      0.72       690\n",
      "  HS_Moderate       0.71      0.59      0.65       338\n",
      "    HS_Strong       0.77      0.65      0.70        79\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      5499\n",
      "    macro avg       0.79      0.62      0.67      5499\n",
      " weighted avg       0.82      0.75      0.78      5499\n",
      "  samples avg       0.45      0.42      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 4.342416013969343e-06\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.07594108581543 seconds\n",
      "\n",
      "Fold 4 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3764, Accuracy: 0.8919, F1 Micro: 0.6169, F1 Macro: 0.3285\n",
      "Epoch 2/10, Train Loss: 0.2552, Accuracy: 0.9145, F1 Micro: 0.7164, F1 Macro: 0.5213\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9225, F1 Micro: 0.75, F1 Macro: 0.5667\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9263, F1 Micro: 0.7789, F1 Macro: 0.6069\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.9244, F1 Micro: 0.7825, F1 Macro: 0.6338\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.928, F1 Micro: 0.7874, F1 Macro: 0.6439\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9265, F1 Micro: 0.7726, F1 Macro: 0.6694\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9264, F1 Micro: 0.7811, F1 Macro: 0.6746\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9291, F1 Micro: 0.786, F1 Macro: 0.6873\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9274, F1 Micro: 0.7792, F1 Macro: 0.6948\n",
      "Best result for 10218 samples: F1 Micro: 0.7874\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1107\n",
      "      Abusive       0.91      0.93      0.92      1030\n",
      "HS_Individual       0.75      0.76      0.76       729\n",
      "     HS_Group       0.72      0.69      0.70       378\n",
      "  HS_Religion       0.80      0.62      0.70       167\n",
      "      HS_Race       0.78      0.48      0.59        88\n",
      "  HS_Physical       0.78      0.09      0.17        74\n",
      "    HS_Gender       0.88      0.09      0.17        75\n",
      "     HS_Other       0.76      0.83      0.80       744\n",
      "      HS_Weak       0.73      0.74      0.73       690\n",
      "  HS_Moderate       0.67      0.60      0.64       338\n",
      "    HS_Strong       0.76      0.65      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5499\n",
      "    macro avg       0.78      0.61      0.64      5499\n",
      " weighted avg       0.79      0.78      0.78      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 6.067588492442158e-06\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.277446269989014 seconds\n",
      "\n",
      "Fold 4 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3703, Accuracy: 0.8939, F1 Micro: 0.6354, F1 Macro: 0.3321\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.9149, F1 Micro: 0.7201, F1 Macro: 0.5281\n",
      "Epoch 3/10, Train Loss: 0.2016, Accuracy: 0.9166, F1 Micro: 0.7657, F1 Macro: 0.5976\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.9249, F1 Micro: 0.7675, F1 Macro: 0.614\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9258, F1 Micro: 0.7841, F1 Macro: 0.6472\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.9259, F1 Micro: 0.7659, F1 Macro: 0.6233\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9279, F1 Micro: 0.7866, F1 Macro: 0.6713\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9228, F1 Micro: 0.7817, F1 Macro: 0.6778\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9277, F1 Micro: 0.7882, F1 Macro: 0.686\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.9275, F1 Micro: 0.7867, F1 Macro: 0.6986\n",
      "Best result for 10418 samples: F1 Micro: 0.7882\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.74      0.64      0.69       378\n",
      "  HS_Religion       0.80      0.63      0.71       167\n",
      "      HS_Race       0.72      0.67      0.69        88\n",
      "  HS_Physical       0.74      0.19      0.30        74\n",
      "    HS_Gender       0.92      0.31      0.46        75\n",
      "     HS_Other       0.78      0.81      0.80       744\n",
      "      HS_Weak       0.70      0.77      0.73       690\n",
      "  HS_Moderate       0.69      0.57      0.62       338\n",
      "    HS_Strong       0.75      0.66      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5499\n",
      "    macro avg       0.77      0.65      0.69      5499\n",
      " weighted avg       0.79      0.79      0.78      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Threshold: 1.5831008795430536e-06\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.912559986114502 seconds\n",
      "\n",
      "Fold 4 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3686, Accuracy: 0.8942, F1 Micro: 0.6722, F1 Macro: 0.3943\n",
      "Epoch 2/10, Train Loss: 0.2476, Accuracy: 0.908, F1 Micro: 0.7087, F1 Macro: 0.4608\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.9207, F1 Micro: 0.7649, F1 Macro: 0.5911\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9254, F1 Micro: 0.7753, F1 Macro: 0.6029\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9273, F1 Micro: 0.782, F1 Macro: 0.6338\n",
      "Epoch 6/10, Train Loss: 0.116, Accuracy: 0.9284, F1 Micro: 0.7869, F1 Macro: 0.6494\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9287, F1 Micro: 0.7789, F1 Macro: 0.6649\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9254, F1 Micro: 0.7841, F1 Macro: 0.6824\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9253, F1 Micro: 0.7794, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9271, F1 Micro: 0.7902, F1 Macro: 0.7002\n",
      "Best result for 10535 samples: F1 Micro: 0.7902\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1107\n",
      "      Abusive       0.88      0.95      0.91      1030\n",
      "HS_Individual       0.72      0.80      0.76       729\n",
      "     HS_Group       0.73      0.65      0.69       378\n",
      "  HS_Religion       0.76      0.67      0.71       167\n",
      "      HS_Race       0.77      0.64      0.70        88\n",
      "  HS_Physical       0.71      0.23      0.35        74\n",
      "    HS_Gender       0.90      0.37      0.53        75\n",
      "     HS_Other       0.76      0.84      0.80       744\n",
      "      HS_Weak       0.69      0.78      0.74       690\n",
      "  HS_Moderate       0.67      0.58      0.62       338\n",
      "    HS_Strong       0.80      0.71      0.75        79\n",
      "\n",
      "    micro avg       0.78      0.81      0.79      5499\n",
      "    macro avg       0.77      0.68      0.70      5499\n",
      " weighted avg       0.78      0.81      0.78      5499\n",
      "  samples avg       0.45      0.45      0.43      5499\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 6610.34 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 658 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.612, Accuracy: 0.8159, F1 Micro: 0.2154, F1 Macro: 0.062\n",
      "Epoch 2/10, Train Loss: 0.4594, Accuracy: 0.8202, F1 Micro: 0.0041, F1 Macro: 0.0018\n",
      "Epoch 3/10, Train Loss: 0.4164, Accuracy: 0.8224, F1 Micro: 0.0357, F1 Macro: 0.0153\n",
      "Epoch 4/10, Train Loss: 0.3858, Accuracy: 0.8233, F1 Micro: 0.0465, F1 Macro: 0.0197\n",
      "Epoch 5/10, Train Loss: 0.3698, Accuracy: 0.8312, F1 Micro: 0.138, F1 Macro: 0.0538\n",
      "Epoch 6/10, Train Loss: 0.3637, Accuracy: 0.843, F1 Micro: 0.2883, F1 Macro: 0.1014\n",
      "Epoch 7/10, Train Loss: 0.3557, Accuracy: 0.8501, F1 Micro: 0.3628, F1 Macro: 0.1452\n",
      "Epoch 8/10, Train Loss: 0.3269, Accuracy: 0.8637, F1 Micro: 0.5003, F1 Macro: 0.2267\n",
      "Epoch 9/10, Train Loss: 0.3068, Accuracy: 0.8713, F1 Micro: 0.5585, F1 Macro: 0.2602\n",
      "Epoch 10/10, Train Loss: 0.295, Accuracy: 0.8748, F1 Micro: 0.5825, F1 Macro: 0.2728\n",
      "Best result for 658 samples: F1 Micro: 0.5825\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.74      0.77      1190\n",
      "      Abusive       0.83      0.71      0.77      1018\n",
      "HS_Individual       0.65      0.56      0.60       768\n",
      "     HS_Group       0.00      0.00      0.00       422\n",
      "  HS_Religion       0.00      0.00      0.00       173\n",
      "      HS_Race       0.00      0.00      0.00       126\n",
      "  HS_Physical       0.00      0.00      0.00        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.64      0.54      0.59       792\n",
      "      HS_Weak       0.62      0.49      0.55       725\n",
      "  HS_Moderate       0.00      0.00      0.00       352\n",
      "    HS_Strong       0.00      0.00      0.00       113\n",
      "\n",
      "    micro avg       0.73      0.49      0.58      5806\n",
      "    macro avg       0.30      0.25      0.27      5806\n",
      " weighted avg       0.56      0.49      0.52      5806\n",
      "  samples avg       0.37      0.29      0.30      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0018451184965670114\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 133.95070886611938 seconds\n",
      "\n",
      "Fold 5 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1646 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5476, Accuracy: 0.8221, F1 Micro: 0.3491, F1 Macro: 0.0969\n",
      "Epoch 2/10, Train Loss: 0.4359, Accuracy: 0.8271, F1 Micro: 0.3952, F1 Macro: 0.1095\n",
      "Epoch 3/10, Train Loss: 0.4159, Accuracy: 0.8407, F1 Micro: 0.4186, F1 Macro: 0.1232\n",
      "Epoch 4/10, Train Loss: 0.3967, Accuracy: 0.8543, F1 Micro: 0.4502, F1 Macro: 0.155\n",
      "Epoch 5/10, Train Loss: 0.3625, Accuracy: 0.8772, F1 Micro: 0.5734, F1 Macro: 0.2831\n",
      "Epoch 6/10, Train Loss: 0.3233, Accuracy: 0.8865, F1 Micro: 0.6701, F1 Macro: 0.4043\n",
      "Epoch 7/10, Train Loss: 0.286, Accuracy: 0.8934, F1 Micro: 0.6708, F1 Macro: 0.4519\n",
      "Epoch 8/10, Train Loss: 0.2572, Accuracy: 0.8953, F1 Micro: 0.6801, F1 Macro: 0.4766\n",
      "Epoch 9/10, Train Loss: 0.2242, Accuracy: 0.8953, F1 Micro: 0.6836, F1 Macro: 0.4757\n",
      "Epoch 10/10, Train Loss: 0.2053, Accuracy: 0.8985, F1 Micro: 0.6896, F1 Macro: 0.4913\n",
      "Best result for 1646 samples: F1 Micro: 0.6896\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.78      0.82      1190\n",
      "      Abusive       0.88      0.80      0.84      1018\n",
      "HS_Individual       0.72      0.64      0.68       768\n",
      "     HS_Group       0.69      0.49      0.57       422\n",
      "  HS_Religion       0.75      0.25      0.37       173\n",
      "      HS_Race       0.84      0.40      0.55       126\n",
      "  HS_Physical       0.00      0.00      0.00        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.71      0.66      0.68       792\n",
      "      HS_Weak       0.67      0.60      0.63       725\n",
      "  HS_Moderate       0.54      0.33      0.41       352\n",
      "    HS_Strong       0.76      0.22      0.34       113\n",
      "\n",
      "    micro avg       0.77      0.63      0.69      5806\n",
      "    macro avg       0.62      0.43      0.49      5806\n",
      " weighted avg       0.74      0.63      0.67      5806\n",
      "  samples avg       0.40      0.36      0.36      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.003497530985623599\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 121.31330370903015 seconds\n",
      "\n",
      "Fold 5 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2535 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5211, Accuracy: 0.8233, F1 Micro: 0.4033, F1 Macro: 0.1098\n",
      "Epoch 2/10, Train Loss: 0.4301, Accuracy: 0.826, F1 Micro: 0.415, F1 Macro: 0.1257\n",
      "Epoch 3/10, Train Loss: 0.4117, Accuracy: 0.8542, F1 Micro: 0.541, F1 Macro: 0.2478\n",
      "Epoch 4/10, Train Loss: 0.3704, Accuracy: 0.8882, F1 Micro: 0.6501, F1 Macro: 0.3867\n",
      "Epoch 5/10, Train Loss: 0.3237, Accuracy: 0.8974, F1 Micro: 0.6816, F1 Macro: 0.46\n",
      "Epoch 6/10, Train Loss: 0.2814, Accuracy: 0.9011, F1 Micro: 0.6952, F1 Macro: 0.4886\n",
      "Epoch 7/10, Train Loss: 0.2411, Accuracy: 0.9037, F1 Micro: 0.7272, F1 Macro: 0.5313\n",
      "Epoch 8/10, Train Loss: 0.2069, Accuracy: 0.9052, F1 Micro: 0.7171, F1 Macro: 0.5196\n",
      "Epoch 9/10, Train Loss: 0.1859, Accuracy: 0.9057, F1 Micro: 0.7325, F1 Macro: 0.5466\n",
      "Epoch 10/10, Train Loss: 0.1615, Accuracy: 0.9046, F1 Micro: 0.728, F1 Macro: 0.5585\n",
      "Best result for 2535 samples: F1 Micro: 0.7325\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1190\n",
      "      Abusive       0.87      0.86      0.87      1018\n",
      "HS_Individual       0.68      0.76      0.72       768\n",
      "     HS_Group       0.74      0.51      0.61       422\n",
      "  HS_Religion       0.72      0.45      0.55       173\n",
      "      HS_Race       0.86      0.49      0.63       126\n",
      "  HS_Physical       1.00      0.03      0.06        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.71      0.79      0.75       792\n",
      "      HS_Weak       0.64      0.74      0.69       725\n",
      "  HS_Moderate       0.57      0.41      0.47       352\n",
      "    HS_Strong       0.90      0.24      0.38       113\n",
      "\n",
      "    micro avg       0.75      0.72      0.73      5806\n",
      "    macro avg       0.71      0.51      0.55      5806\n",
      " weighted avg       0.75      0.72      0.72      5806\n",
      "  samples avg       0.42      0.41      0.40      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.004084533546119928\n",
      "Samples above threshold: 801\n",
      "Acquired samples: 801\n",
      "Sampling duration: 109.29143714904785 seconds\n",
      "\n",
      "Fold 5 - New train size: 3336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5025, Accuracy: 0.8224, F1 Micro: 0.4076, F1 Macro: 0.1104\n",
      "Epoch 2/10, Train Loss: 0.4229, Accuracy: 0.8369, F1 Micro: 0.4277, F1 Macro: 0.1353\n",
      "Epoch 3/10, Train Loss: 0.3833, Accuracy: 0.8816, F1 Micro: 0.6317, F1 Macro: 0.3665\n",
      "Epoch 4/10, Train Loss: 0.3333, Accuracy: 0.8993, F1 Micro: 0.704, F1 Macro: 0.5076\n",
      "Epoch 5/10, Train Loss: 0.2868, Accuracy: 0.9031, F1 Micro: 0.7272, F1 Macro: 0.5464\n",
      "Epoch 6/10, Train Loss: 0.2428, Accuracy: 0.908, F1 Micro: 0.7389, F1 Macro: 0.5591\n",
      "Epoch 7/10, Train Loss: 0.2101, Accuracy: 0.9071, F1 Micro: 0.7405, F1 Macro: 0.5675\n",
      "Epoch 8/10, Train Loss: 0.1781, Accuracy: 0.9115, F1 Micro: 0.7404, F1 Macro: 0.5765\n",
      "Epoch 9/10, Train Loss: 0.1548, Accuracy: 0.9109, F1 Micro: 0.7316, F1 Macro: 0.5637\n",
      "Epoch 10/10, Train Loss: 0.1349, Accuracy: 0.9097, F1 Micro: 0.7411, F1 Macro: 0.5726\n",
      "Best result for 3336 samples: F1 Micro: 0.7411\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1190\n",
      "      Abusive       0.86      0.87      0.86      1018\n",
      "HS_Individual       0.69      0.76      0.73       768\n",
      "     HS_Group       0.76      0.51      0.61       422\n",
      "  HS_Religion       0.69      0.46      0.55       173\n",
      "      HS_Race       0.83      0.59      0.69       126\n",
      "  HS_Physical       1.00      0.05      0.10        60\n",
      "    HS_Gender       1.00      0.01      0.03        67\n",
      "     HS_Other       0.75      0.77      0.76       792\n",
      "      HS_Weak       0.65      0.74      0.70       725\n",
      "  HS_Moderate       0.59      0.41      0.49       352\n",
      "    HS_Strong       0.89      0.36      0.52       113\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5806\n",
      "    macro avg       0.80      0.53      0.57      5806\n",
      " weighted avg       0.77      0.72      0.73      5806\n",
      "  samples avg       0.44      0.42      0.41      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0036494688363745836\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 98.0877115726471 seconds\n",
      "\n",
      "Fold 5 - New train size: 4056\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4056 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4922, Accuracy: 0.8219, F1 Micro: 0.3659, F1 Macro: 0.1\n",
      "Epoch 2/10, Train Loss: 0.4147, Accuracy: 0.8516, F1 Micro: 0.472, F1 Macro: 0.1859\n",
      "Epoch 3/10, Train Loss: 0.3681, Accuracy: 0.8899, F1 Micro: 0.6906, F1 Macro: 0.4817\n",
      "Epoch 4/10, Train Loss: 0.323, Accuracy: 0.9023, F1 Micro: 0.7253, F1 Macro: 0.5249\n",
      "Epoch 5/10, Train Loss: 0.2753, Accuracy: 0.9074, F1 Micro: 0.7286, F1 Macro: 0.5406\n",
      "Epoch 6/10, Train Loss: 0.228, Accuracy: 0.9128, F1 Micro: 0.7477, F1 Macro: 0.5682\n",
      "Epoch 7/10, Train Loss: 0.1997, Accuracy: 0.9127, F1 Micro: 0.7489, F1 Macro: 0.5684\n",
      "Epoch 8/10, Train Loss: 0.1636, Accuracy: 0.9113, F1 Micro: 0.7523, F1 Macro: 0.5832\n",
      "Epoch 9/10, Train Loss: 0.142, Accuracy: 0.9158, F1 Micro: 0.7545, F1 Macro: 0.6016\n",
      "Epoch 10/10, Train Loss: 0.1258, Accuracy: 0.9139, F1 Micro: 0.7514, F1 Macro: 0.6182\n",
      "Best result for 4056 samples: F1 Micro: 0.7545\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.83      0.86      1190\n",
      "      Abusive       0.88      0.85      0.86      1018\n",
      "HS_Individual       0.76      0.71      0.73       768\n",
      "     HS_Group       0.72      0.60      0.66       422\n",
      "  HS_Religion       0.69      0.50      0.58       173\n",
      "      HS_Race       0.79      0.71      0.75       126\n",
      "  HS_Physical       1.00      0.05      0.10        60\n",
      "    HS_Gender       1.00      0.03      0.06        67\n",
      "     HS_Other       0.78      0.75      0.76       792\n",
      "      HS_Weak       0.72      0.69      0.71       725\n",
      "  HS_Moderate       0.60      0.52      0.56       352\n",
      "    HS_Strong       0.81      0.48      0.60       113\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5806\n",
      "    macro avg       0.80      0.56      0.60      5806\n",
      " weighted avg       0.80      0.72      0.75      5806\n",
      "  samples avg       0.44      0.42      0.41      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.002254955749958754\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 88.30949401855469 seconds\n",
      "\n",
      "Fold 5 - New train size: 4704\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4704 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4851, Accuracy: 0.821, F1 Micro: 0.4246, F1 Macro: 0.1255\n",
      "Epoch 2/10, Train Loss: 0.4009, Accuracy: 0.8791, F1 Micro: 0.6058, F1 Macro: 0.3324\n",
      "Epoch 3/10, Train Loss: 0.3496, Accuracy: 0.9005, F1 Micro: 0.7044, F1 Macro: 0.4942\n",
      "Epoch 4/10, Train Loss: 0.297, Accuracy: 0.9093, F1 Micro: 0.7247, F1 Macro: 0.5241\n",
      "Epoch 5/10, Train Loss: 0.2622, Accuracy: 0.9138, F1 Micro: 0.7558, F1 Macro: 0.575\n",
      "Epoch 6/10, Train Loss: 0.221, Accuracy: 0.9149, F1 Micro: 0.7475, F1 Macro: 0.571\n",
      "Epoch 7/10, Train Loss: 0.1873, Accuracy: 0.9152, F1 Micro: 0.7595, F1 Macro: 0.6318\n",
      "Epoch 8/10, Train Loss: 0.155, Accuracy: 0.918, F1 Micro: 0.7629, F1 Macro: 0.6391\n",
      "Epoch 9/10, Train Loss: 0.1325, Accuracy: 0.9143, F1 Micro: 0.7598, F1 Macro: 0.6132\n",
      "Epoch 10/10, Train Loss: 0.1131, Accuracy: 0.9171, F1 Micro: 0.7619, F1 Macro: 0.6434\n",
      "Best result for 4704 samples: F1 Micro: 0.7629\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1190\n",
      "      Abusive       0.86      0.88      0.87      1018\n",
      "HS_Individual       0.75      0.72      0.74       768\n",
      "     HS_Group       0.74      0.62      0.67       422\n",
      "  HS_Religion       0.72      0.51      0.60       173\n",
      "      HS_Race       0.80      0.71      0.76       126\n",
      "  HS_Physical       1.00      0.12      0.21        60\n",
      "    HS_Gender       0.86      0.09      0.16        67\n",
      "     HS_Other       0.79      0.74      0.76       792\n",
      "      HS_Weak       0.72      0.70      0.71       725\n",
      "  HS_Moderate       0.62      0.53      0.58       352\n",
      "    HS_Strong       0.86      0.68      0.76       113\n",
      "\n",
      "    micro avg       0.80      0.73      0.76      5806\n",
      "    macro avg       0.80      0.60      0.64      5806\n",
      " weighted avg       0.80      0.73      0.76      5806\n",
      "  samples avg       0.45      0.43      0.42      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0012489484739489857\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 79.55109310150146 seconds\n",
      "\n",
      "Fold 5 - New train size: 5288\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5288 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.8267, F1 Micro: 0.4397, F1 Macro: 0.1516\n",
      "Epoch 2/10, Train Loss: 0.3917, Accuracy: 0.888, F1 Micro: 0.6499, F1 Macro: 0.4295\n",
      "Epoch 3/10, Train Loss: 0.3362, Accuracy: 0.9052, F1 Micro: 0.7149, F1 Macro: 0.5309\n",
      "Epoch 4/10, Train Loss: 0.2869, Accuracy: 0.9139, F1 Micro: 0.7416, F1 Macro: 0.5765\n",
      "Epoch 5/10, Train Loss: 0.245, Accuracy: 0.9164, F1 Micro: 0.759, F1 Macro: 0.5949\n",
      "Epoch 6/10, Train Loss: 0.2052, Accuracy: 0.9194, F1 Micro: 0.7631, F1 Macro: 0.6103\n",
      "Epoch 7/10, Train Loss: 0.1735, Accuracy: 0.9195, F1 Micro: 0.7556, F1 Macro: 0.6383\n",
      "Epoch 8/10, Train Loss: 0.143, Accuracy: 0.919, F1 Micro: 0.7714, F1 Macro: 0.6588\n",
      "Epoch 9/10, Train Loss: 0.1299, Accuracy: 0.9182, F1 Micro: 0.7706, F1 Macro: 0.6534\n",
      "Epoch 10/10, Train Loss: 0.1102, Accuracy: 0.9204, F1 Micro: 0.7724, F1 Macro: 0.6749\n",
      "Best result for 5288 samples: F1 Micro: 0.7724\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.87      0.89      0.88      1018\n",
      "HS_Individual       0.79      0.69      0.74       768\n",
      "     HS_Group       0.67      0.70      0.69       422\n",
      "  HS_Religion       0.71      0.55      0.62       173\n",
      "      HS_Race       0.78      0.72      0.75       126\n",
      "  HS_Physical       0.91      0.17      0.28        60\n",
      "    HS_Gender       0.86      0.27      0.41        67\n",
      "     HS_Other       0.78      0.78      0.78       792\n",
      "      HS_Weak       0.77      0.68      0.72       725\n",
      "  HS_Moderate       0.58      0.64      0.61       352\n",
      "    HS_Strong       0.88      0.68      0.77       113\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5806\n",
      "    macro avg       0.79      0.63      0.67      5806\n",
      " weighted avg       0.80      0.75      0.77      5806\n",
      "  samples avg       0.45      0.43      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0010890250909142201\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 71.6464455127716 seconds\n",
      "\n",
      "Fold 5 - New train size: 5813\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5813 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4735, Accuracy: 0.8339, F1 Micro: 0.4155, F1 Macro: 0.1305\n",
      "Epoch 2/10, Train Loss: 0.379, Accuracy: 0.892, F1 Micro: 0.6727, F1 Macro: 0.4704\n",
      "Epoch 3/10, Train Loss: 0.3186, Accuracy: 0.9083, F1 Micro: 0.732, F1 Macro: 0.5537\n",
      "Epoch 4/10, Train Loss: 0.2734, Accuracy: 0.9154, F1 Micro: 0.7458, F1 Macro: 0.5926\n",
      "Epoch 5/10, Train Loss: 0.2279, Accuracy: 0.9198, F1 Micro: 0.7634, F1 Macro: 0.6081\n",
      "Epoch 6/10, Train Loss: 0.199, Accuracy: 0.92, F1 Micro: 0.7603, F1 Macro: 0.6298\n",
      "Epoch 7/10, Train Loss: 0.1638, Accuracy: 0.9213, F1 Micro: 0.7717, F1 Macro: 0.6318\n",
      "Epoch 8/10, Train Loss: 0.1375, Accuracy: 0.9216, F1 Micro: 0.7729, F1 Macro: 0.6698\n",
      "Epoch 9/10, Train Loss: 0.1216, Accuracy: 0.9216, F1 Micro: 0.7802, F1 Macro: 0.6884\n",
      "Epoch 10/10, Train Loss: 0.1021, Accuracy: 0.9185, F1 Micro: 0.772, F1 Macro: 0.685\n",
      "Best result for 5813 samples: F1 Micro: 0.7802\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1190\n",
      "      Abusive       0.88      0.89      0.88      1018\n",
      "HS_Individual       0.72      0.77      0.74       768\n",
      "     HS_Group       0.74      0.63      0.68       422\n",
      "  HS_Religion       0.70      0.54      0.61       173\n",
      "      HS_Race       0.77      0.74      0.75       126\n",
      "  HS_Physical       0.92      0.20      0.33        60\n",
      "    HS_Gender       0.87      0.30      0.44        67\n",
      "     HS_Other       0.78      0.81      0.79       792\n",
      "      HS_Weak       0.70      0.76      0.73       725\n",
      "  HS_Moderate       0.65      0.55      0.60       352\n",
      "    HS_Strong       0.91      0.76      0.83       113\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5806\n",
      "    macro avg       0.79      0.65      0.69      5806\n",
      " weighted avg       0.79      0.77      0.78      5806\n",
      "  samples avg       0.45      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.000794566841796041\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 64.6994297504425 seconds\n",
      "\n",
      "Fold 5 - New train size: 6286\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6286 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4663, Accuracy: 0.8427, F1 Micro: 0.4228, F1 Macro: 0.1372\n",
      "Epoch 2/10, Train Loss: 0.3685, Accuracy: 0.8946, F1 Micro: 0.6948, F1 Macro: 0.4932\n",
      "Epoch 3/10, Train Loss: 0.3055, Accuracy: 0.9108, F1 Micro: 0.7461, F1 Macro: 0.5764\n",
      "Epoch 4/10, Train Loss: 0.2568, Accuracy: 0.9166, F1 Micro: 0.7586, F1 Macro: 0.6007\n",
      "Epoch 5/10, Train Loss: 0.2253, Accuracy: 0.9204, F1 Micro: 0.7726, F1 Macro: 0.6322\n",
      "Epoch 6/10, Train Loss: 0.1895, Accuracy: 0.9207, F1 Micro: 0.7791, F1 Macro: 0.6797\n",
      "Epoch 7/10, Train Loss: 0.1536, Accuracy: 0.9213, F1 Micro: 0.7761, F1 Macro: 0.6821\n",
      "Epoch 8/10, Train Loss: 0.1305, Accuracy: 0.9239, F1 Micro: 0.785, F1 Macro: 0.7144\n",
      "Epoch 9/10, Train Loss: 0.1133, Accuracy: 0.9248, F1 Micro: 0.7778, F1 Macro: 0.7078\n",
      "Epoch 10/10, Train Loss: 0.0971, Accuracy: 0.9225, F1 Micro: 0.7773, F1 Macro: 0.7157\n",
      "Best result for 6286 samples: F1 Micro: 0.785\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1190\n",
      "      Abusive       0.88      0.89      0.88      1018\n",
      "HS_Individual       0.74      0.78      0.76       768\n",
      "     HS_Group       0.77      0.64      0.69       422\n",
      "  HS_Religion       0.74      0.57      0.65       173\n",
      "      HS_Race       0.79      0.73      0.76       126\n",
      "  HS_Physical       0.86      0.32      0.46        60\n",
      "    HS_Gender       0.76      0.46      0.57        67\n",
      "     HS_Other       0.80      0.78      0.79       792\n",
      "      HS_Weak       0.71      0.76      0.73       725\n",
      "  HS_Moderate       0.69      0.53      0.60       352\n",
      "    HS_Strong       0.85      0.78      0.81       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5806\n",
      "    macro avg       0.79      0.67      0.71      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.45      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Threshold: 0.0005786802736110982\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 298\n",
      "Sampling duration: 58.8030526638031 seconds\n",
      "\n",
      "Fold 5 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6584 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4604, Accuracy: 0.8548, F1 Micro: 0.5202, F1 Macro: 0.2294\n",
      "Epoch 2/10, Train Loss: 0.3548, Accuracy: 0.8969, F1 Micro: 0.6999, F1 Macro: 0.505\n",
      "Epoch 3/10, Train Loss: 0.2955, Accuracy: 0.9101, F1 Micro: 0.7359, F1 Macro: 0.5404\n",
      "Epoch 4/10, Train Loss: 0.2528, Accuracy: 0.9169, F1 Micro: 0.7626, F1 Macro: 0.596\n",
      "Epoch 5/10, Train Loss: 0.2164, Accuracy: 0.9206, F1 Micro: 0.7668, F1 Macro: 0.6398\n",
      "Epoch 6/10, Train Loss: 0.1801, Accuracy: 0.9223, F1 Micro: 0.7685, F1 Macro: 0.6638\n",
      "Epoch 7/10, Train Loss: 0.1501, Accuracy: 0.9225, F1 Micro: 0.7757, F1 Macro: 0.6875\n",
      "Epoch 8/10, Train Loss: 0.1279, Accuracy: 0.9256, F1 Micro: 0.7854, F1 Macro: 0.7056\n",
      "Epoch 9/10, Train Loss: 0.1096, Accuracy: 0.923, F1 Micro: 0.7828, F1 Macro: 0.7094\n",
      "Epoch 10/10, Train Loss: 0.0952, Accuracy: 0.9225, F1 Micro: 0.7826, F1 Macro: 0.7212\n",
      "Best result for 6584 samples: F1 Micro: 0.7854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.89      0.88      0.89      1018\n",
      "HS_Individual       0.77      0.73      0.75       768\n",
      "     HS_Group       0.76      0.67      0.71       422\n",
      "  HS_Religion       0.75      0.58      0.65       173\n",
      "      HS_Race       0.85      0.65      0.74       126\n",
      "  HS_Physical       0.89      0.27      0.41        60\n",
      "    HS_Gender       0.83      0.36      0.50        67\n",
      "     HS_Other       0.80      0.77      0.78       792\n",
      "      HS_Weak       0.75      0.72      0.73       725\n",
      "  HS_Moderate       0.68      0.60      0.64       352\n",
      "    HS_Strong       0.89      0.73      0.80       113\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5806\n",
      "    macro avg       0.81      0.65      0.71      5806\n",
      " weighted avg       0.82      0.76      0.78      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00035575558431446554\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.38435387611389 seconds\n",
      "\n",
      "Fold 5 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6980 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4546, Accuracy: 0.857, F1 Micro: 0.4935, F1 Macro: 0.1974\n",
      "Epoch 2/10, Train Loss: 0.3515, Accuracy: 0.8981, F1 Micro: 0.6888, F1 Macro: 0.4653\n",
      "Epoch 3/10, Train Loss: 0.2907, Accuracy: 0.9099, F1 Micro: 0.7478, F1 Macro: 0.5705\n",
      "Epoch 4/10, Train Loss: 0.2463, Accuracy: 0.9184, F1 Micro: 0.749, F1 Macro: 0.5966\n",
      "Epoch 5/10, Train Loss: 0.209, Accuracy: 0.9213, F1 Micro: 0.7589, F1 Macro: 0.5995\n",
      "Epoch 6/10, Train Loss: 0.176, Accuracy: 0.9226, F1 Micro: 0.7794, F1 Macro: 0.6699\n",
      "Epoch 7/10, Train Loss: 0.1474, Accuracy: 0.9249, F1 Micro: 0.7729, F1 Macro: 0.6905\n",
      "Epoch 8/10, Train Loss: 0.1224, Accuracy: 0.9247, F1 Micro: 0.7864, F1 Macro: 0.6988\n",
      "Epoch 9/10, Train Loss: 0.1074, Accuracy: 0.9247, F1 Micro: 0.7783, F1 Macro: 0.7038\n",
      "Epoch 10/10, Train Loss: 0.0917, Accuracy: 0.9253, F1 Micro: 0.7888, F1 Macro: 0.7147\n",
      "Best result for 6980 samples: F1 Micro: 0.7888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1190\n",
      "      Abusive       0.89      0.89      0.89      1018\n",
      "HS_Individual       0.76      0.76      0.76       768\n",
      "     HS_Group       0.76      0.65      0.70       422\n",
      "  HS_Religion       0.76      0.54      0.63       173\n",
      "      HS_Race       0.85      0.68      0.76       126\n",
      "  HS_Physical       1.00      0.30      0.46        60\n",
      "    HS_Gender       0.70      0.42      0.52        67\n",
      "     HS_Other       0.77      0.82      0.79       792\n",
      "      HS_Weak       0.74      0.75      0.75       725\n",
      "  HS_Moderate       0.68      0.56      0.61       352\n",
      "    HS_Strong       0.85      0.82      0.84       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5806\n",
      "    macro avg       0.80      0.67      0.71      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.45      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00017994038353208452\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 49.44981789588928 seconds\n",
      "\n",
      "Fold 5 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4439, Accuracy: 0.8726, F1 Micro: 0.5753, F1 Macro: 0.2771\n",
      "Epoch 2/10, Train Loss: 0.3338, Accuracy: 0.9, F1 Micro: 0.7176, F1 Macro: 0.522\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.9143, F1 Micro: 0.7498, F1 Macro: 0.5835\n",
      "Epoch 4/10, Train Loss: 0.2327, Accuracy: 0.9182, F1 Micro: 0.7515, F1 Macro: 0.5809\n",
      "Epoch 5/10, Train Loss: 0.1968, Accuracy: 0.9226, F1 Micro: 0.7735, F1 Macro: 0.6506\n",
      "Epoch 6/10, Train Loss: 0.1646, Accuracy: 0.9222, F1 Micro: 0.7836, F1 Macro: 0.6861\n",
      "Epoch 7/10, Train Loss: 0.1375, Accuracy: 0.9202, F1 Micro: 0.7801, F1 Macro: 0.6986\n",
      "Epoch 8/10, Train Loss: 0.1152, Accuracy: 0.9239, F1 Micro: 0.7841, F1 Macro: 0.7077\n",
      "Epoch 9/10, Train Loss: 0.1009, Accuracy: 0.9238, F1 Micro: 0.7839, F1 Macro: 0.7087\n",
      "Epoch 10/10, Train Loss: 0.088, Accuracy: 0.9208, F1 Micro: 0.7834, F1 Macro: 0.7252\n",
      "Best result for 7336 samples: F1 Micro: 0.7841\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1190\n",
      "      Abusive       0.89      0.88      0.89      1018\n",
      "HS_Individual       0.76      0.73      0.74       768\n",
      "     HS_Group       0.72      0.68      0.70       422\n",
      "  HS_Religion       0.69      0.64      0.66       173\n",
      "      HS_Race       0.81      0.75      0.78       126\n",
      "  HS_Physical       0.93      0.23      0.37        60\n",
      "    HS_Gender       0.83      0.37      0.52        67\n",
      "     HS_Other       0.79      0.78      0.78       792\n",
      "      HS_Weak       0.74      0.72      0.73       725\n",
      "  HS_Moderate       0.66      0.60      0.63       352\n",
      "    HS_Strong       0.86      0.79      0.82       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5806\n",
      "    macro avg       0.80      0.67      0.71      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.45      0.44      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.0001135066551796626\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.48023271560669 seconds\n",
      "\n",
      "Fold 5 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7656 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4432, Accuracy: 0.8736, F1 Micro: 0.5952, F1 Macro: 0.2915\n",
      "Epoch 2/10, Train Loss: 0.331, Accuracy: 0.9003, F1 Micro: 0.7223, F1 Macro: 0.5473\n",
      "Epoch 3/10, Train Loss: 0.269, Accuracy: 0.9156, F1 Micro: 0.7428, F1 Macro: 0.5787\n",
      "Epoch 4/10, Train Loss: 0.225, Accuracy: 0.9177, F1 Micro: 0.767, F1 Macro: 0.6087\n",
      "Epoch 5/10, Train Loss: 0.1927, Accuracy: 0.9227, F1 Micro: 0.7821, F1 Macro: 0.6662\n",
      "Epoch 6/10, Train Loss: 0.1566, Accuracy: 0.9244, F1 Micro: 0.7853, F1 Macro: 0.6795\n",
      "Epoch 7/10, Train Loss: 0.1324, Accuracy: 0.9232, F1 Micro: 0.7777, F1 Macro: 0.6826\n",
      "Epoch 8/10, Train Loss: 0.1145, Accuracy: 0.9251, F1 Micro: 0.7816, F1 Macro: 0.6874\n",
      "Epoch 9/10, Train Loss: 0.0991, Accuracy: 0.9191, F1 Micro: 0.7802, F1 Macro: 0.7124\n",
      "Epoch 10/10, Train Loss: 0.0858, Accuracy: 0.9237, F1 Micro: 0.7786, F1 Macro: 0.7108\n",
      "Best result for 7656 samples: F1 Micro: 0.7853\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1190\n",
      "      Abusive       0.89      0.89      0.89      1018\n",
      "HS_Individual       0.77      0.74      0.76       768\n",
      "     HS_Group       0.74      0.66      0.70       422\n",
      "  HS_Religion       0.75      0.55      0.63       173\n",
      "      HS_Race       0.79      0.74      0.77       126\n",
      "  HS_Physical       0.80      0.13      0.23        60\n",
      "    HS_Gender       1.00      0.22      0.37        67\n",
      "     HS_Other       0.77      0.80      0.78       792\n",
      "      HS_Weak       0.75      0.73      0.74       725\n",
      "  HS_Moderate       0.68      0.57      0.62       352\n",
      "    HS_Strong       0.85      0.76      0.80       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5806\n",
      "    macro avg       0.80      0.64      0.68      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Threshold: 0.00011346027313265948\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 40.049081325531006 seconds\n",
      "\n",
      "Fold 5 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7901 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4381, Accuracy: 0.8793, F1 Micro: 0.6026, F1 Macro: 0.2975\n",
      "Epoch 2/10, Train Loss: 0.3168, Accuracy: 0.9032, F1 Micro: 0.7148, F1 Macro: 0.4987\n",
      "Epoch 3/10, Train Loss: 0.2635, Accuracy: 0.9129, F1 Micro: 0.7361, F1 Macro: 0.5537\n",
      "Epoch 4/10, Train Loss: 0.2211, Accuracy: 0.9214, F1 Micro: 0.7685, F1 Macro: 0.6178\n",
      "Epoch 5/10, Train Loss: 0.1942, Accuracy: 0.9202, F1 Micro: 0.7793, F1 Macro: 0.6548\n",
      "Epoch 6/10, Train Loss: 0.1606, Accuracy: 0.9205, F1 Micro: 0.7806, F1 Macro: 0.6839\n",
      "Epoch 7/10, Train Loss: 0.1365, Accuracy: 0.9187, F1 Micro: 0.7786, F1 Macro: 0.6722\n",
      "Epoch 8/10, Train Loss: 0.1154, Accuracy: 0.9198, F1 Micro: 0.7816, F1 Macro: 0.6913\n",
      "Epoch 9/10, Train Loss: 0.0981, Accuracy: 0.9254, F1 Micro: 0.7856, F1 Macro: 0.7205\n",
      "Epoch 10/10, Train Loss: 0.083, Accuracy: 0.9256, F1 Micro: 0.7855, F1 Macro: 0.7222\n",
      "Best result for 7901 samples: F1 Micro: 0.7856\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.90      0.89      0.90      1018\n",
      "HS_Individual       0.75      0.74      0.75       768\n",
      "     HS_Group       0.77      0.64      0.70       422\n",
      "  HS_Religion       0.78      0.52      0.62       173\n",
      "      HS_Race       0.85      0.67      0.75       126\n",
      "  HS_Physical       0.92      0.37      0.52        60\n",
      "    HS_Gender       0.83      0.45      0.58        67\n",
      "     HS_Other       0.79      0.78      0.78       792\n",
      "      HS_Weak       0.74      0.72      0.73       725\n",
      "  HS_Moderate       0.69      0.55      0.62       352\n",
      "    HS_Strong       0.84      0.81      0.83       113\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5806\n",
      "    macro avg       0.81      0.67      0.72      5806\n",
      " weighted avg       0.81      0.76      0.78      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 3.181421270710415e-05\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.957401275634766 seconds\n",
      "\n",
      "Fold 5 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4297, Accuracy: 0.8821, F1 Micro: 0.6256, F1 Macro: 0.3016\n",
      "Epoch 2/10, Train Loss: 0.3113, Accuracy: 0.9021, F1 Micro: 0.7232, F1 Macro: 0.5436\n",
      "Epoch 3/10, Train Loss: 0.2554, Accuracy: 0.9135, F1 Micro: 0.7501, F1 Macro: 0.5727\n",
      "Epoch 4/10, Train Loss: 0.2137, Accuracy: 0.9162, F1 Micro: 0.7652, F1 Macro: 0.6042\n",
      "Epoch 5/10, Train Loss: 0.1832, Accuracy: 0.9206, F1 Micro: 0.7746, F1 Macro: 0.6485\n",
      "Epoch 6/10, Train Loss: 0.1556, Accuracy: 0.9229, F1 Micro: 0.7781, F1 Macro: 0.6912\n",
      "Epoch 7/10, Train Loss: 0.1277, Accuracy: 0.9223, F1 Micro: 0.7768, F1 Macro: 0.6771\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.9231, F1 Micro: 0.7821, F1 Macro: 0.7058\n",
      "Epoch 9/10, Train Loss: 0.0937, Accuracy: 0.9231, F1 Micro: 0.782, F1 Macro: 0.7093\n",
      "Epoch 10/10, Train Loss: 0.085, Accuracy: 0.9213, F1 Micro: 0.7809, F1 Macro: 0.7293\n",
      "Best result for 8165 samples: F1 Micro: 0.7821\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1190\n",
      "      Abusive       0.91      0.87      0.89      1018\n",
      "HS_Individual       0.73      0.76      0.75       768\n",
      "     HS_Group       0.76      0.63      0.69       422\n",
      "  HS_Religion       0.75      0.55      0.64       173\n",
      "      HS_Race       0.89      0.61      0.72       126\n",
      "  HS_Physical       0.89      0.28      0.43        60\n",
      "    HS_Gender       0.78      0.42      0.54        67\n",
      "     HS_Other       0.78      0.81      0.79       792\n",
      "      HS_Weak       0.71      0.74      0.73       725\n",
      "  HS_Moderate       0.68      0.53      0.60       352\n",
      "    HS_Strong       0.88      0.79      0.83       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5806\n",
      "    macro avg       0.80      0.66      0.71      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.0070525351911783e-05\n",
      "Samples above threshold: 238\n",
      "Acquired samples: 238\n",
      "Sampling duration: 33.17060327529907 seconds\n",
      "\n",
      "Fold 5 - New train size: 8403\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8403 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.423, Accuracy: 0.8808, F1 Micro: 0.5997, F1 Macro: 0.2957\n",
      "Epoch 2/10, Train Loss: 0.2998, Accuracy: 0.9045, F1 Micro: 0.711, F1 Macro: 0.5061\n",
      "Epoch 3/10, Train Loss: 0.2508, Accuracy: 0.9157, F1 Micro: 0.7585, F1 Macro: 0.6004\n",
      "Epoch 4/10, Train Loss: 0.2082, Accuracy: 0.9187, F1 Micro: 0.7525, F1 Macro: 0.5955\n",
      "Epoch 5/10, Train Loss: 0.1789, Accuracy: 0.9205, F1 Micro: 0.7666, F1 Macro: 0.6461\n",
      "Epoch 6/10, Train Loss: 0.1458, Accuracy: 0.9237, F1 Micro: 0.7665, F1 Macro: 0.6582\n",
      "Epoch 7/10, Train Loss: 0.1232, Accuracy: 0.925, F1 Micro: 0.7833, F1 Macro: 0.7074\n",
      "Epoch 8/10, Train Loss: 0.1068, Accuracy: 0.9223, F1 Micro: 0.7831, F1 Macro: 0.7104\n",
      "Epoch 9/10, Train Loss: 0.0881, Accuracy: 0.9226, F1 Micro: 0.7804, F1 Macro: 0.7069\n",
      "Epoch 10/10, Train Loss: 0.0788, Accuracy: 0.9251, F1 Micro: 0.7863, F1 Macro: 0.729\n",
      "Best result for 8403 samples: F1 Micro: 0.7863\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1190\n",
      "      Abusive       0.90      0.88      0.89      1018\n",
      "HS_Individual       0.75      0.75      0.75       768\n",
      "     HS_Group       0.75      0.64      0.69       422\n",
      "  HS_Religion       0.74      0.54      0.63       173\n",
      "      HS_Race       0.84      0.69      0.76       126\n",
      "  HS_Physical       0.74      0.48      0.59        60\n",
      "    HS_Gender       0.72      0.54      0.62        67\n",
      "     HS_Other       0.81      0.77      0.79       792\n",
      "      HS_Weak       0.73      0.73      0.73       725\n",
      "  HS_Moderate       0.69      0.56      0.62       352\n",
      "    HS_Strong       0.87      0.80      0.83       113\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5806\n",
      "    macro avg       0.78      0.69      0.73      5806\n",
      " weighted avg       0.81      0.77      0.78      5806\n",
      "  samples avg       0.45      0.44      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 6.073648273741126e-06\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 29.908594608306885 seconds\n",
      "\n",
      "Fold 5 - New train size: 8617\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8617 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.419, Accuracy: 0.8813, F1 Micro: 0.6196, F1 Macro: 0.2943\n",
      "Epoch 2/10, Train Loss: 0.2998, Accuracy: 0.9041, F1 Micro: 0.7185, F1 Macro: 0.5374\n",
      "Epoch 3/10, Train Loss: 0.2415, Accuracy: 0.9155, F1 Micro: 0.7475, F1 Macro: 0.5765\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.918, F1 Micro: 0.7629, F1 Macro: 0.6077\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9207, F1 Micro: 0.7779, F1 Macro: 0.6597\n",
      "Epoch 6/10, Train Loss: 0.1479, Accuracy: 0.922, F1 Micro: 0.7734, F1 Macro: 0.6717\n",
      "Epoch 7/10, Train Loss: 0.1234, Accuracy: 0.9211, F1 Micro: 0.7821, F1 Macro: 0.6989\n",
      "Epoch 8/10, Train Loss: 0.1017, Accuracy: 0.924, F1 Micro: 0.7813, F1 Macro: 0.7026\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9267, F1 Micro: 0.7877, F1 Macro: 0.7256\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9229, F1 Micro: 0.7858, F1 Macro: 0.7183\n",
      "Best result for 8617 samples: F1 Micro: 0.7877\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.91      0.87      0.89      1018\n",
      "HS_Individual       0.76      0.74      0.75       768\n",
      "     HS_Group       0.78      0.63      0.70       422\n",
      "  HS_Religion       0.79      0.58      0.67       173\n",
      "      HS_Race       0.84      0.66      0.74       126\n",
      "  HS_Physical       0.96      0.37      0.53        60\n",
      "    HS_Gender       0.77      0.51      0.61        67\n",
      "     HS_Other       0.81      0.77      0.79       792\n",
      "      HS_Weak       0.74      0.72      0.73       725\n",
      "  HS_Moderate       0.71      0.53      0.61       352\n",
      "    HS_Strong       0.86      0.78      0.82       113\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5806\n",
      "    macro avg       0.82      0.67      0.73      5806\n",
      " weighted avg       0.82      0.76      0.78      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 5.898292238271097e-06\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.049522876739502 seconds\n",
      "\n",
      "Fold 5 - New train size: 8817\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8817 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4128, Accuracy: 0.8796, F1 Micro: 0.5838, F1 Macro: 0.2729\n",
      "Epoch 2/10, Train Loss: 0.2898, Accuracy: 0.9059, F1 Micro: 0.7193, F1 Macro: 0.5083\n",
      "Epoch 3/10, Train Loss: 0.2357, Accuracy: 0.9117, F1 Micro: 0.7481, F1 Macro: 0.5929\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9167, F1 Micro: 0.7727, F1 Macro: 0.621\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9217, F1 Micro: 0.7733, F1 Macro: 0.6509\n",
      "Epoch 6/10, Train Loss: 0.1397, Accuracy: 0.9165, F1 Micro: 0.7744, F1 Macro: 0.6712\n",
      "Epoch 7/10, Train Loss: 0.1206, Accuracy: 0.924, F1 Micro: 0.7818, F1 Macro: 0.6937\n",
      "Epoch 8/10, Train Loss: 0.0988, Accuracy: 0.9237, F1 Micro: 0.7842, F1 Macro: 0.7036\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9266, F1 Micro: 0.789, F1 Macro: 0.7241\n",
      "Epoch 10/10, Train Loss: 0.0733, Accuracy: 0.9245, F1 Micro: 0.7821, F1 Macro: 0.722\n",
      "Best result for 8817 samples: F1 Micro: 0.789\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.87      1190\n",
      "      Abusive       0.90      0.89      0.90      1018\n",
      "HS_Individual       0.77      0.74      0.75       768\n",
      "     HS_Group       0.76      0.67      0.71       422\n",
      "  HS_Religion       0.71      0.56      0.63       173\n",
      "      HS_Race       0.80      0.71      0.76       126\n",
      "  HS_Physical       0.88      0.37      0.52        60\n",
      "    HS_Gender       0.82      0.48      0.60        67\n",
      "     HS_Other       0.82      0.76      0.79       792\n",
      "      HS_Weak       0.75      0.72      0.73       725\n",
      "  HS_Moderate       0.68      0.60      0.64       352\n",
      "    HS_Strong       0.88      0.74      0.80       113\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5806\n",
      "    macro avg       0.80      0.67      0.72      5806\n",
      " weighted avg       0.82      0.76      0.79      5806\n",
      "  samples avg       0.45      0.44      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 4.700497174781049e-06\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.940935373306274 seconds\n",
      "\n",
      "Fold 5 - New train size: 9017\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9017 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.8791, F1 Micro: 0.5785, F1 Macro: 0.2729\n",
      "Epoch 2/10, Train Loss: 0.2894, Accuracy: 0.8994, F1 Micro: 0.7262, F1 Macro: 0.5396\n",
      "Epoch 3/10, Train Loss: 0.2368, Accuracy: 0.9156, F1 Micro: 0.7503, F1 Macro: 0.5753\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9181, F1 Micro: 0.7599, F1 Macro: 0.6127\n",
      "Epoch 5/10, Train Loss: 0.1669, Accuracy: 0.9229, F1 Micro: 0.7712, F1 Macro: 0.6607\n",
      "Epoch 6/10, Train Loss: 0.1413, Accuracy: 0.9206, F1 Micro: 0.7845, F1 Macro: 0.6758\n",
      "Epoch 7/10, Train Loss: 0.117, Accuracy: 0.9209, F1 Micro: 0.7809, F1 Macro: 0.7024\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9216, F1 Micro: 0.7749, F1 Macro: 0.6859\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9206, F1 Micro: 0.7807, F1 Macro: 0.712\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9199, F1 Micro: 0.7822, F1 Macro: 0.7182\n",
      "Best result for 9017 samples: F1 Micro: 0.7845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.91      0.86      1190\n",
      "      Abusive       0.87      0.92      0.90      1018\n",
      "HS_Individual       0.72      0.80      0.76       768\n",
      "     HS_Group       0.72      0.67      0.69       422\n",
      "  HS_Religion       0.65      0.64      0.64       173\n",
      "      HS_Race       0.79      0.73      0.76       126\n",
      "  HS_Physical       0.90      0.15      0.26        60\n",
      "    HS_Gender       0.88      0.21      0.34        67\n",
      "     HS_Other       0.75      0.85      0.79       792\n",
      "      HS_Weak       0.70      0.79      0.74       725\n",
      "  HS_Moderate       0.65      0.53      0.59       352\n",
      "    HS_Strong       0.80      0.78      0.79       113\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5806\n",
      "    macro avg       0.77      0.66      0.68      5806\n",
      " weighted avg       0.77      0.80      0.78      5806\n",
      "  samples avg       0.46      0.47      0.45      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 2.4572116672061394e-05\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.722896814346313 seconds\n",
      "\n",
      "Fold 5 - New train size: 9217\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9217 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4028, Accuracy: 0.8812, F1 Micro: 0.5985, F1 Macro: 0.2988\n",
      "Epoch 2/10, Train Loss: 0.2789, Accuracy: 0.9049, F1 Micro: 0.7065, F1 Macro: 0.5118\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9156, F1 Micro: 0.7574, F1 Macro: 0.594\n",
      "Epoch 4/10, Train Loss: 0.1949, Accuracy: 0.9096, F1 Micro: 0.7591, F1 Macro: 0.6025\n",
      "Epoch 5/10, Train Loss: 0.1643, Accuracy: 0.9231, F1 Micro: 0.77, F1 Macro: 0.6566\n",
      "Epoch 6/10, Train Loss: 0.135, Accuracy: 0.9207, F1 Micro: 0.7813, F1 Macro: 0.6712\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9242, F1 Micro: 0.7835, F1 Macro: 0.7128\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.9195, F1 Micro: 0.782, F1 Macro: 0.7104\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.9192, F1 Micro: 0.7801, F1 Macro: 0.7057\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9234, F1 Micro: 0.7905, F1 Macro: 0.7249\n",
      "Best result for 9217 samples: F1 Micro: 0.7905\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1190\n",
      "      Abusive       0.88      0.91      0.90      1018\n",
      "HS_Individual       0.73      0.79      0.76       768\n",
      "     HS_Group       0.71      0.67      0.69       422\n",
      "  HS_Religion       0.71      0.59      0.64       173\n",
      "      HS_Race       0.81      0.72      0.76       126\n",
      "  HS_Physical       0.88      0.37      0.52        60\n",
      "    HS_Gender       0.71      0.51      0.59        67\n",
      "     HS_Other       0.75      0.83      0.79       792\n",
      "      HS_Weak       0.71      0.78      0.74       725\n",
      "  HS_Moderate       0.65      0.57      0.61       352\n",
      "    HS_Strong       0.82      0.83      0.83       113\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5806\n",
      "    macro avg       0.77      0.71      0.72      5806\n",
      " weighted avg       0.78      0.80      0.79      5806\n",
      "  samples avg       0.47      0.47      0.45      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Threshold: 3.560624418241787e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 1\n",
      "Sampling duration: 19.421746253967285 seconds\n",
      "\n",
      "Fold 5 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4046, Accuracy: 0.8822, F1 Micro: 0.6268, F1 Macro: 0.3031\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9028, F1 Micro: 0.7134, F1 Macro: 0.4817\n",
      "Epoch 3/10, Train Loss: 0.2313, Accuracy: 0.915, F1 Micro: 0.7465, F1 Macro: 0.5872\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9168, F1 Micro: 0.7597, F1 Macro: 0.6106\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6521\n",
      "Epoch 6/10, Train Loss: 0.1367, Accuracy: 0.9228, F1 Micro: 0.7763, F1 Macro: 0.6748\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9227, F1 Micro: 0.7874, F1 Macro: 0.6918\n",
      "Epoch 8/10, Train Loss: 0.0967, Accuracy: 0.925, F1 Micro: 0.7881, F1 Macro: 0.7211\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9242, F1 Micro: 0.7864, F1 Macro: 0.7254\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9248, F1 Micro: 0.7872, F1 Macro: 0.7346\n",
      "Best result for 9218 samples: F1 Micro: 0.7881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1190\n",
      "      Abusive       0.89      0.89      0.89      1018\n",
      "HS_Individual       0.75      0.78      0.76       768\n",
      "     HS_Group       0.76      0.63      0.69       422\n",
      "  HS_Religion       0.65      0.61      0.63       173\n",
      "      HS_Race       0.78      0.75      0.76       126\n",
      "  HS_Physical       0.90      0.32      0.47        60\n",
      "    HS_Gender       0.69      0.57      0.62        67\n",
      "     HS_Other       0.82      0.76      0.79       792\n",
      "      HS_Weak       0.72      0.75      0.74       725\n",
      "  HS_Moderate       0.68      0.55      0.61       352\n",
      "    HS_Strong       0.86      0.79      0.82       113\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5806\n",
      "    macro avg       0.78      0.69      0.72      5806\n",
      " weighted avg       0.80      0.78      0.79      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 4.9038556880987005e-06\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.07462739944458 seconds\n",
      "\n",
      "Fold 5 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4035, Accuracy: 0.88, F1 Micro: 0.5898, F1 Macro: 0.2854\n",
      "Epoch 2/10, Train Loss: 0.2789, Accuracy: 0.9052, F1 Micro: 0.711, F1 Macro: 0.5183\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9145, F1 Micro: 0.7455, F1 Macro: 0.5763\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9178, F1 Micro: 0.7688, F1 Macro: 0.6064\n",
      "Epoch 5/10, Train Loss: 0.1626, Accuracy: 0.9226, F1 Micro: 0.7711, F1 Macro: 0.6208\n",
      "Epoch 6/10, Train Loss: 0.1374, Accuracy: 0.9229, F1 Micro: 0.7782, F1 Macro: 0.6714\n",
      "Epoch 7/10, Train Loss: 0.1174, Accuracy: 0.9208, F1 Micro: 0.7807, F1 Macro: 0.6931\n",
      "Epoch 8/10, Train Loss: 0.0959, Accuracy: 0.9223, F1 Micro: 0.7857, F1 Macro: 0.7209\n",
      "Epoch 9/10, Train Loss: 0.0825, Accuracy: 0.9208, F1 Micro: 0.7828, F1 Macro: 0.7223\n",
      "Epoch 10/10, Train Loss: 0.0714, Accuracy: 0.9212, F1 Micro: 0.7814, F1 Macro: 0.7255\n",
      "Best result for 9418 samples: F1 Micro: 0.7857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.87      1190\n",
      "      Abusive       0.88      0.92      0.90      1018\n",
      "HS_Individual       0.73      0.76      0.74       768\n",
      "     HS_Group       0.71      0.69      0.70       422\n",
      "  HS_Religion       0.67      0.57      0.62       173\n",
      "      HS_Race       0.83      0.73      0.78       126\n",
      "  HS_Physical       0.88      0.37      0.52        60\n",
      "    HS_Gender       0.79      0.46      0.58        67\n",
      "     HS_Other       0.77      0.82      0.79       792\n",
      "      HS_Weak       0.71      0.73      0.72       725\n",
      "  HS_Moderate       0.65      0.60      0.62       352\n",
      "    HS_Strong       0.83      0.81      0.82       113\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5806\n",
      "    macro avg       0.77      0.70      0.72      5806\n",
      " weighted avg       0.78      0.79      0.78      5806\n",
      "  samples avg       0.46      0.46      0.45      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 5.646453519148054e-06\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.362989902496338 seconds\n",
      "\n",
      "Fold 5 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9618 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3958, Accuracy: 0.8844, F1 Micro: 0.6327, F1 Macro: 0.3184\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.9059, F1 Micro: 0.7163, F1 Macro: 0.5347\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9117, F1 Micro: 0.7542, F1 Macro: 0.5943\n",
      "Epoch 4/10, Train Loss: 0.1861, Accuracy: 0.9208, F1 Micro: 0.7723, F1 Macro: 0.6162\n",
      "Epoch 5/10, Train Loss: 0.1555, Accuracy: 0.9212, F1 Micro: 0.7759, F1 Macro: 0.6452\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9233, F1 Micro: 0.7842, F1 Macro: 0.6903\n",
      "Epoch 7/10, Train Loss: 0.1115, Accuracy: 0.9247, F1 Micro: 0.7867, F1 Macro: 0.7089\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9231, F1 Micro: 0.7829, F1 Macro: 0.7129\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9216, F1 Micro: 0.7858, F1 Macro: 0.7196\n",
      "Epoch 10/10, Train Loss: 0.0714, Accuracy: 0.925, F1 Micro: 0.7882, F1 Macro: 0.7293\n",
      "Best result for 9618 samples: F1 Micro: 0.7882\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1190\n",
      "      Abusive       0.89      0.91      0.90      1018\n",
      "HS_Individual       0.73      0.77      0.75       768\n",
      "     HS_Group       0.77      0.62      0.69       422\n",
      "  HS_Religion       0.74      0.52      0.61       173\n",
      "      HS_Race       0.82      0.72      0.77       126\n",
      "  HS_Physical       0.85      0.47      0.60        60\n",
      "    HS_Gender       0.77      0.51      0.61        67\n",
      "     HS_Other       0.79      0.80      0.79       792\n",
      "      HS_Weak       0.71      0.75      0.73       725\n",
      "  HS_Moderate       0.68      0.54      0.60       352\n",
      "    HS_Strong       0.87      0.79      0.83       113\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5806\n",
      "    macro avg       0.79      0.69      0.73      5806\n",
      " weighted avg       0.80      0.78      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 2.060401584458305e-06\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.584216833114624 seconds\n",
      "\n",
      "Fold 5 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9818 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3918, Accuracy: 0.8808, F1 Micro: 0.5934, F1 Macro: 0.2825\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.902, F1 Micro: 0.7177, F1 Macro: 0.4993\n",
      "Epoch 3/10, Train Loss: 0.221, Accuracy: 0.9172, F1 Micro: 0.7576, F1 Macro: 0.5969\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9177, F1 Micro: 0.7713, F1 Macro: 0.6519\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9172, F1 Micro: 0.7707, F1 Macro: 0.6641\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9233, F1 Micro: 0.7835, F1 Macro: 0.696\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9251, F1 Micro: 0.7798, F1 Macro: 0.6908\n",
      "Epoch 8/10, Train Loss: 0.0924, Accuracy: 0.92, F1 Micro: 0.7828, F1 Macro: 0.718\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.926, F1 Micro: 0.7856, F1 Macro: 0.728\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9277, F1 Micro: 0.7881, F1 Macro: 0.7378\n",
      "Best result for 9818 samples: F1 Micro: 0.7881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.83      0.87      1190\n",
      "      Abusive       0.90      0.89      0.89      1018\n",
      "HS_Individual       0.79      0.71      0.75       768\n",
      "     HS_Group       0.77      0.65      0.70       422\n",
      "  HS_Religion       0.73      0.62      0.67       173\n",
      "      HS_Race       0.85      0.71      0.78       126\n",
      "  HS_Physical       0.91      0.48      0.63        60\n",
      "    HS_Gender       0.78      0.52      0.62        67\n",
      "     HS_Other       0.85      0.72      0.78       792\n",
      "      HS_Weak       0.76      0.69      0.73       725\n",
      "  HS_Moderate       0.68      0.57      0.62       352\n",
      "    HS_Strong       0.88      0.75      0.81       113\n",
      "\n",
      "    micro avg       0.83      0.75      0.79      5806\n",
      "    macro avg       0.82      0.68      0.74      5806\n",
      " weighted avg       0.83      0.75      0.79      5806\n",
      "  samples avg       0.46      0.43      0.43      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 1.775718783392223e-06\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.928470373153687 seconds\n",
      "\n",
      "Fold 5 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10018 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3849, Accuracy: 0.8844, F1 Micro: 0.6285, F1 Macro: 0.3173\n",
      "Epoch 2/10, Train Loss: 0.26, Accuracy: 0.9054, F1 Micro: 0.7161, F1 Macro: 0.5008\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9147, F1 Micro: 0.7518, F1 Macro: 0.568\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9181, F1 Micro: 0.7641, F1 Macro: 0.6281\n",
      "Epoch 5/10, Train Loss: 0.1511, Accuracy: 0.9236, F1 Micro: 0.7742, F1 Macro: 0.6474\n",
      "Epoch 6/10, Train Loss: 0.1222, Accuracy: 0.9245, F1 Micro: 0.7828, F1 Macro: 0.6878\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9229, F1 Micro: 0.7893, F1 Macro: 0.716\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9256, F1 Micro: 0.7848, F1 Macro: 0.7173\n",
      "Epoch 9/10, Train Loss: 0.0755, Accuracy: 0.9228, F1 Micro: 0.786, F1 Macro: 0.7232\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9246, F1 Micro: 0.7884, F1 Macro: 0.7275\n",
      "Best result for 10018 samples: F1 Micro: 0.7893\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1190\n",
      "      Abusive       0.85      0.94      0.89      1018\n",
      "HS_Individual       0.74      0.79      0.76       768\n",
      "     HS_Group       0.72      0.68      0.70       422\n",
      "  HS_Religion       0.68      0.62      0.65       173\n",
      "      HS_Race       0.81      0.70      0.75       126\n",
      "  HS_Physical       0.78      0.35      0.48        60\n",
      "    HS_Gender       0.84      0.39      0.53        67\n",
      "     HS_Other       0.77      0.81      0.79       792\n",
      "      HS_Weak       0.72      0.77      0.74       725\n",
      "  HS_Moderate       0.65      0.60      0.62       352\n",
      "    HS_Strong       0.88      0.74      0.81       113\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5806\n",
      "    macro avg       0.77      0.69      0.72      5806\n",
      " weighted avg       0.78      0.80      0.79      5806\n",
      "  samples avg       0.47      0.47      0.46      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 5.604896523436765e-06\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.144402503967285 seconds\n",
      "\n",
      "Fold 5 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10218 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.8832, F1 Micro: 0.6375, F1 Macro: 0.3179\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9046, F1 Micro: 0.7067, F1 Macro: 0.4693\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.9152, F1 Micro: 0.7521, F1 Macro: 0.5842\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.916, F1 Micro: 0.7671, F1 Macro: 0.6118\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9215, F1 Micro: 0.7685, F1 Macro: 0.6428\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9248, F1 Micro: 0.7846, F1 Macro: 0.6873\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9261, F1 Micro: 0.7862, F1 Macro: 0.6861\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.919, F1 Micro: 0.7826, F1 Macro: 0.7122\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9247, F1 Micro: 0.7872, F1 Macro: 0.726\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9226, F1 Micro: 0.7866, F1 Macro: 0.7324\n",
      "Best result for 10218 samples: F1 Micro: 0.7872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1190\n",
      "      Abusive       0.87      0.92      0.90      1018\n",
      "HS_Individual       0.75      0.76      0.75       768\n",
      "     HS_Group       0.76      0.65      0.70       422\n",
      "  HS_Religion       0.66      0.61      0.63       173\n",
      "      HS_Race       0.79      0.71      0.75       126\n",
      "  HS_Physical       0.93      0.42      0.57        60\n",
      "    HS_Gender       0.82      0.48      0.60        67\n",
      "     HS_Other       0.81      0.77      0.79       792\n",
      "      HS_Weak       0.73      0.75      0.74       725\n",
      "  HS_Moderate       0.68      0.57      0.62       352\n",
      "    HS_Strong       0.87      0.74      0.80       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5806\n",
      "    macro avg       0.79      0.69      0.73      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.46      0.45      0.45      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 2.1166378246562093e-06\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.347505331039429 seconds\n",
      "\n",
      "Fold 5 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10418 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3755, Accuracy: 0.8843, F1 Micro: 0.6557, F1 Macro: 0.3477\n",
      "Epoch 2/10, Train Loss: 0.2529, Accuracy: 0.9038, F1 Micro: 0.7127, F1 Macro: 0.5344\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9155, F1 Micro: 0.7447, F1 Macro: 0.5804\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9202, F1 Micro: 0.7695, F1 Macro: 0.6093\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9144, F1 Micro: 0.7724, F1 Macro: 0.6553\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.923, F1 Micro: 0.7792, F1 Macro: 0.6793\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9234, F1 Micro: 0.7869, F1 Macro: 0.7107\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9231, F1 Micro: 0.7871, F1 Macro: 0.7195\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9222, F1 Micro: 0.785, F1 Macro: 0.7103\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9265, F1 Micro: 0.7924, F1 Macro: 0.7312\n",
      "Best result for 10418 samples: F1 Micro: 0.7924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.88      0.87      1190\n",
      "      Abusive       0.90      0.88      0.89      1018\n",
      "HS_Individual       0.73      0.78      0.76       768\n",
      "     HS_Group       0.79      0.63      0.70       422\n",
      "  HS_Religion       0.71      0.62      0.66       173\n",
      "      HS_Race       0.81      0.72      0.76       126\n",
      "  HS_Physical       0.95      0.35      0.51        60\n",
      "    HS_Gender       0.75      0.57      0.64        67\n",
      "     HS_Other       0.81      0.80      0.80       792\n",
      "      HS_Weak       0.71      0.76      0.74       725\n",
      "  HS_Moderate       0.72      0.53      0.61       352\n",
      "    HS_Strong       0.85      0.81      0.83       113\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5806\n",
      "    macro avg       0.80      0.69      0.73      5806\n",
      " weighted avg       0.81      0.78      0.79      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10536\n",
      "Threshold: 1.1073621635659947e-06\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 118\n",
      "Sampling duration: 2.8982527256011963 seconds\n",
      "\n",
      "Fold 5 - New train size: 10536\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10536 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3715, Accuracy: 0.8845, F1 Micro: 0.6553, F1 Macro: 0.336\n",
      "Epoch 2/10, Train Loss: 0.2485, Accuracy: 0.9067, F1 Micro: 0.711, F1 Macro: 0.5087\n",
      "Epoch 3/10, Train Loss: 0.1996, Accuracy: 0.9142, F1 Micro: 0.7547, F1 Macro: 0.5955\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9187, F1 Micro: 0.7499, F1 Macro: 0.5979\n",
      "Epoch 5/10, Train Loss: 0.1392, Accuracy: 0.9209, F1 Micro: 0.7566, F1 Macro: 0.6488\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9251, F1 Micro: 0.7759, F1 Macro: 0.6835\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9245, F1 Micro: 0.7774, F1 Macro: 0.6924\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9232, F1 Micro: 0.7836, F1 Macro: 0.6964\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9252, F1 Micro: 0.7825, F1 Macro: 0.7128\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9206, F1 Micro: 0.7815, F1 Macro: 0.7203\n",
      "Best result for 10536 samples: F1 Micro: 0.7836\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1190\n",
      "      Abusive       0.90      0.89      0.90      1018\n",
      "HS_Individual       0.76      0.74      0.75       768\n",
      "     HS_Group       0.70      0.69      0.69       422\n",
      "  HS_Religion       0.75      0.46      0.57       173\n",
      "      HS_Race       0.83      0.67      0.74       126\n",
      "  HS_Physical       0.89      0.28      0.43        60\n",
      "    HS_Gender       0.91      0.31      0.47        67\n",
      "     HS_Other       0.76      0.82      0.79       792\n",
      "      HS_Weak       0.74      0.72      0.73       725\n",
      "  HS_Moderate       0.61      0.62      0.61       352\n",
      "    HS_Strong       0.87      0.75      0.81       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5806\n",
      "    macro avg       0.80      0.65      0.70      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 6632.83 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[1:]\n",
    "X = data['Tweet'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    train_fold_df = pd.DataFrame(X_train_fold, columns=['Tweet'])\n",
    "    train_fold_df[label_columns] = y_train_fold\n",
    "\n",
    "    val_fold_df = pd.DataFrame(X_val_fold, columns=['Tweet'])\n",
    "    val_fold_df[label_columns] = y_val_fold\n",
    "\n",
    "    fold_data_dir = 'kfold_splits'\n",
    "    if not os.path.exists(fold_data_dir):\n",
    "        os.makedirs(fold_data_dir)\n",
    "\n",
    "    train_fold_df.to_csv(f'{fold_data_dir}/train_fold_{fold + 1}.csv', index=False)\n",
    "    val_fold_df.to_csv(f'{fold_data_dir}/val_fold_{fold + 1}.csv', index=False)\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    checkpoints = [\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(f'{filename}-fold-{fold + 1}-model')\n",
    "        \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        sampling_args = (\n",
    "            model, X_pool, train_indices, remaining_indices, sampling_dur, \n",
    "            new_samples_shared, fold, X_train_fold, y_train_fold\n",
    "        )\n",
    "        notebook_launcher(monte_carlo_dropout_sampling, sampling_args, num_processes=2)\n",
    "        \n",
    "        # Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4cb4fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T22:54:09.704875Z",
     "iopub.status.busy": "2025-06-25T22:54:09.704531Z",
     "iopub.status.idle": "2025-06-25T22:54:10.444763Z",
     "shell.execute_reply": "2025-06-25T22:54:10.443934Z"
    },
    "papermill": {
     "duration": 1.036489,
     "end_time": "2025-06-25T22:54:10.446741",
     "exception": false,
     "start_time": "2025-06-25T22:54:09.410252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7611336,
     "sourceId": 12090808,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33110.336614,
   "end_time": "2025-06-25T22:54:13.528435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-25T13:42:23.191821",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04b40797787240a08082fc1fe59d9c32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a97e6a9d2e04c58921000a43577e054": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ae25cb3c2ba4f5a8a0f83c6170c3b48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1aae07f9eebe4857a183f9e679fa0523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1dace3dbd6c647119996fa824ac51c5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1fbf410558e4495cbbc9fbcc3f8cb20e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd43da7ecee145dfac3574ccb6c20061",
       "placeholder": "​",
       "style": "IPY_MODEL_4c07f58592b04044885d237713c32a34",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/? [00:00&lt;00:00, 151kB/s]"
      }
     },
     "278e641e58574a1e93f643e868c7af68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73ca33313bd74b8e9c6a15edd04a4ce2",
       "placeholder": "​",
       "style": "IPY_MODEL_76e2c440b0e845e7849a3b89fd4c2a45",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/? [00:00&lt;00:00, 6.73MB/s]"
      }
     },
     "27cab016431944f1b4e2666ea05320f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b7695af97dd4fd3aa24616d3767ad9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "310577e6e9cd4fc3b8df086a2677ab31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31b4a7b615f24ba790261bc8e2891383": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4f862fb90d9432c95dfd8d83ded8cf8",
        "IPY_MODEL_64333935223b406fb5cbaed9924d3e6a",
        "IPY_MODEL_e6f9580c29a34a619ce45b112409c727"
       ],
       "layout": "IPY_MODEL_7faee1982483461c94c15e950c5af378",
       "tabbable": null,
       "tooltip": null
      }
     },
     "327717b8391c4de8806c86f3b23bea85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76903ee4f26248dd88163057b4389f1d",
        "IPY_MODEL_ab7d1f672b5441409bd3beadacb27b19",
        "IPY_MODEL_d33f003aa98d46d8b66b6d07b8a72748"
       ],
       "layout": "IPY_MODEL_04b40797787240a08082fc1fe59d9c32",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3376d87418794388a45644011055a449": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_563cd3570283483294285b0f493f7943",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27cab016431944f1b4e2666ea05320f3",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3593be42fde042828e96a116ee9f1bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe51dd0cc59f4ef49ab6b26ad7f1d718",
        "IPY_MODEL_3376d87418794388a45644011055a449",
        "IPY_MODEL_278e641e58574a1e93f643e868c7af68"
       ],
       "layout": "IPY_MODEL_2b7695af97dd4fd3aa24616d3767ad9c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3633256d94c6485aa5bf3da12edd7ea4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3957ce303fc2433e8696a383383331e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4506366331de4054bcbaa0734092fc96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4c07f58592b04044885d237713c32a34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "563cd3570283483294285b0f493f7943": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "5d87ed2aefb641918f0d83ba0aae4db6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e721922ec33a4c50b31c332d738a4d07",
       "placeholder": "​",
       "style": "IPY_MODEL_72d3fd0c0889414596445c63e620f4b2",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 168B/s]"
      }
     },
     "603ed9dd1b3948babe229685906b1a3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "60f07c04013a4d41bf54d5c8419fbf31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d29a61920c094255a6e171a2b6197d5e",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e18d3d6a471c49a9a7d8f2f54ad8e8bb",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "64333935223b406fb5cbaed9924d3e6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a97e6a9d2e04c58921000a43577e054",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c375631a1f6e4e6f9135c9e787b94b7c",
       "tabbable": null,
       "tooltip": null,
       "value": 497810400.0
      }
     },
     "6d16e5f0fdfd4701b51f94585c892661": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0ae25cb3c2ba4f5a8a0f83c6170c3b48",
       "placeholder": "​",
       "style": "IPY_MODEL_e264b224a2ce492a94ee841505cba4da",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: "
      }
     },
     "72d3fd0c0889414596445c63e620f4b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "73ca33313bd74b8e9c6a15edd04a4ce2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76903ee4f26248dd88163057b4389f1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a7e68807cf5e44479498cb56ec20e593",
       "placeholder": "​",
       "style": "IPY_MODEL_9fdf1ab5ea7b461f864239f2d3fa1d19",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "76aa0047973647be85df8c6e20ce3b4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f5735f847f5640d29c263c794395f6fb",
        "IPY_MODEL_94dd284de9ec499b8dbe6bf1f1534a1d",
        "IPY_MODEL_5d87ed2aefb641918f0d83ba0aae4db6"
       ],
       "layout": "IPY_MODEL_89f2a995462848ed8a569390c8393d1b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "76e2c440b0e845e7849a3b89fd4c2a45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7faee1982483461c94c15e950c5af378": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "823dabc5dd9a4f149b78e13b1f799633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d16e5f0fdfd4701b51f94585c892661",
        "IPY_MODEL_60f07c04013a4d41bf54d5c8419fbf31",
        "IPY_MODEL_1fbf410558e4495cbbc9fbcc3f8cb20e"
       ],
       "layout": "IPY_MODEL_da5a622a36f84c6a9a438af44c4b9717",
       "tabbable": null,
       "tooltip": null
      }
     },
     "89f2a995462848ed8a569390c8393d1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "91f37207841b4c908fc7d1a171934fd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94dd284de9ec499b8dbe6bf1f1534a1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3633256d94c6485aa5bf3da12edd7ea4",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_91f37207841b4c908fc7d1a171934fd1",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "958d9822fa0f4ec48cbdc6fe7670c031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9fdf1ab5ea7b461f864239f2d3fa1d19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a0c5c33b474e4b27abf8ed426817639e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7e68807cf5e44479498cb56ec20e593": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab7d1f672b5441409bd3beadacb27b19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e2998830890c47b6a612bba88d871753",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1aae07f9eebe4857a183f9e679fa0523",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "ad074f3c3a154a90bbdd80860828f90b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd43da7ecee145dfac3574ccb6c20061": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c375631a1f6e4e6f9135c9e787b94b7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d29a61920c094255a6e171a2b6197d5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "d33f003aa98d46d8b66b6d07b8a72748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1dace3dbd6c647119996fa824ac51c5b",
       "placeholder": "​",
       "style": "IPY_MODEL_603ed9dd1b3948babe229685906b1a3b",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 12.2kB/s]"
      }
     },
     "da5a622a36f84c6a9a438af44c4b9717": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db6490eb71b2484aa0751bbb411a6f6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e18d3d6a471c49a9a7d8f2f54ad8e8bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e264b224a2ce492a94ee841505cba4da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e2998830890c47b6a612bba88d871753": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6f9580c29a34a619ce45b112409c727": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_310577e6e9cd4fc3b8df086a2677ab31",
       "placeholder": "​",
       "style": "IPY_MODEL_a0c5c33b474e4b27abf8ed426817639e",
       "tabbable": null,
       "tooltip": null,
       "value": " 498M/498M [00:02&lt;00:00, 259MB/s]"
      }
     },
     "e721922ec33a4c50b31c332d738a4d07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ecbc86a1f91f40bab2faf562b796f26c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4f862fb90d9432c95dfd8d83ded8cf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3957ce303fc2433e8696a383383331e0",
       "placeholder": "​",
       "style": "IPY_MODEL_ad074f3c3a154a90bbdd80860828f90b",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin: 100%"
      }
     },
     "f5735f847f5640d29c263c794395f6fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ecbc86a1f91f40bab2faf562b796f26c",
       "placeholder": "​",
       "style": "IPY_MODEL_958d9822fa0f4ec48cbdc6fe7670c031",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "fe51dd0cc59f4ef49ab6b26ad7f1d718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db6490eb71b2484aa0751bbb411a6f6b",
       "placeholder": "​",
       "style": "IPY_MODEL_4506366331de4054bcbaa0734092fc96",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: "
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
