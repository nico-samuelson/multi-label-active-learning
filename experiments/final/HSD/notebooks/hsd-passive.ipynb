{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10457689,"sourceType":"datasetVersion","datasetId":6473829}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport torch\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.multiprocessing import Manager\nfrom torch.utils.data import DataLoader, Dataset\nfrom accelerate import Accelerator, notebook_launcher\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\nfrom transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.599728Z","iopub.execute_input":"2025-04-12T14:55:02.600127Z","iopub.status.idle":"2025-04-12T14:55:02.605668Z","shell.execute_reply.started":"2025-04-12T14:55:02.600098Z","shell.execute_reply":"2025-04-12T14:55:02.604657Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:02.607197Z","iopub.execute_input":"2025-04-12T14:55:02.607490Z","iopub.status.idle":"2025-04-12T14:55:02.628731Z","shell.execute_reply.started":"2025-04-12T14:55:02.607470Z","shell.execute_reply":"2025-04-12T14:55:02.627781Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:02.630600Z","iopub.execute_input":"2025-04-12T14:55:02.630867Z","iopub.status.idle":"2025-04-12T14:55:02.646562Z","shell.execute_reply.started":"2025-04-12T14:55:02.630848Z","shell.execute_reply":"2025-04-12T14:55:02.645878Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/re_dataset.csv', encoding='latin-1')\n\nalay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/new_kamusalay.csv', encoding='latin-1', header=None)\nalay_dict = alay_dict.rename(columns={0: 'original', \n                                      1: 'replacement'})","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.647954Z","iopub.execute_input":"2025-04-12T14:55:02.648213Z","iopub.status.idle":"2025-04-12T14:55:02.732996Z","shell.execute_reply.started":"2025-04-12T14:55:02.648194Z","shell.execute_reply":"2025-04-12T14:55:02.732368Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(\"Shape: \", data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.733789Z","iopub.execute_input":"2025-04-12T14:55:02.733997Z","iopub.status.idle":"2025-04-12T14:55:02.745778Z","shell.execute_reply.started":"2025-04-12T14:55:02.733978Z","shell.execute_reply":"2025-04-12T14:55:02.744961Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape:  (13169, 13)\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                               Tweet  HS  Abusive  \\\n0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n\n   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n0              1         0            0        0            0          0   \n1              0         0            0        0            0          0   \n2              0         0            0        0            0          0   \n3              0         0            0        0            0          0   \n4              0         1            1        0            0          0   \n\n   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n0         1        1            0          0  \n1         0        0            0          0  \n2         0        0            0          0  \n3         0        0            0          0  \n4         0        0            1          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet</th>\n      <th>HS</th>\n      <th>Abusive</th>\n      <th>HS_Individual</th>\n      <th>HS_Group</th>\n      <th>HS_Religion</th>\n      <th>HS_Race</th>\n      <th>HS_Physical</th>\n      <th>HS_Gender</th>\n      <th>HS_Other</th>\n      <th>HS_Weak</th>\n      <th>HS_Moderate</th>\n      <th>HS_Strong</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"data.HS.value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.746578Z","iopub.execute_input":"2025-04-12T14:55:02.746816Z","iopub.status.idle":"2025-04-12T14:55:02.765850Z","shell.execute_reply.started":"2025-04-12T14:55:02.746787Z","shell.execute_reply":"2025-04-12T14:55:02.765168Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"HS\n0    7608\n1    5561\nName: count, dtype: int64"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"data.Abusive.value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.766721Z","iopub.execute_input":"2025-04-12T14:55:02.766969Z","iopub.status.idle":"2025-04-12T14:55:02.784844Z","shell.execute_reply.started":"2025-04-12T14:55:02.766949Z","shell.execute_reply":"2025-04-12T14:55:02.784171Z"},"trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Abusive\n0    8126\n1    5043\nName: count, dtype: int64"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\nprint(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.787155Z","iopub.execute_input":"2025-04-12T14:55:02.787394Z","iopub.status.idle":"2025-04-12T14:55:02.808748Z","shell.execute_reply.started":"2025-04-12T14:55:02.787376Z","shell.execute_reply":"2025-04-12T14:55:02.808003Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Toxic shape:  (7309, 13)\nNon-toxic shape:  (5860, 13)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(\"Shape: \", alay_dict.shape)\nalay_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.810139Z","iopub.execute_input":"2025-04-12T14:55:02.810421Z","iopub.status.idle":"2025-04-12T14:55:02.826979Z","shell.execute_reply.started":"2025-04-12T14:55:02.810401Z","shell.execute_reply":"2025-04-12T14:55:02.826387Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape:  (15167, 2)\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"               original               replacement\n0   anakjakartaasikasik  anak jakarta asyik asyik\n1          pakcikdahtua         pak cik sudah tua\n2        pakcikmudalagi         pak cik muda lagi\n3           t3tapjokowi              tetap jokowi\n4                    3x                 tiga kali\n5                aamiin                      amin\n6               aamiinn                      amin\n7                 aamin                      amin\n8               aammiin                      amin\n9                  abis                     habis\n10               abisin                  habiskan\n11                 acau                     kacau\n12                achok                      ahok\n13                   ad                       ada\n14                 adek                      adik","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>replacement</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>anakjakartaasikasik</td>\n      <td>anak jakarta asyik asyik</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pakcikdahtua</td>\n      <td>pak cik sudah tua</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pakcikmudalagi</td>\n      <td>pak cik muda lagi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t3tapjokowi</td>\n      <td>tetap jokowi</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3x</td>\n      <td>tiga kali</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>aamiin</td>\n      <td>amin</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>aamiinn</td>\n      <td>amin</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>aamin</td>\n      <td>amin</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>aammiin</td>\n      <td>amin</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>abis</td>\n      <td>habis</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>abisin</td>\n      <td>habiskan</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>acau</td>\n      <td>kacau</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>achok</td>\n      <td>ahok</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ad</td>\n      <td>ada</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>adek</td>\n      <td>adik</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"def lowercase(text):\n    return text.lower()\n\ndef remove_unnecessary_char(text):\n    text = re.sub('\\n',' ',text) # Remove every '\\n'\n    text = re.sub('rt',' ',text) # Remove every retweet symbol\n    text = re.sub('user',' ',text) # Remove every username\n    text = re.sub('url', ' ', text) # Remove every URL\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    return text\n    \ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text\n\nalay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\ndef normalize_alay(text):\n    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n\nprint(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\nprint(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\nprint(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\nprint(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.827911Z","iopub.execute_input":"2025-04-12T14:55:02.828189Z","iopub.status.idle":"2025-04-12T14:55:02.849699Z","shell.execute_reply.started":"2025-04-12T14:55:02.828162Z","shell.execute_reply":"2025-04-12T14:55:02.848937Z"},"trusted":true},"outputs":[{"name":"stdout","text":"remove_nonaplhanumeric:  Halooo duniaa \nlowercase:  halooo, duniaa!\nremove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \nnormalize_alay:  amin adik habis\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def preprocess(text):\n    text = lowercase(text) # 1\n    text = remove_nonaplhanumeric(text) # 2\n    text = remove_unnecessary_char(text) # 2\n    text = normalize_alay(text) # 3\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.850386Z","iopub.execute_input":"2025-04-12T14:55:02.850626Z","iopub.status.idle":"2025-04-12T14:55:02.869824Z","shell.execute_reply.started":"2025-04-12T14:55:02.850608Z","shell.execute_reply":"2025-04-12T14:55:02.869169Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"data['Tweet'] = data['Tweet'].apply(preprocess)\n\n# Define the labels columns for multi-label classification\nlabel_columns = data.columns[1:]  # Assuming label columns start from the third column\n\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n\n# Extract features and labels for training and validation\nX_train = train_data['Tweet'].values\ny_train = train_data[label_columns].values\nX_val = val_data['Tweet'].values\ny_val = val_data[label_columns].values\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:02.870695Z","iopub.execute_input":"2025-04-12T14:55:02.870983Z","iopub.status.idle":"2025-04-12T14:55:03.233098Z","shell.execute_reply.started":"2025-04-12T14:55:02.870956Z","shell.execute_reply":"2025-04-12T14:55:03.232417Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(10535,) (10535, 12)\n(2634,) (2634, 12)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 64\nLEARNING_RATE = 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.233877Z","iopub.execute_input":"2025-04-12T14:55:03.234141Z","iopub.status.idle":"2025-04-12T14:55:03.237733Z","shell.execute_reply.started":"2025-04-12T14:55:03.234104Z","shell.execute_reply":"2025-04-12T14:55:03.236803Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n    labels = torch.tensor(p.label_ids)\n\n    # Hamming accuracy: proportion of correctly predicted labels over total labels\n    accuracy = (preds == labels).float().mean().item()\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n\n    report = classification_report(\n        labels, \n        preds, \n        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n        zero_division=0\n    )   \n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'report': report\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-12T14:55:03.238583Z","iopub.execute_input":"2025-04-12T14:55:03.238892Z","iopub.status.idle":"2025-04-12T14:55:03.279206Z","shell.execute_reply.started":"2025-04-12T14:55:03.238858Z","shell.execute_reply":"2025-04-12T14:55:03.278639Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class HateSpeechDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128, use_float=True):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.use_float = use_float\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        labels = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n        return item\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.279952Z","iopub.execute_input":"2025-04-12T14:55:03.280137Z","iopub.status.idle":"2025-04-12T14:55:03.444003Z","shell.execute_reply.started":"2025-04-12T14:55:03.280120Z","shell.execute_reply":"2025-04-12T14:55:03.443119Z"},"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def get_dataloaders(sequence_length, num_workers=4):\n    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=num_workers\n    )\n\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.444920Z","iopub.execute_input":"2025-04-12T14:55:03.445214Z","iopub.status.idle":"2025-04-12T14:55:03.449701Z","shell.execute_reply.started":"2025-04-12T14:55:03.445185Z","shell.execute_reply":"2025-04-12T14:55:03.449020Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"manager = Manager()\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.450466Z","iopub.execute_input":"2025-04-12T14:55:03.450685Z","iopub.status.idle":"2025-04-12T14:55:03.495419Z","shell.execute_reply.started":"2025-04-12T14:55:03.450666Z","shell.execute_reply":"2025-04-12T14:55:03.494129Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def train_model(sequence_length, model_name, metrics, seed=42, layers_freezed=6, num_workers=4):\n    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n    device = accelerator.device\n\n    with accelerator.main_process_first():\n        model = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=12,\n            problem_type=\"multi_label_classification\"\n        )\n\n    # Freeze the first few layers of the encoder\n    for name, param in model.named_parameters():\n        if \"encoder.layer\" in name:\n            layer_num = name.split(\".\")[3]\n            try:\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                continue\n\n    # Define DataLoaders\n    train_loader, val_loader = get_dataloaders(sequence_length, num_workers)\n\n    # Define optimizer and loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    # Prepare everything with Accelerator\n    model, optimizer, train_loader, val_loader = accelerator.prepare(\n        model, optimizer, train_loader, val_loader\n    )\n\n    best_result = None\n    start_time = time.time()\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for batch in train_loader:\n            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels']\n\n            optimizer.zero_grad()\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels)\n            accelerator.backward(loss)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels']\n                \n                outputs = model(**inputs)\n                preds = torch.sigmoid(outputs.logits).round()\n\n                # Gather predictions and labels from all devices\n                all_preds.append(accelerator.gather(preds))\n                all_labels.append(accelerator.gather(labels))\n\n        all_preds = torch.cat(all_preds).cpu().numpy()\n        all_labels = torch.cat(all_labels).cpu().numpy()\n        \n        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n            accelerator.print(\"Higher F1 achieved, saving model\")\n            best_result = result\n            \n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                f'model-{BATCH_SIZE}-{sequence_length}-{layers_freezed}-{seed}',\n                is_main_process=accelerator.is_main_process,\n                save_function=accelerator.save,\n            )\n\n        accelerator.print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    if accelerator.is_main_process:\n        metrics[0].append(best_result['accuracy'])\n        metrics[1].append(best_result['f1_micro'])\n        metrics[2].append(best_result['f1_macro'])\n        \n    accelerator.print(f\"\\nAccuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n    accelerator.print(best_result['report'])\n    accelerator.print(f\"Duration: {duration}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.496991Z","iopub.execute_input":"2025-04-12T14:55:03.497229Z","iopub.status.idle":"2025-04-12T14:55:03.511108Z","shell.execute_reply.started":"2025-04-12T14:55:03.497211Z","shell.execute_reply":"2025-04-12T14:55:03.510207Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Tokenize each text and calculate their lengths\ntoken_lengths = [len(tokenizer.tokenize(text)) for text in X_train]\n\n# Calculate the average length\naverage_length = sum(token_lengths) / len(token_lengths)\nmax_length = max(token_lengths)\n\nprint(\"Average length of tokenized text:\", average_length)\nprint(\"Max token length:\", max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:03.511946Z","iopub.execute_input":"2025-04-12T14:55:03.512218Z","iopub.status.idle":"2025-04-12T14:55:07.575612Z","shell.execute_reply.started":"2025-04-12T14:55:03.512190Z","shell.execute_reply":"2025-04-12T14:55:07.574680Z"}},"outputs":[{"name":"stdout","text":"Average length of tokenized text: 19.78690080683436\nMax token length: 83\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"counts, bins = np.histogram(token_lengths, range=(0, 500))\nplt.stairs(counts, bins)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:07.576622Z","iopub.execute_input":"2025-04-12T14:55:07.576932Z","iopub.status.idle":"2025-04-12T14:55:07.790067Z","shell.execute_reply.started":"2025-04-12T14:55:07.576901Z","shell.execute_reply":"2025-04-12T14:55:07.789140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmdElEQVR4nO3dfXSU1YHH8V9CmEl4mQkvZibRgNnFgryIAhqnvtQuc4iautKyu6Jp5WgK1SauEY/U7GpEaxsFRV6ksNZV6FmUlz2FUkA0GySsEgJGIiFgxC1tUnSStpAZQiEBcvcPT54yghbqhCSX7+ecOYd57p1n7nOBk++ZzCRxxhgjAAAAy8R39gIAAAA6ApEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEoJnb2AztTW1qZPPvlEffv2VVxcXGcvBwAAnAVjjA4fPqy0tDTFx3/x6zUXdOR88sknSk9P7+xlAACAv0F9fb0uueSSLxy/oCOnb9++kj7bJI/H08mrAQAAZyMSiSg9Pd35Ov5FLujIaf8WlcfjIXIAAOhm/tpbTXjjMQAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsNIF/VvIO9KBpqM6dKS1s5dxTvr1duni5KTOXgYAADFB5HSAA01HFXy+TEePn+zspZyTpJ499D8Pf4PQAQBYgcjpAIeOtOro8ZOae8eVGpLSp7OXc1Y+bmxWwYoqHTrSSuQAAKxA5HSgISl9NPJib2cvAwCACxJvPAYAAFYicgAAgJWIHAAAYKVzjpwtW7botttuU1pamuLi4rRmzZqocWOMioqKlJqaqqSkJAWDQe3bty9qzsGDB5WTkyOPx6Pk5GTl5uaqubk5as6uXbt0ww03KDExUenp6Zo1a9Zpa1m1apWGDRumxMREjRo1Shs2bDjXywEAAJY658g5cuSIRo8erYULF55xfNasWZo/f74WL16siooK9e7dW1lZWTp27JgzJycnRzU1NSopKdG6deu0ZcsWTZs2zRmPRCKaMGGCBg8erMrKSs2ePVszZ87USy+95MzZunWr7rzzTuXm5mrnzp2aOHGiJk6cqN27d5/rJQEAABuZr0CSWb16tXO/ra3N+P1+M3v2bOdYU1OTcbvd5vXXXzfGGLNnzx4jyezYscOZ88Ybb5i4uDhz4MABY4wxP/vZz0y/fv1MS0uLM+dHP/qRGTp0qHP/X/7lX0x2dnbUejIzM80PfvCDs15/OBw2kkw4HD7rx5yN6t83mcE/Wmeqf98U0/N2pO64ZgDAhelsv37H9D05+/fvVygUUjAYdI55vV5lZmaqvLxcklReXq7k5GSNGzfOmRMMBhUfH6+Kigpnzo033iiXy+XMycrKUm1trQ4dOuTMOfV52ue0Pw8AALiwxfTn5IRCIUmSz+eLOu7z+ZyxUCiklJSU6EUkJKh///5RczIyMk47R/tYv379FAqFvvR5zqSlpUUtLS3O/Ugkci6XBwAAupEL6tNVxcXF8nq9zi09Pb2zlwQAADpITCPH7/dLkhoaGqKONzQ0OGN+v1+NjY1R4ydOnNDBgwej5pzpHKc+xxfNaR8/k8LCQoXDYedWX19/rpcIAAC6iZhGTkZGhvx+v0pLS51jkUhEFRUVCgQCkqRAIKCmpiZVVlY6czZt2qS2tjZlZmY6c7Zs2aLjx487c0pKSjR06FD169fPmXPq87TPaX+eM3G73fJ4PFE3AABgp3OOnObmZlVVVamqqkrSZ282rqqqUl1dneLi4lRQUKCnn35aa9euVXV1te6++26lpaVp4sSJkqTLL79cN998s6ZOnart27fr3XffVX5+viZPnqy0tDRJ0l133SWXy6Xc3FzV1NRoxYoVmjdvnqZPn+6s48EHH9TGjRv1/PPP68MPP9TMmTP13nvvKT8//6vvCgAA6P7O9WNbb7/9tpF02m3KlCnGmM8+Rv74448bn89n3G63GT9+vKmtrY06x5/+9Cdz5513mj59+hiPx2Puuecec/jw4ag5H3zwgbn++uuN2+02F198sXnmmWdOW8vKlSvN1772NeNyucyIESPM+vXrz+la+Aj5X3THNQMALkxn+/U7zhhjOrGxOlUkEpHX61U4HI7pt652HwjrWwve0boHru82v4W8O64ZAHBhOtuv3xfUp6sAAMCFg8gBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAVop55Jw8eVKPP/64MjIylJSUpL//+7/Xj3/8YxljnDnGGBUVFSk1NVVJSUkKBoPat29f1HkOHjyonJwceTweJScnKzc3V83NzVFzdu3apRtuuEGJiYlKT0/XrFmzYn05AACgm4p55Dz77LNatGiRXnzxRe3du1fPPvusZs2apQULFjhzZs2apfnz52vx4sWqqKhQ7969lZWVpWPHjjlzcnJyVFNTo5KSEq1bt05btmzRtGnTnPFIJKIJEyZo8ODBqqys1OzZszVz5ky99NJLsb4kAADQDSXE+oRbt27V7bffruzsbEnSpZdeqtdff13bt2+X9NmrOHPnztVjjz2m22+/XZL0i1/8Qj6fT2vWrNHkyZO1d+9ebdy4UTt27NC4ceMkSQsWLNCtt96q5557TmlpaVq2bJlaW1v1yiuvyOVyacSIEaqqqtKcOXOiYggAAFyYYv5Kzte//nWVlpbqo48+kiR98MEHeuedd3TLLbdIkvbv369QKKRgMOg8xuv1KjMzU+Xl5ZKk8vJyJScnO4EjScFgUPHx8aqoqHDm3HjjjXK5XM6crKws1dbW6tChQ2dcW0tLiyKRSNQNAADYKeav5Dz66KOKRCIaNmyYevTooZMnT+onP/mJcnJyJEmhUEiS5PP5oh7n8/mcsVAopJSUlOiFJiSof//+UXMyMjJOO0f7WL9+/U5bW3FxsZ588skYXCUAAOjqYv5KzsqVK7Vs2TK99tprev/997V06VI999xzWrp0aayf6pwVFhYqHA47t/r6+s5eEgAA6CAxfyXnkUce0aOPPqrJkydLkkaNGqXf/e53Ki4u1pQpU+T3+yVJDQ0NSk1NdR7X0NCgK6+8UpLk9/vV2NgYdd4TJ07o4MGDzuP9fr8aGhqi5rTfb5/zeW63W263+6tfJAAA6PJi/krOn//8Z8XHR5+2R48eamtrkyRlZGTI7/ertLTUGY9EIqqoqFAgEJAkBQIBNTU1qbKy0pmzadMmtbW1KTMz05mzZcsWHT9+3JlTUlKioUOHnvFbVQAA4MIS88i57bbb9JOf/ETr16/Xb3/7W61evVpz5szRt7/9bUlSXFycCgoK9PTTT2vt2rWqrq7W3XffrbS0NE2cOFGSdPnll+vmm2/W1KlTtX37dr377rvKz8/X5MmTlZaWJkm666675HK5lJubq5qaGq1YsULz5s3T9OnTY31JAACgG4r5t6sWLFigxx9/XD/84Q/V2NiotLQ0/eAHP1BRUZEzZ8aMGTpy5IimTZumpqYmXX/99dq4caMSExOdOcuWLVN+fr7Gjx+v+Ph4TZo0SfPnz3fGvV6v3nrrLeXl5Wns2LEaOHCgioqK+Pg4AACQJMWZU38U8QUmEonI6/UqHA7L4/HE7Ly7D4T1rQXvaN0D12vkxd6Ynbcjdcc1AwAuTGf79ZvfXQUAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACsROQAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACs1CGRc+DAAX33u9/VgAEDlJSUpFGjRum9995zxo0xKioqUmpqqpKSkhQMBrVv376ocxw8eFA5OTnyeDxKTk5Wbm6umpubo+bs2rVLN9xwgxITE5Wenq5Zs2Z1xOUAAIBuKOaRc+jQIV133XXq2bOn3njjDe3Zs0fPP/+8+vXr58yZNWuW5s+fr8WLF6uiokK9e/dWVlaWjh075szJyclRTU2NSkpKtG7dOm3ZskXTpk1zxiORiCZMmKDBgwersrJSs2fP1syZM/XSSy/F+pIAAEA3lBDrEz777LNKT0/Xq6++6hzLyMhw/myM0dy5c/XYY4/p9ttvlyT94he/kM/n05o1azR58mTt3btXGzdu1I4dOzRu3DhJ0oIFC3TrrbfqueeeU1pampYtW6bW1la98sorcrlcGjFihKqqqjRnzpyoGAIAABemmL+Ss3btWo0bN07//M//rJSUFF111VX6+c9/7ozv379foVBIwWDQOeb1epWZmany8nJJUnl5uZKTk53AkaRgMKj4+HhVVFQ4c2688Ua5XC5nTlZWlmpra3Xo0KEzrq2lpUWRSCTqBgAA7BTzyPnNb36jRYsW6bLLLtObb76p+++/X//6r/+qpUuXSpJCoZAkyefzRT3O5/M5Y6FQSCkpKVHjCQkJ6t+/f9ScM53j1Of4vOLiYnm9XueWnp7+Fa8WAAB0VTGPnLa2No0ZM0Y//elPddVVV2natGmaOnWqFi9eHOunOmeFhYUKh8POrb6+vrOXBAAAOkjMIyc1NVXDhw+POnb55Zerrq5OkuT3+yVJDQ0NUXMaGhqcMb/fr8bGxqjxEydO6ODBg1FzznSOU5/j89xutzweT9QNAADYKeaRc91116m2tjbq2EcffaTBgwdL+uxNyH6/X6Wlpc54JBJRRUWFAoGAJCkQCKipqUmVlZXOnE2bNqmtrU2ZmZnOnC1btuj48ePOnJKSEg0dOjTqk1wAAODCFPPIeeihh7Rt2zb99Kc/1ccff6zXXntNL730kvLy8iRJcXFxKigo0NNPP621a9equrpad999t9LS0jRx4kRJn73yc/PNN2vq1Knavn273n33XeXn52vy5MlKS0uTJN11111yuVzKzc1VTU2NVqxYoXnz5mn69OmxviQAANANxfwj5FdffbVWr16twsJCPfXUU8rIyNDcuXOVk5PjzJkxY4aOHDmiadOmqampSddff702btyoxMREZ86yZcuUn5+v8ePHKz4+XpMmTdL8+fOdca/Xq7feekt5eXkaO3asBg4cqKKiIj4+DgAAJElxxhjT2YvoLJFIRF6vV+FwOKbvz9l9IKxvLXhH6x64XiMv9sbsvB2pO64ZAHBhOtuv3/zuKgAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGClDo+cZ555RnFxcSooKHCOHTt2THl5eRowYID69OmjSZMmqaGhIepxdXV1ys7OVq9evZSSkqJHHnlEJ06ciJqzefNmjRkzRm63W0OGDNGSJUs6+nIAAEA30aGRs2PHDv3Hf/yHrrjiiqjjDz30kH79619r1apVKisr0yeffKLvfOc7zvjJkyeVnZ2t1tZWbd26VUuXLtWSJUtUVFTkzNm/f7+ys7P1zW9+U1VVVSooKND3v/99vfnmmx15SQAAoJvosMhpbm5WTk6Ofv7zn6tfv37O8XA4rP/8z//UnDlz9A//8A8aO3asXn31VW3dulXbtm2TJL311lvas2eP/uu//ktXXnmlbrnlFv34xz/WwoUL1draKklavHixMjIy9Pzzz+vyyy9Xfn6+/umf/kkvvPBCR10SAADoRjoscvLy8pSdna1gMBh1vLKyUsePH486PmzYMA0aNEjl5eWSpPLyco0aNUo+n8+Zk5WVpUgkopqaGmfO58+dlZXlnAMAAFzYEjripMuXL9f777+vHTt2nDYWCoXkcrmUnJwcddzn8ykUCjlzTg2c9vH2sS+bE4lEdPToUSUlJZ323C0tLWppaXHuRyKRc784AADQLcT8lZz6+no9+OCDWrZsmRITE2N9+q+kuLhYXq/XuaWnp3f2kgAAQAeJeeRUVlaqsbFRY8aMUUJCghISElRWVqb58+crISFBPp9Pra2tampqinpcQ0OD/H6/JMnv95/2aav2+39tjsfjOeOrOJJUWFiocDjs3Orr62NxyQAAoAuKeeSMHz9e1dXVqqqqcm7jxo1TTk6O8+eePXuqtLTUeUxtba3q6uoUCAQkSYFAQNXV1WpsbHTmlJSUyOPxaPjw4c6cU8/RPqf9HGfidrvl8XiibgAAwE4xf09O3759NXLkyKhjvXv31oABA5zjubm5mj59uvr37y+Px6MHHnhAgUBA1157rSRpwoQJGj58uL73ve9p1qxZCoVCeuyxx5SXlye32y1Juu+++/Tiiy9qxowZuvfee7Vp0yatXLlS69evj/UlAQCAbqhD3nj817zwwguKj4/XpEmT1NLSoqysLP3sZz9zxnv06KF169bp/vvvVyAQUO/evTVlyhQ99dRTzpyMjAytX79eDz30kObNm6dLLrlEL7/8srKysjrjkgAAQBdzXiJn8+bNUfcTExO1cOFCLVy48AsfM3jwYG3YsOFLz3vTTTdp586dsVgiAACwDL+7CgAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFaKeeQUFxfr6quvVt++fZWSkqKJEyeqtrY2as6xY8eUl5enAQMGqE+fPpo0aZIaGhqi5tTV1Sk7O1u9evVSSkqKHnnkEZ04cSJqzubNmzVmzBi53W4NGTJES5YsifXlAACAbirmkVNWVqa8vDxt27ZNJSUlOn78uCZMmKAjR444cx566CH9+te/1qpVq1RWVqZPPvlE3/nOd5zxkydPKjs7W62trdq6dauWLl2qJUuWqKioyJmzf/9+ZWdn65vf/KaqqqpUUFCg73//+3rzzTdjfUkAAKA7Mh2ssbHRSDJlZWXGGGOamppMz549zapVq5w5e/fuNZJMeXm5McaYDRs2mPj4eBMKhZw5ixYtMh6Px7S0tBhjjJkxY4YZMWJE1HPdcccdJisr66zXFg6HjSQTDof/5us7k+rfN5nBP1pnqn/fFNPzdqTuuGYAwIXpbL9+d/h7csLhsCSpf//+kqTKykodP35cwWDQmTNs2DANGjRI5eXlkqTy8nKNGjVKPp/PmZOVlaVIJKKamhpnzqnnaJ/Tfo4zaWlpUSQSiboBAAA7dWjktLW1qaCgQNddd51GjhwpSQqFQnK5XEpOTo6a6/P5FAqFnDmnBk77ePvYl82JRCI6evToGddTXFwsr9fr3NLT07/yNQIAgK6pQyMnLy9Pu3fv1vLlyzvyac5aYWGhwuGwc6uvr+/sJQEAgA6S0FEnzs/P17p167RlyxZdcsklznG/36/W1lY1NTVFvZrT0NAgv9/vzNm+fXvU+do/fXXqnM9/IquhoUEej0dJSUlnXJPb7Zbb7f7K1wYAALq+mL+SY4xRfn6+Vq9erU2bNikjIyNqfOzYserZs6dKS0udY7W1taqrq1MgEJAkBQIBVVdXq7Gx0ZlTUlIij8ej4cOHO3NOPUf7nPZzAACAC1vMX8nJy8vTa6+9pl/96lfq27ev8x4ar9erpKQkeb1e5ebmavr06erfv788Ho8eeOABBQIBXXvttZKkCRMmaPjw4fre976nWbNmKRQK6bHHHlNeXp7zSsx9992nF198UTNmzNC9996rTZs2aeXKlVq/fn2sLwkAAHRDMX8lZ9GiRQqHw7rpppuUmprq3FasWOHMeeGFF/Stb31LkyZN0o033ii/369f/vKXzniPHj20bt069ejRQ4FAQN/97nd1991366mnnnLmZGRkaP369SopKdHo0aP1/PPP6+WXX1ZWVlasLwkAAHRDMX8lxxjzV+ckJiZq4cKFWrhw4RfOGTx4sDZs2PCl57npppu0c+fOc14jAACwH7+7CgAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgJSIHAABYicgBAABWInIAAICViBwAAGAlIgcAAFiJyAEAAFYicgAAgJWIHAAAYCUiBwAAWInIAQAAViJyAACAlYgcAABgpYTOXgC6lo8bmzt7CeekX2+XLk5O6uxlAAC6ICIHkj6LhaSePVSwoqqzl3JOknr20P88/A1CBwBwGiIHkqSLk5P0Pw9/Q4eOtHb2Us7ax43NKlhRpUNHWokcAMBpiBw4Lk5OIhYAANbgjccAAMBK3T5yFi5cqEsvvVSJiYnKzMzU9u3bO3tJAACgC+jWkbNixQpNnz5dTzzxhN5//32NHj1aWVlZamxs7OylAQCATtatI2fOnDmaOnWq7rnnHg0fPlyLFy9Wr1699Morr3T20gAAQCfrtm88bm1tVWVlpQoLC51j8fHxCgaDKi8vP+NjWlpa1NLS4twPh8OSpEgkEtO1NR+OqK3lz2o+HFEkEhfTc+Mv2vd5128+VfPh2P4dAgC+uov6uHWRJzHm523/um2M+dJ53TZy/vjHP+rkyZPy+XxRx30+nz788MMzPqa4uFhPPvnkacfT09M7ZI2BuR1yWnxOztzOXgEAoDMcPnxYXq/3C8e7beT8LQoLCzV9+nTnfltbmw4ePKgBAwYoLi52r7hEIhGlp6ervr5eHo8nZudFNPb5/GGvzw/2+fxgn8+PjtxnY4wOHz6stLS0L53XbSNn4MCB6tGjhxoaGqKONzQ0yO/3n/Exbrdbbrc76lhycnJHLVEej4f/QOcB+3z+sNfnB/t8frDP50dH7fOXvYLTrtu+8djlcmns2LEqLS11jrW1tam0tFSBQKATVwYAALqCbvtKjiRNnz5dU6ZM0bhx43TNNddo7ty5OnLkiO65557OXhoAAOhk3Tpy7rjjDv3hD39QUVGRQqGQrrzySm3cuPG0NyOfb263W0888cRp3xpDbLHP5w97fX6wz+cH+3x+dIV9jjN/7fNXAAAA3VC3fU8OAADAlyFyAACAlYgcAABgJSIHAABYicjpAAsXLtSll16qxMREZWZmavv27Z29pG5ly5Ytuu2225SWlqa4uDitWbMmatwYo6KiIqWmpiopKUnBYFD79u2LmnPw4EHl5OTI4/EoOTlZubm5am5uPo9X0bUVFxfr6quvVt++fZWSkqKJEyeqtrY2as6xY8eUl5enAQMGqE+fPpo0adJpP3yzrq5O2dnZ6tWrl1JSUvTII4/oxIkT5/NSurxFixbpiiuucH4gWiAQ0BtvvOGMs8+x98wzzyguLk4FBQXOMfY5NmbOnKm4uLio27Bhw5zxLrfPBjG1fPly43K5zCuvvGJqamrM1KlTTXJysmloaOjspXUbGzZsMP/+7/9ufvnLXxpJZvXq1VHjzzzzjPF6vWbNmjXmgw8+MP/4j/9oMjIyzNGjR505N998sxk9erTZtm2b+d///V8zZMgQc+edd57nK+m6srKyzKuvvmp2795tqqqqzK233moGDRpkmpubnTn33XefSU9PN6Wlpea9994z1157rfn617/ujJ84ccKMHDnSBINBs3PnTrNhwwYzcOBAU1hY2BmX1GWtXbvWrF+/3nz00UemtrbW/Nu//Zvp2bOn2b17tzGGfY617du3m0svvdRcccUV5sEHH3SOs8+x8cQTT5gRI0aYTz/91Ln94Q9/cMa72j4TOTF2zTXXmLy8POf+yZMnTVpamikuLu7EVXVfn4+ctrY24/f7zezZs51jTU1Nxu12m9dff90YY8yePXuMJLNjxw5nzhtvvGHi4uLMgQMHztvau5PGxkYjyZSVlRljPtvTnj17mlWrVjlz9u7daySZ8vJyY8xnMRofH29CoZAzZ9GiRcbj8ZiWlpbzewHdTL9+/czLL7/MPsfY4cOHzWWXXWZKSkrMN77xDSdy2OfYeeKJJ8zo0aPPONYV95lvV8VQa2urKisrFQwGnWPx8fEKBoMqLy/vxJXZY//+/QqFQlF77PV6lZmZ6exxeXm5kpOTNW7cOGdOMBhUfHy8Kioqzvuau4NwOCxJ6t+/vySpsrJSx48fj9rnYcOGadCgQVH7PGrUqKgfvpmVlaVIJKKamprzuPru4+TJk1q+fLmOHDmiQCDAPsdYXl6esrOzo/ZT4t9zrO3bt09paWn6u7/7O+Xk5Kiurk5S19znbv0Tj7uaP/7xjzp58uRpP3HZ5/Ppww8/7KRV2SUUCknSGfe4fSwUCiklJSVqPCEhQf3793fm4C/a2tpUUFCg6667TiNHjpT02R66XK7TfoHt5/f5TH8P7WP4i+rqagUCAR07dkx9+vTR6tWrNXz4cFVVVbHPMbJ8+XK9//772rFjx2lj/HuOnczMTC1ZskRDhw7Vp59+qieffFI33HCDdu/e3SX3mcgBLnB5eXnavXu33nnnnc5eirWGDh2qqqoqhcNh/fd//7emTJmisrKyzl6WNerr6/Xggw+qpKREiYmJnb0cq91yyy3On6+44gplZmZq8ODBWrlypZKSkjpxZWfGt6tiaODAgerRo8dp7yRvaGiQ3+/vpFXZpX0fv2yP/X6/Ghsbo8ZPnDihgwcP8vfwOfn5+Vq3bp3efvttXXLJJc5xv9+v1tZWNTU1Rc3//D6f6e+hfQx/4XK5NGTIEI0dO1bFxcUaPXq05s2bxz7HSGVlpRobGzVmzBglJCQoISFBZWVlmj9/vhISEuTz+djnDpKcnKyvfe1r+vjjj7vkv2ciJ4ZcLpfGjh2r0tJS51hbW5tKS0sVCAQ6cWX2yMjIkN/vj9rjSCSiiooKZ48DgYCamppUWVnpzNm0aZPa2tqUmZl53tfcFRljlJ+fr9WrV2vTpk3KyMiIGh87dqx69uwZtc+1tbWqq6uL2ufq6uqooCwpKZHH49Hw4cPPz4V0U21tbWppaWGfY2T8+PGqrq5WVVWVcxs3bpxycnKcP7PPHaO5uVn/93//p9TU1K757znmb2W+wC1fvty43W6zZMkSs2fPHjNt2jSTnJwc9U5yfLnDhw+bnTt3mp07dxpJZs6cOWbnzp3md7/7nTHms4+QJycnm1/96ldm165d5vbbbz/jR8ivuuoqU1FRYd555x1z2WWX8RHyU9x///3G6/WazZs3R30U9M9//rMz57777jODBg0ymzZtMu+9954JBAImEAg44+0fBZ0wYYKpqqoyGzduNBdddBEfuf2cRx991JSVlZn9+/ebXbt2mUcffdTExcWZt956yxjDPneUUz9dZQz7HCsPP/yw2bx5s9m/f7959913TTAYNAMHDjSNjY3GmK63z0ROB1iwYIEZNGiQcblc5pprrjHbtm3r7CV1K2+//baRdNptypQpxpjPPkb++OOPG5/PZ9xutxk/frypra2NOsef/vQnc+edd5o+ffoYj8dj7rnnHnP48OFOuJqu6Uz7K8m8+uqrzpyjR4+aH/7wh6Zfv36mV69e5tvf/rb59NNPo87z29/+1txyyy0mKSnJDBw40Dz88MPm+PHj5/lqurZ7773XDB482LhcLnPRRReZ8ePHO4FjDPvcUT4fOexzbNxxxx0mNTXVuFwuc/HFF5s77rjDfPzxx854V9vnOGOMif3rQwAAAJ2L9+QAAAArETkAAMBKRA4AALASkQMAAKxE5AAAACsROQAAwEpEDgAAsBKRAwAArETkAAAAKxE5AADASkQOAACwEpEDAACs9P/qqETOW2Ut/QAAAABJRU5ErkJggg==\n"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"sizes = [64, 96]\nlengths = [48, 64, 80, 96]\n\nused_sizes = []\nused_lengths = []\n\nfor size in sizes:\n    BATCH_SIZE = size\n    for length in lengths:\n        print(\"=========================================================================================\")\n        print(f\"Batch size: {BATCH_SIZE}, sequence length: {length}\")\n        used_sizes.append(BATCH_SIZE)\n        used_lengths.append(length)\n        \n        args = (length, 'indobenchmark/indobert-base-p1', (accuracies, f1_micros, f1_macros), 42, 6)\n        notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:55:07.791108Z","iopub.execute_input":"2025-04-12T14:55:07.791482Z","iopub.status.idle":"2025-04-12T15:25:17.410418Z","shell.execute_reply.started":"2025-04-12T14:55:07.791447Z","shell.execute_reply":"2025-04-12T15:25:17.409570Z"}},"outputs":[{"name":"stdout","text":"=========================================================================================\nBatch size: 64, sequence length: 48\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3801, Accuracy: 0.8846, F1 Micro: 0.5903, F1 Macro: 0.2898\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2567, Accuracy: 0.9055, F1 Micro: 0.7191, F1 Macro: 0.5368\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2055, Accuracy: 0.913, F1 Micro: 0.7267, F1 Macro: 0.5755\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1728, Accuracy: 0.92, F1 Micro: 0.7585, F1 Macro: 0.6109\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1418, Accuracy: 0.9186, F1 Micro: 0.7664, F1 Macro: 0.6192\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1205, Accuracy: 0.9235, F1 Micro: 0.7737, F1 Macro: 0.6715\nEpoch 7/10, Train Loss: 0.1, Accuracy: 0.9209, F1 Micro: 0.7679, F1 Macro: 0.6674\nEpoch 8/10, Train Loss: 0.0866, Accuracy: 0.9234, F1 Micro: 0.7686, F1 Macro: 0.6729\nEpoch 9/10, Train Loss: 0.074, Accuracy: 0.9198, F1 Micro: 0.7688, F1 Macro: 0.6664\nEpoch 10/10, Train Loss: 0.062, Accuracy: 0.923, F1 Micro: 0.7662, F1 Macro: 0.7004\n\nAccuracy: 0.9235, F1 Micro: 0.7737, F1 Macro: 0.6715\n               precision    recall  f1-score   support\n\n           HS       0.84      0.85      0.85      1134\n      Abusive       0.89      0.90      0.90       992\nHS_Individual       0.72      0.74      0.73       732\n     HS_Group       0.76      0.61      0.68       402\n  HS_Religion       0.76      0.62      0.68       157\n      HS_Race       0.81      0.65      0.72       120\n  HS_Physical       0.62      0.11      0.19        72\n    HS_Gender       0.58      0.29      0.39        51\n     HS_Other       0.78      0.80      0.79       762\n      HS_Weak       0.70      0.72      0.71       689\n  HS_Moderate       0.67      0.53      0.59       331\n    HS_Strong       0.89      0.79      0.84       114\n\n    micro avg       0.79      0.76      0.77      5556\n    macro avg       0.75      0.63      0.67      5556\n weighted avg       0.78      0.76      0.77      5556\n  samples avg       0.43      0.42      0.41      5556\n\nDuration: 175.26555490493774\n=========================================================================================\nBatch size: 64, sequence length: 64\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3798, Accuracy: 0.8858, F1 Micro: 0.6017, F1 Macro: 0.2979\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2553, Accuracy: 0.9053, F1 Micro: 0.7212, F1 Macro: 0.5394\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2043, Accuracy: 0.913, F1 Micro: 0.7263, F1 Macro: 0.5748\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1699, Accuracy: 0.9207, F1 Micro: 0.7608, F1 Macro: 0.6145\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1434, Accuracy: 0.9207, F1 Micro: 0.7708, F1 Macro: 0.6234\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1205, Accuracy: 0.9221, F1 Micro: 0.7737, F1 Macro: 0.6812\nEpoch 7/10, Train Loss: 0.0999, Accuracy: 0.9205, F1 Micro: 0.7709, F1 Macro: 0.6793\nEpoch 8/10, Train Loss: 0.0868, Accuracy: 0.9237, F1 Micro: 0.7666, F1 Macro: 0.6718\nEpoch 9/10, Train Loss: 0.0745, Accuracy: 0.9243, F1 Micro: 0.7737, F1 Macro: 0.6861\nEpoch 10/10, Train Loss: 0.0624, Accuracy: 0.9213, F1 Micro: 0.7682, F1 Macro: 0.6989\n\nAccuracy: 0.9221, F1 Micro: 0.7737, F1 Macro: 0.6812\n               precision    recall  f1-score   support\n\n           HS       0.83      0.87      0.85      1134\n      Abusive       0.87      0.92      0.89       992\nHS_Individual       0.72      0.74      0.73       732\n     HS_Group       0.69      0.64      0.66       402\n  HS_Religion       0.71      0.67      0.69       157\n      HS_Race       0.77      0.72      0.74       120\n  HS_Physical       0.69      0.15      0.25        72\n    HS_Gender       0.53      0.33      0.41        51\n     HS_Other       0.78      0.79      0.79       762\n      HS_Weak       0.70      0.72      0.71       689\n  HS_Moderate       0.62      0.56      0.59       331\n    HS_Strong       0.88      0.83      0.86       114\n\n    micro avg       0.77      0.77      0.77      5556\n    macro avg       0.73      0.66      0.68      5556\n weighted avg       0.77      0.77      0.77      5556\n  samples avg       0.44      0.43      0.42      5556\n\nDuration: 207.71697163581848\n=========================================================================================\nBatch size: 64, sequence length: 80\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3802, Accuracy: 0.8852, F1 Micro: 0.5962, F1 Macro: 0.2915\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2559, Accuracy: 0.9066, F1 Micro: 0.7203, F1 Macro: 0.532\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2051, Accuracy: 0.9125, F1 Micro: 0.7236, F1 Macro: 0.57\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1701, Accuracy: 0.9195, F1 Micro: 0.7573, F1 Macro: 0.6077\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1422, Accuracy: 0.9212, F1 Micro: 0.7735, F1 Macro: 0.6287\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1204, Accuracy: 0.923, F1 Micro: 0.7746, F1 Macro: 0.6704\nEpoch 7/10, Train Loss: 0.0992, Accuracy: 0.9213, F1 Micro: 0.7701, F1 Macro: 0.6681\nEpoch 8/10, Train Loss: 0.0865, Accuracy: 0.9244, F1 Micro: 0.7716, F1 Macro: 0.6641\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0766, Accuracy: 0.925, F1 Micro: 0.7751, F1 Macro: 0.6835\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.0631, Accuracy: 0.9266, F1 Micro: 0.7811, F1 Macro: 0.7044\n\nAccuracy: 0.9266, F1 Micro: 0.7811, F1 Macro: 0.7044\n               precision    recall  f1-score   support\n\n           HS       0.87      0.84      0.85      1134\n      Abusive       0.88      0.92      0.90       992\nHS_Individual       0.76      0.71      0.73       732\n     HS_Group       0.73      0.67      0.70       402\n  HS_Religion       0.77      0.61      0.68       157\n      HS_Race       0.81      0.62      0.70       120\n  HS_Physical       0.78      0.29      0.42        72\n    HS_Gender       0.57      0.39      0.47        51\n     HS_Other       0.79      0.78      0.78       762\n      HS_Weak       0.75      0.69      0.71       689\n  HS_Moderate       0.65      0.60      0.62       331\n    HS_Strong       0.87      0.88      0.87       114\n\n    micro avg       0.80      0.76      0.78      5556\n    macro avg       0.77      0.67      0.70      5556\n weighted avg       0.80      0.76      0.78      5556\n  samples avg       0.45      0.43      0.42      5556\n\nDuration: 251.08484029769897\n=========================================================================================\nBatch size: 64, sequence length: 96\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.379, Accuracy: 0.8833, F1 Micro: 0.5701, F1 Macro: 0.2801\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2552, Accuracy: 0.9058, F1 Micro: 0.7194, F1 Macro: 0.5429\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2041, Accuracy: 0.9135, F1 Micro: 0.72, F1 Macro: 0.5681\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1692, Accuracy: 0.9211, F1 Micro: 0.7585, F1 Macro: 0.6094\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1408, Accuracy: 0.9198, F1 Micro: 0.7687, F1 Macro: 0.6243\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1186, Accuracy: 0.9213, F1 Micro: 0.7726, F1 Macro: 0.671\nEpoch 7/10, Train Loss: 0.1014, Accuracy: 0.9191, F1 Micro: 0.7666, F1 Macro: 0.656\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0891, Accuracy: 0.9236, F1 Micro: 0.7795, F1 Macro: 0.6787\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.074, Accuracy: 0.9233, F1 Micro: 0.7803, F1 Macro: 0.6882\nEpoch 10/10, Train Loss: 0.0627, Accuracy: 0.9219, F1 Micro: 0.7725, F1 Macro: 0.7012\n\nAccuracy: 0.9233, F1 Micro: 0.7803, F1 Macro: 0.6882\n               precision    recall  f1-score   support\n\n           HS       0.83      0.88      0.85      1134\n      Abusive       0.87      0.93      0.90       992\nHS_Individual       0.72      0.75      0.73       732\n     HS_Group       0.70      0.71      0.70       402\n  HS_Religion       0.75      0.61      0.67       157\n      HS_Race       0.80      0.63      0.71       120\n  HS_Physical       1.00      0.15      0.27        72\n    HS_Gender       0.73      0.31      0.44        51\n     HS_Other       0.73      0.84      0.79       762\n      HS_Weak       0.71      0.72      0.72       689\n  HS_Moderate       0.63      0.64      0.63       331\n    HS_Strong       0.87      0.84      0.86       114\n\n    micro avg       0.77      0.79      0.78      5556\n    macro avg       0.78      0.67      0.69      5556\n weighted avg       0.77      0.79      0.78      5556\n  samples avg       0.45      0.44      0.43      5556\n\nDuration: 284.9327142238617\n=========================================================================================\nBatch size: 96, sequence length: 48\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.4168, Accuracy: 0.8775, F1 Micro: 0.5399, F1 Macro: 0.2467\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2817, Accuracy: 0.8974, F1 Micro: 0.6965, F1 Macro: 0.4859\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2333, Accuracy: 0.9071, F1 Micro: 0.7254, F1 Macro: 0.5553\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1963, Accuracy: 0.9153, F1 Micro: 0.7508, F1 Macro: 0.5974\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1668, Accuracy: 0.9208, F1 Micro: 0.758, F1 Macro: 0.5951\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1501, Accuracy: 0.9225, F1 Micro: 0.7656, F1 Macro: 0.6262\nEpoch 7/10, Train Loss: 0.1254, Accuracy: 0.921, F1 Micro: 0.7654, F1 Macro: 0.6355\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.1081, Accuracy: 0.9177, F1 Micro: 0.7708, F1 Macro: 0.6471\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0924, Accuracy: 0.9247, F1 Micro: 0.7747, F1 Macro: 0.6624\nEpoch 10/10, Train Loss: 0.0809, Accuracy: 0.9225, F1 Micro: 0.7699, F1 Macro: 0.6815\n\nAccuracy: 0.9247, F1 Micro: 0.7747, F1 Macro: 0.6624\n               precision    recall  f1-score   support\n\n           HS       0.86      0.85      0.86      1134\n      Abusive       0.89      0.89      0.89       992\nHS_Individual       0.74      0.74      0.74       732\n     HS_Group       0.74      0.61      0.67       402\n  HS_Religion       0.77      0.57      0.65       157\n      HS_Race       0.82      0.66      0.73       120\n  HS_Physical       0.71      0.07      0.13        72\n    HS_Gender       0.54      0.25      0.35        51\n     HS_Other       0.78      0.78      0.78       762\n      HS_Weak       0.72      0.72      0.72       689\n  HS_Moderate       0.66      0.52      0.58       331\n    HS_Strong       0.88      0.82      0.85       114\n\n    micro avg       0.80      0.75      0.77      5556\n    macro avg       0.76      0.62      0.66      5556\n weighted avg       0.79      0.75      0.77      5556\n  samples avg       0.44      0.42      0.41      5556\n\nDuration: 163.41254472732544\n=========================================================================================\nBatch size: 96, sequence length: 64\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.4154, Accuracy: 0.878, F1 Micro: 0.5426, F1 Macro: 0.2485\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2814, Accuracy: 0.8967, F1 Micro: 0.695, F1 Macro: 0.4835\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2338, Accuracy: 0.9056, F1 Micro: 0.7247, F1 Macro: 0.5574\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1955, Accuracy: 0.9168, F1 Micro: 0.738, F1 Macro: 0.5775\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1678, Accuracy: 0.9207, F1 Micro: 0.7587, F1 Macro: 0.5977\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1501, Accuracy: 0.9212, F1 Micro: 0.7671, F1 Macro: 0.6221\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.1261, Accuracy: 0.9208, F1 Micro: 0.7682, F1 Macro: 0.6312\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.1068, Accuracy: 0.9206, F1 Micro: 0.771, F1 Macro: 0.642\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0918, Accuracy: 0.9226, F1 Micro: 0.7724, F1 Macro: 0.672\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.0805, Accuracy: 0.9222, F1 Micro: 0.7747, F1 Macro: 0.6753\n\nAccuracy: 0.9222, F1 Micro: 0.7747, F1 Macro: 0.6753\n               precision    recall  f1-score   support\n\n           HS       0.84      0.87      0.85      1134\n      Abusive       0.87      0.92      0.89       992\nHS_Individual       0.72      0.74      0.73       732\n     HS_Group       0.68      0.66      0.67       402\n  HS_Religion       0.72      0.64      0.68       157\n      HS_Race       0.76      0.69      0.72       120\n  HS_Physical       0.82      0.12      0.22        72\n    HS_Gender       0.67      0.27      0.39        51\n     HS_Other       0.77      0.81      0.79       762\n      HS_Weak       0.70      0.72      0.71       689\n  HS_Moderate       0.59      0.59      0.59       331\n    HS_Strong       0.87      0.83      0.85       114\n\n    micro avg       0.77      0.78      0.77      5556\n    macro avg       0.75      0.66      0.68      5556\n weighted avg       0.77      0.78      0.77      5556\n  samples avg       0.45      0.44      0.43      5556\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.4152, Accuracy: 0.8772, F1 Micro: 0.5344, F1 Macro: 0.2448\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.281, Accuracy: 0.8969, F1 Micro: 0.6998, F1 Macro: 0.4947\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2328, Accuracy: 0.9088, F1 Micro: 0.725, F1 Macro: 0.5539\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1943, Accuracy: 0.9157, F1 Micro: 0.7508, F1 Macro: 0.5941\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1668, Accuracy: 0.9195, F1 Micro: 0.7597, F1 Macro: 0.597\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1492, Accuracy: 0.9226, F1 Micro: 0.7656, F1 Macro: 0.6246\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.1255, Accuracy: 0.9202, F1 Micro: 0.7707, F1 Macro: 0.6422\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.1065, Accuracy: 0.9199, F1 Micro: 0.7733, F1 Macro: 0.6355\nEpoch 9/10, Train Loss: 0.0939, Accuracy: 0.9246, F1 Micro: 0.7712, F1 Macro: 0.6707\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.0819, Accuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6804\n\nAccuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6804\n               precision    recall  f1-score   support\n\n           HS       0.87      0.84      0.85      1134\n      Abusive       0.88      0.90      0.89       992\nHS_Individual       0.75      0.70      0.72       732\n     HS_Group       0.71      0.67      0.69       402\n  HS_Religion       0.82      0.60      0.69       157\n      HS_Race       0.82      0.67      0.73       120\n  HS_Physical       0.78      0.10      0.17        72\n    HS_Gender       0.80      0.31      0.45        51\n     HS_Other       0.78      0.79      0.79       762\n      HS_Weak       0.73      0.68      0.70       689\n  HS_Moderate       0.62      0.58      0.60       331\n    HS_Strong       0.88      0.85      0.87       114\n\n    micro avg       0.80      0.75      0.77      5556\n    macro avg       0.79      0.64      0.68      5556\n weighted avg       0.80      0.75      0.77      5556\n  samples avg       0.44      0.42      0.42      5556\n\nDuration: 236.65838313102722\n=========================================================================================\nBatch size: 96, sequence length: 96\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.4164, Accuracy: 0.8782, F1 Micro: 0.547, F1 Macro: 0.2516\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2808, Accuracy: 0.8962, F1 Micro: 0.6998, F1 Macro: 0.4952\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2339, Accuracy: 0.9075, F1 Micro: 0.7282, F1 Macro: 0.5558\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1957, Accuracy: 0.9165, F1 Micro: 0.7462, F1 Macro: 0.5876\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1678, Accuracy: 0.9192, F1 Micro: 0.7543, F1 Macro: 0.5908\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1492, Accuracy: 0.9204, F1 Micro: 0.7653, F1 Macro: 0.6189\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.1283, Accuracy: 0.9227, F1 Micro: 0.7757, F1 Macro: 0.6324\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.1062, Accuracy: 0.9228, F1 Micro: 0.7765, F1 Macro: 0.6466\nEpoch 9/10, Train Loss: 0.0927, Accuracy: 0.924, F1 Micro: 0.7755, F1 Macro: 0.6711\nEpoch 10/10, Train Loss: 0.0837, Accuracy: 0.9227, F1 Micro: 0.7695, F1 Macro: 0.673\n\nAccuracy: 0.9228, F1 Micro: 0.7765, F1 Macro: 0.6466\n               precision    recall  f1-score   support\n\n           HS       0.84      0.88      0.86      1134\n      Abusive       0.88      0.90      0.89       992\nHS_Individual       0.72      0.77      0.74       732\n     HS_Group       0.72      0.65      0.68       402\n  HS_Religion       0.78      0.59      0.67       157\n      HS_Race       0.78      0.69      0.73       120\n  HS_Physical       0.67      0.03      0.05        72\n    HS_Gender       0.67      0.12      0.20        51\n     HS_Other       0.74      0.83      0.79       762\n      HS_Weak       0.70      0.75      0.72       689\n  HS_Moderate       0.62      0.57      0.60       331\n    HS_Strong       0.87      0.78      0.82       114\n\n    micro avg       0.77      0.78      0.78      5556\n    macro avg       0.75      0.63      0.65      5556\n weighted avg       0.77      0.78      0.77      5556\n  samples avg       0.44      0.43      0.42      5556\n\nDuration: 269.1327612400055\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Batch Size': used_sizes,\n    'Sequence Length': used_lengths,\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'hyperparameters_tuning.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:25:17.411544Z","iopub.execute_input":"2025-04-12T15:25:17.411897Z","iopub.status.idle":"2025-04-12T15:25:17.425084Z","shell.execute_reply.started":"2025-04-12T15:25:17.411862Z","shell.execute_reply":"2025-04-12T15:25:17.424294Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"# USE BEST CONFIG","metadata":{}},{"cell_type":"code","source":"seeds = [50, 81, 14, 3, 94]\n\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()\n\nfor seed in seeds:\n    print(\"=====================\")\n    print(\"SEED:\", seed)\n    set_seed(seed)\n    \n    LEARNING_RATE = 2e-5\n    BATCH_SIZE = 64\n    args = (80, 'indobenchmark/indobert-base-p1', (accuracies, f1_micros, f1_macros), seed, 6)\n    \n    notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:25:27.236882Z","iopub.execute_input":"2025-04-12T15:25:27.237182Z","iopub.status.idle":"2025-04-12T15:46:21.779080Z","shell.execute_reply.started":"2025-04-12T15:25:27.237161Z","shell.execute_reply":"2025-04-12T15:46:21.778200Z"}},"outputs":[{"name":"stdout","text":"=====================\nSEED: 50\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3752, Accuracy: 0.8872, F1 Micro: 0.608, F1 Macro: 0.3094\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2535, Accuracy: 0.907, F1 Micro: 0.7214, F1 Macro: 0.5371\nEpoch 3/10, Train Loss: 0.2037, Accuracy: 0.9119, F1 Micro: 0.7169, F1 Macro: 0.5595\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1685, Accuracy: 0.9193, F1 Micro: 0.7626, F1 Macro: 0.6295\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1395, Accuracy: 0.9213, F1 Micro: 0.7689, F1 Macro: 0.6311\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1182, Accuracy: 0.923, F1 Micro: 0.7742, F1 Macro: 0.6859\nEpoch 7/10, Train Loss: 0.0973, Accuracy: 0.9183, F1 Micro: 0.7647, F1 Macro: 0.6776\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0849, Accuracy: 0.9245, F1 Micro: 0.7749, F1 Macro: 0.6912\nEpoch 9/10, Train Loss: 0.0716, Accuracy: 0.9234, F1 Micro: 0.7665, F1 Macro: 0.6816\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.0611, Accuracy: 0.9224, F1 Micro: 0.7779, F1 Macro: 0.7069\n\nAccuracy: 0.9224, F1 Micro: 0.7779, F1 Macro: 0.7069\n               precision    recall  f1-score   support\n\n           HS       0.83      0.88      0.85      1134\n      Abusive       0.86      0.93      0.89       992\nHS_Individual       0.73      0.73      0.73       732\n     HS_Group       0.66      0.70      0.68       402\n  HS_Religion       0.73      0.61      0.66       157\n      HS_Race       0.78      0.69      0.73       120\n  HS_Physical       0.80      0.33      0.47        72\n    HS_Gender       0.62      0.39      0.48        51\n     HS_Other       0.77      0.83      0.80       762\n      HS_Weak       0.71      0.71      0.71       689\n  HS_Moderate       0.59      0.63      0.61       331\n    HS_Strong       0.86      0.87      0.86       114\n\n    micro avg       0.77      0.79      0.78      5556\n    macro avg       0.75      0.69      0.71      5556\n weighted avg       0.77      0.79      0.78      5556\n  samples avg       0.45      0.44      0.43      5556\n\nDuration: 246.68626403808594\n=====================\nSEED: 81\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3695, Accuracy: 0.8847, F1 Micro: 0.5729, F1 Macro: 0.2869\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2509, Accuracy: 0.9083, F1 Micro: 0.7295, F1 Macro: 0.5529\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.2006, Accuracy: 0.9147, F1 Micro: 0.7325, F1 Macro: 0.5758\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1683, Accuracy: 0.9197, F1 Micro: 0.7625, F1 Macro: 0.6163\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1405, Accuracy: 0.9209, F1 Micro: 0.7703, F1 Macro: 0.6301\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1187, Accuracy: 0.9233, F1 Micro: 0.7744, F1 Macro: 0.6612\nEpoch 7/10, Train Loss: 0.1007, Accuracy: 0.9213, F1 Micro: 0.7628, F1 Macro: 0.6606\nEpoch 8/10, Train Loss: 0.0866, Accuracy: 0.9232, F1 Micro: 0.7731, F1 Macro: 0.6588\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0727, Accuracy: 0.9265, F1 Micro: 0.7809, F1 Macro: 0.6889\nEpoch 10/10, Train Loss: 0.0604, Accuracy: 0.924, F1 Micro: 0.7779, F1 Macro: 0.7015\n\nAccuracy: 0.9265, F1 Micro: 0.7809, F1 Macro: 0.6889\n               precision    recall  f1-score   support\n\n           HS       0.86      0.84      0.85      1134\n      Abusive       0.88      0.91      0.90       992\nHS_Individual       0.75      0.74      0.74       732\n     HS_Group       0.75      0.64      0.69       402\n  HS_Religion       0.79      0.55      0.65       157\n      HS_Race       0.86      0.62      0.72       120\n  HS_Physical       0.92      0.17      0.28        72\n    HS_Gender       0.64      0.35      0.46        51\n     HS_Other       0.79      0.80      0.79       762\n      HS_Weak       0.73      0.72      0.72       689\n  HS_Moderate       0.66      0.55      0.60       331\n    HS_Strong       0.90      0.83      0.86       114\n\n    micro avg       0.80      0.76      0.78      5556\n    macro avg       0.79      0.64      0.69      5556\n weighted avg       0.80      0.76      0.78      5556\n  samples avg       0.44      0.43      0.42      5556\n\nDuration: 248.4030249118805\n=====================\nSEED: 14\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3643, Accuracy: 0.8883, F1 Micro: 0.6067, F1 Macro: 0.3254\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2481, Accuracy: 0.9092, F1 Micro: 0.732, F1 Macro: 0.5566\nEpoch 3/10, Train Loss: 0.2011, Accuracy: 0.9155, F1 Micro: 0.7311, F1 Macro: 0.5653\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1659, Accuracy: 0.9197, F1 Micro: 0.7623, F1 Macro: 0.6256\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1405, Accuracy: 0.9203, F1 Micro: 0.7708, F1 Macro: 0.6342\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1177, Accuracy: 0.9224, F1 Micro: 0.7751, F1 Macro: 0.687\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0972, Accuracy: 0.9211, F1 Micro: 0.7756, F1 Macro: 0.6889\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0838, Accuracy: 0.9258, F1 Micro: 0.7801, F1 Macro: 0.6878\nEpoch 9/10, Train Loss: 0.071, Accuracy: 0.9238, F1 Micro: 0.7764, F1 Macro: 0.6821\nEpoch 10/10, Train Loss: 0.0604, Accuracy: 0.926, F1 Micro: 0.7795, F1 Macro: 0.7103\n\nAccuracy: 0.9258, F1 Micro: 0.7801, F1 Macro: 0.6878\n               precision    recall  f1-score   support\n\n           HS       0.86      0.85      0.86      1134\n      Abusive       0.88      0.91      0.90       992\nHS_Individual       0.75      0.73      0.74       732\n     HS_Group       0.74      0.65      0.69       402\n  HS_Religion       0.76      0.57      0.65       157\n      HS_Race       0.79      0.71      0.75       120\n  HS_Physical       0.91      0.14      0.24        72\n    HS_Gender       0.68      0.33      0.45        51\n     HS_Other       0.78      0.80      0.79       762\n      HS_Weak       0.73      0.70      0.72       689\n  HS_Moderate       0.65      0.58      0.61       331\n    HS_Strong       0.90      0.83      0.87       114\n\n    micro avg       0.80      0.76      0.78      5556\n    macro avg       0.79      0.65      0.69      5556\n weighted avg       0.80      0.76      0.77      5556\n  samples avg       0.44      0.43      0.42      5556\n\nDuration: 249.1054289340973\n=====================\nSEED: 3\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3724, Accuracy: 0.8876, F1 Micro: 0.6063, F1 Macro: 0.3051\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2523, Accuracy: 0.9059, F1 Micro: 0.7265, F1 Macro: 0.5438\nEpoch 3/10, Train Loss: 0.2022, Accuracy: 0.9132, F1 Micro: 0.7176, F1 Macro: 0.5534\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1681, Accuracy: 0.9208, F1 Micro: 0.7634, F1 Macro: 0.6222\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.138, Accuracy: 0.923, F1 Micro: 0.7678, F1 Macro: 0.624\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1165, Accuracy: 0.9226, F1 Micro: 0.7757, F1 Macro: 0.6882\nEpoch 7/10, Train Loss: 0.0967, Accuracy: 0.9217, F1 Micro: 0.7691, F1 Macro: 0.6842\nEpoch 8/10, Train Loss: 0.0857, Accuracy: 0.9245, F1 Micro: 0.7749, F1 Macro: 0.6898\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0701, Accuracy: 0.9214, F1 Micro: 0.7771, F1 Macro: 0.7114\nEpoch 10/10, Train Loss: 0.0621, Accuracy: 0.9225, F1 Micro: 0.7751, F1 Macro: 0.7019\n\nAccuracy: 0.9214, F1 Micro: 0.7771, F1 Macro: 0.7114\n               precision    recall  f1-score   support\n\n           HS       0.82      0.89      0.85      1134\n      Abusive       0.85      0.94      0.89       992\nHS_Individual       0.71      0.75      0.73       732\n     HS_Group       0.68      0.69      0.68       402\n  HS_Religion       0.68      0.66      0.67       157\n      HS_Race       0.77      0.68      0.73       120\n  HS_Physical       0.92      0.31      0.46        72\n    HS_Gender       0.65      0.51      0.57        51\n     HS_Other       0.76      0.82      0.79       762\n      HS_Weak       0.69      0.72      0.71       689\n  HS_Moderate       0.61      0.62      0.62       331\n    HS_Strong       0.89      0.82      0.85       114\n\n    micro avg       0.76      0.80      0.78      5556\n    macro avg       0.75      0.70      0.71      5556\n weighted avg       0.76      0.80      0.77      5556\n  samples avg       0.45      0.45      0.43      5556\n\nDuration: 248.9212930202484\n=====================\nSEED: 94\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3709, Accuracy: 0.8885, F1 Micro: 0.6166, F1 Macro: 0.3134\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2505, Accuracy: 0.9068, F1 Micro: 0.7293, F1 Macro: 0.5472\nEpoch 3/10, Train Loss: 0.2024, Accuracy: 0.9127, F1 Micro: 0.7219, F1 Macro: 0.5723\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1685, Accuracy: 0.9198, F1 Micro: 0.7674, F1 Macro: 0.617\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1405, Accuracy: 0.922, F1 Micro: 0.7681, F1 Macro: 0.6206\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1175, Accuracy: 0.923, F1 Micro: 0.7775, F1 Macro: 0.6773\nEpoch 7/10, Train Loss: 0.0988, Accuracy: 0.9236, F1 Micro: 0.7732, F1 Macro: 0.6883\nEpoch 8/10, Train Loss: 0.0863, Accuracy: 0.9257, F1 Micro: 0.7773, F1 Macro: 0.6871\nEpoch 9/10, Train Loss: 0.0717, Accuracy: 0.9241, F1 Micro: 0.7769, F1 Macro: 0.6997\nEpoch 10/10, Train Loss: 0.0625, Accuracy: 0.9209, F1 Micro: 0.7677, F1 Macro: 0.6916\n\nAccuracy: 0.923, F1 Micro: 0.7775, F1 Macro: 0.6773\n               precision    recall  f1-score   support\n\n           HS       0.82      0.89      0.86      1134\n      Abusive       0.87      0.93      0.90       992\nHS_Individual       0.71      0.78      0.74       732\n     HS_Group       0.76      0.61      0.68       402\n  HS_Religion       0.71      0.62      0.66       157\n      HS_Race       0.74      0.65      0.69       120\n  HS_Physical       0.85      0.15      0.26        72\n    HS_Gender       0.59      0.33      0.42        51\n     HS_Other       0.77      0.81      0.79       762\n      HS_Weak       0.69      0.75      0.71       689\n  HS_Moderate       0.68      0.51      0.59       331\n    HS_Strong       0.89      0.78      0.83       114\n\n    micro avg       0.77      0.78      0.78      5556\n    macro avg       0.76      0.65      0.68      5556\n weighted avg       0.77      0.78      0.77      5556\n  samples avg       0.45      0.44      0.43      5556\n\nDuration: 246.60347366333008\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Trial': [1,2,3,4,5],\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'result.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:46:21.780672Z","iopub.execute_input":"2025-04-12T15:46:21.781008Z","iopub.status.idle":"2025-04-12T15:46:21.790334Z","shell.execute_reply.started":"2025-04-12T15:46:21.780976Z","shell.execute_reply":"2025-04-12T15:46:21.789523Z"}},"outputs":[],"execution_count":50}]}