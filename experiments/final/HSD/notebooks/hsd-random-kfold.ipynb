{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91dca1ff",
   "metadata": {
    "papermill": {
     "duration": 0.012085,
     "end_time": "2025-06-25T13:37:20.075126",
     "exception": false,
     "start_time": "2025-06-25T13:37:20.063041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a8ba62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:20.098233Z",
     "iopub.status.busy": "2025-06-25T13:37:20.097926Z",
     "iopub.status.idle": "2025-06-25T13:37:44.739570Z",
     "shell.execute_reply": "2025-06-25T13:37:44.738921Z"
    },
    "papermill": {
     "duration": 24.654783,
     "end_time": "2025-06-25T13:37:44.741176",
     "exception": false,
     "start_time": "2025-06-25T13:37:20.086393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90273f04",
   "metadata": {
    "papermill": {
     "duration": 0.01063,
     "end_time": "2025-06-25T13:37:44.763019",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.752389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e8dfef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:44.786205Z",
     "iopub.status.busy": "2025-06-25T13:37:44.785684Z",
     "iopub.status.idle": "2025-06-25T13:37:44.789399Z",
     "shell.execute_reply": "2025-06-25T13:37:44.788513Z"
    },
    "papermill": {
     "duration": 0.016664,
     "end_time": "2025-06-25T13:37:44.790786",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.774122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a86ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:44.813722Z",
     "iopub.status.busy": "2025-06-25T13:37:44.813495Z",
     "iopub.status.idle": "2025-06-25T13:37:44.817186Z",
     "shell.execute_reply": "2025-06-25T13:37:44.816429Z"
    },
    "papermill": {
     "duration": 0.016617,
     "end_time": "2025-06-25T13:37:44.818457",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.801840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77f38cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:44.841153Z",
     "iopub.status.busy": "2025-06-25T13:37:44.840929Z",
     "iopub.status.idle": "2025-06-25T13:37:44.851195Z",
     "shell.execute_reply": "2025-06-25T13:37:44.850334Z"
    },
    "papermill": {
     "duration": 0.023032,
     "end_time": "2025-06-25T13:37:44.852750",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.829718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36426b1",
   "metadata": {
    "papermill": {
     "duration": 0.010834,
     "end_time": "2025-06-25T13:37:44.874780",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.863946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c8c8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:44.897187Z",
     "iopub.status.busy": "2025-06-25T13:37:44.896980Z",
     "iopub.status.idle": "2025-06-25T13:37:44.950491Z",
     "shell.execute_reply": "2025-06-25T13:37:44.949093Z"
    },
    "papermill": {
     "duration": 0.066345,
     "end_time": "2025-06-25T13:37:44.952150",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.885805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-random-kfold'\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "sequence_length = 80\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3fbfd",
   "metadata": {
    "papermill": {
     "duration": 0.010483,
     "end_time": "2025-06-25T13:37:44.973658",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.963175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8f2b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:44.996142Z",
     "iopub.status.busy": "2025-06-25T13:37:44.995795Z",
     "iopub.status.idle": "2025-06-25T13:37:45.132612Z",
     "shell.execute_reply": "2025-06-25T13:37:45.131847Z"
    },
    "papermill": {
     "duration": 0.149527,
     "end_time": "2025-06-25T13:37:45.133787",
     "exception": false,
     "start_time": "2025-06-25T13:37:44.984260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fe426a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.156909Z",
     "iopub.status.busy": "2025-06-25T13:37:45.156668Z",
     "iopub.status.idle": "2025-06-25T13:37:45.169227Z",
     "shell.execute_reply": "2025-06-25T13:37:45.168560Z"
    },
    "papermill": {
     "duration": 0.025386,
     "end_time": "2025-06-25T13:37:45.170560",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.145174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfd54e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.194015Z",
     "iopub.status.busy": "2025-06-25T13:37:45.193787Z",
     "iopub.status.idle": "2025-06-25T13:37:45.199132Z",
     "shell.execute_reply": "2025-06-25T13:37:45.198324Z"
    },
    "papermill": {
     "duration": 0.018378,
     "end_time": "2025-06-25T13:37:45.200608",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.182230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc1e30a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.225176Z",
     "iopub.status.busy": "2025-06-25T13:37:45.224956Z",
     "iopub.status.idle": "2025-06-25T13:37:45.237795Z",
     "shell.execute_reply": "2025-06-25T13:37:45.236885Z"
    },
    "papermill": {
     "duration": 0.026324,
     "end_time": "2025-06-25T13:37:45.239252",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.212928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (7309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afded33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.264900Z",
     "iopub.status.busy": "2025-06-25T13:37:45.264644Z",
     "iopub.status.idle": "2025-06-25T13:37:45.272806Z",
     "shell.execute_reply": "2025-06-25T13:37:45.272160Z"
    },
    "papermill": {
     "duration": 0.022291,
     "end_time": "2025-06-25T13:37:45.274065",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.251774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff346d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.298415Z",
     "iopub.status.busy": "2025-06-25T13:37:45.298211Z",
     "iopub.status.idle": "2025-06-25T13:37:45.310388Z",
     "shell.execute_reply": "2025-06-25T13:37:45.309567Z"
    },
    "papermill": {
     "duration": 0.025967,
     "end_time": "2025-06-25T13:37:45.311564",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.285597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ad7cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.335356Z",
     "iopub.status.busy": "2025-06-25T13:37:45.335159Z",
     "iopub.status.idle": "2025-06-25T13:37:45.338169Z",
     "shell.execute_reply": "2025-06-25T13:37:45.337582Z"
    },
    "papermill": {
     "duration": 0.016556,
     "end_time": "2025-06-25T13:37:45.339501",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.322945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc2fc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.363256Z",
     "iopub.status.busy": "2025-06-25T13:37:45.363058Z",
     "iopub.status.idle": "2025-06-25T13:37:45.724610Z",
     "shell.execute_reply": "2025-06-25T13:37:45.723934Z"
    },
    "papermill": {
     "duration": 0.37515,
     "end_time": "2025-06-25T13:37:45.726313",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.351163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "label_columns = data.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35659f0",
   "metadata": {
    "papermill": {
     "duration": 0.011472,
     "end_time": "2025-06-25T13:37:45.750572",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.739100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c4d85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:45.774341Z",
     "iopub.status.busy": "2025-06-25T13:37:45.774102Z",
     "iopub.status.idle": "2025-06-25T13:37:46.473552Z",
     "shell.execute_reply": "2025-06-25T13:37:46.472899Z"
    },
    "papermill": {
     "duration": 0.712903,
     "end_time": "2025-06-25T13:37:46.474914",
     "exception": false,
     "start_time": "2025-06-25T13:37:45.762011",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbee567db00749a38e6b76f1222c23fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eddd42909545e9bcf57adb38f3b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad558bd214b481e90ce97312f964ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8801372936ee4d03ae816e4b1cafc856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ca5bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.500191Z",
     "iopub.status.busy": "2025-06-25T13:37:46.499957Z",
     "iopub.status.idle": "2025-06-25T13:37:46.504213Z",
     "shell.execute_reply": "2025-06-25T13:37:46.503564Z"
    },
    "papermill": {
     "duration": 0.01796,
     "end_time": "2025-06-25T13:37:46.505422",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.487462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c5fe0",
   "metadata": {
    "papermill": {
     "duration": 0.011716,
     "end_time": "2025-06-25T13:37:46.528989",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.517273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "028ce362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.554516Z",
     "iopub.status.busy": "2025-06-25T13:37:46.554266Z",
     "iopub.status.idle": "2025-06-25T13:37:46.558944Z",
     "shell.execute_reply": "2025-06-25T13:37:46.558325Z"
    },
    "papermill": {
     "duration": 0.018969,
     "end_time": "2025-06-25T13:37:46.560044",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.541075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5771993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.585075Z",
     "iopub.status.busy": "2025-06-25T13:37:46.584849Z",
     "iopub.status.idle": "2025-06-25T13:37:46.597683Z",
     "shell.execute_reply": "2025-06-25T13:37:46.597081Z"
    },
    "papermill": {
     "duration": 0.026518,
     "end_time": "2025-06-25T13:37:46.598816",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.572298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples...\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        # This print can be commented out to reduce log spam in a k-fold loop\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc22790",
   "metadata": {
    "papermill": {
     "duration": 0.011847,
     "end_time": "2025-06-25T13:37:46.622828",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.610981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d7693e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.648236Z",
     "iopub.status.busy": "2025-06-25T13:37:46.648011Z",
     "iopub.status.idle": "2025-06-25T13:37:46.653123Z",
     "shell.execute_reply": "2025-06-25T13:37:46.652525Z"
    },
    "papermill": {
     "duration": 0.018904,
     "end_time": "2025-06-25T13:37:46.654347",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.635443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7d5d1",
   "metadata": {
    "papermill": {
     "duration": 0.011863,
     "end_time": "2025-06-25T13:37:46.678135",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.666272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9722494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.702504Z",
     "iopub.status.busy": "2025-06-25T13:37:46.702241Z",
     "iopub.status.idle": "2025-06-25T13:37:46.710102Z",
     "shell.execute_reply": "2025-06-25T13:37:46.709504Z"
    },
    "papermill": {
     "duration": 0.021314,
     "end_time": "2025-06-25T13:37:46.711386",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.690072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_sampling(current_train_size, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, n_samples=min_increment):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    nearest_cp = 0\n",
    "    arrived_at_cp = False\n",
    "    for cp in checkpoints:\n",
    "        if cp > current_train_size:\n",
    "            nearest_cp = cp\n",
    "            break\n",
    "\n",
    "    num_of_candidates = math.ceil(0.1 * len(remaining_indices))\n",
    "\n",
    "    if num_of_candidates <= n_samples and n_samples < nearest_cp - current_train_size:\n",
    "        num_of_candidates = n_samples\n",
    "    elif num_of_candidates > n_samples and num_of_candidates < nearest_cp - current_train_size:\n",
    "        num_of_candidates = max(n_samples, num_of_candidates)\n",
    "    else:\n",
    "        num_of_candidates = nearest_cp - current_train_size\n",
    "        arrived_at_cp = True\n",
    "\n",
    "    if len(remaining_indices) < n_samples:\n",
    "        random_indices = range(len(X_pool))\n",
    "    else:\n",
    "        random_indices = random.sample(range(len(X_pool)), num_of_candidates)\n",
    "\n",
    "    if arrived_at_cp:\n",
    "        temp = train_indices.copy()\n",
    "        temp.extend([remaining_indices[i] for i in random_indices])\n",
    "            \n",
    "        # Save acquired data up to checkpoint\n",
    "        acquired_data = pd.DataFrame({\n",
    "            'processed_text': [X_train_fold[i] for i in temp],\n",
    "            'HS': [y_train_fold[i][0] for i in temp],\n",
    "            'Abusive': [y_train_fold[i][1] for i in temp],\n",
    "            'HS_Individual': [y_train_fold[i][2] for i in temp],\n",
    "            'HS_Group': [y_train_fold[i][3] for i in temp],\n",
    "            'HS_Religion': [y_train_fold[i][4] for i in temp],\n",
    "            'HS_Race': [y_train_fold[i][5] for i in temp],\n",
    "            'HS_Physical': [y_train_fold[i][6] for i in temp],\n",
    "            'HS_Gender': [y_train_fold[i][7] for i in temp],\n",
    "            'HS_Other': [y_train_fold[i][8] for i in temp],\n",
    "            'HS_Weak': [y_train_fold[i][9] for i in temp],\n",
    "            'HS_Moderate': [y_train_fold[i][10] for i in temp],\n",
    "            'HS_Strong': [y_train_fold[i][11] for i in temp],\n",
    "        })\n",
    "\n",
    "        acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "    end_time = time.time() \n",
    "    duration = end_time - start_time\n",
    "\n",
    "    sampling_dur.append(duration)\n",
    "    for i in random_indices:\n",
    "        new_samples.append(remaining_indices[i])\n",
    "        \n",
    "    print(\"Nearest checkpoint:\", nearest_cp)\n",
    "    print(\"Acquired samples:\", len(random_indices))\n",
    "    print(f\"Sampling duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e6b0d",
   "metadata": {
    "papermill": {
     "duration": 0.011895,
     "end_time": "2025-06-25T13:37:46.735201",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.723306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2caa9b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T13:37:46.759577Z",
     "iopub.status.busy": "2025-06-25T13:37:46.759346Z",
     "iopub.status.idle": "2025-06-25T21:41:45.492645Z",
     "shell.execute_reply": "2025-06-25T21:41:45.491603Z"
    },
    "papermill": {
     "duration": 29038.747446,
     "end_time": "2025-06-25T21:41:45.494411",
     "exception": false,
     "start_time": "2025-06-25T13:37:46.746965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 658 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e325ad2959c436e9238837d32a4948a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6381, Accuracy: 0.7985, F1 Micro: 0.3882, F1 Macro: 0.1172\n",
      "Epoch 2/10, Train Loss: 0.4768, Accuracy: 0.8343, F1 Micro: 0.1455, F1 Macro: 0.0462\n",
      "Epoch 3/10, Train Loss: 0.4068, Accuracy: 0.8333, F1 Micro: 0.1123, F1 Macro: 0.0388\n",
      "Epoch 4/10, Train Loss: 0.3813, Accuracy: 0.8332, F1 Micro: 0.1021, F1 Macro: 0.0367\n",
      "Epoch 5/10, Train Loss: 0.3774, Accuracy: 0.8373, F1 Micro: 0.1641, F1 Macro: 0.0541\n",
      "Epoch 6/10, Train Loss: 0.3665, Accuracy: 0.844, F1 Micro: 0.2355, F1 Macro: 0.0791\n",
      "Epoch 7/10, Train Loss: 0.355, Accuracy: 0.8526, F1 Micro: 0.3214, F1 Macro: 0.1069\n",
      "Epoch 8/10, Train Loss: 0.3289, Accuracy: 0.8608, F1 Micro: 0.3921, F1 Macro: 0.1407\n",
      "Epoch 9/10, Train Loss: 0.3142, Accuracy: 0.871, F1 Micro: 0.4906, F1 Macro: 0.2141\n",
      "Epoch 10/10, Train Loss: 0.2951, Accuracy: 0.8741, F1 Micro: 0.5196, F1 Macro: 0.2373\n",
      "Best result for 658 samples: F1 Micro: 0.5196\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.61      0.71      1141\n",
      "      Abusive       0.81      0.74      0.77      1012\n",
      "HS_Individual       0.67      0.36      0.47       737\n",
      "     HS_Group       0.00      0.00      0.00       404\n",
      "  HS_Religion       0.00      0.00      0.00       164\n",
      "      HS_Race       0.00      0.00      0.00       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.72      0.35      0.47       779\n",
      "      HS_Weak       0.67      0.31      0.43       686\n",
      "  HS_Moderate       0.00      0.00      0.00       356\n",
      "    HS_Strong       0.00      0.00      0.00        99\n",
      "\n",
      "    micro avg       0.77      0.39      0.52      5608\n",
      "    macro avg       0.31      0.20      0.24      5608\n",
      " weighted avg       0.59      0.39      0.46      5608\n",
      "  samples avg       0.36      0.25      0.27      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 0.0006797313690185547 seconds\n",
      "\n",
      "Fold 1 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1646 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.8305, F1 Micro: 0.0748, F1 Macro: 0.0283\n",
      "Epoch 2/10, Train Loss: 0.4025, Accuracy: 0.8305, F1 Micro: 0.0579, F1 Macro: 0.0232\n",
      "Epoch 3/10, Train Loss: 0.3704, Accuracy: 0.8371, F1 Micro: 0.1401, F1 Macro: 0.0496\n",
      "Epoch 4/10, Train Loss: 0.3403, Accuracy: 0.864, F1 Micro: 0.4243, F1 Macro: 0.1621\n",
      "Epoch 5/10, Train Loss: 0.3101, Accuracy: 0.8796, F1 Micro: 0.5683, F1 Macro: 0.2728\n",
      "Epoch 6/10, Train Loss: 0.2746, Accuracy: 0.8866, F1 Micro: 0.6074, F1 Macro: 0.3192\n",
      "Epoch 7/10, Train Loss: 0.251, Accuracy: 0.8902, F1 Micro: 0.6275, F1 Macro: 0.3793\n",
      "Epoch 8/10, Train Loss: 0.2197, Accuracy: 0.8936, F1 Micro: 0.6346, F1 Macro: 0.4135\n",
      "Epoch 9/10, Train Loss: 0.2055, Accuracy: 0.8965, F1 Micro: 0.682, F1 Macro: 0.4801\n",
      "Epoch 10/10, Train Loss: 0.188, Accuracy: 0.8985, F1 Micro: 0.6783, F1 Macro: 0.4768\n",
      "Best result for 1646 samples: F1 Micro: 0.682\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.81      0.81      1141\n",
      "      Abusive       0.82      0.82      0.82      1012\n",
      "HS_Individual       0.70      0.62      0.66       737\n",
      "     HS_Group       0.60      0.54      0.57       404\n",
      "  HS_Religion       0.66      0.30      0.42       164\n",
      "      HS_Race       0.75      0.45      0.56       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.74      0.65      0.69       779\n",
      "      HS_Weak       0.66      0.55      0.60       686\n",
      "  HS_Moderate       0.52      0.45      0.48       356\n",
      "    HS_Strong       1.00      0.09      0.17        99\n",
      "\n",
      "    micro avg       0.73      0.64      0.68      5608\n",
      "    macro avg       0.60      0.44      0.48      5608\n",
      " weighted avg       0.72      0.64      0.67      5608\n",
      "  samples avg       0.39      0.36      0.35      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 0.0005519390106201172 seconds\n",
      "\n",
      "Fold 1 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5009, Accuracy: 0.8316, F1 Micro: 0.0784, F1 Macro: 0.0301\n",
      "Epoch 2/10, Train Loss: 0.3813, Accuracy: 0.8497, F1 Micro: 0.2762, F1 Macro: 0.0951\n",
      "Epoch 3/10, Train Loss: 0.3362, Accuracy: 0.8801, F1 Micro: 0.5879, F1 Macro: 0.2757\n",
      "Epoch 4/10, Train Loss: 0.294, Accuracy: 0.8859, F1 Micro: 0.5936, F1 Macro: 0.2942\n",
      "Epoch 5/10, Train Loss: 0.2532, Accuracy: 0.8967, F1 Micro: 0.6806, F1 Macro: 0.4658\n",
      "Epoch 6/10, Train Loss: 0.2265, Accuracy: 0.899, F1 Micro: 0.6941, F1 Macro: 0.4816\n",
      "Epoch 7/10, Train Loss: 0.1973, Accuracy: 0.897, F1 Micro: 0.6338, F1 Macro: 0.4532\n",
      "Epoch 8/10, Train Loss: 0.18, Accuracy: 0.9036, F1 Micro: 0.6864, F1 Macro: 0.4783\n",
      "Epoch 9/10, Train Loss: 0.1589, Accuracy: 0.9057, F1 Micro: 0.7177, F1 Macro: 0.5523\n",
      "Epoch 10/10, Train Loss: 0.1409, Accuracy: 0.9062, F1 Micro: 0.7172, F1 Macro: 0.519\n",
      "Best result for 2535 samples: F1 Micro: 0.7177\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.82      1141\n",
      "      Abusive       0.85      0.85      0.85      1012\n",
      "HS_Individual       0.71      0.66      0.69       737\n",
      "     HS_Group       0.62      0.58      0.60       404\n",
      "  HS_Religion       0.64      0.49      0.55       164\n",
      "      HS_Race       0.68      0.65      0.66       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.76      0.70      0.73       779\n",
      "      HS_Weak       0.68      0.63      0.65       686\n",
      "  HS_Moderate       0.55      0.49      0.52       356\n",
      "    HS_Strong       0.68      0.46      0.55        99\n",
      "\n",
      "    micro avg       0.75      0.69      0.72      5608\n",
      "    macro avg       0.58      0.53      0.55      5608\n",
      " weighted avg       0.73      0.69      0.71      5608\n",
      "  samples avg       0.41      0.39      0.38      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 0.0004687309265136719 seconds\n",
      "\n",
      "Fold 1 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3335 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4758, Accuracy: 0.8331, F1 Micro: 0.0918, F1 Macro: 0.0357\n",
      "Epoch 2/10, Train Loss: 0.3574, Accuracy: 0.8661, F1 Micro: 0.4298, F1 Macro: 0.1818\n",
      "Epoch 3/10, Train Loss: 0.2982, Accuracy: 0.886, F1 Micro: 0.6052, F1 Macro: 0.2937\n",
      "Epoch 4/10, Train Loss: 0.2603, Accuracy: 0.8946, F1 Micro: 0.6972, F1 Macro: 0.4489\n",
      "Epoch 5/10, Train Loss: 0.2249, Accuracy: 0.9023, F1 Micro: 0.7008, F1 Macro: 0.4992\n",
      "Epoch 6/10, Train Loss: 0.1935, Accuracy: 0.9053, F1 Micro: 0.7017, F1 Macro: 0.5043\n",
      "Epoch 7/10, Train Loss: 0.1786, Accuracy: 0.9069, F1 Micro: 0.7249, F1 Macro: 0.5541\n",
      "Epoch 8/10, Train Loss: 0.1524, Accuracy: 0.909, F1 Micro: 0.7131, F1 Macro: 0.5236\n",
      "Epoch 9/10, Train Loss: 0.1322, Accuracy: 0.9107, F1 Micro: 0.7185, F1 Macro: 0.5602\n",
      "Epoch 10/10, Train Loss: 0.1165, Accuracy: 0.9076, F1 Micro: 0.7195, F1 Macro: 0.572\n",
      "Best result for 3335 samples: F1 Micro: 0.7249\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1141\n",
      "      Abusive       0.84      0.84      0.84      1012\n",
      "HS_Individual       0.69      0.71      0.70       737\n",
      "     HS_Group       0.66      0.57      0.61       404\n",
      "  HS_Religion       0.67      0.46      0.55       164\n",
      "      HS_Race       0.77      0.56      0.65       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.73      0.75      0.74       779\n",
      "      HS_Weak       0.66      0.66      0.66       686\n",
      "  HS_Moderate       0.60      0.44      0.51       356\n",
      "    HS_Strong       0.73      0.44      0.55        99\n",
      "\n",
      "    micro avg       0.75      0.71      0.72      5608\n",
      "    macro avg       0.60      0.52      0.55      5608\n",
      " weighted avg       0.73      0.71      0.71      5608\n",
      "  samples avg       0.41      0.40      0.38      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 0.0004684925079345703 seconds\n",
      "\n",
      "Fold 1 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4055 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.457, Accuracy: 0.8369, F1 Micro: 0.1397, F1 Macro: 0.0513\n",
      "Epoch 2/10, Train Loss: 0.3431, Accuracy: 0.8784, F1 Micro: 0.5437, F1 Macro: 0.256\n",
      "Epoch 3/10, Train Loss: 0.2837, Accuracy: 0.8963, F1 Micro: 0.6652, F1 Macro: 0.4091\n",
      "Epoch 4/10, Train Loss: 0.2449, Accuracy: 0.8974, F1 Micro: 0.6378, F1 Macro: 0.4639\n",
      "Epoch 5/10, Train Loss: 0.2134, Accuracy: 0.9063, F1 Micro: 0.6931, F1 Macro: 0.5049\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9089, F1 Micro: 0.7319, F1 Macro: 0.5514\n",
      "Epoch 7/10, Train Loss: 0.1615, Accuracy: 0.9134, F1 Micro: 0.7445, F1 Macro: 0.5772\n",
      "Epoch 8/10, Train Loss: 0.1433, Accuracy: 0.9146, F1 Micro: 0.7385, F1 Macro: 0.5788\n",
      "Epoch 9/10, Train Loss: 0.1248, Accuracy: 0.912, F1 Micro: 0.7322, F1 Macro: 0.5444\n",
      "Epoch 10/10, Train Loss: 0.1103, Accuracy: 0.917, F1 Micro: 0.7501, F1 Macro: 0.5914\n",
      "Best result for 4055 samples: F1 Micro: 0.7501\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1141\n",
      "      Abusive       0.88      0.85      0.87      1012\n",
      "HS_Individual       0.73      0.73      0.73       737\n",
      "     HS_Group       0.73      0.57      0.64       404\n",
      "  HS_Religion       0.68      0.59      0.63       164\n",
      "      HS_Race       0.78      0.64      0.70       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.80      0.71      0.75       779\n",
      "      HS_Weak       0.68      0.71      0.69       686\n",
      "  HS_Moderate       0.64      0.48      0.55       356\n",
      "    HS_Strong       0.82      0.59      0.68        99\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5608\n",
      "    macro avg       0.63      0.56      0.59      5608\n",
      " weighted avg       0.77      0.72      0.74      5608\n",
      "  samples avg       0.43      0.41      0.40      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 0.0004794597625732422 seconds\n",
      "\n",
      "Fold 1 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4703 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4482, Accuracy: 0.8464, F1 Micro: 0.241, F1 Macro: 0.0853\n",
      "Epoch 2/10, Train Loss: 0.3296, Accuracy: 0.8848, F1 Micro: 0.6073, F1 Macro: 0.296\n",
      "Epoch 3/10, Train Loss: 0.2661, Accuracy: 0.9003, F1 Micro: 0.6682, F1 Macro: 0.4425\n",
      "Epoch 4/10, Train Loss: 0.2248, Accuracy: 0.9063, F1 Micro: 0.7187, F1 Macro: 0.5243\n",
      "Epoch 5/10, Train Loss: 0.1986, Accuracy: 0.9092, F1 Micro: 0.6985, F1 Macro: 0.5102\n",
      "Epoch 6/10, Train Loss: 0.1738, Accuracy: 0.9122, F1 Micro: 0.717, F1 Macro: 0.5507\n",
      "Epoch 7/10, Train Loss: 0.1485, Accuracy: 0.9097, F1 Micro: 0.7456, F1 Macro: 0.5873\n",
      "Epoch 8/10, Train Loss: 0.1296, Accuracy: 0.9146, F1 Micro: 0.7442, F1 Macro: 0.5832\n",
      "Epoch 9/10, Train Loss: 0.1099, Accuracy: 0.9183, F1 Micro: 0.7506, F1 Macro: 0.6023\n",
      "Epoch 10/10, Train Loss: 0.0921, Accuracy: 0.916, F1 Micro: 0.7437, F1 Macro: 0.5907\n",
      "Best result for 4703 samples: F1 Micro: 0.7506\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.81      0.84      1141\n",
      "      Abusive       0.91      0.83      0.87      1012\n",
      "HS_Individual       0.77      0.66      0.71       737\n",
      "     HS_Group       0.69      0.66      0.67       404\n",
      "  HS_Religion       0.72      0.58      0.64       164\n",
      "      HS_Race       0.76      0.66      0.70       119\n",
      "  HS_Physical       0.00      0.00      0.00        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.80      0.73      0.76       779\n",
      "      HS_Weak       0.73      0.63      0.68       686\n",
      "  HS_Moderate       0.63      0.56      0.59       356\n",
      "    HS_Strong       0.80      0.73      0.76        99\n",
      "\n",
      "    micro avg       0.80      0.71      0.75      5608\n",
      "    macro avg       0.64      0.57      0.60      5608\n",
      " weighted avg       0.78      0.71      0.74      5608\n",
      "  samples avg       0.42      0.40      0.39      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.00039196014404296875 seconds\n",
      "\n",
      "Fold 1 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5287 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4377, Accuracy: 0.858, F1 Micro: 0.3929, F1 Macro: 0.1219\n",
      "Epoch 2/10, Train Loss: 0.315, Accuracy: 0.884, F1 Micro: 0.5677, F1 Macro: 0.2869\n",
      "Epoch 3/10, Train Loss: 0.251, Accuracy: 0.9025, F1 Micro: 0.6764, F1 Macro: 0.452\n",
      "Epoch 4/10, Train Loss: 0.2149, Accuracy: 0.9088, F1 Micro: 0.7308, F1 Macro: 0.5391\n",
      "Epoch 5/10, Train Loss: 0.183, Accuracy: 0.9152, F1 Micro: 0.7408, F1 Macro: 0.5711\n",
      "Epoch 6/10, Train Loss: 0.1583, Accuracy: 0.9149, F1 Micro: 0.7507, F1 Macro: 0.5852\n",
      "Epoch 7/10, Train Loss: 0.1411, Accuracy: 0.9175, F1 Micro: 0.749, F1 Macro: 0.5989\n",
      "Epoch 8/10, Train Loss: 0.119, Accuracy: 0.9151, F1 Micro: 0.7607, F1 Macro: 0.6112\n",
      "Epoch 9/10, Train Loss: 0.1018, Accuracy: 0.9173, F1 Micro: 0.7632, F1 Macro: 0.613\n",
      "Epoch 10/10, Train Loss: 0.0882, Accuracy: 0.9219, F1 Micro: 0.7683, F1 Macro: 0.633\n",
      "Best result for 5287 samples: F1 Micro: 0.7683\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1141\n",
      "      Abusive       0.89      0.88      0.89      1012\n",
      "HS_Individual       0.74      0.73      0.74       737\n",
      "     HS_Group       0.72      0.64      0.68       404\n",
      "  HS_Religion       0.75      0.58      0.65       164\n",
      "      HS_Race       0.78      0.70      0.73       119\n",
      "  HS_Physical       0.67      0.04      0.07        53\n",
      "    HS_Gender       0.83      0.09      0.16        58\n",
      "     HS_Other       0.80      0.78      0.79       779\n",
      "      HS_Weak       0.71      0.69      0.70       686\n",
      "  HS_Moderate       0.65      0.54      0.59       356\n",
      "    HS_Strong       0.76      0.76      0.76        99\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5608\n",
      "    macro avg       0.76      0.60      0.63      5608\n",
      " weighted avg       0.79      0.74      0.76      5608\n",
      "  samples avg       0.44      0.42      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0003743171691894531 seconds\n",
      "\n",
      "Fold 1 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5812 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4336, Accuracy: 0.8592, F1 Micro: 0.3789, F1 Macro: 0.1314\n",
      "Epoch 2/10, Train Loss: 0.3053, Accuracy: 0.8912, F1 Micro: 0.6176, F1 Macro: 0.3546\n",
      "Epoch 3/10, Train Loss: 0.251, Accuracy: 0.9067, F1 Micro: 0.7104, F1 Macro: 0.5013\n",
      "Epoch 4/10, Train Loss: 0.2129, Accuracy: 0.9092, F1 Micro: 0.6992, F1 Macro: 0.5087\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.9157, F1 Micro: 0.7407, F1 Macro: 0.5417\n",
      "Epoch 6/10, Train Loss: 0.1577, Accuracy: 0.9194, F1 Micro: 0.748, F1 Macro: 0.5662\n",
      "Epoch 7/10, Train Loss: 0.1318, Accuracy: 0.9189, F1 Micro: 0.7644, F1 Macro: 0.6022\n",
      "Epoch 8/10, Train Loss: 0.1148, Accuracy: 0.9207, F1 Micro: 0.7647, F1 Macro: 0.6074\n",
      "Epoch 9/10, Train Loss: 0.0992, Accuracy: 0.9179, F1 Micro: 0.7625, F1 Macro: 0.6111\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.9186, F1 Micro: 0.7633, F1 Macro: 0.6358\n",
      "Best result for 5812 samples: F1 Micro: 0.7647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1141\n",
      "      Abusive       0.90      0.85      0.88      1012\n",
      "HS_Individual       0.73      0.75      0.74       737\n",
      "     HS_Group       0.72      0.60      0.65       404\n",
      "  HS_Religion       0.75      0.53      0.62       164\n",
      "      HS_Race       0.79      0.63      0.70       119\n",
      "  HS_Physical       1.00      0.02      0.04        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.79      0.79      0.79       779\n",
      "      HS_Weak       0.69      0.73      0.71       686\n",
      "  HS_Moderate       0.65      0.51      0.57       356\n",
      "    HS_Strong       0.78      0.69      0.73        99\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5608\n",
      "    macro avg       0.72      0.58      0.61      5608\n",
      " weighted avg       0.78      0.74      0.76      5608\n",
      "  samples avg       0.43      0.42      0.41      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0003554821014404297 seconds\n",
      "\n",
      "Fold 1 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6285 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4249, Accuracy: 0.8652, F1 Micro: 0.4312, F1 Macro: 0.1729\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.8972, F1 Micro: 0.6629, F1 Macro: 0.3832\n",
      "Epoch 3/10, Train Loss: 0.245, Accuracy: 0.9057, F1 Micro: 0.6925, F1 Macro: 0.5151\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9112, F1 Micro: 0.7199, F1 Macro: 0.5044\n",
      "Epoch 5/10, Train Loss: 0.1756, Accuracy: 0.9173, F1 Micro: 0.7569, F1 Macro: 0.5936\n",
      "Epoch 6/10, Train Loss: 0.1463, Accuracy: 0.9174, F1 Micro: 0.7621, F1 Macro: 0.6091\n",
      "Epoch 7/10, Train Loss: 0.1307, Accuracy: 0.9209, F1 Micro: 0.7658, F1 Macro: 0.6153\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9199, F1 Micro: 0.7649, F1 Macro: 0.6354\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.9213, F1 Micro: 0.7669, F1 Macro: 0.6406\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.921, F1 Micro: 0.7684, F1 Macro: 0.6568\n",
      "Best result for 6285 samples: F1 Micro: 0.7684\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1141\n",
      "      Abusive       0.86      0.92      0.89      1012\n",
      "HS_Individual       0.71      0.74      0.73       737\n",
      "     HS_Group       0.75      0.59      0.66       404\n",
      "  HS_Religion       0.72      0.59      0.65       164\n",
      "      HS_Race       0.75      0.71      0.73       119\n",
      "  HS_Physical       0.67      0.11      0.19        53\n",
      "    HS_Gender       0.80      0.21      0.33        58\n",
      "     HS_Other       0.82      0.76      0.79       779\n",
      "      HS_Weak       0.69      0.72      0.71       686\n",
      "  HS_Moderate       0.66      0.52      0.58       356\n",
      "    HS_Strong       0.81      0.76      0.78        99\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5608\n",
      "    macro avg       0.76      0.62      0.66      5608\n",
      " weighted avg       0.78      0.75      0.76      5608\n",
      "  samples avg       0.45      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 0.1967766284942627 seconds\n",
      "\n",
      "Fold 1 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6584 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4227, Accuracy: 0.8622, F1 Micro: 0.3957, F1 Macro: 0.1612\n",
      "Epoch 2/10, Train Loss: 0.295, Accuracy: 0.8905, F1 Micro: 0.6041, F1 Macro: 0.3483\n",
      "Epoch 3/10, Train Loss: 0.2361, Accuracy: 0.9094, F1 Micro: 0.7233, F1 Macro: 0.5265\n",
      "Epoch 4/10, Train Loss: 0.2008, Accuracy: 0.9151, F1 Micro: 0.7414, F1 Macro: 0.5559\n",
      "Epoch 5/10, Train Loss: 0.174, Accuracy: 0.9179, F1 Micro: 0.7574, F1 Macro: 0.5986\n",
      "Epoch 6/10, Train Loss: 0.1465, Accuracy: 0.9123, F1 Micro: 0.7549, F1 Macro: 0.6023\n",
      "Epoch 7/10, Train Loss: 0.1272, Accuracy: 0.9212, F1 Micro: 0.7627, F1 Macro: 0.6172\n",
      "Epoch 8/10, Train Loss: 0.1102, Accuracy: 0.9182, F1 Micro: 0.7624, F1 Macro: 0.6238\n",
      "Epoch 9/10, Train Loss: 0.0955, Accuracy: 0.9187, F1 Micro: 0.764, F1 Macro: 0.646\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9201, F1 Micro: 0.7585, F1 Macro: 0.6495\n",
      "Best result for 6584 samples: F1 Micro: 0.764\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1141\n",
      "      Abusive       0.88      0.89      0.88      1012\n",
      "HS_Individual       0.70      0.75      0.72       737\n",
      "     HS_Group       0.72      0.61      0.66       404\n",
      "  HS_Religion       0.68      0.68      0.68       164\n",
      "      HS_Race       0.75      0.74      0.75       119\n",
      "  HS_Physical       0.57      0.08      0.13        53\n",
      "    HS_Gender       0.69      0.16      0.25        58\n",
      "     HS_Other       0.81      0.77      0.79       779\n",
      "      HS_Weak       0.66      0.73      0.69       686\n",
      "  HS_Moderate       0.64      0.53      0.58       356\n",
      "    HS_Strong       0.80      0.75      0.77        99\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5608\n",
      "    macro avg       0.73      0.63      0.65      5608\n",
      " weighted avg       0.77      0.76      0.76      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 0.0004088878631591797 seconds\n",
      "\n",
      "Fold 1 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6980 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4157, Accuracy: 0.8729, F1 Micro: 0.5008, F1 Macro: 0.2165\n",
      "Epoch 2/10, Train Loss: 0.2863, Accuracy: 0.8981, F1 Micro: 0.6517, F1 Macro: 0.3992\n",
      "Epoch 3/10, Train Loss: 0.2338, Accuracy: 0.9092, F1 Micro: 0.7332, F1 Macro: 0.5303\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9159, F1 Micro: 0.744, F1 Macro: 0.5623\n",
      "Epoch 5/10, Train Loss: 0.1696, Accuracy: 0.9194, F1 Micro: 0.7564, F1 Macro: 0.5874\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9201, F1 Micro: 0.7659, F1 Macro: 0.6034\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.9173, F1 Micro: 0.7646, F1 Macro: 0.6111\n",
      "Epoch 8/10, Train Loss: 0.1094, Accuracy: 0.9227, F1 Micro: 0.771, F1 Macro: 0.6199\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6298\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9228, F1 Micro: 0.7686, F1 Macro: 0.6514\n",
      "Best result for 6980 samples: F1 Micro: 0.7713\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1141\n",
      "      Abusive       0.89      0.89      0.89      1012\n",
      "HS_Individual       0.73      0.74      0.74       737\n",
      "     HS_Group       0.70      0.62      0.66       404\n",
      "  HS_Religion       0.73      0.62      0.67       164\n",
      "      HS_Race       0.82      0.67      0.74       119\n",
      "  HS_Physical       1.00      0.02      0.04        53\n",
      "    HS_Gender       0.60      0.05      0.10        58\n",
      "     HS_Other       0.78      0.80      0.79       779\n",
      "      HS_Weak       0.70      0.71      0.71       686\n",
      "  HS_Moderate       0.64      0.56      0.60       356\n",
      "    HS_Strong       0.80      0.77      0.78        99\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5608\n",
      "    macro avg       0.77      0.61      0.63      5608\n",
      " weighted avg       0.78      0.76      0.76      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 0.0003666877746582031 seconds\n",
      "\n",
      "Fold 1 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4124, Accuracy: 0.879, F1 Micro: 0.5734, F1 Macro: 0.2638\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9047, F1 Micro: 0.7002, F1 Macro: 0.4835\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.9113, F1 Micro: 0.736, F1 Macro: 0.5448\n",
      "Epoch 4/10, Train Loss: 0.1991, Accuracy: 0.9184, F1 Micro: 0.7463, F1 Macro: 0.5847\n",
      "Epoch 5/10, Train Loss: 0.1659, Accuracy: 0.9191, F1 Micro: 0.7626, F1 Macro: 0.6125\n",
      "Epoch 6/10, Train Loss: 0.1405, Accuracy: 0.9197, F1 Micro: 0.7685, F1 Macro: 0.6177\n",
      "Epoch 7/10, Train Loss: 0.1184, Accuracy: 0.9148, F1 Micro: 0.7629, F1 Macro: 0.6138\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9227, F1 Micro: 0.7733, F1 Macro: 0.6444\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.9205, F1 Micro: 0.7658, F1 Macro: 0.6365\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9237, F1 Micro: 0.768, F1 Macro: 0.6424\n",
      "Best result for 7336 samples: F1 Micro: 0.7733\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1141\n",
      "      Abusive       0.86      0.91      0.89      1012\n",
      "HS_Individual       0.74      0.74      0.74       737\n",
      "     HS_Group       0.71      0.64      0.67       404\n",
      "  HS_Religion       0.74      0.54      0.62       164\n",
      "      HS_Race       0.78      0.72      0.75       119\n",
      "  HS_Physical       1.00      0.06      0.11        53\n",
      "    HS_Gender       0.88      0.12      0.21        58\n",
      "     HS_Other       0.81      0.78      0.80       779\n",
      "      HS_Weak       0.72      0.71      0.71       686\n",
      "  HS_Moderate       0.65      0.56      0.60       356\n",
      "    HS_Strong       0.81      0.76      0.78        99\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5608\n",
      "    macro avg       0.79      0.62      0.64      5608\n",
      " weighted avg       0.79      0.76      0.77      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 0.0002758502960205078 seconds\n",
      "\n",
      "Fold 1 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7656 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4072, Accuracy: 0.8785, F1 Micro: 0.5594, F1 Macro: 0.2578\n",
      "Epoch 2/10, Train Loss: 0.2791, Accuracy: 0.9026, F1 Micro: 0.7031, F1 Macro: 0.4803\n",
      "Epoch 3/10, Train Loss: 0.2316, Accuracy: 0.911, F1 Micro: 0.7312, F1 Macro: 0.531\n",
      "Epoch 4/10, Train Loss: 0.1945, Accuracy: 0.917, F1 Micro: 0.7615, F1 Macro: 0.5997\n",
      "Epoch 5/10, Train Loss: 0.1658, Accuracy: 0.9216, F1 Micro: 0.7648, F1 Macro: 0.6089\n",
      "Epoch 6/10, Train Loss: 0.1417, Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6142\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9218, F1 Micro: 0.7711, F1 Macro: 0.625\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9219, F1 Micro: 0.7704, F1 Macro: 0.6459\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.9235, F1 Micro: 0.772, F1 Macro: 0.664\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9239, F1 Micro: 0.7789, F1 Macro: 0.6687\n",
      "Best result for 7656 samples: F1 Micro: 0.7789\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1141\n",
      "      Abusive       0.89      0.91      0.90      1012\n",
      "HS_Individual       0.71      0.78      0.74       737\n",
      "     HS_Group       0.76      0.59      0.67       404\n",
      "  HS_Religion       0.76      0.55      0.64       164\n",
      "      HS_Race       0.80      0.70      0.74       119\n",
      "  HS_Physical       0.65      0.21      0.31        53\n",
      "    HS_Gender       0.69      0.16      0.25        58\n",
      "     HS_Other       0.80      0.81      0.80       779\n",
      "      HS_Weak       0.69      0.76      0.72       686\n",
      "  HS_Moderate       0.68      0.54      0.60       356\n",
      "    HS_Strong       0.85      0.73      0.78        99\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5608\n",
      "    macro avg       0.76      0.63      0.67      5608\n",
      " weighted avg       0.79      0.77      0.77      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 0.22933387756347656 seconds\n",
      "\n",
      "Fold 1 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 7901 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4082, Accuracy: 0.8793, F1 Micro: 0.552, F1 Macro: 0.2589\n",
      "Epoch 2/10, Train Loss: 0.2802, Accuracy: 0.902, F1 Micro: 0.7047, F1 Macro: 0.5007\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.9139, F1 Micro: 0.7371, F1 Macro: 0.562\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9179, F1 Micro: 0.7351, F1 Macro: 0.5473\n",
      "Epoch 5/10, Train Loss: 0.1626, Accuracy: 0.9202, F1 Micro: 0.7647, F1 Macro: 0.6065\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9212, F1 Micro: 0.7724, F1 Macro: 0.6266\n",
      "Epoch 7/10, Train Loss: 0.1173, Accuracy: 0.9215, F1 Micro: 0.7618, F1 Macro: 0.6016\n",
      "Epoch 8/10, Train Loss: 0.1014, Accuracy: 0.9201, F1 Micro: 0.7714, F1 Macro: 0.6408\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.9227, F1 Micro: 0.7729, F1 Macro: 0.6501\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9229, F1 Micro: 0.7701, F1 Macro: 0.6497\n",
      "Best result for 7901 samples: F1 Micro: 0.7729\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1141\n",
      "      Abusive       0.91      0.88      0.90      1012\n",
      "HS_Individual       0.72      0.77      0.74       737\n",
      "     HS_Group       0.74      0.57      0.64       404\n",
      "  HS_Religion       0.73      0.58      0.64       164\n",
      "      HS_Race       0.78      0.71      0.75       119\n",
      "  HS_Physical       0.55      0.11      0.19        53\n",
      "    HS_Gender       0.73      0.14      0.23        58\n",
      "     HS_Other       0.80      0.81      0.80       779\n",
      "      HS_Weak       0.69      0.75      0.72       686\n",
      "  HS_Moderate       0.67      0.51      0.58       356\n",
      "    HS_Strong       0.79      0.75      0.77        99\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5608\n",
      "    macro avg       0.74      0.62      0.65      5608\n",
      " weighted avg       0.79      0.76      0.77      5608\n",
      "  samples avg       0.43      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 0.00021696090698242188 seconds\n",
      "\n",
      "Fold 1 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8165 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4015, Accuracy: 0.8789, F1 Micro: 0.5425, F1 Macro: 0.2518\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.904, F1 Micro: 0.6909, F1 Macro: 0.4355\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.913, F1 Micro: 0.7221, F1 Macro: 0.5316\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9196, F1 Micro: 0.7597, F1 Macro: 0.6003\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9225, F1 Micro: 0.7685, F1 Macro: 0.6124\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9213, F1 Micro: 0.7738, F1 Macro: 0.6247\n",
      "Epoch 7/10, Train Loss: 0.1139, Accuracy: 0.9231, F1 Micro: 0.7761, F1 Macro: 0.6583\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9217, F1 Micro: 0.7741, F1 Macro: 0.6456\n",
      "Epoch 9/10, Train Loss: 0.083, Accuracy: 0.9257, F1 Micro: 0.7768, F1 Macro: 0.6645\n",
      "Epoch 10/10, Train Loss: 0.0733, Accuracy: 0.9249, F1 Micro: 0.7788, F1 Macro: 0.6835\n",
      "Best result for 8165 samples: F1 Micro: 0.7788\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1141\n",
      "      Abusive       0.90      0.89      0.89      1012\n",
      "HS_Individual       0.74      0.75      0.74       737\n",
      "     HS_Group       0.71      0.62      0.66       404\n",
      "  HS_Religion       0.75      0.63      0.68       164\n",
      "      HS_Race       0.78      0.70      0.74       119\n",
      "  HS_Physical       0.55      0.23      0.32        53\n",
      "    HS_Gender       0.73      0.28      0.40        58\n",
      "     HS_Other       0.82      0.79      0.80       779\n",
      "      HS_Weak       0.72      0.71      0.72       686\n",
      "  HS_Moderate       0.66      0.55      0.60       356\n",
      "    HS_Strong       0.77      0.80      0.79        99\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5608\n",
      "    macro avg       0.75      0.65      0.68      5608\n",
      " weighted avg       0.79      0.76      0.77      5608\n",
      "  samples avg       0.45      0.43      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 0.00021648406982421875 seconds\n",
      "\n",
      "Fold 1 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8402 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4, Accuracy: 0.8817, F1 Micro: 0.5805, F1 Macro: 0.2796\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.9039, F1 Micro: 0.6749, F1 Macro: 0.4449\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9156, F1 Micro: 0.7442, F1 Macro: 0.5753\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.921, F1 Micro: 0.7594, F1 Macro: 0.598\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9223, F1 Micro: 0.7727, F1 Macro: 0.618\n",
      "Epoch 6/10, Train Loss: 0.1346, Accuracy: 0.9229, F1 Micro: 0.7799, F1 Macro: 0.6331\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9209, F1 Micro: 0.7772, F1 Macro: 0.6443\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.9261, F1 Micro: 0.7738, F1 Macro: 0.656\n",
      "Epoch 9/10, Train Loss: 0.0866, Accuracy: 0.9227, F1 Micro: 0.7751, F1 Macro: 0.647\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9219, F1 Micro: 0.7759, F1 Macro: 0.6752\n",
      "Best result for 8402 samples: F1 Micro: 0.7799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1141\n",
      "      Abusive       0.85      0.92      0.88      1012\n",
      "HS_Individual       0.73      0.76      0.74       737\n",
      "     HS_Group       0.69      0.68      0.68       404\n",
      "  HS_Religion       0.74      0.61      0.67       164\n",
      "      HS_Race       0.82      0.66      0.73       119\n",
      "  HS_Physical       0.75      0.06      0.11        53\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.77      0.84      0.80       779\n",
      "      HS_Weak       0.71      0.73      0.72       686\n",
      "  HS_Moderate       0.64      0.59      0.62       356\n",
      "    HS_Strong       0.78      0.77      0.78        99\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5608\n",
      "    macro avg       0.69      0.63      0.63      5608\n",
      " weighted avg       0.76      0.79      0.77      5608\n",
      "  samples avg       0.44      0.45      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 0.00022268295288085938 seconds\n",
      "\n",
      "Fold 1 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8616 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3997, Accuracy: 0.8805, F1 Micro: 0.5618, F1 Macro: 0.2639\n",
      "Epoch 2/10, Train Loss: 0.2755, Accuracy: 0.9042, F1 Micro: 0.7161, F1 Macro: 0.4965\n",
      "Epoch 3/10, Train Loss: 0.2194, Accuracy: 0.9134, F1 Micro: 0.7233, F1 Macro: 0.5271\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9204, F1 Micro: 0.7606, F1 Macro: 0.6031\n",
      "Epoch 5/10, Train Loss: 0.155, Accuracy: 0.9229, F1 Micro: 0.764, F1 Macro: 0.6177\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9215, F1 Micro: 0.7743, F1 Macro: 0.6237\n",
      "Epoch 7/10, Train Loss: 0.1148, Accuracy: 0.9227, F1 Micro: 0.7739, F1 Macro: 0.6451\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9235, F1 Micro: 0.7737, F1 Macro: 0.6644\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9234, F1 Micro: 0.7742, F1 Macro: 0.6699\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9234, F1 Micro: 0.7726, F1 Macro: 0.6799\n",
      "Best result for 8616 samples: F1 Micro: 0.7743\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1141\n",
      "      Abusive       0.88      0.91      0.89      1012\n",
      "HS_Individual       0.72      0.77      0.74       737\n",
      "     HS_Group       0.70      0.63      0.66       404\n",
      "  HS_Religion       0.74      0.55      0.63       164\n",
      "      HS_Race       0.85      0.64      0.73       119\n",
      "  HS_Physical       1.00      0.04      0.07        53\n",
      "    HS_Gender       1.00      0.02      0.03        58\n",
      "     HS_Other       0.76      0.85      0.80       779\n",
      "      HS_Weak       0.70      0.74      0.72       686\n",
      "  HS_Moderate       0.66      0.53      0.58       356\n",
      "    HS_Strong       0.77      0.76      0.76        99\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5608\n",
      "    macro avg       0.80      0.61      0.62      5608\n",
      " weighted avg       0.78      0.77      0.76      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018334388732910156 seconds\n",
      "\n",
      "Fold 1 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 8816 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.398, Accuracy: 0.8828, F1 Micro: 0.5908, F1 Macro: 0.2792\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.9038, F1 Micro: 0.7201, F1 Macro: 0.4811\n",
      "Epoch 3/10, Train Loss: 0.2221, Accuracy: 0.9164, F1 Micro: 0.7473, F1 Macro: 0.5839\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9218, F1 Micro: 0.7677, F1 Macro: 0.577\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9237, F1 Micro: 0.7697, F1 Macro: 0.6187\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.9246, F1 Micro: 0.7748, F1 Macro: 0.623\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9255, F1 Micro: 0.7781, F1 Macro: 0.6532\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9213, F1 Micro: 0.7726, F1 Macro: 0.6628\n",
      "Epoch 9/10, Train Loss: 0.085, Accuracy: 0.924, F1 Micro: 0.7689, F1 Macro: 0.6666\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9271, F1 Micro: 0.7803, F1 Macro: 0.6831\n",
      "Best result for 8816 samples: F1 Micro: 0.7803\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1141\n",
      "      Abusive       0.93      0.86      0.89      1012\n",
      "HS_Individual       0.75      0.74      0.75       737\n",
      "     HS_Group       0.77      0.59      0.67       404\n",
      "  HS_Religion       0.77      0.65      0.70       164\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.55      0.23      0.32        53\n",
      "    HS_Gender       0.74      0.24      0.36        58\n",
      "     HS_Other       0.84      0.76      0.80       779\n",
      "      HS_Weak       0.73      0.72      0.72       686\n",
      "  HS_Moderate       0.70      0.52      0.60       356\n",
      "    HS_Strong       0.77      0.76      0.77        99\n",
      "\n",
      "    micro avg       0.82      0.74      0.78      5608\n",
      "    macro avg       0.76      0.64      0.68      5608\n",
      " weighted avg       0.82      0.74      0.78      5608\n",
      "  samples avg       0.45      0.42      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001952648162841797 seconds\n",
      "\n",
      "Fold 1 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9016 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3902, Accuracy: 0.8845, F1 Micro: 0.6078, F1 Macro: 0.2879\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.908, F1 Micro: 0.7189, F1 Macro: 0.5036\n",
      "Epoch 3/10, Train Loss: 0.218, Accuracy: 0.9182, F1 Micro: 0.7497, F1 Macro: 0.5722\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.9203, F1 Micro: 0.767, F1 Macro: 0.6052\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9216, F1 Micro: 0.7737, F1 Macro: 0.6198\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9213, F1 Micro: 0.7746, F1 Macro: 0.6286\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9244, F1 Micro: 0.7755, F1 Macro: 0.6355\n",
      "Epoch 8/10, Train Loss: 0.0952, Accuracy: 0.9236, F1 Micro: 0.7733, F1 Macro: 0.6732\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9248, F1 Micro: 0.7811, F1 Macro: 0.6866\n",
      "Epoch 10/10, Train Loss: 0.0693, Accuracy: 0.9245, F1 Micro: 0.7749, F1 Macro: 0.6829\n",
      "Best result for 9016 samples: F1 Micro: 0.7811\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1141\n",
      "      Abusive       0.89      0.90      0.90      1012\n",
      "HS_Individual       0.75      0.73      0.74       737\n",
      "     HS_Group       0.68      0.67      0.67       404\n",
      "  HS_Religion       0.72      0.66      0.69       164\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.58      0.21      0.31        53\n",
      "    HS_Gender       0.80      0.28      0.41        58\n",
      "     HS_Other       0.81      0.80      0.81       779\n",
      "      HS_Weak       0.73      0.69      0.71       686\n",
      "  HS_Moderate       0.62      0.60      0.61       356\n",
      "    HS_Strong       0.79      0.80      0.79        99\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5608\n",
      "    macro avg       0.75      0.66      0.69      5608\n",
      " weighted avg       0.79      0.77      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.000186920166015625 seconds\n",
      "\n",
      "Fold 1 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9216 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3951, Accuracy: 0.8801, F1 Micro: 0.5469, F1 Macro: 0.2568\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.9075, F1 Micro: 0.7064, F1 Macro: 0.5118\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9147, F1 Micro: 0.7483, F1 Macro: 0.5365\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9213, F1 Micro: 0.7512, F1 Macro: 0.5949\n",
      "Epoch 5/10, Train Loss: 0.1541, Accuracy: 0.925, F1 Micro: 0.7695, F1 Macro: 0.6279\n",
      "Epoch 6/10, Train Loss: 0.1304, Accuracy: 0.9233, F1 Micro: 0.7749, F1 Macro: 0.6261\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9244, F1 Micro: 0.7704, F1 Macro: 0.6476\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9249, F1 Micro: 0.7799, F1 Macro: 0.6917\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9215, F1 Micro: 0.7779, F1 Macro: 0.6852\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9258, F1 Micro: 0.7785, F1 Macro: 0.6936\n",
      "Best result for 9216 samples: F1 Micro: 0.7799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1141\n",
      "      Abusive       0.91      0.88      0.90      1012\n",
      "HS_Individual       0.73      0.76      0.75       737\n",
      "     HS_Group       0.72      0.63      0.67       404\n",
      "  HS_Religion       0.71      0.68      0.70       164\n",
      "      HS_Race       0.72      0.78      0.75       119\n",
      "  HS_Physical       0.44      0.23      0.30        53\n",
      "    HS_Gender       0.80      0.34      0.48        58\n",
      "     HS_Other       0.84      0.75      0.79       779\n",
      "      HS_Weak       0.71      0.75      0.73       686\n",
      "  HS_Moderate       0.65      0.56      0.60       356\n",
      "    HS_Strong       0.79      0.78      0.79        99\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5608\n",
      "    macro avg       0.74      0.67      0.69      5608\n",
      " weighted avg       0.79      0.77      0.78      5608\n",
      "  samples avg       0.44      0.43      0.42      5608\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 0.26421022415161133 seconds\n",
      "\n",
      "Fold 1 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8848, F1 Micro: 0.5981, F1 Macro: 0.2905\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.908, F1 Micro: 0.7077, F1 Macro: 0.4913\n",
      "Epoch 3/10, Train Loss: 0.2184, Accuracy: 0.9181, F1 Micro: 0.7598, F1 Macro: 0.5945\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.9226, F1 Micro: 0.7551, F1 Macro: 0.5935\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9219, F1 Micro: 0.7802, F1 Macro: 0.6247\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.9247, F1 Micro: 0.781, F1 Macro: 0.6373\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9251, F1 Micro: 0.7828, F1 Macro: 0.6524\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9255, F1 Micro: 0.7785, F1 Macro: 0.679\n",
      "Epoch 9/10, Train Loss: 0.0782, Accuracy: 0.9241, F1 Micro: 0.7828, F1 Macro: 0.6614\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9256, F1 Micro: 0.78, F1 Macro: 0.6921\n",
      "Best result for 9218 samples: F1 Micro: 0.7828\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1141\n",
      "      Abusive       0.91      0.89      0.90      1012\n",
      "HS_Individual       0.73      0.78      0.75       737\n",
      "     HS_Group       0.69      0.64      0.67       404\n",
      "  HS_Religion       0.71      0.68      0.69       164\n",
      "      HS_Race       0.72      0.78      0.75       119\n",
      "  HS_Physical       0.75      0.11      0.20        53\n",
      "    HS_Gender       0.78      0.12      0.21        58\n",
      "     HS_Other       0.78      0.83      0.81       779\n",
      "      HS_Weak       0.70      0.76      0.73       686\n",
      "  HS_Moderate       0.63      0.54      0.58       356\n",
      "    HS_Strong       0.75      0.84      0.79        99\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5608\n",
      "    macro avg       0.75      0.66      0.66      5608\n",
      " weighted avg       0.78      0.79      0.78      5608\n",
      "  samples avg       0.45      0.45      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019931793212890625 seconds\n",
      "\n",
      "Fold 1 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3937, Accuracy: 0.8845, F1 Micro: 0.5848, F1 Macro: 0.2894\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9057, F1 Micro: 0.7185, F1 Macro: 0.4704\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9142, F1 Micro: 0.7537, F1 Macro: 0.5882\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9231, F1 Micro: 0.7656, F1 Macro: 0.6073\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.925, F1 Micro: 0.7668, F1 Macro: 0.6231\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9252, F1 Micro: 0.7693, F1 Macro: 0.6286\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9233, F1 Micro: 0.7708, F1 Macro: 0.634\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9253, F1 Micro: 0.7858, F1 Macro: 0.6689\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9244, F1 Micro: 0.7776, F1 Macro: 0.6763\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.925, F1 Micro: 0.7792, F1 Macro: 0.6747\n",
      "Best result for 9418 samples: F1 Micro: 0.7858\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1141\n",
      "      Abusive       0.88      0.92      0.90      1012\n",
      "HS_Individual       0.72      0.76      0.74       737\n",
      "     HS_Group       0.71      0.66      0.68       404\n",
      "  HS_Religion       0.70      0.71      0.71       164\n",
      "      HS_Race       0.72      0.74      0.73       119\n",
      "  HS_Physical       0.70      0.13      0.22        53\n",
      "    HS_Gender       0.75      0.16      0.26        58\n",
      "     HS_Other       0.80      0.83      0.81       779\n",
      "      HS_Weak       0.70      0.75      0.72       686\n",
      "  HS_Moderate       0.66      0.61      0.63       356\n",
      "    HS_Strong       0.79      0.72      0.75        99\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5608\n",
      "    macro avg       0.75      0.65      0.67      5608\n",
      " weighted avg       0.78      0.79      0.78      5608\n",
      "  samples avg       0.45      0.45      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001857280731201172 seconds\n",
      "\n",
      "Fold 1 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9618 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3876, Accuracy: 0.8846, F1 Micro: 0.6018, F1 Macro: 0.2874\n",
      "Epoch 2/10, Train Loss: 0.2652, Accuracy: 0.903, F1 Micro: 0.6604, F1 Macro: 0.4293\n",
      "Epoch 3/10, Train Loss: 0.2102, Accuracy: 0.9198, F1 Micro: 0.7552, F1 Macro: 0.5777\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9235, F1 Micro: 0.7726, F1 Macro: 0.6152\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.924, F1 Micro: 0.7801, F1 Macro: 0.6454\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9233, F1 Micro: 0.7799, F1 Macro: 0.6575\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9264, F1 Micro: 0.7858, F1 Macro: 0.655\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9261, F1 Micro: 0.7814, F1 Macro: 0.678\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9266, F1 Micro: 0.7856, F1 Macro: 0.6899\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9271, F1 Micro: 0.7868, F1 Macro: 0.7068\n",
      "Best result for 9618 samples: F1 Micro: 0.7868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1141\n",
      "      Abusive       0.90      0.91      0.90      1012\n",
      "HS_Individual       0.74      0.75      0.74       737\n",
      "     HS_Group       0.74      0.64      0.68       404\n",
      "  HS_Religion       0.77      0.66      0.71       164\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.64      0.30      0.41        53\n",
      "    HS_Gender       0.69      0.34      0.46        58\n",
      "     HS_Other       0.81      0.80      0.81       779\n",
      "      HS_Weak       0.71      0.73      0.72       686\n",
      "  HS_Moderate       0.69      0.57      0.62       356\n",
      "    HS_Strong       0.81      0.80      0.80        99\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5608\n",
      "    macro avg       0.76      0.68      0.71      5608\n",
      " weighted avg       0.80      0.77      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00035190582275390625 seconds\n",
      "\n",
      "Fold 1 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 9818 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3877, Accuracy: 0.8836, F1 Micro: 0.5718, F1 Macro: 0.2724\n",
      "Epoch 2/10, Train Loss: 0.261, Accuracy: 0.9105, F1 Micro: 0.718, F1 Macro: 0.489\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.9195, F1 Micro: 0.7582, F1 Macro: 0.5888\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.922, F1 Micro: 0.7742, F1 Macro: 0.619\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9266, F1 Micro: 0.7835, F1 Macro: 0.6415\n",
      "Epoch 6/10, Train Loss: 0.1256, Accuracy: 0.9276, F1 Micro: 0.7834, F1 Macro: 0.6537\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9246, F1 Micro: 0.7757, F1 Macro: 0.6483\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9254, F1 Micro: 0.7843, F1 Macro: 0.679\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9285, F1 Micro: 0.7842, F1 Macro: 0.6722\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9268, F1 Micro: 0.7869, F1 Macro: 0.7057\n",
      "Best result for 9818 samples: F1 Micro: 0.7869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1141\n",
      "      Abusive       0.88      0.92      0.90      1012\n",
      "HS_Individual       0.73      0.79      0.76       737\n",
      "     HS_Group       0.77      0.60      0.67       404\n",
      "  HS_Religion       0.74      0.65      0.69       164\n",
      "      HS_Race       0.79      0.71      0.74       119\n",
      "  HS_Physical       0.62      0.28      0.39        53\n",
      "    HS_Gender       0.73      0.41      0.53        58\n",
      "     HS_Other       0.82      0.79      0.80       779\n",
      "      HS_Weak       0.70      0.76      0.73       686\n",
      "  HS_Moderate       0.70      0.53      0.60       356\n",
      "    HS_Strong       0.80      0.79      0.80        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.76      0.67      0.71      5608\n",
      " weighted avg       0.79      0.78      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002605915069580078 seconds\n",
      "\n",
      "Fold 1 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10018 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3868, Accuracy: 0.8866, F1 Micro: 0.6524, F1 Macro: 0.326\n",
      "Epoch 2/10, Train Loss: 0.2569, Accuracy: 0.9096, F1 Micro: 0.6988, F1 Macro: 0.4859\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9199, F1 Micro: 0.7568, F1 Macro: 0.5834\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9242, F1 Micro: 0.7732, F1 Macro: 0.6205\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9258, F1 Micro: 0.7816, F1 Macro: 0.6439\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9269, F1 Micro: 0.7746, F1 Macro: 0.6245\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9276, F1 Micro: 0.7896, F1 Macro: 0.6539\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9274, F1 Micro: 0.7877, F1 Macro: 0.6897\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9285, F1 Micro: 0.7893, F1 Macro: 0.6966\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9289, F1 Micro: 0.7886, F1 Macro: 0.702\n",
      "Best result for 10018 samples: F1 Micro: 0.7896\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1141\n",
      "      Abusive       0.90      0.91      0.90      1012\n",
      "HS_Individual       0.74      0.78      0.76       737\n",
      "     HS_Group       0.71      0.62      0.66       404\n",
      "  HS_Religion       0.83      0.56      0.67       164\n",
      "      HS_Race       0.80      0.70      0.74       119\n",
      "  HS_Physical       1.00      0.06      0.11        53\n",
      "    HS_Gender       0.75      0.10      0.18        58\n",
      "     HS_Other       0.80      0.84      0.82       779\n",
      "      HS_Weak       0.72      0.76      0.74       686\n",
      "  HS_Moderate       0.65      0.56      0.60       356\n",
      "    HS_Strong       0.81      0.78      0.79        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.80      0.63      0.65      5608\n",
      " weighted avg       0.80      0.78      0.78      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018596649169921875 seconds\n",
      "\n",
      "Fold 1 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3823, Accuracy: 0.8885, F1 Micro: 0.6267, F1 Macro: 0.3065\n",
      "Epoch 2/10, Train Loss: 0.26, Accuracy: 0.9099, F1 Micro: 0.7046, F1 Macro: 0.5249\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9197, F1 Micro: 0.7522, F1 Macro: 0.5852\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.9249, F1 Micro: 0.7698, F1 Macro: 0.607\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9263, F1 Micro: 0.7808, F1 Macro: 0.638\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.9266, F1 Micro: 0.778, F1 Macro: 0.6403\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9274, F1 Micro: 0.7851, F1 Macro: 0.6466\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9273, F1 Micro: 0.7836, F1 Macro: 0.6489\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9285, F1 Micro: 0.7904, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9261, F1 Micro: 0.7841, F1 Macro: 0.6804\n",
      "Best result for 10218 samples: F1 Micro: 0.7904\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1141\n",
      "      Abusive       0.90      0.90      0.90      1012\n",
      "HS_Individual       0.78      0.73      0.75       737\n",
      "     HS_Group       0.70      0.71      0.70       404\n",
      "  HS_Religion       0.67      0.69      0.68       164\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.65      0.21      0.31        53\n",
      "    HS_Gender       0.71      0.38      0.49        58\n",
      "     HS_Other       0.83      0.79      0.81       779\n",
      "      HS_Weak       0.76      0.70      0.73       686\n",
      "  HS_Moderate       0.64      0.64      0.64       356\n",
      "    HS_Strong       0.80      0.82      0.81        99\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5608\n",
      "    macro avg       0.75      0.68      0.70      5608\n",
      " weighted avg       0.80      0.78      0.79      5608\n",
      "  samples avg       0.45      0.44      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017547607421875 seconds\n",
      "\n",
      "Fold 1 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.8819, F1 Micro: 0.5484, F1 Macro: 0.261\n",
      "Epoch 2/10, Train Loss: 0.2525, Accuracy: 0.9123, F1 Micro: 0.7284, F1 Macro: 0.5371\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9218, F1 Micro: 0.7543, F1 Macro: 0.5869\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9263, F1 Micro: 0.7765, F1 Macro: 0.6191\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9277, F1 Micro: 0.7845, F1 Macro: 0.6461\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9241, F1 Micro: 0.7859, F1 Macro: 0.6422\n",
      "Epoch 7/10, Train Loss: 0.1024, Accuracy: 0.9273, F1 Micro: 0.7906, F1 Macro: 0.6601\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9242, F1 Micro: 0.787, F1 Macro: 0.6909\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9275, F1 Micro: 0.7828, F1 Macro: 0.6963\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9287, F1 Micro: 0.7925, F1 Macro: 0.7094\n",
      "Best result for 10418 samples: F1 Micro: 0.7925\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1141\n",
      "      Abusive       0.91      0.91      0.91      1012\n",
      "HS_Individual       0.76      0.77      0.76       737\n",
      "     HS_Group       0.73      0.66      0.69       404\n",
      "  HS_Religion       0.70      0.67      0.68       164\n",
      "      HS_Race       0.74      0.74      0.74       119\n",
      "  HS_Physical       0.71      0.32      0.44        53\n",
      "    HS_Gender       0.69      0.34      0.46        58\n",
      "     HS_Other       0.82      0.81      0.81       779\n",
      "      HS_Weak       0.73      0.73      0.73       686\n",
      "  HS_Moderate       0.67      0.58      0.62       356\n",
      "    HS_Strong       0.77      0.83      0.80        99\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5608\n",
      "    macro avg       0.76      0.69      0.71      5608\n",
      " weighted avg       0.80      0.78      0.79      5608\n",
      "  samples avg       0.45      0.45      0.43      5608\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 0.2968621253967285 seconds\n",
      "\n",
      "Fold 1 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 10535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3829, Accuracy: 0.8893, F1 Micro: 0.6514, F1 Macro: 0.3327\n",
      "Epoch 2/10, Train Loss: 0.2559, Accuracy: 0.9124, F1 Micro: 0.734, F1 Macro: 0.5389\n",
      "Epoch 3/10, Train Loss: 0.2033, Accuracy: 0.9194, F1 Micro: 0.7657, F1 Macro: 0.6034\n",
      "Epoch 4/10, Train Loss: 0.1711, Accuracy: 0.9242, F1 Micro: 0.7688, F1 Macro: 0.6014\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9257, F1 Micro: 0.7691, F1 Macro: 0.6166\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9282, F1 Micro: 0.7887, F1 Macro: 0.6541\n",
      "Epoch 7/10, Train Loss: 0.1009, Accuracy: 0.9271, F1 Micro: 0.7842, F1 Macro: 0.684\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9282, F1 Micro: 0.7934, F1 Macro: 0.6987\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9259, F1 Micro: 0.7833, F1 Macro: 0.6967\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9272, F1 Micro: 0.7886, F1 Macro: 0.7068\n",
      "Best result for 10535 samples: F1 Micro: 0.7934\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1141\n",
      "      Abusive       0.90      0.91      0.90      1012\n",
      "HS_Individual       0.75      0.76      0.76       737\n",
      "     HS_Group       0.71      0.69      0.70       404\n",
      "  HS_Religion       0.74      0.66      0.70       164\n",
      "      HS_Race       0.79      0.70      0.74       119\n",
      "  HS_Physical       0.65      0.25      0.36        53\n",
      "    HS_Gender       0.76      0.28      0.41        58\n",
      "     HS_Other       0.80      0.84      0.82       779\n",
      "      HS_Weak       0.72      0.74      0.73       686\n",
      "  HS_Moderate       0.66      0.63      0.64       356\n",
      "    HS_Strong       0.81      0.73      0.77        99\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5608\n",
      "    macro avg       0.76      0.67      0.70      5608\n",
      " weighted avg       0.79      0.79      0.79      5608\n",
      "  samples avg       0.45      0.45      0.44      5608\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 5792.40 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 658 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6074, Accuracy: 0.8369, F1 Micro: 0.2789, F1 Macro: 0.0958\n",
      "Epoch 2/10, Train Loss: 0.4725, Accuracy: 0.8453, F1 Micro: 0.2388, F1 Macro: 0.078\n",
      "Epoch 3/10, Train Loss: 0.414, Accuracy: 0.8392, F1 Micro: 0.1207, F1 Macro: 0.0438\n",
      "Epoch 4/10, Train Loss: 0.3877, Accuracy: 0.849, F1 Micro: 0.2496, F1 Macro: 0.0847\n",
      "Epoch 5/10, Train Loss: 0.3756, Accuracy: 0.8507, F1 Micro: 0.271, F1 Macro: 0.0898\n",
      "Epoch 6/10, Train Loss: 0.3627, Accuracy: 0.8593, F1 Micro: 0.3917, F1 Macro: 0.1349\n",
      "Epoch 7/10, Train Loss: 0.347, Accuracy: 0.8698, F1 Micro: 0.4861, F1 Macro: 0.2087\n",
      "Epoch 8/10, Train Loss: 0.3046, Accuracy: 0.8757, F1 Micro: 0.576, F1 Macro: 0.2681\n",
      "Epoch 9/10, Train Loss: 0.295, Accuracy: 0.8795, F1 Micro: 0.5968, F1 Macro: 0.3013\n",
      "Epoch 10/10, Train Loss: 0.2598, Accuracy: 0.8811, F1 Micro: 0.6072, F1 Macro: 0.318\n",
      "Best result for 658 samples: F1 Micro: 0.6072\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.76      0.75      1094\n",
      "      Abusive       0.81      0.73      0.77      1072\n",
      "HS_Individual       0.60      0.58      0.59       689\n",
      "     HS_Group       0.66      0.29      0.40       405\n",
      "  HS_Religion       0.00      0.00      0.00       124\n",
      "      HS_Race       0.00      0.00      0.00       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.64      0.59      0.62       754\n",
      "      HS_Weak       0.59      0.55      0.57       664\n",
      "  HS_Moderate       0.48      0.07      0.12       346\n",
      "    HS_Strong       0.00      0.00      0.00        84\n",
      "\n",
      "    micro avg       0.69      0.54      0.61      5476\n",
      "    macro avg       0.38      0.30      0.32      5476\n",
      " weighted avg       0.62      0.54      0.57      5476\n",
      "  samples avg       0.37      0.32      0.32      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 0.0005939006805419922 seconds\n",
      "\n",
      "Fold 2 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1646 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5313, Accuracy: 0.8426, F1 Micro: 0.1968, F1 Macro: 0.066\n",
      "Epoch 2/10, Train Loss: 0.3966, Accuracy: 0.8515, F1 Micro: 0.3148, F1 Macro: 0.1006\n",
      "Epoch 3/10, Train Loss: 0.3677, Accuracy: 0.8612, F1 Micro: 0.4346, F1 Macro: 0.1635\n",
      "Epoch 4/10, Train Loss: 0.3359, Accuracy: 0.8753, F1 Micro: 0.5286, F1 Macro: 0.2421\n",
      "Epoch 5/10, Train Loss: 0.2921, Accuracy: 0.8816, F1 Micro: 0.6101, F1 Macro: 0.3005\n",
      "Epoch 6/10, Train Loss: 0.2675, Accuracy: 0.8877, F1 Micro: 0.6271, F1 Macro: 0.3322\n",
      "Epoch 7/10, Train Loss: 0.2358, Accuracy: 0.8908, F1 Micro: 0.6542, F1 Macro: 0.3997\n",
      "Epoch 8/10, Train Loss: 0.2152, Accuracy: 0.8937, F1 Micro: 0.666, F1 Macro: 0.436\n",
      "Epoch 9/10, Train Loss: 0.1858, Accuracy: 0.8932, F1 Micro: 0.6331, F1 Macro: 0.3966\n",
      "Epoch 10/10, Train Loss: 0.1649, Accuracy: 0.8976, F1 Micro: 0.6798, F1 Macro: 0.4717\n",
      "Best result for 1646 samples: F1 Micro: 0.6798\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.79      0.79      1094\n",
      "      Abusive       0.86      0.82      0.84      1072\n",
      "HS_Individual       0.63      0.65      0.64       689\n",
      "     HS_Group       0.68      0.50      0.58       405\n",
      "  HS_Religion       0.68      0.35      0.46       124\n",
      "      HS_Race       0.92      0.27      0.42       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.68      0.66      0.67       754\n",
      "      HS_Weak       0.61      0.62      0.62       664\n",
      "  HS_Moderate       0.58      0.36      0.44       346\n",
      "    HS_Strong       0.83      0.12      0.21        84\n",
      "\n",
      "    micro avg       0.72      0.64      0.68      5476\n",
      "    macro avg       0.61      0.43      0.47      5476\n",
      " weighted avg       0.71      0.64      0.66      5476\n",
      "  samples avg       0.39      0.37      0.36      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 0.0006151199340820312 seconds\n",
      "\n",
      "Fold 2 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.485, Accuracy: 0.8423, F1 Micro: 0.1694, F1 Macro: 0.0614\n",
      "Epoch 2/10, Train Loss: 0.3829, Accuracy: 0.8563, F1 Micro: 0.3287, F1 Macro: 0.1169\n",
      "Epoch 3/10, Train Loss: 0.3255, Accuracy: 0.8787, F1 Micro: 0.5797, F1 Macro: 0.2725\n",
      "Epoch 4/10, Train Loss: 0.2876, Accuracy: 0.8887, F1 Micro: 0.6425, F1 Macro: 0.3383\n",
      "Epoch 5/10, Train Loss: 0.2538, Accuracy: 0.894, F1 Micro: 0.6491, F1 Macro: 0.3743\n",
      "Epoch 6/10, Train Loss: 0.224, Accuracy: 0.8961, F1 Micro: 0.6704, F1 Macro: 0.435\n",
      "Epoch 7/10, Train Loss: 0.1988, Accuracy: 0.8983, F1 Micro: 0.6907, F1 Macro: 0.4606\n",
      "Epoch 8/10, Train Loss: 0.1699, Accuracy: 0.9014, F1 Micro: 0.7007, F1 Macro: 0.4895\n",
      "Epoch 9/10, Train Loss: 0.1504, Accuracy: 0.9028, F1 Micro: 0.7086, F1 Macro: 0.5225\n",
      "Epoch 10/10, Train Loss: 0.1335, Accuracy: 0.9029, F1 Micro: 0.7036, F1 Macro: 0.5385\n",
      "Best result for 2535 samples: F1 Micro: 0.7086\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.83      0.80      1094\n",
      "      Abusive       0.87      0.85      0.86      1072\n",
      "HS_Individual       0.63      0.72      0.67       689\n",
      "     HS_Group       0.71      0.54      0.62       405\n",
      "  HS_Religion       0.72      0.42      0.53       124\n",
      "      HS_Race       0.83      0.31      0.45       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.67      0.73      0.70       754\n",
      "      HS_Weak       0.61      0.70      0.66       664\n",
      "  HS_Moderate       0.58      0.38      0.46       346\n",
      "    HS_Strong       0.89      0.37      0.52        84\n",
      "\n",
      "    micro avg       0.72      0.70      0.71      5476\n",
      "    macro avg       0.61      0.49      0.52      5476\n",
      " weighted avg       0.71      0.70      0.69      5476\n",
      "  samples avg       0.41      0.40      0.38      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 0.0004813671112060547 seconds\n",
      "\n",
      "Fold 2 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3335 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4631, Accuracy: 0.8404, F1 Micro: 0.1334, F1 Macro: 0.0493\n",
      "Epoch 2/10, Train Loss: 0.3517, Accuracy: 0.8771, F1 Micro: 0.526, F1 Macro: 0.2371\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.888, F1 Micro: 0.6037, F1 Macro: 0.3081\n",
      "Epoch 4/10, Train Loss: 0.2497, Accuracy: 0.8935, F1 Micro: 0.6428, F1 Macro: 0.3621\n",
      "Epoch 5/10, Train Loss: 0.2217, Accuracy: 0.8972, F1 Micro: 0.63, F1 Macro: 0.4219\n",
      "Epoch 6/10, Train Loss: 0.1855, Accuracy: 0.9045, F1 Micro: 0.7096, F1 Macro: 0.5018\n",
      "Epoch 7/10, Train Loss: 0.1567, Accuracy: 0.9039, F1 Micro: 0.705, F1 Macro: 0.4789\n",
      "Epoch 8/10, Train Loss: 0.1373, Accuracy: 0.9049, F1 Micro: 0.7272, F1 Macro: 0.5577\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9062, F1 Micro: 0.7202, F1 Macro: 0.5444\n",
      "Epoch 10/10, Train Loss: 0.1077, Accuracy: 0.9081, F1 Micro: 0.7266, F1 Macro: 0.5674\n",
      "Best result for 3335 samples: F1 Micro: 0.7272\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.88      0.81      1094\n",
      "      Abusive       0.85      0.87      0.86      1072\n",
      "HS_Individual       0.62      0.79      0.70       689\n",
      "     HS_Group       0.68      0.54      0.60       405\n",
      "  HS_Religion       0.67      0.59      0.63       124\n",
      "      HS_Race       0.81      0.47      0.60       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.67      0.78      0.72       754\n",
      "      HS_Weak       0.61      0.78      0.69       664\n",
      "  HS_Moderate       0.60      0.44      0.51       346\n",
      "    HS_Strong       0.84      0.44      0.58        84\n",
      "\n",
      "    micro avg       0.71      0.75      0.73      5476\n",
      "    macro avg       0.59      0.55      0.56      5476\n",
      " weighted avg       0.70      0.75      0.71      5476\n",
      "  samples avg       0.42      0.43      0.40      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 0.00048732757568359375 seconds\n",
      "\n",
      "Fold 2 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4055 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4452, Accuracy: 0.8468, F1 Micro: 0.2219, F1 Macro: 0.0733\n",
      "Epoch 2/10, Train Loss: 0.3397, Accuracy: 0.8826, F1 Micro: 0.5853, F1 Macro: 0.2799\n",
      "Epoch 3/10, Train Loss: 0.2759, Accuracy: 0.8941, F1 Micro: 0.6394, F1 Macro: 0.351\n",
      "Epoch 4/10, Train Loss: 0.2395, Accuracy: 0.8988, F1 Micro: 0.6558, F1 Macro: 0.4245\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.9049, F1 Micro: 0.7073, F1 Macro: 0.5113\n",
      "Epoch 6/10, Train Loss: 0.1701, Accuracy: 0.9065, F1 Micro: 0.7231, F1 Macro: 0.5544\n",
      "Epoch 7/10, Train Loss: 0.1459, Accuracy: 0.9076, F1 Micro: 0.7251, F1 Macro: 0.5557\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9059, F1 Micro: 0.7235, F1 Macro: 0.5368\n",
      "Epoch 9/10, Train Loss: 0.1124, Accuracy: 0.9112, F1 Micro: 0.719, F1 Macro: 0.5723\n",
      "Epoch 10/10, Train Loss: 0.0945, Accuracy: 0.9129, F1 Micro: 0.7353, F1 Macro: 0.5913\n",
      "Best result for 4055 samples: F1 Micro: 0.7353\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.83      0.81      1094\n",
      "      Abusive       0.90      0.85      0.87      1072\n",
      "HS_Individual       0.67      0.74      0.70       689\n",
      "     HS_Group       0.72      0.54      0.62       405\n",
      "  HS_Religion       0.68      0.57      0.62       124\n",
      "      HS_Race       0.87      0.57      0.69       125\n",
      "  HS_Physical       1.00      0.05      0.09        61\n",
      "    HS_Gender       0.67      0.03      0.07        58\n",
      "     HS_Other       0.75      0.72      0.74       754\n",
      "      HS_Weak       0.65      0.71      0.68       664\n",
      "  HS_Moderate       0.63      0.44      0.52       346\n",
      "    HS_Strong       0.83      0.58      0.69        84\n",
      "\n",
      "    micro avg       0.76      0.71      0.74      5476\n",
      "    macro avg       0.76      0.55      0.59      5476\n",
      " weighted avg       0.76      0.71      0.73      5476\n",
      "  samples avg       0.43      0.41      0.40      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 0.0004563331604003906 seconds\n",
      "\n",
      "Fold 2 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4703 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4344, Accuracy: 0.8572, F1 Micro: 0.3664, F1 Macro: 0.1142\n",
      "Epoch 2/10, Train Loss: 0.3168, Accuracy: 0.8825, F1 Micro: 0.6522, F1 Macro: 0.3391\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.898, F1 Micro: 0.6816, F1 Macro: 0.4605\n",
      "Epoch 4/10, Train Loss: 0.2163, Accuracy: 0.9025, F1 Micro: 0.7034, F1 Macro: 0.4755\n",
      "Epoch 5/10, Train Loss: 0.1899, Accuracy: 0.9062, F1 Micro: 0.7242, F1 Macro: 0.5153\n",
      "Epoch 6/10, Train Loss: 0.1672, Accuracy: 0.9092, F1 Micro: 0.7288, F1 Macro: 0.55\n",
      "Epoch 7/10, Train Loss: 0.1424, Accuracy: 0.9078, F1 Micro: 0.7323, F1 Macro: 0.5571\n",
      "Epoch 8/10, Train Loss: 0.1175, Accuracy: 0.9107, F1 Micro: 0.7335, F1 Macro: 0.5737\n",
      "Epoch 9/10, Train Loss: 0.1009, Accuracy: 0.91, F1 Micro: 0.7214, F1 Macro: 0.557\n",
      "Epoch 10/10, Train Loss: 0.0868, Accuracy: 0.9119, F1 Micro: 0.728, F1 Macro: 0.5946\n",
      "Best result for 4703 samples: F1 Micro: 0.7335\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.84      0.81      1094\n",
      "      Abusive       0.91      0.84      0.87      1072\n",
      "HS_Individual       0.66      0.74      0.69       689\n",
      "     HS_Group       0.69      0.58      0.63       405\n",
      "  HS_Religion       0.66      0.57      0.61       124\n",
      "      HS_Race       0.85      0.50      0.63       125\n",
      "  HS_Physical       0.00      0.00      0.00        61\n",
      "    HS_Gender       0.00      0.00      0.00        58\n",
      "     HS_Other       0.71      0.77      0.74       754\n",
      "      HS_Weak       0.64      0.73      0.68       664\n",
      "  HS_Moderate       0.59      0.47      0.52       346\n",
      "    HS_Strong       0.83      0.58      0.69        84\n",
      "\n",
      "    micro avg       0.74      0.72      0.73      5476\n",
      "    macro avg       0.61      0.55      0.57      5476\n",
      " weighted avg       0.73      0.72      0.72      5476\n",
      "  samples avg       0.41      0.41      0.39      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.00043582916259765625 seconds\n",
      "\n",
      "Fold 2 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5287 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.433, Accuracy: 0.8622, F1 Micro: 0.4205, F1 Macro: 0.1435\n",
      "Epoch 2/10, Train Loss: 0.312, Accuracy: 0.8908, F1 Micro: 0.6526, F1 Macro: 0.3543\n",
      "Epoch 3/10, Train Loss: 0.2524, Accuracy: 0.8998, F1 Micro: 0.6974, F1 Macro: 0.4761\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9082, F1 Micro: 0.7059, F1 Macro: 0.5094\n",
      "Epoch 5/10, Train Loss: 0.1808, Accuracy: 0.9105, F1 Micro: 0.7293, F1 Macro: 0.5483\n",
      "Epoch 6/10, Train Loss: 0.1526, Accuracy: 0.9136, F1 Micro: 0.7336, F1 Macro: 0.5637\n",
      "Epoch 7/10, Train Loss: 0.1303, Accuracy: 0.9138, F1 Micro: 0.7279, F1 Macro: 0.5907\n",
      "Epoch 8/10, Train Loss: 0.1118, Accuracy: 0.9092, F1 Micro: 0.746, F1 Macro: 0.6146\n",
      "Epoch 9/10, Train Loss: 0.0926, Accuracy: 0.9138, F1 Micro: 0.7375, F1 Macro: 0.6058\n",
      "Epoch 10/10, Train Loss: 0.0794, Accuracy: 0.9149, F1 Micro: 0.7426, F1 Macro: 0.6195\n",
      "Best result for 5287 samples: F1 Micro: 0.746\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.91      0.82      1094\n",
      "      Abusive       0.87      0.89      0.88      1072\n",
      "HS_Individual       0.62      0.77      0.69       689\n",
      "     HS_Group       0.65      0.68      0.66       405\n",
      "  HS_Religion       0.65      0.67      0.66       124\n",
      "      HS_Race       0.81      0.66      0.73       125\n",
      "  HS_Physical       0.60      0.05      0.09        61\n",
      "    HS_Gender       0.44      0.07      0.12        58\n",
      "     HS_Other       0.71      0.81      0.76       754\n",
      "      HS_Weak       0.61      0.76      0.68       664\n",
      "  HS_Moderate       0.57      0.58      0.57       346\n",
      "    HS_Strong       0.79      0.65      0.71        84\n",
      "\n",
      "    micro avg       0.71      0.79      0.75      5476\n",
      "    macro avg       0.67      0.63      0.61      5476\n",
      " weighted avg       0.71      0.79      0.74      5476\n",
      "  samples avg       0.43      0.45      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.00040221214294433594 seconds\n",
      "\n",
      "Fold 2 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5812 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4198, Accuracy: 0.8651, F1 Micro: 0.3975, F1 Macro: 0.1599\n",
      "Epoch 2/10, Train Loss: 0.2957, Accuracy: 0.8934, F1 Micro: 0.631, F1 Macro: 0.3441\n",
      "Epoch 3/10, Train Loss: 0.2443, Accuracy: 0.9025, F1 Micro: 0.695, F1 Macro: 0.4535\n",
      "Epoch 4/10, Train Loss: 0.2066, Accuracy: 0.9098, F1 Micro: 0.721, F1 Macro: 0.5435\n",
      "Epoch 5/10, Train Loss: 0.1777, Accuracy: 0.9116, F1 Micro: 0.7327, F1 Macro: 0.5657\n",
      "Epoch 6/10, Train Loss: 0.1456, Accuracy: 0.9152, F1 Micro: 0.729, F1 Macro: 0.5742\n",
      "Epoch 7/10, Train Loss: 0.1254, Accuracy: 0.9152, F1 Micro: 0.7486, F1 Macro: 0.6046\n",
      "Epoch 8/10, Train Loss: 0.1071, Accuracy: 0.9157, F1 Micro: 0.7462, F1 Macro: 0.6058\n",
      "Epoch 9/10, Train Loss: 0.0911, Accuracy: 0.915, F1 Micro: 0.7499, F1 Macro: 0.6163\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.912, F1 Micro: 0.747, F1 Macro: 0.6389\n",
      "Best result for 5812 samples: F1 Micro: 0.7499\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.85      0.82      1094\n",
      "      Abusive       0.88      0.89      0.88      1072\n",
      "HS_Individual       0.65      0.79      0.71       689\n",
      "     HS_Group       0.75      0.55      0.63       405\n",
      "  HS_Religion       0.67      0.60      0.63       124\n",
      "      HS_Race       0.85      0.62      0.72       125\n",
      "  HS_Physical       0.50      0.07      0.12        61\n",
      "    HS_Gender       0.62      0.09      0.15        58\n",
      "     HS_Other       0.73      0.75      0.74       754\n",
      "      HS_Weak       0.64      0.78      0.70       664\n",
      "  HS_Moderate       0.67      0.46      0.54       346\n",
      "    HS_Strong       0.82      0.67      0.74        84\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5476\n",
      "    macro avg       0.72      0.59      0.62      5476\n",
      " weighted avg       0.75      0.75      0.74      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.00036525726318359375 seconds\n",
      "\n",
      "Fold 2 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6285 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4152, Accuracy: 0.8743, F1 Micro: 0.5649, F1 Macro: 0.2526\n",
      "Epoch 2/10, Train Loss: 0.2918, Accuracy: 0.8954, F1 Micro: 0.681, F1 Macro: 0.4222\n",
      "Epoch 3/10, Train Loss: 0.2326, Accuracy: 0.9045, F1 Micro: 0.6896, F1 Macro: 0.4933\n",
      "Epoch 4/10, Train Loss: 0.193, Accuracy: 0.9107, F1 Micro: 0.7367, F1 Macro: 0.5669\n",
      "Epoch 5/10, Train Loss: 0.1656, Accuracy: 0.9165, F1 Micro: 0.7497, F1 Macro: 0.5928\n",
      "Epoch 6/10, Train Loss: 0.1397, Accuracy: 0.9152, F1 Micro: 0.7547, F1 Macro: 0.6147\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.9172, F1 Micro: 0.7445, F1 Macro: 0.5967\n",
      "Epoch 8/10, Train Loss: 0.1021, Accuracy: 0.9174, F1 Micro: 0.7382, F1 Macro: 0.6051\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.918, F1 Micro: 0.7559, F1 Macro: 0.6289\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9185, F1 Micro: 0.7507, F1 Macro: 0.6375\n",
      "Best result for 6285 samples: F1 Micro: 0.7559\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1094\n",
      "      Abusive       0.91      0.86      0.88      1072\n",
      "HS_Individual       0.68      0.76      0.72       689\n",
      "     HS_Group       0.71      0.56      0.63       405\n",
      "  HS_Religion       0.76      0.58      0.66       124\n",
      "      HS_Race       0.84      0.66      0.74       125\n",
      "  HS_Physical       0.56      0.08      0.14        61\n",
      "    HS_Gender       0.50      0.10      0.17        58\n",
      "     HS_Other       0.73      0.80      0.76       754\n",
      "      HS_Weak       0.67      0.73      0.70       664\n",
      "  HS_Moderate       0.64      0.48      0.55       346\n",
      "    HS_Strong       0.78      0.74      0.76        84\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5476\n",
      "    macro avg       0.72      0.60      0.63      5476\n",
      " weighted avg       0.76      0.75      0.75      5476\n",
      "  samples avg       0.43      0.43      0.41      5476\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 0.18481779098510742 seconds\n",
      "\n",
      "Fold 2 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6584 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4123, Accuracy: 0.8757, F1 Micro: 0.5153, F1 Macro: 0.2294\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8961, F1 Micro: 0.6661, F1 Macro: 0.4431\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.906, F1 Micro: 0.7133, F1 Macro: 0.501\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9101, F1 Micro: 0.7038, F1 Macro: 0.4877\n",
      "Epoch 5/10, Train Loss: 0.1682, Accuracy: 0.9148, F1 Micro: 0.7297, F1 Macro: 0.5682\n",
      "Epoch 6/10, Train Loss: 0.1389, Accuracy: 0.9163, F1 Micro: 0.7533, F1 Macro: 0.6112\n",
      "Epoch 7/10, Train Loss: 0.1179, Accuracy: 0.919, F1 Micro: 0.7482, F1 Macro: 0.6168\n",
      "Epoch 8/10, Train Loss: 0.1037, Accuracy: 0.9158, F1 Micro: 0.7496, F1 Macro: 0.6318\n",
      "Epoch 9/10, Train Loss: 0.0859, Accuracy: 0.9167, F1 Micro: 0.7515, F1 Macro: 0.6195\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9179, F1 Micro: 0.7471, F1 Macro: 0.639\n",
      "Best result for 6584 samples: F1 Micro: 0.7533\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.88      0.83      1094\n",
      "      Abusive       0.90      0.87      0.88      1072\n",
      "HS_Individual       0.67      0.79      0.72       689\n",
      "     HS_Group       0.74      0.58      0.65       405\n",
      "  HS_Religion       0.68      0.60      0.64       124\n",
      "      HS_Race       0.84      0.58      0.69       125\n",
      "  HS_Physical       0.62      0.08      0.14        61\n",
      "    HS_Gender       0.50      0.03      0.06        58\n",
      "     HS_Other       0.73      0.76      0.75       754\n",
      "      HS_Weak       0.65      0.76      0.70       664\n",
      "  HS_Moderate       0.65      0.47      0.55       346\n",
      "    HS_Strong       0.82      0.63      0.71        84\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5476\n",
      "    macro avg       0.72      0.59      0.61      5476\n",
      " weighted avg       0.75      0.75      0.74      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 0.0003666877746582031 seconds\n",
      "\n",
      "Fold 2 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6980 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4103, Accuracy: 0.8783, F1 Micro: 0.5974, F1 Macro: 0.2739\n",
      "Epoch 2/10, Train Loss: 0.284, Accuracy: 0.8968, F1 Micro: 0.6369, F1 Macro: 0.3783\n",
      "Epoch 3/10, Train Loss: 0.2294, Accuracy: 0.9072, F1 Micro: 0.723, F1 Macro: 0.5178\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9131, F1 Micro: 0.7324, F1 Macro: 0.5469\n",
      "Epoch 5/10, Train Loss: 0.1609, Accuracy: 0.9121, F1 Micro: 0.7447, F1 Macro: 0.5736\n",
      "Epoch 6/10, Train Loss: 0.1345, Accuracy: 0.9172, F1 Micro: 0.7355, F1 Macro: 0.6081\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9197, F1 Micro: 0.7515, F1 Macro: 0.6197\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9192, F1 Micro: 0.7549, F1 Macro: 0.6385\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.918, F1 Micro: 0.7582, F1 Macro: 0.6481\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9145, F1 Micro: 0.7578, F1 Macro: 0.6511\n",
      "Best result for 6980 samples: F1 Micro: 0.7582\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1094\n",
      "      Abusive       0.89      0.89      0.89      1072\n",
      "HS_Individual       0.67      0.75      0.71       689\n",
      "     HS_Group       0.70      0.59      0.64       405\n",
      "  HS_Religion       0.67      0.67      0.67       124\n",
      "      HS_Race       0.79      0.61      0.69       125\n",
      "  HS_Physical       0.46      0.18      0.26        61\n",
      "    HS_Gender       0.52      0.19      0.28        58\n",
      "     HS_Other       0.76      0.78      0.77       754\n",
      "      HS_Weak       0.67      0.74      0.70       664\n",
      "  HS_Moderate       0.63      0.51      0.57       346\n",
      "    HS_Strong       0.81      0.73      0.77        84\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5476\n",
      "    macro avg       0.70      0.62      0.65      5476\n",
      " weighted avg       0.76      0.76      0.75      5476\n",
      "  samples avg       0.44      0.44      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 0.00036144256591796875 seconds\n",
      "\n",
      "Fold 2 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4028, Accuracy: 0.8801, F1 Micro: 0.5807, F1 Macro: 0.2724\n",
      "Epoch 2/10, Train Loss: 0.2736, Accuracy: 0.8978, F1 Micro: 0.6841, F1 Macro: 0.4017\n",
      "Epoch 3/10, Train Loss: 0.2212, Accuracy: 0.9096, F1 Micro: 0.7212, F1 Macro: 0.5376\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9158, F1 Micro: 0.7465, F1 Macro: 0.5765\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.9191, F1 Micro: 0.7628, F1 Macro: 0.6114\n",
      "Epoch 6/10, Train Loss: 0.1276, Accuracy: 0.9174, F1 Micro: 0.7451, F1 Macro: 0.6032\n",
      "Epoch 7/10, Train Loss: 0.1103, Accuracy: 0.9217, F1 Micro: 0.7622, F1 Macro: 0.639\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9204, F1 Micro: 0.7616, F1 Macro: 0.654\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9195, F1 Micro: 0.7534, F1 Macro: 0.6643\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9191, F1 Micro: 0.7659, F1 Macro: 0.672\n",
      "Best result for 7336 samples: F1 Micro: 0.7659\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.87      0.83      1094\n",
      "      Abusive       0.87      0.91      0.89      1072\n",
      "HS_Individual       0.68      0.75      0.71       689\n",
      "     HS_Group       0.70      0.66      0.68       405\n",
      "  HS_Religion       0.71      0.66      0.68       124\n",
      "      HS_Race       0.80      0.77      0.78       125\n",
      "  HS_Physical       0.53      0.13      0.21        61\n",
      "    HS_Gender       0.63      0.29      0.40        58\n",
      "     HS_Other       0.73      0.80      0.76       754\n",
      "      HS_Weak       0.67      0.73      0.70       664\n",
      "  HS_Moderate       0.63      0.59      0.61       346\n",
      "    HS_Strong       0.79      0.80      0.79        84\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5476\n",
      "    macro avg       0.71      0.66      0.67      5476\n",
      " weighted avg       0.75      0.78      0.76      5476\n",
      "  samples avg       0.44      0.45      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 0.0002586841583251953 seconds\n",
      "\n",
      "Fold 2 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7656 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8818, F1 Micro: 0.5817, F1 Macro: 0.2776\n",
      "Epoch 2/10, Train Loss: 0.2734, Accuracy: 0.8992, F1 Micro: 0.6929, F1 Macro: 0.4722\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9085, F1 Micro: 0.69, F1 Macro: 0.5144\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9162, F1 Micro: 0.7515, F1 Macro: 0.596\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9186, F1 Micro: 0.7637, F1 Macro: 0.6319\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.9222, F1 Micro: 0.7706, F1 Macro: 0.6411\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9162, F1 Micro: 0.7612, F1 Macro: 0.6164\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9199, F1 Micro: 0.7545, F1 Macro: 0.6444\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9181, F1 Micro: 0.7614, F1 Macro: 0.6549\n",
      "Epoch 10/10, Train Loss: 0.0693, Accuracy: 0.9191, F1 Micro: 0.7591, F1 Macro: 0.6666\n",
      "Best result for 7656 samples: F1 Micro: 0.7706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1094\n",
      "      Abusive       0.87      0.91      0.89      1072\n",
      "HS_Individual       0.69      0.79      0.74       689\n",
      "     HS_Group       0.75      0.58      0.65       405\n",
      "  HS_Religion       0.75      0.66      0.70       124\n",
      "      HS_Race       0.82      0.69      0.75       125\n",
      "  HS_Physical       0.50      0.13      0.21        61\n",
      "    HS_Gender       0.50      0.05      0.09        58\n",
      "     HS_Other       0.76      0.79      0.77       754\n",
      "      HS_Weak       0.68      0.77      0.72       664\n",
      "  HS_Moderate       0.68      0.47      0.55       346\n",
      "    HS_Strong       0.82      0.71      0.76        84\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5476\n",
      "    macro avg       0.72      0.62      0.64      5476\n",
      " weighted avg       0.77      0.77      0.76      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 0.23250150680541992 seconds\n",
      "\n",
      "Fold 2 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 7901 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3977, Accuracy: 0.883, F1 Micro: 0.5859, F1 Macro: 0.2799\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.8996, F1 Micro: 0.68, F1 Macro: 0.4047\n",
      "Epoch 3/10, Train Loss: 0.2186, Accuracy: 0.9098, F1 Micro: 0.7025, F1 Macro: 0.4994\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9162, F1 Micro: 0.7486, F1 Macro: 0.5728\n",
      "Epoch 5/10, Train Loss: 0.1492, Accuracy: 0.9186, F1 Micro: 0.7638, F1 Macro: 0.6123\n",
      "Epoch 6/10, Train Loss: 0.1282, Accuracy: 0.9183, F1 Micro: 0.7635, F1 Macro: 0.6365\n",
      "Epoch 7/10, Train Loss: 0.1106, Accuracy: 0.9199, F1 Micro: 0.757, F1 Macro: 0.6404\n",
      "Epoch 8/10, Train Loss: 0.0966, Accuracy: 0.9192, F1 Micro: 0.7707, F1 Macro: 0.6627\n",
      "Epoch 9/10, Train Loss: 0.0834, Accuracy: 0.9215, F1 Micro: 0.7635, F1 Macro: 0.6629\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9208, F1 Micro: 0.7696, F1 Macro: 0.6797\n",
      "Best result for 7901 samples: F1 Micro: 0.7707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.84      1094\n",
      "      Abusive       0.86      0.92      0.89      1072\n",
      "HS_Individual       0.67      0.80      0.73       689\n",
      "     HS_Group       0.71      0.64      0.68       405\n",
      "  HS_Religion       0.75      0.67      0.71       124\n",
      "      HS_Race       0.82      0.66      0.73       125\n",
      "  HS_Physical       0.32      0.15      0.20        61\n",
      "    HS_Gender       0.59      0.22      0.32        58\n",
      "     HS_Other       0.73      0.83      0.78       754\n",
      "      HS_Weak       0.66      0.79      0.72       664\n",
      "  HS_Moderate       0.63      0.59      0.61       346\n",
      "    HS_Strong       0.79      0.71      0.75        84\n",
      "\n",
      "    micro avg       0.74      0.80      0.77      5476\n",
      "    macro avg       0.69      0.66      0.66      5476\n",
      " weighted avg       0.74      0.80      0.77      5476\n",
      "  samples avg       0.45      0.46      0.44      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 0.0002639293670654297 seconds\n",
      "\n",
      "Fold 2 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8165 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3954, Accuracy: 0.8823, F1 Micro: 0.5863, F1 Macro: 0.287\n",
      "Epoch 2/10, Train Loss: 0.2725, Accuracy: 0.901, F1 Micro: 0.6856, F1 Macro: 0.4722\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.912, F1 Micro: 0.728, F1 Macro: 0.5569\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9171, F1 Micro: 0.7414, F1 Macro: 0.5605\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9138, F1 Micro: 0.7592, F1 Macro: 0.6095\n",
      "Epoch 6/10, Train Loss: 0.129, Accuracy: 0.9224, F1 Micro: 0.764, F1 Macro: 0.6284\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.9199, F1 Micro: 0.7574, F1 Macro: 0.6308\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9202, F1 Micro: 0.7669, F1 Macro: 0.6636\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.9205, F1 Micro: 0.7603, F1 Macro: 0.6662\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9204, F1 Micro: 0.7559, F1 Macro: 0.669\n",
      "Best result for 8165 samples: F1 Micro: 0.7669\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1094\n",
      "      Abusive       0.89      0.90      0.89      1072\n",
      "HS_Individual       0.66      0.79      0.72       689\n",
      "     HS_Group       0.73      0.58      0.65       405\n",
      "  HS_Religion       0.75      0.63      0.68       124\n",
      "      HS_Race       0.78      0.75      0.77       125\n",
      "  HS_Physical       0.42      0.13      0.20        61\n",
      "    HS_Gender       0.59      0.28      0.38        58\n",
      "     HS_Other       0.76      0.78      0.77       754\n",
      "      HS_Weak       0.66      0.77      0.71       664\n",
      "  HS_Moderate       0.68      0.51      0.58       346\n",
      "    HS_Strong       0.80      0.75      0.77        84\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5476\n",
      "    macro avg       0.71      0.64      0.66      5476\n",
      " weighted avg       0.76      0.77      0.76      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 0.0002028942108154297 seconds\n",
      "\n",
      "Fold 2 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8402 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3903, Accuracy: 0.8841, F1 Micro: 0.6128, F1 Macro: 0.2963\n",
      "Epoch 2/10, Train Loss: 0.2635, Accuracy: 0.9026, F1 Micro: 0.6735, F1 Macro: 0.4484\n",
      "Epoch 3/10, Train Loss: 0.2164, Accuracy: 0.9084, F1 Micro: 0.6873, F1 Macro: 0.5008\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.9197, F1 Micro: 0.7513, F1 Macro: 0.5908\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9188, F1 Micro: 0.7605, F1 Macro: 0.6184\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.9202, F1 Micro: 0.7602, F1 Macro: 0.6528\n",
      "Epoch 7/10, Train Loss: 0.1075, Accuracy: 0.9149, F1 Micro: 0.7607, F1 Macro: 0.6517\n",
      "Epoch 8/10, Train Loss: 0.0905, Accuracy: 0.921, F1 Micro: 0.7644, F1 Macro: 0.6666\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9202, F1 Micro: 0.7619, F1 Macro: 0.6654\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9206, F1 Micro: 0.7554, F1 Macro: 0.6755\n",
      "Best result for 8402 samples: F1 Micro: 0.7644\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1094\n",
      "      Abusive       0.91      0.89      0.90      1072\n",
      "HS_Individual       0.67      0.79      0.72       689\n",
      "     HS_Group       0.78      0.55      0.65       405\n",
      "  HS_Religion       0.66      0.70      0.68       124\n",
      "      HS_Race       0.83      0.72      0.77       125\n",
      "  HS_Physical       0.83      0.16      0.27        61\n",
      "    HS_Gender       0.75      0.26      0.38        58\n",
      "     HS_Other       0.78      0.73      0.75       754\n",
      "      HS_Weak       0.66      0.78      0.72       664\n",
      "  HS_Moderate       0.70      0.45      0.54       346\n",
      "    HS_Strong       0.81      0.75      0.78        84\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5476\n",
      "    macro avg       0.77      0.64      0.67      5476\n",
      " weighted avg       0.78      0.75      0.76      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 0.00019812583923339844 seconds\n",
      "\n",
      "Fold 2 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8616 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3905, Accuracy: 0.8734, F1 Micro: 0.4739, F1 Macro: 0.2199\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.9049, F1 Micro: 0.694, F1 Macro: 0.4563\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9114, F1 Micro: 0.736, F1 Macro: 0.5449\n",
      "Epoch 4/10, Train Loss: 0.1753, Accuracy: 0.9171, F1 Micro: 0.733, F1 Macro: 0.5803\n",
      "Epoch 5/10, Train Loss: 0.1505, Accuracy: 0.9205, F1 Micro: 0.7634, F1 Macro: 0.6069\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9187, F1 Micro: 0.7677, F1 Macro: 0.6403\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9208, F1 Micro: 0.7716, F1 Macro: 0.6593\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9236, F1 Micro: 0.766, F1 Macro: 0.6726\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9197, F1 Micro: 0.7664, F1 Macro: 0.6823\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9203, F1 Micro: 0.7638, F1 Macro: 0.69\n",
      "Best result for 8616 samples: F1 Micro: 0.7716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1094\n",
      "      Abusive       0.90      0.90      0.90      1072\n",
      "HS_Individual       0.69      0.76      0.72       689\n",
      "     HS_Group       0.69      0.69      0.69       405\n",
      "  HS_Religion       0.78      0.65      0.70       124\n",
      "      HS_Race       0.83      0.70      0.76       125\n",
      "  HS_Physical       0.46      0.10      0.16        61\n",
      "    HS_Gender       0.50      0.16      0.24        58\n",
      "     HS_Other       0.72      0.84      0.78       754\n",
      "      HS_Weak       0.68      0.73      0.70       664\n",
      "  HS_Moderate       0.62      0.59      0.61       346\n",
      "    HS_Strong       0.79      0.83      0.81        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5476\n",
      "    macro avg       0.70      0.65      0.66      5476\n",
      " weighted avg       0.75      0.79      0.77      5476\n",
      "  samples avg       0.44      0.45      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001735687255859375 seconds\n",
      "\n",
      "Fold 2 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 8816 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8803, F1 Micro: 0.5341, F1 Macro: 0.2523\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.9045, F1 Micro: 0.6909, F1 Macro: 0.4545\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.912, F1 Micro: 0.7317, F1 Macro: 0.5367\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9157, F1 Micro: 0.723, F1 Macro: 0.5603\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9204, F1 Micro: 0.7625, F1 Macro: 0.6266\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9202, F1 Micro: 0.7682, F1 Macro: 0.6534\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.9212, F1 Micro: 0.7595, F1 Macro: 0.6498\n",
      "Epoch 8/10, Train Loss: 0.0868, Accuracy: 0.9125, F1 Micro: 0.7573, F1 Macro: 0.6583\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9209, F1 Micro: 0.7653, F1 Macro: 0.6584\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9201, F1 Micro: 0.7669, F1 Macro: 0.6778\n",
      "Best result for 8816 samples: F1 Micro: 0.7682\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1094\n",
      "      Abusive       0.87      0.91      0.89      1072\n",
      "HS_Individual       0.68      0.77      0.72       689\n",
      "     HS_Group       0.70      0.63      0.66       405\n",
      "  HS_Religion       0.74      0.61      0.67       124\n",
      "      HS_Race       0.77      0.78      0.77       125\n",
      "  HS_Physical       0.58      0.11      0.19        61\n",
      "    HS_Gender       0.50      0.17      0.26        58\n",
      "     HS_Other       0.75      0.81      0.78       754\n",
      "      HS_Weak       0.67      0.75      0.71       664\n",
      "  HS_Moderate       0.64      0.54      0.58       346\n",
      "    HS_Strong       0.83      0.70      0.76        84\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5476\n",
      "    macro avg       0.71      0.64      0.65      5476\n",
      " weighted avg       0.75      0.78      0.76      5476\n",
      "  samples avg       0.45      0.45      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001690387725830078 seconds\n",
      "\n",
      "Fold 2 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9016 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3872, Accuracy: 0.8874, F1 Micro: 0.6132, F1 Macro: 0.3027\n",
      "Epoch 2/10, Train Loss: 0.2578, Accuracy: 0.9044, F1 Micro: 0.6787, F1 Macro: 0.4783\n",
      "Epoch 3/10, Train Loss: 0.2065, Accuracy: 0.9156, F1 Micro: 0.736, F1 Macro: 0.5689\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.9198, F1 Micro: 0.7558, F1 Macro: 0.599\n",
      "Epoch 5/10, Train Loss: 0.1471, Accuracy: 0.9196, F1 Micro: 0.7641, F1 Macro: 0.6413\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9193, F1 Micro: 0.7652, F1 Macro: 0.6527\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.9209, F1 Micro: 0.7616, F1 Macro: 0.6537\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9214, F1 Micro: 0.7657, F1 Macro: 0.6833\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9223, F1 Micro: 0.7699, F1 Macro: 0.6762\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9218, F1 Micro: 0.7663, F1 Macro: 0.6761\n",
      "Best result for 9016 samples: F1 Micro: 0.7699\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1094\n",
      "      Abusive       0.91      0.88      0.89      1072\n",
      "HS_Individual       0.68      0.79      0.73       689\n",
      "     HS_Group       0.75      0.58      0.65       405\n",
      "  HS_Religion       0.73      0.66      0.69       124\n",
      "      HS_Race       0.81      0.66      0.73       125\n",
      "  HS_Physical       0.58      0.18      0.28        61\n",
      "    HS_Gender       0.68      0.33      0.44        58\n",
      "     HS_Other       0.76      0.79      0.78       754\n",
      "      HS_Weak       0.67      0.77      0.72       664\n",
      "  HS_Moderate       0.70      0.51      0.59       346\n",
      "    HS_Strong       0.81      0.75      0.78        84\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5476\n",
      "    macro avg       0.74      0.65      0.68      5476\n",
      " weighted avg       0.78      0.77      0.77      5476\n",
      "  samples avg       0.44      0.44      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017595291137695312 seconds\n",
      "\n",
      "Fold 2 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9216 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3893, Accuracy: 0.8882, F1 Micro: 0.616, F1 Macro: 0.311\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.9034, F1 Micro: 0.6796, F1 Macro: 0.4513\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9137, F1 Micro: 0.7413, F1 Macro: 0.5628\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9192, F1 Micro: 0.7464, F1 Macro: 0.5935\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9208, F1 Micro: 0.7583, F1 Macro: 0.6173\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9214, F1 Micro: 0.7627, F1 Macro: 0.6406\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9223, F1 Micro: 0.7672, F1 Macro: 0.6648\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9162, F1 Micro: 0.7621, F1 Macro: 0.6593\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9227, F1 Micro: 0.7678, F1 Macro: 0.6906\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9219, F1 Micro: 0.7678, F1 Macro: 0.6852\n",
      "Best result for 9216 samples: F1 Micro: 0.7678\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1094\n",
      "      Abusive       0.90      0.89      0.89      1072\n",
      "HS_Individual       0.68      0.76      0.72       689\n",
      "     HS_Group       0.73      0.59      0.65       405\n",
      "  HS_Religion       0.67      0.70      0.69       124\n",
      "      HS_Race       0.79      0.71      0.75       125\n",
      "  HS_Physical       0.71      0.16      0.27        61\n",
      "    HS_Gender       0.70      0.45      0.55        58\n",
      "     HS_Other       0.79      0.74      0.76       754\n",
      "      HS_Weak       0.68      0.75      0.71       664\n",
      "  HS_Moderate       0.68      0.54      0.60       346\n",
      "    HS_Strong       0.80      0.79      0.80        84\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5476\n",
      "    macro avg       0.75      0.66      0.69      5476\n",
      " weighted avg       0.78      0.76      0.76      5476\n",
      "  samples avg       0.44      0.44      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 0.25791263580322266 seconds\n",
      "\n",
      "Fold 2 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3811, Accuracy: 0.8883, F1 Micro: 0.6179, F1 Macro: 0.3141\n",
      "Epoch 2/10, Train Loss: 0.2552, Accuracy: 0.9048, F1 Micro: 0.6807, F1 Macro: 0.4518\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9114, F1 Micro: 0.702, F1 Macro: 0.5221\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9198, F1 Micro: 0.7524, F1 Macro: 0.5919\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9213, F1 Micro: 0.7687, F1 Macro: 0.616\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9211, F1 Micro: 0.7721, F1 Macro: 0.6597\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9217, F1 Micro: 0.7576, F1 Macro: 0.6385\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.9178, F1 Micro: 0.7666, F1 Macro: 0.6799\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9226, F1 Micro: 0.7721, F1 Macro: 0.6864\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9195, F1 Micro: 0.766, F1 Macro: 0.6658\n",
      "Best result for 9218 samples: F1 Micro: 0.7721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1094\n",
      "      Abusive       0.91      0.89      0.90      1072\n",
      "HS_Individual       0.68      0.78      0.73       689\n",
      "     HS_Group       0.73      0.61      0.66       405\n",
      "  HS_Religion       0.69      0.69      0.69       124\n",
      "      HS_Race       0.80      0.73      0.76       125\n",
      "  HS_Physical       0.70      0.23      0.35        61\n",
      "    HS_Gender       0.70      0.33      0.45        58\n",
      "     HS_Other       0.77      0.76      0.77       754\n",
      "      HS_Weak       0.67      0.77      0.72       664\n",
      "  HS_Moderate       0.65      0.53      0.59       346\n",
      "    HS_Strong       0.80      0.77      0.79        84\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5476\n",
      "    macro avg       0.74      0.66      0.69      5476\n",
      " weighted avg       0.77      0.77      0.77      5476\n",
      "  samples avg       0.44      0.44      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018858909606933594 seconds\n",
      "\n",
      "Fold 2 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3806, Accuracy: 0.8872, F1 Micro: 0.6249, F1 Macro: 0.3215\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9047, F1 Micro: 0.7041, F1 Macro: 0.4993\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.915, F1 Micro: 0.7314, F1 Macro: 0.5538\n",
      "Epoch 4/10, Train Loss: 0.172, Accuracy: 0.9169, F1 Micro: 0.7286, F1 Macro: 0.5759\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9216, F1 Micro: 0.7686, F1 Macro: 0.6534\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.919, F1 Micro: 0.7725, F1 Macro: 0.6746\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9227, F1 Micro: 0.7689, F1 Macro: 0.6747\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9213, F1 Micro: 0.7642, F1 Macro: 0.6505\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9226, F1 Micro: 0.775, F1 Macro: 0.6952\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9203, F1 Micro: 0.7646, F1 Macro: 0.659\n",
      "Best result for 9418 samples: F1 Micro: 0.775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1094\n",
      "      Abusive       0.90      0.89      0.90      1072\n",
      "HS_Individual       0.70      0.76      0.73       689\n",
      "     HS_Group       0.68      0.67      0.67       405\n",
      "  HS_Religion       0.71      0.73      0.72       124\n",
      "      HS_Race       0.73      0.82      0.77       125\n",
      "  HS_Physical       0.44      0.23      0.30        61\n",
      "    HS_Gender       0.62      0.41      0.49        58\n",
      "     HS_Other       0.77      0.77      0.77       754\n",
      "      HS_Weak       0.70      0.75      0.72       664\n",
      "  HS_Moderate       0.62      0.60      0.61       346\n",
      "    HS_Strong       0.80      0.81      0.80        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5476\n",
      "    macro avg       0.71      0.69      0.70      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.44      0.45      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.000194549560546875 seconds\n",
      "\n",
      "Fold 2 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9618 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3829, Accuracy: 0.8878, F1 Micro: 0.6334, F1 Macro: 0.3348\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.9026, F1 Micro: 0.721, F1 Macro: 0.5129\n",
      "Epoch 3/10, Train Loss: 0.2065, Accuracy: 0.9141, F1 Micro: 0.7274, F1 Macro: 0.5416\n",
      "Epoch 4/10, Train Loss: 0.1708, Accuracy: 0.9177, F1 Micro: 0.7679, F1 Macro: 0.6204\n",
      "Epoch 5/10, Train Loss: 0.1423, Accuracy: 0.9221, F1 Micro: 0.7591, F1 Macro: 0.6233\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.923, F1 Micro: 0.7667, F1 Macro: 0.6368\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9243, F1 Micro: 0.773, F1 Macro: 0.6704\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9219, F1 Micro: 0.7672, F1 Macro: 0.6667\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9234, F1 Micro: 0.7687, F1 Macro: 0.6887\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9227, F1 Micro: 0.7658, F1 Macro: 0.6829\n",
      "Best result for 9618 samples: F1 Micro: 0.773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1094\n",
      "      Abusive       0.91      0.89      0.90      1072\n",
      "HS_Individual       0.72      0.74      0.73       689\n",
      "     HS_Group       0.73      0.63      0.67       405\n",
      "  HS_Religion       0.77      0.62      0.69       124\n",
      "      HS_Race       0.84      0.66      0.74       125\n",
      "  HS_Physical       0.50      0.11      0.19        61\n",
      "    HS_Gender       0.59      0.33      0.42        58\n",
      "     HS_Other       0.77      0.78      0.78       754\n",
      "      HS_Weak       0.70      0.73      0.72       664\n",
      "  HS_Moderate       0.65      0.53      0.59       346\n",
      "    HS_Strong       0.85      0.73      0.78        84\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5476\n",
      "    macro avg       0.74      0.63      0.67      5476\n",
      " weighted avg       0.78      0.76      0.77      5476\n",
      "  samples avg       0.45      0.44      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019073486328125 seconds\n",
      "\n",
      "Fold 2 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 9818 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3771, Accuracy: 0.8874, F1 Micro: 0.5938, F1 Macro: 0.3\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.907, F1 Micro: 0.7155, F1 Macro: 0.498\n",
      "Epoch 3/10, Train Loss: 0.2027, Accuracy: 0.9147, F1 Micro: 0.7365, F1 Macro: 0.545\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9158, F1 Micro: 0.721, F1 Macro: 0.5656\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9208, F1 Micro: 0.7648, F1 Macro: 0.6211\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9238, F1 Micro: 0.7687, F1 Macro: 0.6734\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9245, F1 Micro: 0.7621, F1 Macro: 0.6711\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9248, F1 Micro: 0.771, F1 Macro: 0.6838\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9221, F1 Micro: 0.774, F1 Macro: 0.7057\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9232, F1 Micro: 0.7714, F1 Macro: 0.6915\n",
      "Best result for 9818 samples: F1 Micro: 0.774\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1094\n",
      "      Abusive       0.91      0.90      0.91      1072\n",
      "HS_Individual       0.67      0.78      0.72       689\n",
      "     HS_Group       0.72      0.62      0.67       405\n",
      "  HS_Religion       0.67      0.76      0.71       124\n",
      "      HS_Race       0.82      0.78      0.80       125\n",
      "  HS_Physical       0.50      0.26      0.34        61\n",
      "    HS_Gender       0.67      0.55      0.60        58\n",
      "     HS_Other       0.77      0.77      0.77       754\n",
      "      HS_Weak       0.67      0.76      0.71       664\n",
      "  HS_Moderate       0.67      0.56      0.61       346\n",
      "    HS_Strong       0.80      0.79      0.80        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5476\n",
      "    macro avg       0.72      0.70      0.71      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.45      0.45      0.44      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001823902130126953 seconds\n",
      "\n",
      "Fold 2 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10018 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.8866, F1 Micro: 0.578, F1 Macro: 0.2853\n",
      "Epoch 2/10, Train Loss: 0.2484, Accuracy: 0.9079, F1 Micro: 0.7259, F1 Macro: 0.5202\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9183, F1 Micro: 0.7581, F1 Macro: 0.5916\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9186, F1 Micro: 0.7351, F1 Macro: 0.5667\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9211, F1 Micro: 0.7623, F1 Macro: 0.6298\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9225, F1 Micro: 0.7664, F1 Macro: 0.6588\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9207, F1 Micro: 0.7413, F1 Macro: 0.6412\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.9251, F1 Micro: 0.7757, F1 Macro: 0.6891\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9248, F1 Micro: 0.7735, F1 Macro: 0.7006\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9215, F1 Micro: 0.7697, F1 Macro: 0.6844\n",
      "Best result for 10018 samples: F1 Micro: 0.7757\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1094\n",
      "      Abusive       0.91      0.88      0.90      1072\n",
      "HS_Individual       0.72      0.75      0.73       689\n",
      "     HS_Group       0.72      0.64      0.68       405\n",
      "  HS_Religion       0.73      0.71      0.72       124\n",
      "      HS_Race       0.85      0.72      0.78       125\n",
      "  HS_Physical       0.48      0.21      0.30        61\n",
      "    HS_Gender       0.69      0.34      0.46        58\n",
      "     HS_Other       0.79      0.76      0.77       754\n",
      "      HS_Weak       0.71      0.72      0.71       664\n",
      "  HS_Moderate       0.65      0.56      0.60       346\n",
      "    HS_Strong       0.76      0.77      0.77        84\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5476\n",
      "    macro avg       0.74      0.66      0.69      5476\n",
      " weighted avg       0.79      0.76      0.77      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002694129943847656 seconds\n",
      "\n",
      "Fold 2 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3724, Accuracy: 0.8886, F1 Micro: 0.6133, F1 Macro: 0.3047\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9086, F1 Micro: 0.7104, F1 Macro: 0.5388\n",
      "Epoch 3/10, Train Loss: 0.1993, Accuracy: 0.9173, F1 Micro: 0.7376, F1 Macro: 0.5809\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9237, F1 Micro: 0.7701, F1 Macro: 0.6143\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.923, F1 Micro: 0.7707, F1 Macro: 0.6497\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9228, F1 Micro: 0.7691, F1 Macro: 0.6687\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9229, F1 Micro: 0.7681, F1 Macro: 0.6776\n",
      "Epoch 8/10, Train Loss: 0.0832, Accuracy: 0.9193, F1 Micro: 0.7563, F1 Macro: 0.6645\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.92, F1 Micro: 0.7685, F1 Macro: 0.6677\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9228, F1 Micro: 0.7629, F1 Macro: 0.6897\n",
      "Best result for 10218 samples: F1 Micro: 0.7707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1094\n",
      "      Abusive       0.91      0.88      0.89      1072\n",
      "HS_Individual       0.70      0.76      0.73       689\n",
      "     HS_Group       0.72      0.61      0.66       405\n",
      "  HS_Religion       0.73      0.66      0.69       124\n",
      "      HS_Race       0.84      0.72      0.78       125\n",
      "  HS_Physical       0.71      0.08      0.15        61\n",
      "    HS_Gender       0.54      0.12      0.20        58\n",
      "     HS_Other       0.76      0.78      0.77       754\n",
      "      HS_Weak       0.70      0.74      0.72       664\n",
      "  HS_Moderate       0.66      0.52      0.58       346\n",
      "    HS_Strong       0.81      0.75      0.78        84\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5476\n",
      "    macro avg       0.74      0.62      0.65      5476\n",
      " weighted avg       0.78      0.76      0.76      5476\n",
      "  samples avg       0.44      0.43      0.42      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001614093780517578 seconds\n",
      "\n",
      "Fold 2 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3755, Accuracy: 0.8881, F1 Micro: 0.6542, F1 Macro: 0.3529\n",
      "Epoch 2/10, Train Loss: 0.2496, Accuracy: 0.9064, F1 Micro: 0.7028, F1 Macro: 0.467\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9177, F1 Micro: 0.7468, F1 Macro: 0.5908\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9167, F1 Micro: 0.7518, F1 Macro: 0.6015\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.924, F1 Micro: 0.7714, F1 Macro: 0.6497\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9236, F1 Micro: 0.7689, F1 Macro: 0.6559\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.9224, F1 Micro: 0.7562, F1 Macro: 0.6641\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9238, F1 Micro: 0.763, F1 Macro: 0.6809\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.923, F1 Micro: 0.7723, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9238, F1 Micro: 0.7676, F1 Macro: 0.7002\n",
      "Best result for 10418 samples: F1 Micro: 0.7723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1094\n",
      "      Abusive       0.92      0.88      0.90      1072\n",
      "HS_Individual       0.72      0.73      0.73       689\n",
      "     HS_Group       0.68      0.66      0.67       405\n",
      "  HS_Religion       0.69      0.69      0.69       124\n",
      "      HS_Race       0.81      0.78      0.80       125\n",
      "  HS_Physical       0.65      0.25      0.36        61\n",
      "    HS_Gender       0.64      0.31      0.42        58\n",
      "     HS_Other       0.77      0.77      0.77       754\n",
      "      HS_Weak       0.71      0.72      0.71       664\n",
      "  HS_Moderate       0.62      0.61      0.62       346\n",
      "    HS_Strong       0.77      0.80      0.78        84\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5476\n",
      "    macro avg       0.73      0.67      0.69      5476\n",
      " weighted avg       0.77      0.77      0.77      5476\n",
      "  samples avg       0.45      0.44      0.43      5476\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 0.3055534362792969 seconds\n",
      "\n",
      "Fold 2 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 10535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3706, Accuracy: 0.8868, F1 Micro: 0.5863, F1 Macro: 0.3084\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9076, F1 Micro: 0.6963, F1 Macro: 0.4914\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9183, F1 Micro: 0.7474, F1 Macro: 0.5627\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9195, F1 Micro: 0.7688, F1 Macro: 0.6182\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9243, F1 Micro: 0.7709, F1 Macro: 0.6546\n",
      "Epoch 6/10, Train Loss: 0.113, Accuracy: 0.9229, F1 Micro: 0.7664, F1 Macro: 0.6442\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.923, F1 Micro: 0.7775, F1 Macro: 0.6914\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9238, F1 Micro: 0.7657, F1 Macro: 0.6828\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9226, F1 Micro: 0.7697, F1 Macro: 0.6704\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9231, F1 Micro: 0.7732, F1 Macro: 0.6954\n",
      "Best result for 10535 samples: F1 Micro: 0.7775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1094\n",
      "      Abusive       0.89      0.92      0.90      1072\n",
      "HS_Individual       0.68      0.80      0.74       689\n",
      "     HS_Group       0.74      0.62      0.67       405\n",
      "  HS_Religion       0.66      0.73      0.69       124\n",
      "      HS_Race       0.74      0.85      0.79       125\n",
      "  HS_Physical       0.39      0.21      0.28        61\n",
      "    HS_Gender       0.67      0.38      0.48        58\n",
      "     HS_Other       0.79      0.76      0.77       754\n",
      "      HS_Weak       0.67      0.79      0.72       664\n",
      "  HS_Moderate       0.67      0.55      0.60       346\n",
      "    HS_Strong       0.79      0.81      0.80        84\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5476\n",
      "    macro avg       0.71      0.69      0.69      5476\n",
      " weighted avg       0.76      0.79      0.77      5476\n",
      "  samples avg       0.45      0.45      0.44      5476\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 5787.28 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 658 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.589, Accuracy: 0.8192, F1 Micro: 0.3329, F1 Macro: 0.121\n",
      "Epoch 2/10, Train Loss: 0.462, Accuracy: 0.8328, F1 Micro: 0.1424, F1 Macro: 0.043\n",
      "Epoch 3/10, Train Loss: 0.4096, Accuracy: 0.8379, F1 Micro: 0.1751, F1 Macro: 0.0644\n",
      "Epoch 4/10, Train Loss: 0.3943, Accuracy: 0.8443, F1 Micro: 0.243, F1 Macro: 0.0848\n",
      "Epoch 5/10, Train Loss: 0.3778, Accuracy: 0.8511, F1 Micro: 0.3367, F1 Macro: 0.1065\n",
      "Epoch 6/10, Train Loss: 0.361, Accuracy: 0.8584, F1 Micro: 0.3902, F1 Macro: 0.1412\n",
      "Epoch 7/10, Train Loss: 0.3343, Accuracy: 0.8714, F1 Micro: 0.5554, F1 Macro: 0.2443\n",
      "Epoch 8/10, Train Loss: 0.3088, Accuracy: 0.8736, F1 Micro: 0.5244, F1 Macro: 0.246\n",
      "Epoch 9/10, Train Loss: 0.2861, Accuracy: 0.8776, F1 Micro: 0.6054, F1 Macro: 0.2994\n",
      "Epoch 10/10, Train Loss: 0.2691, Accuracy: 0.8796, F1 Micro: 0.6353, F1 Macro: 0.3468\n",
      "Best result for 658 samples: F1 Micro: 0.6353\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.74      0.82      0.78      1142\n",
      "      Abusive       0.78      0.82      0.80      1026\n",
      "HS_Individual       0.61      0.65      0.63       723\n",
      "     HS_Group       0.59      0.35      0.44       419\n",
      "  HS_Religion       0.00      0.00      0.00       177\n",
      "      HS_Race       0.00      0.00      0.00       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.61      0.71      0.66       746\n",
      "      HS_Weak       0.59      0.58      0.58       685\n",
      "  HS_Moderate       0.61      0.16      0.26       352\n",
      "    HS_Strong       1.00      0.01      0.02       105\n",
      "\n",
      "    micro avg       0.67      0.60      0.64      5634\n",
      "    macro avg       0.46      0.34      0.35      5634\n",
      " weighted avg       0.62      0.60      0.59      5634\n",
      "  samples avg       0.39      0.34      0.34      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 0.0006327629089355469 seconds\n",
      "\n",
      "Fold 3 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1646 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5144, Accuracy: 0.8304, F1 Micro: 0.0904, F1 Macro: 0.0329\n",
      "Epoch 2/10, Train Loss: 0.3928, Accuracy: 0.841, F1 Micro: 0.2035, F1 Macro: 0.0732\n",
      "Epoch 3/10, Train Loss: 0.3663, Accuracy: 0.8618, F1 Micro: 0.437, F1 Macro: 0.1544\n",
      "Epoch 4/10, Train Loss: 0.327, Accuracy: 0.8771, F1 Micro: 0.6092, F1 Macro: 0.2877\n",
      "Epoch 5/10, Train Loss: 0.301, Accuracy: 0.8843, F1 Micro: 0.6258, F1 Macro: 0.3094\n",
      "Epoch 6/10, Train Loss: 0.2642, Accuracy: 0.8894, F1 Micro: 0.6319, F1 Macro: 0.3617\n",
      "Epoch 7/10, Train Loss: 0.2373, Accuracy: 0.895, F1 Micro: 0.6729, F1 Macro: 0.4091\n",
      "Epoch 8/10, Train Loss: 0.2164, Accuracy: 0.8968, F1 Micro: 0.6878, F1 Macro: 0.4681\n",
      "Epoch 9/10, Train Loss: 0.1932, Accuracy: 0.8915, F1 Micro: 0.6846, F1 Macro: 0.4018\n",
      "Epoch 10/10, Train Loss: 0.1725, Accuracy: 0.9011, F1 Micro: 0.6962, F1 Macro: 0.4969\n",
      "Best result for 1646 samples: F1 Micro: 0.6962\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.83      1142\n",
      "      Abusive       0.87      0.80      0.83      1026\n",
      "HS_Individual       0.70      0.63      0.66       723\n",
      "     HS_Group       0.64      0.60      0.62       419\n",
      "  HS_Religion       0.71      0.20      0.32       177\n",
      "      HS_Race       0.94      0.28      0.43       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.69      0.72      0.71       746\n",
      "      HS_Weak       0.68      0.58      0.63       685\n",
      "  HS_Moderate       0.57      0.45      0.50       352\n",
      "    HS_Strong       0.80      0.30      0.44       105\n",
      "\n",
      "    micro avg       0.75      0.65      0.70      5634\n",
      "    macro avg       0.62      0.45      0.50      5634\n",
      " weighted avg       0.73      0.65      0.68      5634\n",
      "  samples avg       0.40      0.37      0.36      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 0.0005664825439453125 seconds\n",
      "\n",
      "Fold 3 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4843, Accuracy: 0.8427, F1 Micro: 0.2422, F1 Macro: 0.0838\n",
      "Epoch 2/10, Train Loss: 0.3679, Accuracy: 0.8621, F1 Micro: 0.4246, F1 Macro: 0.1621\n",
      "Epoch 3/10, Train Loss: 0.3187, Accuracy: 0.8821, F1 Micro: 0.5978, F1 Macro: 0.2849\n",
      "Epoch 4/10, Train Loss: 0.2822, Accuracy: 0.8909, F1 Micro: 0.663, F1 Macro: 0.3927\n",
      "Epoch 5/10, Train Loss: 0.2447, Accuracy: 0.8983, F1 Micro: 0.6644, F1 Macro: 0.4248\n",
      "Epoch 6/10, Train Loss: 0.222, Accuracy: 0.9023, F1 Micro: 0.6959, F1 Macro: 0.4684\n",
      "Epoch 7/10, Train Loss: 0.1982, Accuracy: 0.9026, F1 Micro: 0.7201, F1 Macro: 0.5226\n",
      "Epoch 8/10, Train Loss: 0.1706, Accuracy: 0.9048, F1 Micro: 0.7094, F1 Macro: 0.5055\n",
      "Epoch 9/10, Train Loss: 0.1504, Accuracy: 0.9046, F1 Micro: 0.7102, F1 Macro: 0.503\n",
      "Epoch 10/10, Train Loss: 0.1342, Accuracy: 0.904, F1 Micro: 0.714, F1 Macro: 0.5182\n",
      "Best result for 2535 samples: F1 Micro: 0.7201\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.87      0.84      1142\n",
      "      Abusive       0.87      0.86      0.86      1026\n",
      "HS_Individual       0.66      0.74      0.70       723\n",
      "     HS_Group       0.66      0.59      0.62       419\n",
      "  HS_Religion       0.77      0.17      0.28       177\n",
      "      HS_Race       0.78      0.49      0.60       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.66      0.81      0.73       746\n",
      "      HS_Weak       0.64      0.72      0.67       685\n",
      "  HS_Moderate       0.56      0.45      0.50       352\n",
      "    HS_Strong       0.77      0.34      0.47       105\n",
      "\n",
      "    micro avg       0.72      0.72      0.72      5634\n",
      "    macro avg       0.60      0.50      0.52      5634\n",
      " weighted avg       0.71      0.72      0.70      5634\n",
      "  samples avg       0.42      0.41      0.39      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 0.0005118846893310547 seconds\n",
      "\n",
      "Fold 3 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3335 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4534, Accuracy: 0.8463, F1 Micro: 0.2731, F1 Macro: 0.0924\n",
      "Epoch 2/10, Train Loss: 0.3427, Accuracy: 0.8766, F1 Micro: 0.5542, F1 Macro: 0.2581\n",
      "Epoch 3/10, Train Loss: 0.2938, Accuracy: 0.8869, F1 Micro: 0.5999, F1 Macro: 0.3119\n",
      "Epoch 4/10, Train Loss: 0.2482, Accuracy: 0.897, F1 Micro: 0.6543, F1 Macro: 0.3765\n",
      "Epoch 5/10, Train Loss: 0.2271, Accuracy: 0.9032, F1 Micro: 0.7026, F1 Macro: 0.4649\n",
      "Epoch 6/10, Train Loss: 0.1956, Accuracy: 0.907, F1 Micro: 0.7161, F1 Macro: 0.5258\n",
      "Epoch 7/10, Train Loss: 0.1678, Accuracy: 0.9094, F1 Micro: 0.7215, F1 Macro: 0.544\n",
      "Epoch 8/10, Train Loss: 0.1475, Accuracy: 0.9054, F1 Micro: 0.7205, F1 Macro: 0.5129\n",
      "Epoch 9/10, Train Loss: 0.1242, Accuracy: 0.9071, F1 Micro: 0.7289, F1 Macro: 0.5626\n",
      "Epoch 10/10, Train Loss: 0.1126, Accuracy: 0.9085, F1 Micro: 0.7123, F1 Macro: 0.5539\n",
      "Best result for 3335 samples: F1 Micro: 0.7289\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.83      1142\n",
      "      Abusive       0.86      0.90      0.88      1026\n",
      "HS_Individual       0.72      0.63      0.67       723\n",
      "     HS_Group       0.60      0.69      0.64       419\n",
      "  HS_Religion       0.85      0.41      0.56       177\n",
      "      HS_Race       0.84      0.50      0.62       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.70      0.79      0.74       746\n",
      "      HS_Weak       0.70      0.60      0.65       685\n",
      "  HS_Moderate       0.53      0.65      0.58       352\n",
      "    HS_Strong       0.84      0.45      0.58       105\n",
      "\n",
      "    micro avg       0.74      0.71      0.73      5634\n",
      "    macro avg       0.62      0.54      0.56      5634\n",
      " weighted avg       0.73      0.71      0.72      5634\n",
      "  samples avg       0.42      0.41      0.40      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 0.00044536590576171875 seconds\n",
      "\n",
      "Fold 3 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4055 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.44, Accuracy: 0.8504, F1 Micro: 0.3398, F1 Macro: 0.1056\n",
      "Epoch 2/10, Train Loss: 0.3282, Accuracy: 0.8829, F1 Micro: 0.6212, F1 Macro: 0.2998\n",
      "Epoch 3/10, Train Loss: 0.2778, Accuracy: 0.8973, F1 Micro: 0.6938, F1 Macro: 0.4261\n",
      "Epoch 4/10, Train Loss: 0.2318, Accuracy: 0.9024, F1 Micro: 0.7206, F1 Macro: 0.4953\n",
      "Epoch 5/10, Train Loss: 0.2038, Accuracy: 0.9091, F1 Micro: 0.7342, F1 Macro: 0.5279\n",
      "Epoch 6/10, Train Loss: 0.1787, Accuracy: 0.9084, F1 Micro: 0.7427, F1 Macro: 0.5558\n",
      "Epoch 7/10, Train Loss: 0.1517, Accuracy: 0.9111, F1 Micro: 0.745, F1 Macro: 0.5587\n",
      "Epoch 8/10, Train Loss: 0.1295, Accuracy: 0.9137, F1 Micro: 0.7463, F1 Macro: 0.5736\n",
      "Epoch 9/10, Train Loss: 0.1073, Accuracy: 0.9125, F1 Micro: 0.7405, F1 Macro: 0.5699\n",
      "Epoch 10/10, Train Loss: 0.0933, Accuracy: 0.911, F1 Micro: 0.7427, F1 Macro: 0.5767\n",
      "Best result for 4055 samples: F1 Micro: 0.7463\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1142\n",
      "      Abusive       0.90      0.87      0.88      1026\n",
      "HS_Individual       0.69      0.75      0.72       723\n",
      "     HS_Group       0.76      0.57      0.65       419\n",
      "  HS_Religion       0.76      0.45      0.57       177\n",
      "      HS_Race       0.91      0.51      0.66       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.71      0.79      0.75       746\n",
      "      HS_Weak       0.67      0.74      0.70       685\n",
      "  HS_Moderate       0.67      0.46      0.54       352\n",
      "    HS_Strong       0.84      0.44      0.58       105\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5634\n",
      "    macro avg       0.64      0.54      0.57      5634\n",
      " weighted avg       0.75      0.73      0.73      5634\n",
      "  samples avg       0.43      0.41      0.40      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 0.0003981590270996094 seconds\n",
      "\n",
      "Fold 3 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4703 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4314, Accuracy: 0.8554, F1 Micro: 0.364, F1 Macro: 0.1176\n",
      "Epoch 2/10, Train Loss: 0.3132, Accuracy: 0.8813, F1 Micro: 0.5643, F1 Macro: 0.2724\n",
      "Epoch 3/10, Train Loss: 0.2591, Accuracy: 0.9003, F1 Micro: 0.6761, F1 Macro: 0.4275\n",
      "Epoch 4/10, Train Loss: 0.2161, Accuracy: 0.9063, F1 Micro: 0.715, F1 Macro: 0.4876\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.9118, F1 Micro: 0.7468, F1 Macro: 0.562\n",
      "Epoch 6/10, Train Loss: 0.164, Accuracy: 0.9107, F1 Micro: 0.74, F1 Macro: 0.5518\n",
      "Epoch 7/10, Train Loss: 0.14, Accuracy: 0.9137, F1 Micro: 0.7476, F1 Macro: 0.5892\n",
      "Epoch 8/10, Train Loss: 0.1215, Accuracy: 0.9163, F1 Micro: 0.7503, F1 Macro: 0.568\n",
      "Epoch 9/10, Train Loss: 0.1053, Accuracy: 0.9129, F1 Micro: 0.7551, F1 Macro: 0.5946\n",
      "Epoch 10/10, Train Loss: 0.0909, Accuracy: 0.9122, F1 Micro: 0.756, F1 Macro: 0.601\n",
      "Best result for 4703 samples: F1 Micro: 0.756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1142\n",
      "      Abusive       0.89      0.90      0.90      1026\n",
      "HS_Individual       0.64      0.82      0.72       723\n",
      "     HS_Group       0.73      0.57      0.64       419\n",
      "  HS_Religion       0.73      0.60      0.66       177\n",
      "      HS_Race       0.78      0.62      0.69       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       1.00      0.02      0.03        60\n",
      "     HS_Other       0.69      0.84      0.76       746\n",
      "      HS_Weak       0.62      0.81      0.70       685\n",
      "  HS_Moderate       0.65      0.51      0.57       352\n",
      "    HS_Strong       0.82      0.61      0.70       105\n",
      "\n",
      "    micro avg       0.73      0.78      0.76      5634\n",
      "    macro avg       0.70      0.60      0.60      5634\n",
      " weighted avg       0.73      0.78      0.74      5634\n",
      "  samples avg       0.44      0.44      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.00041961669921875 seconds\n",
      "\n",
      "Fold 3 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5287 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4188, Accuracy: 0.8589, F1 Micro: 0.3812, F1 Macro: 0.1396\n",
      "Epoch 2/10, Train Loss: 0.3016, Accuracy: 0.8904, F1 Micro: 0.6345, F1 Macro: 0.3347\n",
      "Epoch 3/10, Train Loss: 0.2478, Accuracy: 0.8985, F1 Micro: 0.6588, F1 Macro: 0.403\n",
      "Epoch 4/10, Train Loss: 0.212, Accuracy: 0.9061, F1 Micro: 0.7331, F1 Macro: 0.5156\n",
      "Epoch 5/10, Train Loss: 0.1827, Accuracy: 0.9123, F1 Micro: 0.747, F1 Macro: 0.579\n",
      "Epoch 6/10, Train Loss: 0.157, Accuracy: 0.9111, F1 Micro: 0.7514, F1 Macro: 0.5774\n",
      "Epoch 7/10, Train Loss: 0.1352, Accuracy: 0.9149, F1 Micro: 0.7576, F1 Macro: 0.5988\n",
      "Epoch 8/10, Train Loss: 0.1133, Accuracy: 0.9156, F1 Micro: 0.7579, F1 Macro: 0.5926\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9161, F1 Micro: 0.7563, F1 Macro: 0.6012\n",
      "Epoch 10/10, Train Loss: 0.0797, Accuracy: 0.9175, F1 Micro: 0.7578, F1 Macro: 0.6008\n",
      "Best result for 5287 samples: F1 Micro: 0.7579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1142\n",
      "      Abusive       0.88      0.92      0.90      1026\n",
      "HS_Individual       0.70      0.73      0.72       723\n",
      "     HS_Group       0.72      0.67      0.69       419\n",
      "  HS_Religion       0.73      0.53      0.61       177\n",
      "      HS_Race       0.88      0.55      0.67       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.70      0.82      0.75       746\n",
      "      HS_Weak       0.69      0.71      0.70       685\n",
      "  HS_Moderate       0.63      0.58      0.61       352\n",
      "    HS_Strong       0.80      0.50      0.62       105\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5634\n",
      "    macro avg       0.63      0.57      0.59      5634\n",
      " weighted avg       0.74      0.76      0.75      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0003743171691894531 seconds\n",
      "\n",
      "Fold 3 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5812 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4176, Accuracy: 0.8723, F1 Micro: 0.5574, F1 Macro: 0.2408\n",
      "Epoch 2/10, Train Loss: 0.2979, Accuracy: 0.8939, F1 Micro: 0.6741, F1 Macro: 0.395\n",
      "Epoch 3/10, Train Loss: 0.2462, Accuracy: 0.9053, F1 Micro: 0.7174, F1 Macro: 0.5036\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9093, F1 Micro: 0.7255, F1 Macro: 0.5314\n",
      "Epoch 5/10, Train Loss: 0.1707, Accuracy: 0.9136, F1 Micro: 0.7447, F1 Macro: 0.5432\n",
      "Epoch 6/10, Train Loss: 0.1464, Accuracy: 0.9169, F1 Micro: 0.7633, F1 Macro: 0.5978\n",
      "Epoch 7/10, Train Loss: 0.1268, Accuracy: 0.9178, F1 Micro: 0.7577, F1 Macro: 0.5973\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9183, F1 Micro: 0.7594, F1 Macro: 0.609\n",
      "Epoch 9/10, Train Loss: 0.0952, Accuracy: 0.9165, F1 Micro: 0.7597, F1 Macro: 0.6112\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9169, F1 Micro: 0.7666, F1 Macro: 0.6298\n",
      "Best result for 5812 samples: F1 Micro: 0.7666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1142\n",
      "      Abusive       0.87      0.93      0.90      1026\n",
      "HS_Individual       0.71      0.75      0.73       723\n",
      "     HS_Group       0.65      0.69      0.67       419\n",
      "  HS_Religion       0.75      0.60      0.66       177\n",
      "      HS_Race       0.80      0.68      0.74       119\n",
      "  HS_Physical       0.33      0.01      0.02        80\n",
      "    HS_Gender       0.80      0.13      0.23        60\n",
      "     HS_Other       0.73      0.82      0.77       746\n",
      "      HS_Weak       0.68      0.73      0.70       685\n",
      "  HS_Moderate       0.58      0.65      0.61       352\n",
      "    HS_Strong       0.81      0.56      0.66       105\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5634\n",
      "    macro avg       0.71      0.62      0.63      5634\n",
      " weighted avg       0.75      0.78      0.76      5634\n",
      "  samples avg       0.45      0.45      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0003936290740966797 seconds\n",
      "\n",
      "Fold 3 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6285 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8743, F1 Micro: 0.5349, F1 Macro: 0.2388\n",
      "Epoch 2/10, Train Loss: 0.2862, Accuracy: 0.8983, F1 Micro: 0.67, F1 Macro: 0.4202\n",
      "Epoch 3/10, Train Loss: 0.2362, Accuracy: 0.9068, F1 Micro: 0.713, F1 Macro: 0.5169\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9135, F1 Micro: 0.7235, F1 Macro: 0.5488\n",
      "Epoch 5/10, Train Loss: 0.1703, Accuracy: 0.9195, F1 Micro: 0.7517, F1 Macro: 0.5857\n",
      "Epoch 6/10, Train Loss: 0.1423, Accuracy: 0.9145, F1 Micro: 0.7534, F1 Macro: 0.5846\n",
      "Epoch 7/10, Train Loss: 0.1219, Accuracy: 0.9153, F1 Micro: 0.7532, F1 Macro: 0.5828\n",
      "Epoch 8/10, Train Loss: 0.1071, Accuracy: 0.9183, F1 Micro: 0.7613, F1 Macro: 0.6105\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.9197, F1 Micro: 0.7663, F1 Macro: 0.6297\n",
      "Epoch 10/10, Train Loss: 0.0769, Accuracy: 0.9177, F1 Micro: 0.7689, F1 Macro: 0.6452\n",
      "Best result for 6285 samples: F1 Micro: 0.7689\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1142\n",
      "      Abusive       0.89      0.92      0.90      1026\n",
      "HS_Individual       0.70      0.74      0.72       723\n",
      "     HS_Group       0.67      0.73      0.70       419\n",
      "  HS_Religion       0.68      0.67      0.67       177\n",
      "      HS_Race       0.79      0.71      0.75       119\n",
      "  HS_Physical       0.50      0.03      0.05        80\n",
      "    HS_Gender       0.77      0.17      0.27        60\n",
      "     HS_Other       0.72      0.83      0.77       746\n",
      "      HS_Weak       0.69      0.71      0.70       685\n",
      "  HS_Moderate       0.61      0.65      0.63       352\n",
      "    HS_Strong       0.79      0.68      0.73       105\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5634\n",
      "    macro avg       0.72      0.64      0.65      5634\n",
      " weighted avg       0.75      0.78      0.76      5634\n",
      "  samples avg       0.45      0.45      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 0.18663597106933594 seconds\n",
      "\n",
      "Fold 3 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6584 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4017, Accuracy: 0.8768, F1 Micro: 0.5832, F1 Macro: 0.2678\n",
      "Epoch 2/10, Train Loss: 0.2879, Accuracy: 0.8975, F1 Micro: 0.6496, F1 Macro: 0.3759\n",
      "Epoch 3/10, Train Loss: 0.2317, Accuracy: 0.9054, F1 Micro: 0.7214, F1 Macro: 0.5158\n",
      "Epoch 4/10, Train Loss: 0.1936, Accuracy: 0.9111, F1 Micro: 0.7505, F1 Macro: 0.584\n",
      "Epoch 5/10, Train Loss: 0.1683, Accuracy: 0.9129, F1 Micro: 0.7544, F1 Macro: 0.5805\n",
      "Epoch 6/10, Train Loss: 0.1386, Accuracy: 0.9141, F1 Micro: 0.7595, F1 Macro: 0.5962\n",
      "Epoch 7/10, Train Loss: 0.1188, Accuracy: 0.9188, F1 Micro: 0.7647, F1 Macro: 0.5973\n",
      "Epoch 8/10, Train Loss: 0.0997, Accuracy: 0.9152, F1 Micro: 0.7577, F1 Macro: 0.5997\n",
      "Epoch 9/10, Train Loss: 0.0841, Accuracy: 0.9151, F1 Micro: 0.7586, F1 Macro: 0.6112\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9181, F1 Micro: 0.7629, F1 Macro: 0.6254\n",
      "Best result for 6584 samples: F1 Micro: 0.7647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1142\n",
      "      Abusive       0.89      0.90      0.90      1026\n",
      "HS_Individual       0.70      0.79      0.74       723\n",
      "     HS_Group       0.76      0.58      0.66       419\n",
      "  HS_Religion       0.75      0.48      0.58       177\n",
      "      HS_Race       0.86      0.61      0.71       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.73      0.82      0.77       746\n",
      "      HS_Weak       0.68      0.75      0.71       685\n",
      "  HS_Moderate       0.69      0.49      0.58       352\n",
      "    HS_Strong       0.78      0.57      0.66       105\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5634\n",
      "    macro avg       0.64      0.57      0.60      5634\n",
      " weighted avg       0.76      0.76      0.75      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 0.0003426074981689453 seconds\n",
      "\n",
      "Fold 3 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6980 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4013, Accuracy: 0.8778, F1 Micro: 0.5607, F1 Macro: 0.2635\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.9003, F1 Micro: 0.6738, F1 Macro: 0.4525\n",
      "Epoch 3/10, Train Loss: 0.2283, Accuracy: 0.9117, F1 Micro: 0.7372, F1 Macro: 0.5438\n",
      "Epoch 4/10, Train Loss: 0.1968, Accuracy: 0.9134, F1 Micro: 0.7317, F1 Macro: 0.5174\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9202, F1 Micro: 0.7543, F1 Macro: 0.5928\n",
      "Epoch 6/10, Train Loss: 0.1405, Accuracy: 0.9217, F1 Micro: 0.7643, F1 Macro: 0.608\n",
      "Epoch 7/10, Train Loss: 0.1145, Accuracy: 0.9192, F1 Micro: 0.7535, F1 Macro: 0.5865\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.9197, F1 Micro: 0.7688, F1 Macro: 0.6393\n",
      "Epoch 9/10, Train Loss: 0.089, Accuracy: 0.9161, F1 Micro: 0.7628, F1 Macro: 0.6498\n",
      "Epoch 10/10, Train Loss: 0.0734, Accuracy: 0.9201, F1 Micro: 0.7687, F1 Macro: 0.6497\n",
      "Best result for 6980 samples: F1 Micro: 0.7688\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1142\n",
      "      Abusive       0.88      0.92      0.90      1026\n",
      "HS_Individual       0.74      0.70      0.72       723\n",
      "     HS_Group       0.67      0.70      0.69       419\n",
      "  HS_Religion       0.77      0.61      0.68       177\n",
      "      HS_Race       0.79      0.72      0.75       119\n",
      "  HS_Physical       0.75      0.04      0.07        80\n",
      "    HS_Gender       0.86      0.10      0.18        60\n",
      "     HS_Other       0.74      0.79      0.77       746\n",
      "      HS_Weak       0.73      0.67      0.70       685\n",
      "  HS_Moderate       0.61      0.66      0.64       352\n",
      "    HS_Strong       0.80      0.67      0.73       105\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5634\n",
      "    macro avg       0.76      0.62      0.64      5634\n",
      " weighted avg       0.77      0.76      0.76      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 0.00039839744567871094 seconds\n",
      "\n",
      "Fold 3 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3988, Accuracy: 0.8793, F1 Micro: 0.607, F1 Macro: 0.2838\n",
      "Epoch 2/10, Train Loss: 0.2773, Accuracy: 0.8957, F1 Micro: 0.7009, F1 Macro: 0.4408\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9062, F1 Micro: 0.7355, F1 Macro: 0.5377\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.9132, F1 Micro: 0.7525, F1 Macro: 0.5802\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.92, F1 Micro: 0.7695, F1 Macro: 0.6151\n",
      "Epoch 6/10, Train Loss: 0.1324, Accuracy: 0.9181, F1 Micro: 0.7653, F1 Macro: 0.6137\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9179, F1 Micro: 0.7642, F1 Macro: 0.6135\n",
      "Epoch 8/10, Train Loss: 0.0998, Accuracy: 0.9213, F1 Micro: 0.7695, F1 Macro: 0.6336\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9145, F1 Micro: 0.7633, F1 Macro: 0.6606\n",
      "Epoch 10/10, Train Loss: 0.074, Accuracy: 0.9152, F1 Micro: 0.7635, F1 Macro: 0.655\n",
      "Best result for 7336 samples: F1 Micro: 0.7695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1142\n",
      "      Abusive       0.90      0.89      0.90      1026\n",
      "HS_Individual       0.72      0.74      0.73       723\n",
      "     HS_Group       0.75      0.65      0.70       419\n",
      "  HS_Religion       0.74      0.63      0.68       177\n",
      "      HS_Race       0.82      0.69      0.75       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.75      0.10      0.18        60\n",
      "     HS_Other       0.77      0.79      0.78       746\n",
      "      HS_Weak       0.70      0.70      0.70       685\n",
      "  HS_Moderate       0.67      0.60      0.63       352\n",
      "    HS_Strong       0.79      0.67      0.72       105\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5634\n",
      "    macro avg       0.70      0.61      0.63      5634\n",
      " weighted avg       0.78      0.75      0.76      5634\n",
      "  samples avg       0.45      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 0.0002734661102294922 seconds\n",
      "\n",
      "Fold 3 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7656 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3945, Accuracy: 0.8805, F1 Micro: 0.5967, F1 Macro: 0.2831\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.9006, F1 Micro: 0.6802, F1 Macro: 0.453\n",
      "Epoch 3/10, Train Loss: 0.2251, Accuracy: 0.9073, F1 Micro: 0.7367, F1 Macro: 0.5601\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.916, F1 Micro: 0.7554, F1 Macro: 0.5785\n",
      "Epoch 5/10, Train Loss: 0.1585, Accuracy: 0.9187, F1 Micro: 0.7588, F1 Macro: 0.5978\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9188, F1 Micro: 0.7671, F1 Macro: 0.6067\n",
      "Epoch 7/10, Train Loss: 0.1129, Accuracy: 0.9207, F1 Micro: 0.7619, F1 Macro: 0.6109\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9177, F1 Micro: 0.7671, F1 Macro: 0.6487\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9175, F1 Micro: 0.7607, F1 Macro: 0.6382\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.919, F1 Micro: 0.7651, F1 Macro: 0.6343\n",
      "Best result for 7656 samples: F1 Micro: 0.7671\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1142\n",
      "      Abusive       0.87      0.92      0.89      1026\n",
      "HS_Individual       0.72      0.73      0.73       723\n",
      "     HS_Group       0.71      0.70      0.71       419\n",
      "  HS_Religion       0.76      0.52      0.62       177\n",
      "      HS_Race       0.88      0.61      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        80\n",
      "    HS_Gender       0.00      0.00      0.00        60\n",
      "     HS_Other       0.74      0.81      0.77       746\n",
      "      HS_Weak       0.70      0.71      0.71       685\n",
      "  HS_Moderate       0.63      0.64      0.64       352\n",
      "    HS_Strong       0.81      0.54      0.65       105\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5634\n",
      "    macro avg       0.64      0.59      0.61      5634\n",
      " weighted avg       0.75      0.77      0.76      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 0.22279143333435059 seconds\n",
      "\n",
      "Fold 3 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 7901 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3865, Accuracy: 0.8813, F1 Micro: 0.5799, F1 Macro: 0.2803\n",
      "Epoch 2/10, Train Loss: 0.2725, Accuracy: 0.9038, F1 Micro: 0.7174, F1 Macro: 0.5121\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9113, F1 Micro: 0.7407, F1 Macro: 0.5447\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9156, F1 Micro: 0.7559, F1 Macro: 0.5975\n",
      "Epoch 5/10, Train Loss: 0.1579, Accuracy: 0.9162, F1 Micro: 0.7604, F1 Macro: 0.6028\n",
      "Epoch 6/10, Train Loss: 0.1304, Accuracy: 0.9189, F1 Micro: 0.7671, F1 Macro: 0.6223\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9177, F1 Micro: 0.7684, F1 Macro: 0.639\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9207, F1 Micro: 0.7731, F1 Macro: 0.649\n",
      "Epoch 9/10, Train Loss: 0.0825, Accuracy: 0.9168, F1 Micro: 0.7632, F1 Macro: 0.6241\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.9195, F1 Micro: 0.7683, F1 Macro: 0.6627\n",
      "Best result for 7901 samples: F1 Micro: 0.7731\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1142\n",
      "      Abusive       0.89      0.92      0.91      1026\n",
      "HS_Individual       0.72      0.74      0.73       723\n",
      "     HS_Group       0.70      0.71      0.71       419\n",
      "  HS_Religion       0.79      0.58      0.67       177\n",
      "      HS_Race       0.84      0.66      0.74       119\n",
      "  HS_Physical       0.70      0.09      0.16        80\n",
      "    HS_Gender       0.54      0.12      0.19        60\n",
      "     HS_Other       0.72      0.81      0.76       746\n",
      "      HS_Weak       0.71      0.72      0.71       685\n",
      "  HS_Moderate       0.63      0.67      0.65       352\n",
      "    HS_Strong       0.82      0.63      0.71       105\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5634\n",
      "    macro avg       0.74      0.63      0.65      5634\n",
      " weighted avg       0.77      0.77      0.77      5634\n",
      "  samples avg       0.44      0.44      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 0.00021839141845703125 seconds\n",
      "\n",
      "Fold 3 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8165 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3899, Accuracy: 0.8809, F1 Micro: 0.6089, F1 Macro: 0.3046\n",
      "Epoch 2/10, Train Loss: 0.2674, Accuracy: 0.9014, F1 Micro: 0.708, F1 Macro: 0.4501\n",
      "Epoch 3/10, Train Loss: 0.2227, Accuracy: 0.912, F1 Micro: 0.7347, F1 Macro: 0.5327\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.9141, F1 Micro: 0.7438, F1 Macro: 0.5411\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.921, F1 Micro: 0.7604, F1 Macro: 0.5965\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9221, F1 Micro: 0.7617, F1 Macro: 0.6051\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9218, F1 Micro: 0.771, F1 Macro: 0.6227\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.919, F1 Micro: 0.7747, F1 Macro: 0.6504\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.9211, F1 Micro: 0.7682, F1 Macro: 0.6253\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.916, F1 Micro: 0.7663, F1 Macro: 0.6619\n",
      "Best result for 8165 samples: F1 Micro: 0.7747\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1142\n",
      "      Abusive       0.87      0.93      0.90      1026\n",
      "HS_Individual       0.70      0.78      0.74       723\n",
      "     HS_Group       0.70      0.71      0.70       419\n",
      "  HS_Religion       0.70      0.56      0.62       177\n",
      "      HS_Race       0.75      0.74      0.74       119\n",
      "  HS_Physical       0.56      0.06      0.11        80\n",
      "    HS_Gender       0.59      0.17      0.26        60\n",
      "     HS_Other       0.72      0.84      0.78       746\n",
      "      HS_Weak       0.68      0.75      0.71       685\n",
      "  HS_Moderate       0.65      0.65      0.65       352\n",
      "    HS_Strong       0.78      0.70      0.74       105\n",
      "\n",
      "    micro avg       0.75      0.80      0.77      5634\n",
      "    macro avg       0.71      0.65      0.65      5634\n",
      " weighted avg       0.75      0.80      0.77      5634\n",
      "  samples avg       0.46      0.46      0.44      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 0.0001995563507080078 seconds\n",
      "\n",
      "Fold 3 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8402 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3868, Accuracy: 0.8745, F1 Micro: 0.5124, F1 Macro: 0.2349\n",
      "Epoch 2/10, Train Loss: 0.2673, Accuracy: 0.9044, F1 Micro: 0.7204, F1 Macro: 0.5125\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.9153, F1 Micro: 0.7387, F1 Macro: 0.5652\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9165, F1 Micro: 0.7618, F1 Macro: 0.6084\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9189, F1 Micro: 0.7663, F1 Macro: 0.615\n",
      "Epoch 6/10, Train Loss: 0.129, Accuracy: 0.9197, F1 Micro: 0.7675, F1 Macro: 0.6216\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.923, F1 Micro: 0.7697, F1 Macro: 0.6242\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.919, F1 Micro: 0.7704, F1 Macro: 0.6442\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9234, F1 Micro: 0.7708, F1 Macro: 0.6399\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9201, F1 Micro: 0.77, F1 Macro: 0.6651\n",
      "Best result for 8402 samples: F1 Micro: 0.7708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1142\n",
      "      Abusive       0.90      0.91      0.91      1026\n",
      "HS_Individual       0.73      0.74      0.73       723\n",
      "     HS_Group       0.78      0.58      0.66       419\n",
      "  HS_Religion       0.75      0.52      0.62       177\n",
      "      HS_Race       0.83      0.61      0.70       119\n",
      "  HS_Physical       0.67      0.07      0.13        80\n",
      "    HS_Gender       0.83      0.17      0.28        60\n",
      "     HS_Other       0.79      0.77      0.78       746\n",
      "      HS_Weak       0.71      0.72      0.71       685\n",
      "  HS_Moderate       0.73      0.50      0.59       352\n",
      "    HS_Strong       0.81      0.63      0.71       105\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5634\n",
      "    macro avg       0.78      0.59      0.64      5634\n",
      " weighted avg       0.80      0.74      0.76      5634\n",
      "  samples avg       0.45      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 0.00020051002502441406 seconds\n",
      "\n",
      "Fold 3 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8616 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.383, Accuracy: 0.8828, F1 Micro: 0.6072, F1 Macro: 0.2954\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9059, F1 Micro: 0.7027, F1 Macro: 0.4855\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.9156, F1 Micro: 0.7312, F1 Macro: 0.5656\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9186, F1 Micro: 0.7653, F1 Macro: 0.6086\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9193, F1 Micro: 0.7659, F1 Macro: 0.6127\n",
      "Epoch 6/10, Train Loss: 0.1245, Accuracy: 0.92, F1 Micro: 0.7706, F1 Macro: 0.6204\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.923, F1 Micro: 0.7656, F1 Macro: 0.6305\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.922, F1 Micro: 0.7748, F1 Macro: 0.6494\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9196, F1 Micro: 0.77, F1 Macro: 0.6548\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.921, F1 Micro: 0.7748, F1 Macro: 0.6714\n",
      "Best result for 8616 samples: F1 Micro: 0.7748\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.90      0.91      0.91      1026\n",
      "HS_Individual       0.71      0.75      0.73       723\n",
      "     HS_Group       0.73      0.67      0.70       419\n",
      "  HS_Religion       0.71      0.63      0.67       177\n",
      "      HS_Race       0.83      0.63      0.72       119\n",
      "  HS_Physical       0.67      0.07      0.13        80\n",
      "    HS_Gender       0.70      0.12      0.20        60\n",
      "     HS_Other       0.75      0.81      0.78       746\n",
      "      HS_Weak       0.69      0.72      0.70       685\n",
      "  HS_Moderate       0.68      0.61      0.64       352\n",
      "    HS_Strong       0.80      0.72      0.76       105\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5634\n",
      "    macro avg       0.75      0.62      0.65      5634\n",
      " weighted avg       0.78      0.77      0.77      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017404556274414062 seconds\n",
      "\n",
      "Fold 3 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 8816 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3839, Accuracy: 0.8816, F1 Micro: 0.5815, F1 Macro: 0.2767\n",
      "Epoch 2/10, Train Loss: 0.2604, Accuracy: 0.9076, F1 Micro: 0.711, F1 Macro: 0.5187\n",
      "Epoch 3/10, Train Loss: 0.212, Accuracy: 0.9169, F1 Micro: 0.751, F1 Macro: 0.5741\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9193, F1 Micro: 0.7661, F1 Macro: 0.6017\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9184, F1 Micro: 0.7638, F1 Macro: 0.6003\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9192, F1 Micro: 0.7672, F1 Macro: 0.6234\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9173, F1 Micro: 0.7696, F1 Macro: 0.6409\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9194, F1 Micro: 0.7715, F1 Macro: 0.6502\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.924, F1 Micro: 0.7717, F1 Macro: 0.6466\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9205, F1 Micro: 0.7739, F1 Macro: 0.6741\n",
      "Best result for 8816 samples: F1 Micro: 0.7739\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1142\n",
      "      Abusive       0.90      0.92      0.91      1026\n",
      "HS_Individual       0.68      0.76      0.72       723\n",
      "     HS_Group       0.72      0.65      0.68       419\n",
      "  HS_Religion       0.74      0.59      0.66       177\n",
      "      HS_Race       0.85      0.65      0.73       119\n",
      "  HS_Physical       0.78      0.17      0.29        80\n",
      "    HS_Gender       0.72      0.30      0.42        60\n",
      "     HS_Other       0.75      0.81      0.78       746\n",
      "      HS_Weak       0.67      0.74      0.70       685\n",
      "  HS_Moderate       0.65      0.61      0.63       352\n",
      "    HS_Strong       0.80      0.63      0.70       105\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5634\n",
      "    macro avg       0.76      0.64      0.67      5634\n",
      " weighted avg       0.77      0.78      0.77      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018167495727539062 seconds\n",
      "\n",
      "Fold 3 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9016 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.375, Accuracy: 0.8845, F1 Micro: 0.6503, F1 Macro: 0.3498\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.9059, F1 Micro: 0.719, F1 Macro: 0.4739\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9155, F1 Micro: 0.7536, F1 Macro: 0.591\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9186, F1 Micro: 0.7673, F1 Macro: 0.6124\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9193, F1 Micro: 0.7677, F1 Macro: 0.6053\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.9211, F1 Micro: 0.7666, F1 Macro: 0.6216\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9227, F1 Micro: 0.7681, F1 Macro: 0.6342\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9217, F1 Micro: 0.7724, F1 Macro: 0.6482\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9168, F1 Micro: 0.7666, F1 Macro: 0.6578\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9174, F1 Micro: 0.7663, F1 Macro: 0.6748\n",
      "Best result for 9016 samples: F1 Micro: 0.7724\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.91      0.89      0.90      1026\n",
      "HS_Individual       0.72      0.75      0.74       723\n",
      "     HS_Group       0.72      0.65      0.68       419\n",
      "  HS_Religion       0.79      0.55      0.65       177\n",
      "      HS_Race       0.84      0.67      0.75       119\n",
      "  HS_Physical       0.78      0.09      0.16        80\n",
      "    HS_Gender       0.64      0.15      0.24        60\n",
      "     HS_Other       0.75      0.81      0.78       746\n",
      "      HS_Weak       0.70      0.73      0.72       685\n",
      "  HS_Moderate       0.63      0.59      0.61       352\n",
      "    HS_Strong       0.79      0.63      0.70       105\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5634\n",
      "    macro avg       0.76      0.61      0.65      5634\n",
      " weighted avg       0.78      0.76      0.77      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00030612945556640625 seconds\n",
      "\n",
      "Fold 3 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9216 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3783, Accuracy: 0.8824, F1 Micro: 0.6514, F1 Macro: 0.3256\n",
      "Epoch 2/10, Train Loss: 0.2614, Accuracy: 0.9058, F1 Micro: 0.7073, F1 Macro: 0.4512\n",
      "Epoch 3/10, Train Loss: 0.2123, Accuracy: 0.9158, F1 Micro: 0.7427, F1 Macro: 0.5755\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9184, F1 Micro: 0.7552, F1 Macro: 0.5999\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9198, F1 Micro: 0.765, F1 Macro: 0.6118\n",
      "Epoch 6/10, Train Loss: 0.1273, Accuracy: 0.9238, F1 Micro: 0.7742, F1 Macro: 0.6486\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.9219, F1 Micro: 0.7745, F1 Macro: 0.6498\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9213, F1 Micro: 0.7752, F1 Macro: 0.66\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.917, F1 Micro: 0.7713, F1 Macro: 0.6683\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9247, F1 Micro: 0.7757, F1 Macro: 0.6793\n",
      "Best result for 9216 samples: F1 Micro: 0.7757\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1142\n",
      "      Abusive       0.91      0.90      0.91      1026\n",
      "HS_Individual       0.74      0.71      0.73       723\n",
      "     HS_Group       0.74      0.63      0.68       419\n",
      "  HS_Religion       0.77      0.64      0.70       177\n",
      "      HS_Race       0.89      0.66      0.75       119\n",
      "  HS_Physical       0.82      0.17      0.29        80\n",
      "    HS_Gender       0.76      0.32      0.45        60\n",
      "     HS_Other       0.80      0.78      0.79       746\n",
      "      HS_Weak       0.72      0.68      0.70       685\n",
      "  HS_Moderate       0.67      0.56      0.61       352\n",
      "    HS_Strong       0.77      0.64      0.70       105\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5634\n",
      "    macro avg       0.79      0.63      0.68      5634\n",
      " weighted avg       0.81      0.75      0.77      5634\n",
      "  samples avg       0.44      0.42      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 0.26216673851013184 seconds\n",
      "\n",
      "Fold 3 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3749, Accuracy: 0.8877, F1 Micro: 0.6458, F1 Macro: 0.3297\n",
      "Epoch 2/10, Train Loss: 0.2574, Accuracy: 0.9066, F1 Micro: 0.7297, F1 Macro: 0.5424\n",
      "Epoch 3/10, Train Loss: 0.2107, Accuracy: 0.9151, F1 Micro: 0.7516, F1 Macro: 0.5742\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.915, F1 Micro: 0.7603, F1 Macro: 0.6019\n",
      "Epoch 5/10, Train Loss: 0.1478, Accuracy: 0.9228, F1 Micro: 0.7702, F1 Macro: 0.6163\n",
      "Epoch 6/10, Train Loss: 0.1249, Accuracy: 0.9207, F1 Micro: 0.7686, F1 Macro: 0.627\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9246, F1 Micro: 0.7806, F1 Macro: 0.6555\n",
      "Epoch 8/10, Train Loss: 0.0895, Accuracy: 0.9209, F1 Micro: 0.774, F1 Macro: 0.6649\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9194, F1 Micro: 0.7759, F1 Macro: 0.682\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.921, F1 Micro: 0.7766, F1 Macro: 0.6736\n",
      "Best result for 9218 samples: F1 Micro: 0.7806\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1142\n",
      "      Abusive       0.88      0.92      0.90      1026\n",
      "HS_Individual       0.74      0.74      0.74       723\n",
      "     HS_Group       0.76      0.67      0.71       419\n",
      "  HS_Religion       0.78      0.58      0.67       177\n",
      "      HS_Race       0.84      0.61      0.70       119\n",
      "  HS_Physical       0.75      0.11      0.20        80\n",
      "    HS_Gender       0.73      0.13      0.23        60\n",
      "     HS_Other       0.75      0.81      0.78       746\n",
      "      HS_Weak       0.72      0.72      0.72       685\n",
      "  HS_Moderate       0.68      0.62      0.65       352\n",
      "    HS_Strong       0.82      0.62      0.71       105\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5634\n",
      "    macro avg       0.78      0.62      0.66      5634\n",
      " weighted avg       0.79      0.77      0.77      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002009868621826172 seconds\n",
      "\n",
      "Fold 3 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3766, Accuracy: 0.8843, F1 Micro: 0.6405, F1 Macro: 0.3198\n",
      "Epoch 2/10, Train Loss: 0.2596, Accuracy: 0.9045, F1 Micro: 0.6933, F1 Macro: 0.4217\n",
      "Epoch 3/10, Train Loss: 0.2131, Accuracy: 0.9172, F1 Micro: 0.7459, F1 Macro: 0.5724\n",
      "Epoch 4/10, Train Loss: 0.1736, Accuracy: 0.9179, F1 Micro: 0.7629, F1 Macro: 0.6089\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.923, F1 Micro: 0.7647, F1 Macro: 0.6071\n",
      "Epoch 6/10, Train Loss: 0.1252, Accuracy: 0.9231, F1 Micro: 0.7762, F1 Macro: 0.631\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9211, F1 Micro: 0.7769, F1 Macro: 0.6544\n",
      "Epoch 8/10, Train Loss: 0.0905, Accuracy: 0.9231, F1 Micro: 0.7704, F1 Macro: 0.6449\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9231, F1 Micro: 0.7769, F1 Macro: 0.6699\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9208, F1 Micro: 0.7763, F1 Macro: 0.6838\n",
      "Best result for 9418 samples: F1 Micro: 0.7769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1142\n",
      "      Abusive       0.88      0.94      0.91      1026\n",
      "HS_Individual       0.73      0.73      0.73       723\n",
      "     HS_Group       0.68      0.74      0.71       419\n",
      "  HS_Religion       0.74      0.64      0.69       177\n",
      "      HS_Race       0.79      0.72      0.75       119\n",
      "  HS_Physical       0.78      0.09      0.16        80\n",
      "    HS_Gender       0.57      0.13      0.22        60\n",
      "     HS_Other       0.74      0.82      0.78       746\n",
      "      HS_Weak       0.71      0.72      0.71       685\n",
      "  HS_Moderate       0.62      0.69      0.65       352\n",
      "    HS_Strong       0.79      0.63      0.70       105\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5634\n",
      "    macro avg       0.74      0.64      0.65      5634\n",
      " weighted avg       0.77      0.79      0.77      5634\n",
      "  samples avg       0.45      0.45      0.44      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018262863159179688 seconds\n",
      "\n",
      "Fold 3 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9618 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3731, Accuracy: 0.8859, F1 Micro: 0.6605, F1 Macro: 0.3525\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.9059, F1 Micro: 0.733, F1 Macro: 0.5347\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9136, F1 Micro: 0.7469, F1 Macro: 0.5687\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9216, F1 Micro: 0.7621, F1 Macro: 0.5942\n",
      "Epoch 5/10, Train Loss: 0.1444, Accuracy: 0.9235, F1 Micro: 0.7756, F1 Macro: 0.6239\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9238, F1 Micro: 0.7708, F1 Macro: 0.628\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.9211, F1 Micro: 0.7654, F1 Macro: 0.636\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9236, F1 Micro: 0.7761, F1 Macro: 0.6677\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.9218, F1 Micro: 0.7729, F1 Macro: 0.6749\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9222, F1 Micro: 0.774, F1 Macro: 0.6819\n",
      "Best result for 9618 samples: F1 Micro: 0.7761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1142\n",
      "      Abusive       0.88      0.93      0.90      1026\n",
      "HS_Individual       0.72      0.77      0.74       723\n",
      "     HS_Group       0.78      0.59      0.67       419\n",
      "  HS_Religion       0.74      0.59      0.66       177\n",
      "      HS_Race       0.89      0.60      0.71       119\n",
      "  HS_Physical       0.72      0.16      0.27        80\n",
      "    HS_Gender       0.64      0.27      0.38        60\n",
      "     HS_Other       0.79      0.77      0.78       746\n",
      "      HS_Weak       0.69      0.75      0.72       685\n",
      "  HS_Moderate       0.71      0.51      0.59       352\n",
      "    HS_Strong       0.87      0.63      0.73       105\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5634\n",
      "    macro avg       0.77      0.62      0.67      5634\n",
      " weighted avg       0.79      0.76      0.77      5634\n",
      "  samples avg       0.46      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019598007202148438 seconds\n",
      "\n",
      "Fold 3 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 9818 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3661, Accuracy: 0.8817, F1 Micro: 0.5652, F1 Macro: 0.2898\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.9076, F1 Micro: 0.7015, F1 Macro: 0.4626\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9165, F1 Micro: 0.7296, F1 Macro: 0.5493\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.918, F1 Micro: 0.7686, F1 Macro: 0.6086\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9218, F1 Micro: 0.76, F1 Macro: 0.6005\n",
      "Epoch 6/10, Train Loss: 0.1239, Accuracy: 0.9239, F1 Micro: 0.7759, F1 Macro: 0.6589\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.923, F1 Micro: 0.7668, F1 Macro: 0.651\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9212, F1 Micro: 0.7742, F1 Macro: 0.661\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9185, F1 Micro: 0.7664, F1 Macro: 0.6676\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.922, F1 Micro: 0.7729, F1 Macro: 0.6889\n",
      "Best result for 9818 samples: F1 Micro: 0.7759\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1142\n",
      "      Abusive       0.91      0.89      0.90      1026\n",
      "HS_Individual       0.77      0.70      0.73       723\n",
      "     HS_Group       0.69      0.70      0.70       419\n",
      "  HS_Religion       0.76      0.63      0.69       177\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.78      0.09      0.16        80\n",
      "    HS_Gender       0.67      0.13      0.22        60\n",
      "     HS_Other       0.77      0.78      0.78       746\n",
      "      HS_Weak       0.75      0.67      0.71       685\n",
      "  HS_Moderate       0.64      0.62      0.63       352\n",
      "    HS_Strong       0.76      0.76      0.76       105\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5634\n",
      "    macro avg       0.76      0.63      0.66      5634\n",
      " weighted avg       0.80      0.75      0.77      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017881393432617188 seconds\n",
      "\n",
      "Fold 3 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10018 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.8878, F1 Micro: 0.6605, F1 Macro: 0.3585\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9115, F1 Micro: 0.7395, F1 Macro: 0.5481\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.9137, F1 Micro: 0.7519, F1 Macro: 0.5651\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.9218, F1 Micro: 0.7636, F1 Macro: 0.6072\n",
      "Epoch 5/10, Train Loss: 0.1413, Accuracy: 0.9111, F1 Micro: 0.7565, F1 Macro: 0.5932\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9196, F1 Micro: 0.7768, F1 Macro: 0.6604\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9221, F1 Micro: 0.7785, F1 Macro: 0.6657\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.926, F1 Micro: 0.7798, F1 Macro: 0.6808\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9192, F1 Micro: 0.7763, F1 Macro: 0.6848\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9197, F1 Micro: 0.772, F1 Macro: 0.6862\n",
      "Best result for 10018 samples: F1 Micro: 0.7798\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1142\n",
      "      Abusive       0.92      0.90      0.91      1026\n",
      "HS_Individual       0.76      0.72      0.74       723\n",
      "     HS_Group       0.75      0.66      0.70       419\n",
      "  HS_Religion       0.77      0.56      0.65       177\n",
      "      HS_Race       0.86      0.68      0.76       119\n",
      "  HS_Physical       0.85      0.21      0.34        80\n",
      "    HS_Gender       0.82      0.23      0.36        60\n",
      "     HS_Other       0.79      0.78      0.78       746\n",
      "      HS_Weak       0.74      0.69      0.72       685\n",
      "  HS_Moderate       0.67      0.59      0.63       352\n",
      "    HS_Strong       0.82      0.65      0.72       105\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5634\n",
      "    macro avg       0.80      0.63      0.68      5634\n",
      " weighted avg       0.81      0.75      0.77      5634\n",
      "  samples avg       0.45      0.43      0.42      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017762184143066406 seconds\n",
      "\n",
      "Fold 3 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3645, Accuracy: 0.8895, F1 Micro: 0.627, F1 Macro: 0.3515\n",
      "Epoch 2/10, Train Loss: 0.245, Accuracy: 0.9115, F1 Micro: 0.7253, F1 Macro: 0.5207\n",
      "Epoch 3/10, Train Loss: 0.2054, Accuracy: 0.9167, F1 Micro: 0.7518, F1 Macro: 0.5789\n",
      "Epoch 4/10, Train Loss: 0.1701, Accuracy: 0.9238, F1 Micro: 0.7787, F1 Macro: 0.6224\n",
      "Epoch 5/10, Train Loss: 0.1409, Accuracy: 0.9213, F1 Micro: 0.7764, F1 Macro: 0.6326\n",
      "Epoch 6/10, Train Loss: 0.1201, Accuracy: 0.922, F1 Micro: 0.7736, F1 Macro: 0.6404\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.9235, F1 Micro: 0.7636, F1 Macro: 0.6428\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9228, F1 Micro: 0.7777, F1 Macro: 0.6593\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9227, F1 Micro: 0.7787, F1 Macro: 0.6773\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9249, F1 Micro: 0.7801, F1 Macro: 0.6953\n",
      "Best result for 10218 samples: F1 Micro: 0.7801\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1142\n",
      "      Abusive       0.93      0.90      0.91      1026\n",
      "HS_Individual       0.74      0.73      0.73       723\n",
      "     HS_Group       0.71      0.67      0.69       419\n",
      "  HS_Religion       0.69      0.67      0.68       177\n",
      "      HS_Race       0.80      0.75      0.77       119\n",
      "  HS_Physical       0.86      0.24      0.37        80\n",
      "    HS_Gender       0.70      0.32      0.44        60\n",
      "     HS_Other       0.79      0.79      0.79       746\n",
      "      HS_Weak       0.72      0.70      0.71       685\n",
      "  HS_Moderate       0.65      0.60      0.62       352\n",
      "    HS_Strong       0.83      0.72      0.77       105\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5634\n",
      "    macro avg       0.77      0.66      0.70      5634\n",
      " weighted avg       0.80      0.76      0.78      5634\n",
      "  samples avg       0.45      0.43      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001647472381591797 seconds\n",
      "\n",
      "Fold 3 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3678, Accuracy: 0.8892, F1 Micro: 0.6224, F1 Macro: 0.3404\n",
      "Epoch 2/10, Train Loss: 0.2485, Accuracy: 0.9108, F1 Micro: 0.7295, F1 Macro: 0.5218\n",
      "Epoch 3/10, Train Loss: 0.2047, Accuracy: 0.9167, F1 Micro: 0.7667, F1 Macro: 0.6107\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9226, F1 Micro: 0.7754, F1 Macro: 0.6151\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9205, F1 Micro: 0.7702, F1 Macro: 0.6463\n",
      "Epoch 6/10, Train Loss: 0.1201, Accuracy: 0.921, F1 Micro: 0.7728, F1 Macro: 0.6484\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9259, F1 Micro: 0.7824, F1 Macro: 0.6712\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9225, F1 Micro: 0.7771, F1 Macro: 0.6916\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9254, F1 Micro: 0.7855, F1 Macro: 0.7064\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9242, F1 Micro: 0.7811, F1 Macro: 0.7038\n",
      "Best result for 10418 samples: F1 Micro: 0.7855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1142\n",
      "      Abusive       0.92      0.91      0.91      1026\n",
      "HS_Individual       0.72      0.76      0.74       723\n",
      "     HS_Group       0.74      0.68      0.70       419\n",
      "  HS_Religion       0.74      0.62      0.67       177\n",
      "      HS_Race       0.81      0.74      0.77       119\n",
      "  HS_Physical       0.81      0.31      0.45        80\n",
      "    HS_Gender       0.70      0.35      0.47        60\n",
      "     HS_Other       0.76      0.81      0.78       746\n",
      "      HS_Weak       0.71      0.74      0.72       685\n",
      "  HS_Moderate       0.68      0.61      0.64       352\n",
      "    HS_Strong       0.85      0.67      0.75       105\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5634\n",
      "    macro avg       0.77      0.67      0.71      5634\n",
      " weighted avg       0.79      0.78      0.78      5634\n",
      "  samples avg       0.45      0.44      0.43      5634\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 0.29882121086120605 seconds\n",
      "\n",
      "Fold 3 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 10535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3664, Accuracy: 0.8905, F1 Micro: 0.6222, F1 Macro: 0.338\n",
      "Epoch 2/10, Train Loss: 0.2477, Accuracy: 0.9117, F1 Micro: 0.7225, F1 Macro: 0.5315\n",
      "Epoch 3/10, Train Loss: 0.1999, Accuracy: 0.9138, F1 Micro: 0.7566, F1 Macro: 0.6044\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9193, F1 Micro: 0.772, F1 Macro: 0.6207\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9227, F1 Micro: 0.7784, F1 Macro: 0.635\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.9216, F1 Micro: 0.7766, F1 Macro: 0.6566\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9172, F1 Micro: 0.7751, F1 Macro: 0.665\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.9206, F1 Micro: 0.7783, F1 Macro: 0.6848\n",
      "Epoch 9/10, Train Loss: 0.071, Accuracy: 0.9247, F1 Micro: 0.7783, F1 Macro: 0.7042\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9266, F1 Micro: 0.7826, F1 Macro: 0.7094\n",
      "Best result for 10535 samples: F1 Micro: 0.7826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1142\n",
      "      Abusive       0.92      0.89      0.91      1026\n",
      "HS_Individual       0.78      0.68      0.73       723\n",
      "     HS_Group       0.70      0.70      0.70       419\n",
      "  HS_Religion       0.75      0.63      0.69       177\n",
      "      HS_Race       0.82      0.75      0.78       119\n",
      "  HS_Physical       0.85      0.29      0.43        80\n",
      "    HS_Gender       0.79      0.37      0.50        60\n",
      "     HS_Other       0.80      0.80      0.80       746\n",
      "      HS_Weak       0.76      0.66      0.71       685\n",
      "  HS_Moderate       0.63      0.64      0.64       352\n",
      "    HS_Strong       0.84      0.72      0.78       105\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5634\n",
      "    macro avg       0.79      0.67      0.71      5634\n",
      " weighted avg       0.81      0.76      0.78      5634\n",
      "  samples avg       0.44      0.43      0.42      5634\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 5838.45 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 658 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5946, Accuracy: 0.8335, F1 Micro: 0.1121, F1 Macro: 0.0446\n",
      "Epoch 2/10, Train Loss: 0.4551, Accuracy: 0.831, F1 Micro: 0.0309, F1 Macro: 0.0129\n",
      "Epoch 3/10, Train Loss: 0.3992, Accuracy: 0.8345, F1 Micro: 0.0736, F1 Macro: 0.0279\n",
      "Epoch 4/10, Train Loss: 0.365, Accuracy: 0.8375, F1 Micro: 0.1139, F1 Macro: 0.0398\n",
      "Epoch 5/10, Train Loss: 0.3536, Accuracy: 0.8479, F1 Micro: 0.2404, F1 Macro: 0.0824\n",
      "Epoch 6/10, Train Loss: 0.3469, Accuracy: 0.8564, F1 Micro: 0.3297, F1 Macro: 0.1173\n",
      "Epoch 7/10, Train Loss: 0.3272, Accuracy: 0.8666, F1 Micro: 0.429, F1 Macro: 0.1693\n",
      "Epoch 8/10, Train Loss: 0.3093, Accuracy: 0.8769, F1 Micro: 0.534, F1 Macro: 0.2344\n",
      "Epoch 9/10, Train Loss: 0.2709, Accuracy: 0.8785, F1 Micro: 0.5631, F1 Macro: 0.2606\n",
      "Epoch 10/10, Train Loss: 0.261, Accuracy: 0.8819, F1 Micro: 0.5885, F1 Macro: 0.2918\n",
      "Best result for 658 samples: F1 Micro: 0.5885\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.75      0.77      1107\n",
      "      Abusive       0.82      0.74      0.78      1030\n",
      "HS_Individual       0.64      0.49      0.55       729\n",
      "     HS_Group       0.61      0.12      0.21       378\n",
      "  HS_Religion       0.00      0.00      0.00       167\n",
      "      HS_Race       0.00      0.00      0.00        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.66      0.58      0.62       744\n",
      "      HS_Weak       0.60      0.41      0.49       690\n",
      "  HS_Moderate       0.48      0.05      0.09       338\n",
      "    HS_Strong       0.00      0.00      0.00        79\n",
      "\n",
      "    micro avg       0.72      0.50      0.59      5499\n",
      "    macro avg       0.38      0.26      0.29      5499\n",
      " weighted avg       0.64      0.50      0.54      5499\n",
      "  samples avg       0.38      0.29      0.30      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 0.0006439685821533203 seconds\n",
      "\n",
      "Fold 4 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1646 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5166, Accuracy: 0.8313, F1 Micro: 0.0402, F1 Macro: 0.0165\n",
      "Epoch 2/10, Train Loss: 0.3903, Accuracy: 0.8381, F1 Micro: 0.1194, F1 Macro: 0.0427\n",
      "Epoch 3/10, Train Loss: 0.3624, Accuracy: 0.8673, F1 Micro: 0.4866, F1 Macro: 0.1762\n",
      "Epoch 4/10, Train Loss: 0.3241, Accuracy: 0.8776, F1 Micro: 0.525, F1 Macro: 0.2319\n",
      "Epoch 5/10, Train Loss: 0.2924, Accuracy: 0.8849, F1 Micro: 0.5871, F1 Macro: 0.2967\n",
      "Epoch 6/10, Train Loss: 0.2648, Accuracy: 0.8913, F1 Micro: 0.6372, F1 Macro: 0.3438\n",
      "Epoch 7/10, Train Loss: 0.2437, Accuracy: 0.8931, F1 Micro: 0.6433, F1 Macro: 0.3485\n",
      "Epoch 8/10, Train Loss: 0.2094, Accuracy: 0.8965, F1 Micro: 0.6604, F1 Macro: 0.384\n",
      "Epoch 9/10, Train Loss: 0.1875, Accuracy: 0.8979, F1 Micro: 0.6682, F1 Macro: 0.4077\n",
      "Epoch 10/10, Train Loss: 0.1749, Accuracy: 0.8981, F1 Micro: 0.6947, F1 Macro: 0.4462\n",
      "Best result for 1646 samples: F1 Micro: 0.6947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1107\n",
      "      Abusive       0.84      0.86      0.85      1030\n",
      "HS_Individual       0.63      0.72      0.67       729\n",
      "     HS_Group       0.71      0.47      0.56       378\n",
      "  HS_Religion       0.76      0.11      0.20       167\n",
      "      HS_Race       0.87      0.15      0.25        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.64      0.79      0.70       744\n",
      "      HS_Weak       0.60      0.68      0.64       690\n",
      "  HS_Moderate       0.62      0.36      0.46       338\n",
      "    HS_Strong       0.90      0.11      0.20        79\n",
      "\n",
      "    micro avg       0.71      0.68      0.69      5499\n",
      "    macro avg       0.61      0.42      0.45      5499\n",
      " weighted avg       0.70      0.68      0.67      5499\n",
      "  samples avg       0.40      0.38      0.37      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 0.0005443096160888672 seconds\n",
      "\n",
      "Fold 4 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4856, Accuracy: 0.8351, F1 Micro: 0.0796, F1 Macro: 0.0301\n",
      "Epoch 2/10, Train Loss: 0.3757, Accuracy: 0.8672, F1 Micro: 0.4692, F1 Macro: 0.1699\n",
      "Epoch 3/10, Train Loss: 0.3255, Accuracy: 0.8848, F1 Micro: 0.6116, F1 Macro: 0.3002\n",
      "Epoch 4/10, Train Loss: 0.2867, Accuracy: 0.8907, F1 Micro: 0.6032, F1 Macro: 0.3252\n",
      "Epoch 5/10, Train Loss: 0.2524, Accuracy: 0.8995, F1 Micro: 0.6866, F1 Macro: 0.4482\n",
      "Epoch 6/10, Train Loss: 0.2232, Accuracy: 0.9002, F1 Micro: 0.7017, F1 Macro: 0.4581\n",
      "Epoch 7/10, Train Loss: 0.2021, Accuracy: 0.9069, F1 Micro: 0.7077, F1 Macro: 0.4819\n",
      "Epoch 8/10, Train Loss: 0.1671, Accuracy: 0.9079, F1 Micro: 0.7023, F1 Macro: 0.5066\n",
      "Epoch 9/10, Train Loss: 0.1535, Accuracy: 0.911, F1 Micro: 0.7263, F1 Macro: 0.5149\n",
      "Epoch 10/10, Train Loss: 0.1354, Accuracy: 0.9102, F1 Micro: 0.7289, F1 Macro: 0.5339\n",
      "Best result for 2535 samples: F1 Micro: 0.7289\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1107\n",
      "      Abusive       0.86      0.87      0.86      1030\n",
      "HS_Individual       0.70      0.71      0.71       729\n",
      "     HS_Group       0.70      0.60      0.64       378\n",
      "  HS_Religion       0.80      0.43      0.56       167\n",
      "      HS_Race       0.80      0.38      0.51        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.70      0.77      0.73       744\n",
      "      HS_Weak       0.67      0.69      0.68       690\n",
      "  HS_Moderate       0.61      0.51      0.56       338\n",
      "    HS_Strong       0.94      0.20      0.33        79\n",
      "\n",
      "    micro avg       0.75      0.71      0.73      5499\n",
      "    macro avg       0.63      0.50      0.53      5499\n",
      " weighted avg       0.73      0.71      0.71      5499\n",
      "  samples avg       0.41      0.39      0.38      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 0.0004878044128417969 seconds\n",
      "\n",
      "Fold 4 - New train size: 3335\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3335 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4583, Accuracy: 0.841, F1 Micro: 0.1571, F1 Macro: 0.0549\n",
      "Epoch 2/10, Train Loss: 0.3552, Accuracy: 0.8806, F1 Micro: 0.5656, F1 Macro: 0.2478\n",
      "Epoch 3/10, Train Loss: 0.2919, Accuracy: 0.8931, F1 Micro: 0.6391, F1 Macro: 0.3369\n",
      "Epoch 4/10, Train Loss: 0.2442, Accuracy: 0.8995, F1 Micro: 0.6791, F1 Macro: 0.4206\n",
      "Epoch 5/10, Train Loss: 0.2208, Accuracy: 0.9062, F1 Micro: 0.698, F1 Macro: 0.4782\n",
      "Epoch 6/10, Train Loss: 0.1909, Accuracy: 0.9078, F1 Micro: 0.6905, F1 Macro: 0.4618\n",
      "Epoch 7/10, Train Loss: 0.1704, Accuracy: 0.9118, F1 Micro: 0.7109, F1 Macro: 0.5095\n",
      "Epoch 8/10, Train Loss: 0.1424, Accuracy: 0.9145, F1 Micro: 0.7324, F1 Macro: 0.5461\n",
      "Epoch 9/10, Train Loss: 0.1215, Accuracy: 0.9113, F1 Micro: 0.7253, F1 Macro: 0.5228\n",
      "Epoch 10/10, Train Loss: 0.1087, Accuracy: 0.9138, F1 Micro: 0.7434, F1 Macro: 0.5647\n",
      "Best result for 3335 samples: F1 Micro: 0.7434\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1107\n",
      "      Abusive       0.86      0.90      0.88      1030\n",
      "HS_Individual       0.69      0.74      0.72       729\n",
      "     HS_Group       0.73      0.59      0.65       378\n",
      "  HS_Religion       0.76      0.46      0.57       167\n",
      "      HS_Race       0.77      0.39      0.52        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.71      0.79      0.75       744\n",
      "      HS_Weak       0.67      0.72      0.69       690\n",
      "  HS_Moderate       0.62      0.52      0.56       338\n",
      "    HS_Strong       0.84      0.47      0.60        79\n",
      "\n",
      "    micro avg       0.75      0.73      0.74      5499\n",
      "    macro avg       0.62      0.53      0.56      5499\n",
      " weighted avg       0.73      0.73      0.73      5499\n",
      "  samples avg       0.42      0.41      0.40      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 0.0005097389221191406 seconds\n",
      "\n",
      "Fold 4 - New train size: 4055\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4055 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4456, Accuracy: 0.8516, F1 Micro: 0.2761, F1 Macro: 0.0952\n",
      "Epoch 2/10, Train Loss: 0.3347, Accuracy: 0.8876, F1 Micro: 0.6059, F1 Macro: 0.2927\n",
      "Epoch 3/10, Train Loss: 0.2709, Accuracy: 0.8998, F1 Micro: 0.6756, F1 Macro: 0.3846\n",
      "Epoch 4/10, Train Loss: 0.2359, Accuracy: 0.9059, F1 Micro: 0.7064, F1 Macro: 0.4525\n",
      "Epoch 5/10, Train Loss: 0.1987, Accuracy: 0.9113, F1 Micro: 0.7366, F1 Macro: 0.532\n",
      "Epoch 6/10, Train Loss: 0.1726, Accuracy: 0.9151, F1 Micro: 0.7374, F1 Macro: 0.5486\n",
      "Epoch 7/10, Train Loss: 0.1458, Accuracy: 0.9173, F1 Micro: 0.7471, F1 Macro: 0.5668\n",
      "Epoch 8/10, Train Loss: 0.1286, Accuracy: 0.9161, F1 Micro: 0.7517, F1 Macro: 0.5761\n",
      "Epoch 9/10, Train Loss: 0.1088, Accuracy: 0.9182, F1 Micro: 0.743, F1 Macro: 0.5749\n",
      "Epoch 10/10, Train Loss: 0.092, Accuracy: 0.9184, F1 Micro: 0.746, F1 Macro: 0.5865\n",
      "Best result for 4055 samples: F1 Micro: 0.7517\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1107\n",
      "      Abusive       0.85      0.90      0.88      1030\n",
      "HS_Individual       0.68      0.80      0.74       729\n",
      "     HS_Group       0.79      0.53      0.63       378\n",
      "  HS_Religion       0.82      0.47      0.60       167\n",
      "      HS_Race       0.80      0.47      0.59        88\n",
      "  HS_Physical       0.00      0.00      0.00        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.72      0.78      0.75       744\n",
      "      HS_Weak       0.65      0.78      0.71       690\n",
      "  HS_Moderate       0.72      0.45      0.55       338\n",
      "    HS_Strong       0.85      0.49      0.62        79\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5499\n",
      "    macro avg       0.64      0.54      0.58      5499\n",
      " weighted avg       0.74      0.74      0.74      5499\n",
      "  samples avg       0.43      0.41      0.40      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 0.0006890296936035156 seconds\n",
      "\n",
      "Fold 4 - New train size: 4703\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4703 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4383, Accuracy: 0.8624, F1 Micro: 0.4233, F1 Macro: 0.1459\n",
      "Epoch 2/10, Train Loss: 0.3256, Accuracy: 0.8909, F1 Micro: 0.6566, F1 Macro: 0.3504\n",
      "Epoch 3/10, Train Loss: 0.2631, Accuracy: 0.9027, F1 Micro: 0.6837, F1 Macro: 0.4218\n",
      "Epoch 4/10, Train Loss: 0.2245, Accuracy: 0.9107, F1 Micro: 0.7164, F1 Macro: 0.5082\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9142, F1 Micro: 0.7381, F1 Macro: 0.5601\n",
      "Epoch 6/10, Train Loss: 0.1642, Accuracy: 0.9178, F1 Micro: 0.7442, F1 Macro: 0.5881\n",
      "Epoch 7/10, Train Loss: 0.1369, Accuracy: 0.9188, F1 Micro: 0.7508, F1 Macro: 0.5627\n",
      "Epoch 8/10, Train Loss: 0.1221, Accuracy: 0.9195, F1 Micro: 0.7462, F1 Macro: 0.5772\n",
      "Epoch 9/10, Train Loss: 0.1056, Accuracy: 0.9213, F1 Micro: 0.7659, F1 Macro: 0.5999\n",
      "Epoch 10/10, Train Loss: 0.0932, Accuracy: 0.9202, F1 Micro: 0.7594, F1 Macro: 0.6077\n",
      "Best result for 4703 samples: F1 Micro: 0.7659\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1107\n",
      "      Abusive       0.88      0.91      0.89      1030\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.74      0.58      0.65       378\n",
      "  HS_Religion       0.79      0.53      0.64       167\n",
      "      HS_Race       0.70      0.52      0.60        88\n",
      "  HS_Physical       1.00      0.03      0.05        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.77      0.78      0.77       744\n",
      "      HS_Weak       0.68      0.77      0.72       690\n",
      "  HS_Moderate       0.69      0.52      0.59       338\n",
      "    HS_Strong       0.81      0.59      0.69        79\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5499\n",
      "    macro avg       0.72      0.57      0.60      5499\n",
      " weighted avg       0.77      0.76      0.75      5499\n",
      "  samples avg       0.45      0.42      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.0004031658172607422 seconds\n",
      "\n",
      "Fold 4 - New train size: 5287\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5287 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4267, Accuracy: 0.8686, F1 Micro: 0.4841, F1 Macro: 0.1842\n",
      "Epoch 2/10, Train Loss: 0.308, Accuracy: 0.8904, F1 Micro: 0.6016, F1 Macro: 0.3356\n",
      "Epoch 3/10, Train Loss: 0.2496, Accuracy: 0.9053, F1 Micro: 0.7054, F1 Macro: 0.455\n",
      "Epoch 4/10, Train Loss: 0.212, Accuracy: 0.9121, F1 Micro: 0.7379, F1 Macro: 0.5326\n",
      "Epoch 5/10, Train Loss: 0.1856, Accuracy: 0.9173, F1 Micro: 0.7515, F1 Macro: 0.5713\n",
      "Epoch 6/10, Train Loss: 0.1572, Accuracy: 0.9195, F1 Micro: 0.7487, F1 Macro: 0.5867\n",
      "Epoch 7/10, Train Loss: 0.1317, Accuracy: 0.9201, F1 Micro: 0.7572, F1 Macro: 0.5882\n",
      "Epoch 8/10, Train Loss: 0.1114, Accuracy: 0.9172, F1 Micro: 0.7561, F1 Macro: 0.5909\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.9204, F1 Micro: 0.7599, F1 Macro: 0.5888\n",
      "Epoch 10/10, Train Loss: 0.085, Accuracy: 0.9189, F1 Micro: 0.7515, F1 Macro: 0.6032\n",
      "Best result for 5287 samples: F1 Micro: 0.7599\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1107\n",
      "      Abusive       0.87      0.90      0.89      1030\n",
      "HS_Individual       0.71      0.75      0.73       729\n",
      "     HS_Group       0.79      0.54      0.64       378\n",
      "  HS_Religion       0.83      0.49      0.62       167\n",
      "      HS_Race       0.78      0.52      0.63        88\n",
      "  HS_Physical       1.00      0.01      0.03        74\n",
      "    HS_Gender       0.00      0.00      0.00        75\n",
      "     HS_Other       0.75      0.81      0.78       744\n",
      "      HS_Weak       0.68      0.74      0.71       690\n",
      "  HS_Moderate       0.73      0.48      0.58       338\n",
      "    HS_Strong       0.80      0.52      0.63        79\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5499\n",
      "    macro avg       0.73      0.55      0.59      5499\n",
      " weighted avg       0.78      0.74      0.75      5499\n",
      "  samples avg       0.43      0.41      0.41      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.00043320655822753906 seconds\n",
      "\n",
      "Fold 4 - New train size: 5812\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5812 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4179, Accuracy: 0.8746, F1 Micro: 0.5205, F1 Macro: 0.2118\n",
      "Epoch 2/10, Train Loss: 0.2994, Accuracy: 0.8977, F1 Micro: 0.6608, F1 Macro: 0.3453\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.9078, F1 Micro: 0.7227, F1 Macro: 0.5198\n",
      "Epoch 4/10, Train Loss: 0.2068, Accuracy: 0.9128, F1 Micro: 0.7232, F1 Macro: 0.5236\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.917, F1 Micro: 0.7335, F1 Macro: 0.5454\n",
      "Epoch 6/10, Train Loss: 0.1576, Accuracy: 0.9173, F1 Micro: 0.7494, F1 Macro: 0.5461\n",
      "Epoch 7/10, Train Loss: 0.1299, Accuracy: 0.9226, F1 Micro: 0.7697, F1 Macro: 0.6066\n",
      "Epoch 8/10, Train Loss: 0.1062, Accuracy: 0.9216, F1 Micro: 0.7669, F1 Macro: 0.61\n",
      "Epoch 9/10, Train Loss: 0.0928, Accuracy: 0.9221, F1 Micro: 0.7641, F1 Macro: 0.6094\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.923, F1 Micro: 0.7721, F1 Macro: 0.657\n",
      "Best result for 5812 samples: F1 Micro: 0.7721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1107\n",
      "      Abusive       0.88      0.92      0.90      1030\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.73      0.58      0.65       378\n",
      "  HS_Religion       0.72      0.63      0.67       167\n",
      "      HS_Race       0.78      0.66      0.72        88\n",
      "  HS_Physical       0.57      0.11      0.18        74\n",
      "    HS_Gender       0.89      0.23      0.36        75\n",
      "     HS_Other       0.78      0.77      0.77       744\n",
      "      HS_Weak       0.68      0.77      0.72       690\n",
      "  HS_Moderate       0.70      0.51      0.59       338\n",
      "    HS_Strong       0.81      0.66      0.73        79\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5499\n",
      "    macro avg       0.76      0.62      0.66      5499\n",
      " weighted avg       0.78      0.77      0.76      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0003573894500732422 seconds\n",
      "\n",
      "Fold 4 - New train size: 6285\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6285 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4148, Accuracy: 0.8705, F1 Micro: 0.4521, F1 Macro: 0.1794\n",
      "Epoch 2/10, Train Loss: 0.2901, Accuracy: 0.8989, F1 Micro: 0.6631, F1 Macro: 0.3662\n",
      "Epoch 3/10, Train Loss: 0.2395, Accuracy: 0.912, F1 Micro: 0.716, F1 Macro: 0.5068\n",
      "Epoch 4/10, Train Loss: 0.2004, Accuracy: 0.9174, F1 Micro: 0.754, F1 Macro: 0.5534\n",
      "Epoch 5/10, Train Loss: 0.1666, Accuracy: 0.9209, F1 Micro: 0.757, F1 Macro: 0.5951\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9199, F1 Micro: 0.7632, F1 Macro: 0.5855\n",
      "Epoch 7/10, Train Loss: 0.1231, Accuracy: 0.9222, F1 Micro: 0.7566, F1 Macro: 0.5964\n",
      "Epoch 8/10, Train Loss: 0.1038, Accuracy: 0.9227, F1 Micro: 0.7668, F1 Macro: 0.6235\n",
      "Epoch 9/10, Train Loss: 0.0904, Accuracy: 0.923, F1 Micro: 0.7705, F1 Macro: 0.6363\n",
      "Epoch 10/10, Train Loss: 0.0768, Accuracy: 0.9248, F1 Micro: 0.775, F1 Macro: 0.6556\n",
      "Best result for 6285 samples: F1 Micro: 0.775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1107\n",
      "      Abusive       0.90      0.91      0.90      1030\n",
      "HS_Individual       0.74      0.75      0.74       729\n",
      "     HS_Group       0.73      0.63      0.68       378\n",
      "  HS_Religion       0.77      0.59      0.67       167\n",
      "      HS_Race       0.77      0.53      0.63        88\n",
      "  HS_Physical       0.53      0.14      0.22        74\n",
      "    HS_Gender       0.73      0.25      0.38        75\n",
      "     HS_Other       0.76      0.80      0.78       744\n",
      "      HS_Weak       0.71      0.74      0.72       690\n",
      "  HS_Moderate       0.67      0.57      0.62       338\n",
      "    HS_Strong       0.83      0.57      0.68        79\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5499\n",
      "    macro avg       0.75      0.61      0.66      5499\n",
      " weighted avg       0.79      0.76      0.77      5499\n",
      "  samples avg       0.44      0.42      0.41      5499\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 0.18480706214904785 seconds\n",
      "\n",
      "Fold 4 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6584 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4083, Accuracy: 0.8748, F1 Micro: 0.4966, F1 Macro: 0.2055\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.8977, F1 Micro: 0.6442, F1 Macro: 0.362\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.9122, F1 Micro: 0.7266, F1 Macro: 0.5136\n",
      "Epoch 4/10, Train Loss: 0.2005, Accuracy: 0.9168, F1 Micro: 0.7565, F1 Macro: 0.5833\n",
      "Epoch 5/10, Train Loss: 0.1702, Accuracy: 0.9198, F1 Micro: 0.7641, F1 Macro: 0.5962\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9234, F1 Micro: 0.7625, F1 Macro: 0.5882\n",
      "Epoch 7/10, Train Loss: 0.1179, Accuracy: 0.918, F1 Micro: 0.7676, F1 Macro: 0.6138\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.922, F1 Micro: 0.7689, F1 Macro: 0.6051\n",
      "Epoch 9/10, Train Loss: 0.089, Accuracy: 0.922, F1 Micro: 0.7653, F1 Macro: 0.6238\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.924, F1 Micro: 0.7735, F1 Macro: 0.6344\n",
      "Best result for 6584 samples: F1 Micro: 0.7735\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1107\n",
      "      Abusive       0.89      0.92      0.90      1030\n",
      "HS_Individual       0.72      0.77      0.75       729\n",
      "     HS_Group       0.74      0.59      0.66       378\n",
      "  HS_Religion       0.83      0.55      0.66       167\n",
      "      HS_Race       0.79      0.39      0.52        88\n",
      "  HS_Physical       0.64      0.12      0.20        74\n",
      "    HS_Gender       0.83      0.13      0.23        75\n",
      "     HS_Other       0.74      0.83      0.78       744\n",
      "      HS_Weak       0.70      0.74      0.72       690\n",
      "  HS_Moderate       0.68      0.54      0.60       338\n",
      "    HS_Strong       0.84      0.65      0.73        79\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5499\n",
      "    macro avg       0.77      0.59      0.63      5499\n",
      " weighted avg       0.78      0.76      0.76      5499\n",
      "  samples avg       0.44      0.42      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 0.0003962516784667969 seconds\n",
      "\n",
      "Fold 4 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6980 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4058, Accuracy: 0.8823, F1 Micro: 0.5885, F1 Macro: 0.2705\n",
      "Epoch 2/10, Train Loss: 0.2774, Accuracy: 0.9025, F1 Micro: 0.6896, F1 Macro: 0.426\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.9116, F1 Micro: 0.7053, F1 Macro: 0.5083\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9189, F1 Micro: 0.7567, F1 Macro: 0.5906\n",
      "Epoch 5/10, Train Loss: 0.16, Accuracy: 0.9218, F1 Micro: 0.7546, F1 Macro: 0.5899\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9192, F1 Micro: 0.7596, F1 Macro: 0.5947\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9241, F1 Micro: 0.7729, F1 Macro: 0.6075\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9209, F1 Micro: 0.7671, F1 Macro: 0.6132\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9218, F1 Micro: 0.7691, F1 Macro: 0.623\n",
      "Epoch 10/10, Train Loss: 0.0769, Accuracy: 0.9258, F1 Micro: 0.7719, F1 Macro: 0.6627\n",
      "Best result for 6980 samples: F1 Micro: 0.7729\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1107\n",
      "      Abusive       0.88      0.92      0.90      1030\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.79      0.60      0.68       378\n",
      "  HS_Religion       0.81      0.56      0.66       167\n",
      "      HS_Race       0.82      0.35      0.49        88\n",
      "  HS_Physical       1.00      0.05      0.10        74\n",
      "    HS_Gender       0.50      0.03      0.05        75\n",
      "     HS_Other       0.73      0.81      0.77       744\n",
      "      HS_Weak       0.70      0.76      0.73       690\n",
      "  HS_Moderate       0.72      0.54      0.62       338\n",
      "    HS_Strong       0.81      0.59      0.69        79\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5499\n",
      "    macro avg       0.78      0.57      0.61      5499\n",
      " weighted avg       0.79      0.76      0.76      5499\n",
      "  samples avg       0.44      0.42      0.41      5499\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 0.0003528594970703125 seconds\n",
      "\n",
      "Fold 4 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3981, Accuracy: 0.8803, F1 Micro: 0.5557, F1 Macro: 0.2514\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.9024, F1 Micro: 0.6925, F1 Macro: 0.4242\n",
      "Epoch 3/10, Train Loss: 0.2267, Accuracy: 0.9161, F1 Micro: 0.7351, F1 Macro: 0.5528\n",
      "Epoch 4/10, Train Loss: 0.1922, Accuracy: 0.9201, F1 Micro: 0.7519, F1 Macro: 0.5796\n",
      "Epoch 5/10, Train Loss: 0.1609, Accuracy: 0.9223, F1 Micro: 0.7503, F1 Macro: 0.5906\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9235, F1 Micro: 0.7685, F1 Macro: 0.5966\n",
      "Epoch 7/10, Train Loss: 0.1158, Accuracy: 0.9228, F1 Micro: 0.7713, F1 Macro: 0.6052\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9218, F1 Micro: 0.7636, F1 Macro: 0.6201\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.923, F1 Micro: 0.7731, F1 Macro: 0.6381\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9234, F1 Micro: 0.7747, F1 Macro: 0.6356\n",
      "Best result for 7336 samples: F1 Micro: 0.7747\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1107\n",
      "      Abusive       0.90      0.92      0.91      1030\n",
      "HS_Individual       0.70      0.79      0.74       729\n",
      "     HS_Group       0.77      0.59      0.67       378\n",
      "  HS_Religion       0.76      0.58      0.66       167\n",
      "      HS_Race       0.77      0.56      0.64        88\n",
      "  HS_Physical       0.64      0.09      0.16        74\n",
      "    HS_Gender       0.78      0.09      0.17        75\n",
      "     HS_Other       0.74      0.82      0.78       744\n",
      "      HS_Weak       0.67      0.78      0.72       690\n",
      "  HS_Moderate       0.70      0.53      0.60       338\n",
      "    HS_Strong       0.82      0.63      0.71        79\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5499\n",
      "    macro avg       0.76      0.60      0.64      5499\n",
      " weighted avg       0.78      0.77      0.76      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 0.0003161430358886719 seconds\n",
      "\n",
      "Fold 4 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7656 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.8827, F1 Micro: 0.6049, F1 Macro: 0.2835\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9036, F1 Micro: 0.6955, F1 Macro: 0.4282\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9159, F1 Micro: 0.7513, F1 Macro: 0.5709\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9202, F1 Micro: 0.7438, F1 Macro: 0.5721\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9195, F1 Micro: 0.7676, F1 Macro: 0.5872\n",
      "Epoch 6/10, Train Loss: 0.135, Accuracy: 0.9227, F1 Micro: 0.7733, F1 Macro: 0.6048\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.925, F1 Micro: 0.7721, F1 Macro: 0.6058\n",
      "Epoch 8/10, Train Loss: 0.1, Accuracy: 0.9256, F1 Micro: 0.7748, F1 Macro: 0.6454\n",
      "Epoch 9/10, Train Loss: 0.0831, Accuracy: 0.9204, F1 Micro: 0.7739, F1 Macro: 0.6426\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9234, F1 Micro: 0.7768, F1 Macro: 0.6503\n",
      "Best result for 7656 samples: F1 Micro: 0.7768\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1107\n",
      "      Abusive       0.87      0.94      0.90      1030\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.75      0.62      0.68       378\n",
      "  HS_Religion       0.75      0.59      0.66       167\n",
      "      HS_Race       0.74      0.59      0.66        88\n",
      "  HS_Physical       0.53      0.14      0.22        74\n",
      "    HS_Gender       0.69      0.15      0.24        75\n",
      "     HS_Other       0.75      0.83      0.79       744\n",
      "      HS_Weak       0.68      0.77      0.72       690\n",
      "  HS_Moderate       0.69      0.57      0.62       338\n",
      "    HS_Strong       0.83      0.63      0.72        79\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5499\n",
      "    macro avg       0.74      0.62      0.65      5499\n",
      " weighted avg       0.77      0.78      0.77      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 0.22397994995117188 seconds\n",
      "\n",
      "Fold 4 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 7901 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.394, Accuracy: 0.8853, F1 Micro: 0.6101, F1 Macro: 0.2817\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.9061, F1 Micro: 0.6906, F1 Macro: 0.4605\n",
      "Epoch 3/10, Train Loss: 0.2161, Accuracy: 0.917, F1 Micro: 0.7481, F1 Macro: 0.5532\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9202, F1 Micro: 0.7575, F1 Macro: 0.5549\n",
      "Epoch 5/10, Train Loss: 0.1523, Accuracy: 0.923, F1 Micro: 0.7559, F1 Macro: 0.5877\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.9222, F1 Micro: 0.7743, F1 Macro: 0.6143\n",
      "Epoch 7/10, Train Loss: 0.1104, Accuracy: 0.9244, F1 Micro: 0.7698, F1 Macro: 0.6109\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.9242, F1 Micro: 0.7635, F1 Macro: 0.6293\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9238, F1 Micro: 0.7763, F1 Macro: 0.6542\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9222, F1 Micro: 0.7702, F1 Macro: 0.6781\n",
      "Best result for 7901 samples: F1 Micro: 0.7763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1107\n",
      "      Abusive       0.87      0.94      0.90      1030\n",
      "HS_Individual       0.71      0.78      0.75       729\n",
      "     HS_Group       0.77      0.61      0.68       378\n",
      "  HS_Religion       0.79      0.58      0.67       167\n",
      "      HS_Race       0.77      0.52      0.62        88\n",
      "  HS_Physical       0.53      0.14      0.22        74\n",
      "    HS_Gender       0.74      0.23      0.35        75\n",
      "     HS_Other       0.75      0.81      0.78       744\n",
      "      HS_Weak       0.68      0.77      0.72       690\n",
      "  HS_Moderate       0.70      0.56      0.62       338\n",
      "    HS_Strong       0.84      0.59      0.70        79\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5499\n",
      "    macro avg       0.75      0.62      0.65      5499\n",
      " weighted avg       0.77      0.78      0.77      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 0.00021409988403320312 seconds\n",
      "\n",
      "Fold 4 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8165 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3894, Accuracy: 0.8849, F1 Micro: 0.6044, F1 Macro: 0.2869\n",
      "Epoch 2/10, Train Loss: 0.2679, Accuracy: 0.9037, F1 Micro: 0.6582, F1 Macro: 0.4058\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.9167, F1 Micro: 0.7281, F1 Macro: 0.5452\n",
      "Epoch 4/10, Train Loss: 0.1857, Accuracy: 0.9216, F1 Micro: 0.7616, F1 Macro: 0.6005\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9206, F1 Micro: 0.772, F1 Macro: 0.6171\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.9258, F1 Micro: 0.7665, F1 Macro: 0.6172\n",
      "Epoch 7/10, Train Loss: 0.1107, Accuracy: 0.9205, F1 Micro: 0.7455, F1 Macro: 0.6098\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.924, F1 Micro: 0.7801, F1 Macro: 0.6556\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.9209, F1 Micro: 0.7681, F1 Macro: 0.6543\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9239, F1 Micro: 0.7759, F1 Macro: 0.6547\n",
      "Best result for 8165 samples: F1 Micro: 0.7801\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1107\n",
      "      Abusive       0.89      0.92      0.90      1030\n",
      "HS_Individual       0.69      0.82      0.75       729\n",
      "     HS_Group       0.76      0.62      0.68       378\n",
      "  HS_Religion       0.75      0.64      0.69       167\n",
      "      HS_Race       0.75      0.56      0.64        88\n",
      "  HS_Physical       0.73      0.11      0.19        74\n",
      "    HS_Gender       0.68      0.20      0.31        75\n",
      "     HS_Other       0.75      0.82      0.78       744\n",
      "      HS_Weak       0.67      0.81      0.73       690\n",
      "  HS_Moderate       0.71      0.54      0.62       338\n",
      "    HS_Strong       0.83      0.62      0.71        79\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5499\n",
      "    macro avg       0.75      0.63      0.66      5499\n",
      " weighted avg       0.77      0.79      0.77      5499\n",
      "  samples avg       0.44      0.44      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 0.0002071857452392578 seconds\n",
      "\n",
      "Fold 4 - New train size: 8402\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8402 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3865, Accuracy: 0.8852, F1 Micro: 0.5966, F1 Macro: 0.2828\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.9069, F1 Micro: 0.6984, F1 Macro: 0.4494\n",
      "Epoch 3/10, Train Loss: 0.2233, Accuracy: 0.9133, F1 Micro: 0.7068, F1 Macro: 0.4982\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9222, F1 Micro: 0.7614, F1 Macro: 0.595\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.923, F1 Micro: 0.7744, F1 Macro: 0.6125\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.9253, F1 Micro: 0.7683, F1 Macro: 0.6174\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.9258, F1 Micro: 0.7774, F1 Macro: 0.6292\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9253, F1 Micro: 0.7779, F1 Macro: 0.6607\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9241, F1 Micro: 0.7734, F1 Macro: 0.6496\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9204, F1 Micro: 0.7712, F1 Macro: 0.6596\n",
      "Best result for 8402 samples: F1 Micro: 0.7779\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1107\n",
      "      Abusive       0.87      0.94      0.90      1030\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.74      0.65      0.69       378\n",
      "  HS_Religion       0.80      0.56      0.66       167\n",
      "      HS_Race       0.80      0.49      0.61        88\n",
      "  HS_Physical       0.46      0.15      0.22        74\n",
      "    HS_Gender       0.76      0.29      0.42        75\n",
      "     HS_Other       0.77      0.80      0.78       744\n",
      "      HS_Weak       0.72      0.73      0.72       690\n",
      "  HS_Moderate       0.67      0.57      0.62       338\n",
      "    HS_Strong       0.83      0.61      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5499\n",
      "    macro avg       0.75      0.62      0.66      5499\n",
      " weighted avg       0.78      0.77      0.77      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 0.00019693374633789062 seconds\n",
      "\n",
      "Fold 4 - New train size: 8616\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8616 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3824, Accuracy: 0.8859, F1 Micro: 0.5961, F1 Macro: 0.2935\n",
      "Epoch 2/10, Train Loss: 0.2642, Accuracy: 0.9044, F1 Micro: 0.7096, F1 Macro: 0.4338\n",
      "Epoch 3/10, Train Loss: 0.2165, Accuracy: 0.9202, F1 Micro: 0.7529, F1 Macro: 0.5804\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9179, F1 Micro: 0.7654, F1 Macro: 0.6052\n",
      "Epoch 5/10, Train Loss: 0.1444, Accuracy: 0.9249, F1 Micro: 0.7641, F1 Macro: 0.6122\n",
      "Epoch 6/10, Train Loss: 0.1279, Accuracy: 0.9244, F1 Micro: 0.7745, F1 Macro: 0.6252\n",
      "Epoch 7/10, Train Loss: 0.1046, Accuracy: 0.9221, F1 Micro: 0.7752, F1 Macro: 0.6189\n",
      "Epoch 8/10, Train Loss: 0.0924, Accuracy: 0.9245, F1 Micro: 0.7754, F1 Macro: 0.6584\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9239, F1 Micro: 0.7743, F1 Macro: 0.6466\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9268, F1 Micro: 0.7748, F1 Macro: 0.6702\n",
      "Best result for 8616 samples: F1 Micro: 0.7754\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1107\n",
      "      Abusive       0.89      0.93      0.91      1030\n",
      "HS_Individual       0.75      0.72      0.73       729\n",
      "     HS_Group       0.69      0.67      0.68       378\n",
      "  HS_Religion       0.74      0.68      0.71       167\n",
      "      HS_Race       0.71      0.59      0.65        88\n",
      "  HS_Physical       0.59      0.14      0.22        74\n",
      "    HS_Gender       0.83      0.20      0.32        75\n",
      "     HS_Other       0.78      0.79      0.79       744\n",
      "      HS_Weak       0.72      0.70      0.71       690\n",
      "  HS_Moderate       0.63      0.60      0.62       338\n",
      "    HS_Strong       0.79      0.66      0.72        79\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5499\n",
      "    macro avg       0.75      0.63      0.66      5499\n",
      " weighted avg       0.78      0.76      0.77      5499\n",
      "  samples avg       0.45      0.43      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017333030700683594 seconds\n",
      "\n",
      "Fold 4 - New train size: 8816\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 8816 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3896, Accuracy: 0.8876, F1 Micro: 0.6332, F1 Macro: 0.3052\n",
      "Epoch 2/10, Train Loss: 0.2602, Accuracy: 0.909, F1 Micro: 0.7074, F1 Macro: 0.486\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.9151, F1 Micro: 0.7509, F1 Macro: 0.5698\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9221, F1 Micro: 0.7717, F1 Macro: 0.5905\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9253, F1 Micro: 0.7708, F1 Macro: 0.6079\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.926, F1 Micro: 0.7752, F1 Macro: 0.6243\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9259, F1 Micro: 0.7746, F1 Macro: 0.6576\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9248, F1 Micro: 0.7822, F1 Macro: 0.672\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9249, F1 Micro: 0.7679, F1 Macro: 0.6588\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9261, F1 Micro: 0.7779, F1 Macro: 0.6855\n",
      "Best result for 8816 samples: F1 Micro: 0.7822\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1107\n",
      "      Abusive       0.90      0.92      0.91      1030\n",
      "HS_Individual       0.71      0.77      0.74       729\n",
      "     HS_Group       0.71      0.68      0.70       378\n",
      "  HS_Religion       0.79      0.67      0.73       167\n",
      "      HS_Race       0.77      0.50      0.61        88\n",
      "  HS_Physical       0.43      0.16      0.24        74\n",
      "    HS_Gender       0.78      0.28      0.41        75\n",
      "     HS_Other       0.74      0.84      0.79       744\n",
      "      HS_Weak       0.69      0.76      0.73       690\n",
      "  HS_Moderate       0.66      0.61      0.63       338\n",
      "    HS_Strong       0.83      0.66      0.73        79\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5499\n",
      "    macro avg       0.74      0.65      0.67      5499\n",
      " weighted avg       0.77      0.79      0.78      5499\n",
      "  samples avg       0.44      0.44      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0003039836883544922 seconds\n",
      "\n",
      "Fold 4 - New train size: 9016\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9016 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3792, Accuracy: 0.8901, F1 Micro: 0.6341, F1 Macro: 0.3206\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.9081, F1 Micro: 0.7222, F1 Macro: 0.4926\n",
      "Epoch 3/10, Train Loss: 0.2099, Accuracy: 0.9186, F1 Micro: 0.7577, F1 Macro: 0.5631\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9221, F1 Micro: 0.7653, F1 Macro: 0.5994\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9176, F1 Micro: 0.7707, F1 Macro: 0.6135\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.924, F1 Micro: 0.7765, F1 Macro: 0.6116\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9199, F1 Micro: 0.774, F1 Macro: 0.646\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9231, F1 Micro: 0.7813, F1 Macro: 0.6708\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.9245, F1 Micro: 0.7749, F1 Macro: 0.6691\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9267, F1 Micro: 0.7785, F1 Macro: 0.6738\n",
      "Best result for 9016 samples: F1 Micro: 0.7813\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.91      0.86      1107\n",
      "      Abusive       0.87      0.94      0.90      1030\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.67      0.71      0.69       378\n",
      "  HS_Religion       0.73      0.66      0.69       167\n",
      "      HS_Race       0.72      0.61      0.66        88\n",
      "  HS_Physical       0.67      0.14      0.22        74\n",
      "    HS_Gender       0.86      0.25      0.39        75\n",
      "     HS_Other       0.73      0.86      0.79       744\n",
      "      HS_Weak       0.70      0.76      0.73       690\n",
      "  HS_Moderate       0.61      0.64      0.63       338\n",
      "    HS_Strong       0.79      0.70      0.74        79\n",
      "\n",
      "    micro avg       0.76      0.81      0.78      5499\n",
      "    macro avg       0.74      0.66      0.67      5499\n",
      " weighted avg       0.76      0.81      0.78      5499\n",
      "  samples avg       0.45      0.45      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00020194053649902344 seconds\n",
      "\n",
      "Fold 4 - New train size: 9216\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9216 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3808, Accuracy: 0.8913, F1 Micro: 0.6325, F1 Macro: 0.3242\n",
      "Epoch 2/10, Train Loss: 0.2622, Accuracy: 0.9106, F1 Micro: 0.7101, F1 Macro: 0.5173\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9173, F1 Micro: 0.7332, F1 Macro: 0.531\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9229, F1 Micro: 0.7559, F1 Macro: 0.596\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9273, F1 Micro: 0.7773, F1 Macro: 0.6194\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9265, F1 Micro: 0.7827, F1 Macro: 0.6487\n",
      "Epoch 7/10, Train Loss: 0.1019, Accuracy: 0.9269, F1 Micro: 0.7695, F1 Macro: 0.6448\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9262, F1 Micro: 0.7742, F1 Macro: 0.6768\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9269, F1 Micro: 0.7787, F1 Macro: 0.6699\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9265, F1 Micro: 0.7742, F1 Macro: 0.6813\n",
      "Best result for 9216 samples: F1 Micro: 0.7827\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1107\n",
      "      Abusive       0.91      0.90      0.90      1030\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.74      0.65      0.69       378\n",
      "  HS_Religion       0.84      0.61      0.71       167\n",
      "      HS_Race       0.76      0.61      0.68        88\n",
      "  HS_Physical       0.53      0.12      0.20        74\n",
      "    HS_Gender       0.86      0.08      0.15        75\n",
      "     HS_Other       0.77      0.83      0.80       744\n",
      "      HS_Weak       0.70      0.76      0.73       690\n",
      "  HS_Moderate       0.67      0.59      0.62       338\n",
      "    HS_Strong       0.81      0.61      0.70        79\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5499\n",
      "    macro avg       0.76      0.62      0.65      5499\n",
      " weighted avg       0.79      0.78      0.77      5499\n",
      "  samples avg       0.43      0.42      0.41      5499\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 0.259857177734375 seconds\n",
      "\n",
      "Fold 4 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3786, Accuracy: 0.8874, F1 Micro: 0.5953, F1 Macro: 0.3133\n",
      "Epoch 2/10, Train Loss: 0.2591, Accuracy: 0.908, F1 Micro: 0.727, F1 Macro: 0.4842\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9207, F1 Micro: 0.7567, F1 Macro: 0.5778\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9246, F1 Micro: 0.7712, F1 Macro: 0.6023\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9256, F1 Micro: 0.7748, F1 Macro: 0.6184\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.9259, F1 Micro: 0.7776, F1 Macro: 0.6419\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9183, F1 Micro: 0.7653, F1 Macro: 0.6414\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9271, F1 Micro: 0.775, F1 Macro: 0.6602\n",
      "Epoch 9/10, Train Loss: 0.0755, Accuracy: 0.9274, F1 Micro: 0.7749, F1 Macro: 0.6675\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9256, F1 Micro: 0.7846, F1 Macro: 0.6854\n",
      "Best result for 9218 samples: F1 Micro: 0.7846\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.72      0.80      0.76       729\n",
      "     HS_Group       0.73      0.63      0.68       378\n",
      "  HS_Religion       0.78      0.62      0.69       167\n",
      "      HS_Race       0.72      0.59      0.65        88\n",
      "  HS_Physical       0.50      0.24      0.33        74\n",
      "    HS_Gender       0.87      0.35      0.50        75\n",
      "     HS_Other       0.76      0.83      0.79       744\n",
      "      HS_Weak       0.69      0.78      0.73       690\n",
      "  HS_Moderate       0.68      0.56      0.61       338\n",
      "    HS_Strong       0.76      0.70      0.73        79\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5499\n",
      "    macro avg       0.74      0.66      0.69      5499\n",
      " weighted avg       0.77      0.79      0.78      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018167495727539062 seconds\n",
      "\n",
      "Fold 4 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.377, Accuracy: 0.8888, F1 Micro: 0.6215, F1 Macro: 0.3081\n",
      "Epoch 2/10, Train Loss: 0.2578, Accuracy: 0.9125, F1 Micro: 0.7271, F1 Macro: 0.5431\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9169, F1 Micro: 0.7349, F1 Macro: 0.5429\n",
      "Epoch 4/10, Train Loss: 0.1783, Accuracy: 0.9248, F1 Micro: 0.7674, F1 Macro: 0.5895\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9266, F1 Micro: 0.7694, F1 Macro: 0.6136\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.929, F1 Micro: 0.7786, F1 Macro: 0.6496\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9264, F1 Micro: 0.7832, F1 Macro: 0.6506\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.9282, F1 Micro: 0.778, F1 Macro: 0.6483\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9256, F1 Micro: 0.7842, F1 Macro: 0.6824\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.927, F1 Micro: 0.776, F1 Macro: 0.6708\n",
      "Best result for 9418 samples: F1 Micro: 0.7842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1107\n",
      "      Abusive       0.89      0.93      0.91      1030\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.68      0.69      0.69       378\n",
      "  HS_Religion       0.78      0.67      0.72       167\n",
      "      HS_Race       0.79      0.61      0.69        88\n",
      "  HS_Physical       0.54      0.20      0.29        74\n",
      "    HS_Gender       0.79      0.29      0.43        75\n",
      "     HS_Other       0.75      0.83      0.79       744\n",
      "      HS_Weak       0.70      0.75      0.72       690\n",
      "  HS_Moderate       0.63      0.65      0.64       338\n",
      "    HS_Strong       0.84      0.59      0.70        79\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5499\n",
      "    macro avg       0.75      0.66      0.68      5499\n",
      " weighted avg       0.77      0.79      0.78      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018262863159179688 seconds\n",
      "\n",
      "Fold 4 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9618 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3749, Accuracy: 0.8918, F1 Micro: 0.6286, F1 Macro: 0.333\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.9074, F1 Micro: 0.7029, F1 Macro: 0.4274\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9198, F1 Micro: 0.7565, F1 Macro: 0.5662\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.924, F1 Micro: 0.7539, F1 Macro: 0.5764\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.9271, F1 Micro: 0.7814, F1 Macro: 0.618\n",
      "Epoch 6/10, Train Loss: 0.1201, Accuracy: 0.924, F1 Micro: 0.7791, F1 Macro: 0.6528\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9234, F1 Micro: 0.7795, F1 Macro: 0.6386\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9266, F1 Micro: 0.7822, F1 Macro: 0.6754\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.928, F1 Micro: 0.7841, F1 Macro: 0.6797\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9303, F1 Micro: 0.7894, F1 Macro: 0.6982\n",
      "Best result for 9618 samples: F1 Micro: 0.7894\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1107\n",
      "      Abusive       0.91      0.92      0.92      1030\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.75      0.68      0.71       378\n",
      "  HS_Religion       0.81      0.63      0.71       167\n",
      "      HS_Race       0.70      0.65      0.67        88\n",
      "  HS_Physical       0.64      0.28      0.39        74\n",
      "    HS_Gender       0.93      0.33      0.49        75\n",
      "     HS_Other       0.80      0.80      0.80       744\n",
      "      HS_Weak       0.75      0.71      0.73       690\n",
      "  HS_Moderate       0.67      0.61      0.64       338\n",
      "    HS_Strong       0.78      0.66      0.71        79\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5499\n",
      "    macro avg       0.78      0.65      0.70      5499\n",
      " weighted avg       0.81      0.77      0.79      5499\n",
      "  samples avg       0.45      0.42      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017499923706054688 seconds\n",
      "\n",
      "Fold 4 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 9818 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3735, Accuracy: 0.8924, F1 Micro: 0.6522, F1 Macro: 0.3432\n",
      "Epoch 2/10, Train Loss: 0.2503, Accuracy: 0.908, F1 Micro: 0.7346, F1 Macro: 0.5373\n",
      "Epoch 3/10, Train Loss: 0.2037, Accuracy: 0.9215, F1 Micro: 0.7614, F1 Macro: 0.5917\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9245, F1 Micro: 0.7568, F1 Macro: 0.5817\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9261, F1 Micro: 0.7778, F1 Macro: 0.6154\n",
      "Epoch 6/10, Train Loss: 0.1149, Accuracy: 0.9197, F1 Micro: 0.7718, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9266, F1 Micro: 0.7801, F1 Macro: 0.636\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9249, F1 Micro: 0.7829, F1 Macro: 0.6773\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9261, F1 Micro: 0.7827, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9251, F1 Micro: 0.7842, F1 Macro: 0.6727\n",
      "Best result for 9818 samples: F1 Micro: 0.7842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1107\n",
      "      Abusive       0.88      0.94      0.91      1030\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.71      0.65      0.67       378\n",
      "  HS_Religion       0.74      0.71      0.72       167\n",
      "      HS_Race       0.73      0.60      0.66        88\n",
      "  HS_Physical       0.77      0.14      0.23        74\n",
      "    HS_Gender       0.95      0.25      0.40        75\n",
      "     HS_Other       0.76      0.84      0.80       744\n",
      "      HS_Weak       0.69      0.77      0.73       690\n",
      "  HS_Moderate       0.63      0.61      0.62       338\n",
      "    HS_Strong       0.84      0.65      0.73        79\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5499\n",
      "    macro avg       0.77      0.65      0.67      5499\n",
      " weighted avg       0.77      0.80      0.78      5499\n",
      "  samples avg       0.45      0.45      0.44      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017642974853515625 seconds\n",
      "\n",
      "Fold 4 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10018 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3715, Accuracy: 0.8896, F1 Micro: 0.6589, F1 Macro: 0.3328\n",
      "Epoch 2/10, Train Loss: 0.2509, Accuracy: 0.9118, F1 Micro: 0.7345, F1 Macro: 0.5414\n",
      "Epoch 3/10, Train Loss: 0.2019, Accuracy: 0.9232, F1 Micro: 0.7698, F1 Macro: 0.5985\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9243, F1 Micro: 0.7775, F1 Macro: 0.6125\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9269, F1 Micro: 0.7801, F1 Macro: 0.6287\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9272, F1 Micro: 0.7719, F1 Macro: 0.6201\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.9251, F1 Micro: 0.7868, F1 Macro: 0.6878\n",
      "Epoch 8/10, Train Loss: 0.0877, Accuracy: 0.9258, F1 Micro: 0.7771, F1 Macro: 0.6699\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9253, F1 Micro: 0.7786, F1 Macro: 0.6508\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.926, F1 Micro: 0.7826, F1 Macro: 0.6949\n",
      "Best result for 10018 samples: F1 Micro: 0.7868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.91      0.86      1107\n",
      "      Abusive       0.91      0.90      0.91      1030\n",
      "HS_Individual       0.71      0.81      0.76       729\n",
      "     HS_Group       0.71      0.68      0.70       378\n",
      "  HS_Religion       0.77      0.66      0.71       167\n",
      "      HS_Race       0.76      0.68      0.72        88\n",
      "  HS_Physical       0.46      0.22      0.29        74\n",
      "    HS_Gender       0.88      0.29      0.44        75\n",
      "     HS_Other       0.74      0.86      0.80       744\n",
      "      HS_Weak       0.68      0.81      0.74       690\n",
      "  HS_Moderate       0.62      0.64      0.63       338\n",
      "    HS_Strong       0.82      0.62      0.71        79\n",
      "\n",
      "    micro avg       0.76      0.81      0.79      5499\n",
      "    macro avg       0.74      0.67      0.69      5499\n",
      " weighted avg       0.77      0.81      0.78      5499\n",
      "  samples avg       0.43      0.44      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00016617774963378906 seconds\n",
      "\n",
      "Fold 4 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3751, Accuracy: 0.8932, F1 Micro: 0.6452, F1 Macro: 0.3606\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.9119, F1 Micro: 0.7257, F1 Macro: 0.5023\n",
      "Epoch 3/10, Train Loss: 0.2053, Accuracy: 0.9226, F1 Micro: 0.7603, F1 Macro: 0.5932\n",
      "Epoch 4/10, Train Loss: 0.1697, Accuracy: 0.9252, F1 Micro: 0.7606, F1 Macro: 0.5855\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.9244, F1 Micro: 0.7786, F1 Macro: 0.6202\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9272, F1 Micro: 0.7818, F1 Macro: 0.6382\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9239, F1 Micro: 0.7796, F1 Macro: 0.6748\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9258, F1 Micro: 0.782, F1 Macro: 0.656\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9284, F1 Micro: 0.777, F1 Macro: 0.6698\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9257, F1 Micro: 0.7746, F1 Macro: 0.6756\n",
      "Best result for 10218 samples: F1 Micro: 0.782\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1107\n",
      "      Abusive       0.91      0.91      0.91      1030\n",
      "HS_Individual       0.73      0.76      0.75       729\n",
      "     HS_Group       0.70      0.68      0.69       378\n",
      "  HS_Religion       0.82      0.62      0.71       167\n",
      "      HS_Race       0.73      0.58      0.65        88\n",
      "  HS_Physical       0.71      0.14      0.23        74\n",
      "    HS_Gender       0.83      0.13      0.23        75\n",
      "     HS_Other       0.75      0.85      0.80       744\n",
      "      HS_Weak       0.71      0.75      0.73       690\n",
      "  HS_Moderate       0.65      0.61      0.63       338\n",
      "    HS_Strong       0.78      0.66      0.71        79\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5499\n",
      "    macro avg       0.76      0.63      0.66      5499\n",
      " weighted avg       0.78      0.78      0.77      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00015473365783691406 seconds\n",
      "\n",
      "Fold 4 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3665, Accuracy: 0.8916, F1 Micro: 0.675, F1 Macro: 0.3913\n",
      "Epoch 2/10, Train Loss: 0.2482, Accuracy: 0.9123, F1 Micro: 0.7338, F1 Macro: 0.5157\n",
      "Epoch 3/10, Train Loss: 0.2004, Accuracy: 0.9227, F1 Micro: 0.7628, F1 Macro: 0.5919\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9267, F1 Micro: 0.771, F1 Macro: 0.6095\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9279, F1 Micro: 0.7828, F1 Macro: 0.6275\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9261, F1 Micro: 0.7838, F1 Macro: 0.6339\n",
      "Epoch 7/10, Train Loss: 0.0994, Accuracy: 0.9265, F1 Micro: 0.7802, F1 Macro: 0.6597\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9274, F1 Micro: 0.7852, F1 Macro: 0.6745\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9276, F1 Micro: 0.7813, F1 Macro: 0.661\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9289, F1 Micro: 0.7872, F1 Macro: 0.6945\n",
      "Best result for 10418 samples: F1 Micro: 0.7872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1107\n",
      "      Abusive       0.91      0.93      0.92      1030\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.75      0.62      0.68       378\n",
      "  HS_Religion       0.84      0.58      0.69       167\n",
      "      HS_Race       0.77      0.62      0.69        88\n",
      "  HS_Physical       0.64      0.24      0.35        74\n",
      "    HS_Gender       0.81      0.40      0.54        75\n",
      "     HS_Other       0.80      0.78      0.79       744\n",
      "      HS_Weak       0.71      0.75      0.73       690\n",
      "  HS_Moderate       0.68      0.56      0.61       338\n",
      "    HS_Strong       0.82      0.65      0.72        79\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5499\n",
      "    macro avg       0.78      0.65      0.69      5499\n",
      " weighted avg       0.80      0.77      0.78      5499\n",
      "  samples avg       0.44      0.43      0.42      5499\n",
      "\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 0.2957909107208252 seconds\n",
      "\n",
      "Fold 4 - New train size: 10535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 10535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.369, Accuracy: 0.8932, F1 Micro: 0.643, F1 Macro: 0.3601\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9159, F1 Micro: 0.7268, F1 Macro: 0.5333\n",
      "Epoch 3/10, Train Loss: 0.2027, Accuracy: 0.9225, F1 Micro: 0.764, F1 Macro: 0.5992\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.9236, F1 Micro: 0.779, F1 Macro: 0.6099\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9277, F1 Micro: 0.7783, F1 Macro: 0.6456\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9218, F1 Micro: 0.7725, F1 Macro: 0.6344\n",
      "Epoch 7/10, Train Loss: 0.0984, Accuracy: 0.9272, F1 Micro: 0.7835, F1 Macro: 0.6412\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9256, F1 Micro: 0.7812, F1 Macro: 0.6625\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.93, F1 Micro: 0.7931, F1 Macro: 0.681\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9252, F1 Micro: 0.7867, F1 Macro: 0.6834\n",
      "Best result for 10535 samples: F1 Micro: 0.7931\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1107\n",
      "      Abusive       0.90      0.93      0.92      1030\n",
      "HS_Individual       0.74      0.79      0.76       729\n",
      "     HS_Group       0.76      0.65      0.70       378\n",
      "  HS_Religion       0.77      0.66      0.71       167\n",
      "      HS_Race       0.78      0.53      0.64        88\n",
      "  HS_Physical       0.81      0.18      0.29        74\n",
      "    HS_Gender       0.87      0.27      0.41        75\n",
      "     HS_Other       0.78      0.82      0.80       744\n",
      "      HS_Weak       0.71      0.78      0.74       690\n",
      "  HS_Moderate       0.69      0.58      0.63       338\n",
      "    HS_Strong       0.86      0.61      0.71        79\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5499\n",
      "    macro avg       0.79      0.64      0.68      5499\n",
      " weighted avg       0.80      0.79      0.79      5499\n",
      "  samples avg       0.45      0.44      0.43      5499\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 5806.75 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 658 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6136, Accuracy: 0.8169, F1 Micro: 0.2051, F1 Macro: 0.06\n",
      "Epoch 2/10, Train Loss: 0.4588, Accuracy: 0.8201, F1 Micro: 0.0038, F1 Macro: 0.0016\n",
      "Epoch 3/10, Train Loss: 0.4187, Accuracy: 0.8233, F1 Micro: 0.0481, F1 Macro: 0.02\n",
      "Epoch 4/10, Train Loss: 0.386, Accuracy: 0.8238, F1 Micro: 0.0508, F1 Macro: 0.0212\n",
      "Epoch 5/10, Train Loss: 0.3707, Accuracy: 0.8329, F1 Micro: 0.1604, F1 Macro: 0.0612\n",
      "Epoch 6/10, Train Loss: 0.3624, Accuracy: 0.8458, F1 Micro: 0.3243, F1 Macro: 0.1116\n",
      "Epoch 7/10, Train Loss: 0.3537, Accuracy: 0.8506, F1 Micro: 0.3637, F1 Macro: 0.145\n",
      "Epoch 8/10, Train Loss: 0.3241, Accuracy: 0.8632, F1 Micro: 0.4924, F1 Macro: 0.2234\n",
      "Epoch 9/10, Train Loss: 0.3069, Accuracy: 0.8715, F1 Micro: 0.5743, F1 Macro: 0.2679\n",
      "Epoch 10/10, Train Loss: 0.2933, Accuracy: 0.8748, F1 Micro: 0.5872, F1 Macro: 0.2747\n",
      "Best result for 658 samples: F1 Micro: 0.5872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.75      0.77      1190\n",
      "      Abusive       0.82      0.72      0.77      1018\n",
      "HS_Individual       0.65      0.57      0.61       768\n",
      "     HS_Group       0.00      0.00      0.00       422\n",
      "  HS_Religion       0.00      0.00      0.00       173\n",
      "      HS_Race       0.00      0.00      0.00       126\n",
      "  HS_Physical       0.00      0.00      0.00        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.64      0.56      0.60       792\n",
      "      HS_Weak       0.61      0.50      0.55       725\n",
      "  HS_Moderate       0.00      0.00      0.00       352\n",
      "    HS_Strong       0.00      0.00      0.00       113\n",
      "\n",
      "    micro avg       0.72      0.50      0.59      5806\n",
      "    macro avg       0.29      0.26      0.27      5806\n",
      " weighted avg       0.56      0.50      0.52      5806\n",
      "  samples avg       0.38      0.29      0.30      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 0.0006015300750732422 seconds\n",
      "\n",
      "Fold 5 - New train size: 1646\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1646 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.8248, F1 Micro: 0.0974, F1 Macro: 0.033\n",
      "Epoch 2/10, Train Loss: 0.4013, Accuracy: 0.8243, F1 Micro: 0.0575, F1 Macro: 0.0242\n",
      "Epoch 3/10, Train Loss: 0.3731, Accuracy: 0.841, F1 Micro: 0.2516, F1 Macro: 0.0915\n",
      "Epoch 4/10, Train Loss: 0.3457, Accuracy: 0.8695, F1 Micro: 0.5483, F1 Macro: 0.2473\n",
      "Epoch 5/10, Train Loss: 0.3079, Accuracy: 0.8746, F1 Micro: 0.5796, F1 Macro: 0.2694\n",
      "Epoch 6/10, Train Loss: 0.2836, Accuracy: 0.8802, F1 Micro: 0.6112, F1 Macro: 0.2968\n",
      "Epoch 7/10, Train Loss: 0.2523, Accuracy: 0.8861, F1 Micro: 0.645, F1 Macro: 0.3416\n",
      "Epoch 8/10, Train Loss: 0.2296, Accuracy: 0.8878, F1 Micro: 0.6773, F1 Macro: 0.437\n",
      "Epoch 9/10, Train Loss: 0.2009, Accuracy: 0.8898, F1 Micro: 0.6572, F1 Macro: 0.4143\n",
      "Epoch 10/10, Train Loss: 0.1799, Accuracy: 0.8917, F1 Micro: 0.6847, F1 Macro: 0.4851\n",
      "Best result for 1646 samples: F1 Micro: 0.6847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1190\n",
      "      Abusive       0.84      0.85      0.85      1018\n",
      "HS_Individual       0.67      0.60      0.63       768\n",
      "     HS_Group       0.63      0.57      0.60       422\n",
      "  HS_Religion       0.61      0.16      0.26       173\n",
      "      HS_Race       0.82      0.42      0.55       126\n",
      "  HS_Physical       0.00      0.00      0.00        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.67      0.67      0.67       792\n",
      "      HS_Weak       0.64      0.60      0.62       725\n",
      "  HS_Moderate       0.52      0.42      0.47       352\n",
      "    HS_Strong       0.73      0.24      0.36       113\n",
      "\n",
      "    micro avg       0.72      0.65      0.68      5806\n",
      "    macro avg       0.58      0.45      0.49      5806\n",
      " weighted avg       0.70      0.65      0.67      5806\n",
      "  samples avg       0.41      0.38      0.37      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 0.0005910396575927734 seconds\n",
      "\n",
      "Fold 5 - New train size: 2535\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2535 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4893, Accuracy: 0.8255, F1 Micro: 0.077, F1 Macro: 0.0299\n",
      "Epoch 2/10, Train Loss: 0.3755, Accuracy: 0.8515, F1 Micro: 0.405, F1 Macro: 0.1485\n",
      "Epoch 3/10, Train Loss: 0.3304, Accuracy: 0.8721, F1 Micro: 0.5569, F1 Macro: 0.2577\n",
      "Epoch 4/10, Train Loss: 0.2878, Accuracy: 0.8833, F1 Micro: 0.627, F1 Macro: 0.3238\n",
      "Epoch 5/10, Train Loss: 0.2521, Accuracy: 0.8912, F1 Micro: 0.654, F1 Macro: 0.3966\n",
      "Epoch 6/10, Train Loss: 0.2125, Accuracy: 0.8905, F1 Micro: 0.6392, F1 Macro: 0.4097\n",
      "Epoch 7/10, Train Loss: 0.1836, Accuracy: 0.8952, F1 Micro: 0.6999, F1 Macro: 0.4933\n",
      "Epoch 8/10, Train Loss: 0.1713, Accuracy: 0.896, F1 Micro: 0.683, F1 Macro: 0.5081\n",
      "Epoch 9/10, Train Loss: 0.1466, Accuracy: 0.8979, F1 Micro: 0.7043, F1 Macro: 0.5088\n",
      "Epoch 10/10, Train Loss: 0.1318, Accuracy: 0.8982, F1 Micro: 0.7096, F1 Macro: 0.5485\n",
      "Best result for 2535 samples: F1 Micro: 0.7096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.82      1190\n",
      "      Abusive       0.87      0.86      0.86      1018\n",
      "HS_Individual       0.69      0.66      0.67       768\n",
      "     HS_Group       0.62      0.60      0.61       422\n",
      "  HS_Religion       0.64      0.38      0.47       173\n",
      "      HS_Race       0.76      0.64      0.70       126\n",
      "  HS_Physical       1.00      0.03      0.06        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.68      0.71      0.70       792\n",
      "      HS_Weak       0.65      0.63      0.64       725\n",
      "  HS_Moderate       0.52      0.49      0.51       352\n",
      "    HS_Strong       0.75      0.42      0.53       113\n",
      "\n",
      "    micro avg       0.73      0.69      0.71      5806\n",
      "    macro avg       0.67      0.52      0.55      5806\n",
      " weighted avg       0.72      0.69      0.70      5806\n",
      "  samples avg       0.42      0.40      0.39      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 801\n",
      "Sampling duration: 0.0004737377166748047 seconds\n",
      "\n",
      "Fold 5 - New train size: 3336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4631, Accuracy: 0.83, F1 Micro: 0.1301, F1 Macro: 0.0476\n",
      "Epoch 2/10, Train Loss: 0.3537, Accuracy: 0.8698, F1 Micro: 0.5378, F1 Macro: 0.2472\n",
      "Epoch 3/10, Train Loss: 0.299, Accuracy: 0.8838, F1 Micro: 0.6328, F1 Macro: 0.3221\n",
      "Epoch 4/10, Train Loss: 0.2557, Accuracy: 0.893, F1 Micro: 0.6841, F1 Macro: 0.4365\n",
      "Epoch 5/10, Train Loss: 0.2194, Accuracy: 0.8962, F1 Micro: 0.7085, F1 Macro: 0.4996\n",
      "Epoch 6/10, Train Loss: 0.1923, Accuracy: 0.8957, F1 Micro: 0.7178, F1 Macro: 0.5486\n",
      "Epoch 7/10, Train Loss: 0.1712, Accuracy: 0.9027, F1 Micro: 0.7109, F1 Macro: 0.5284\n",
      "Epoch 8/10, Train Loss: 0.1422, Accuracy: 0.9031, F1 Micro: 0.7153, F1 Macro: 0.5462\n",
      "Epoch 9/10, Train Loss: 0.1201, Accuracy: 0.9043, F1 Micro: 0.7203, F1 Macro: 0.5541\n",
      "Epoch 10/10, Train Loss: 0.1062, Accuracy: 0.9061, F1 Micro: 0.7204, F1 Macro: 0.5659\n",
      "Best result for 3336 samples: F1 Micro: 0.7204\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1190\n",
      "      Abusive       0.91      0.82      0.86      1018\n",
      "HS_Individual       0.72      0.65      0.69       768\n",
      "     HS_Group       0.68      0.54      0.60       422\n",
      "  HS_Religion       0.76      0.35      0.48       173\n",
      "      HS_Race       0.83      0.56      0.67       126\n",
      "  HS_Physical       1.00      0.07      0.12        60\n",
      "    HS_Gender       0.00      0.00      0.00        67\n",
      "     HS_Other       0.72      0.73      0.72       792\n",
      "      HS_Weak       0.70      0.63      0.66       725\n",
      "  HS_Moderate       0.57      0.43      0.49       352\n",
      "    HS_Strong       0.84      0.55      0.66       113\n",
      "\n",
      "    micro avg       0.78      0.67      0.72      5806\n",
      "    macro avg       0.71      0.51      0.57      5806\n",
      " weighted avg       0.77      0.67      0.71      5806\n",
      "  samples avg       0.42      0.39      0.39      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 0.0005218982696533203 seconds\n",
      "\n",
      "Fold 5 - New train size: 4056\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4056 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4544, Accuracy: 0.8404, F1 Micro: 0.265, F1 Macro: 0.0893\n",
      "Epoch 2/10, Train Loss: 0.3349, Accuracy: 0.8761, F1 Micro: 0.5815, F1 Macro: 0.2661\n",
      "Epoch 3/10, Train Loss: 0.2783, Accuracy: 0.8893, F1 Micro: 0.653, F1 Macro: 0.3611\n",
      "Epoch 4/10, Train Loss: 0.2451, Accuracy: 0.8984, F1 Micro: 0.7118, F1 Macro: 0.5085\n",
      "Epoch 5/10, Train Loss: 0.2094, Accuracy: 0.9019, F1 Micro: 0.7123, F1 Macro: 0.5139\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9045, F1 Micro: 0.7267, F1 Macro: 0.5467\n",
      "Epoch 7/10, Train Loss: 0.1529, Accuracy: 0.9071, F1 Micro: 0.724, F1 Macro: 0.5459\n",
      "Epoch 8/10, Train Loss: 0.1267, Accuracy: 0.9075, F1 Micro: 0.7198, F1 Macro: 0.5493\n",
      "Epoch 9/10, Train Loss: 0.1109, Accuracy: 0.9082, F1 Micro: 0.7282, F1 Macro: 0.577\n",
      "Epoch 10/10, Train Loss: 0.1013, Accuracy: 0.9098, F1 Micro: 0.7373, F1 Macro: 0.6083\n",
      "Best result for 4056 samples: F1 Micro: 0.7373\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1190\n",
      "      Abusive       0.89      0.85      0.87      1018\n",
      "HS_Individual       0.71      0.70      0.70       768\n",
      "     HS_Group       0.71      0.54      0.61       422\n",
      "  HS_Religion       0.69      0.48      0.57       173\n",
      "      HS_Race       0.83      0.65      0.73       126\n",
      "  HS_Physical       1.00      0.13      0.24        60\n",
      "    HS_Gender       1.00      0.06      0.11        67\n",
      "     HS_Other       0.75      0.73      0.74       792\n",
      "      HS_Weak       0.68      0.68      0.68       725\n",
      "  HS_Moderate       0.61      0.45      0.51       352\n",
      "    HS_Strong       0.85      0.59      0.70       113\n",
      "\n",
      "    micro avg       0.78      0.70      0.74      5806\n",
      "    macro avg       0.80      0.56      0.61      5806\n",
      " weighted avg       0.78      0.70      0.73      5806\n",
      "  samples avg       0.43      0.41      0.40      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 0.0004100799560546875 seconds\n",
      "\n",
      "Fold 5 - New train size: 4704\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4704 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4415, Accuracy: 0.8473, F1 Micro: 0.3382, F1 Macro: 0.1185\n",
      "Epoch 2/10, Train Loss: 0.3271, Accuracy: 0.8741, F1 Micro: 0.5394, F1 Macro: 0.2485\n",
      "Epoch 3/10, Train Loss: 0.2642, Accuracy: 0.895, F1 Micro: 0.6677, F1 Macro: 0.4066\n",
      "Epoch 4/10, Train Loss: 0.2291, Accuracy: 0.9042, F1 Micro: 0.7174, F1 Macro: 0.4992\n",
      "Epoch 5/10, Train Loss: 0.1996, Accuracy: 0.9063, F1 Micro: 0.7208, F1 Macro: 0.5368\n",
      "Epoch 6/10, Train Loss: 0.1625, Accuracy: 0.9062, F1 Micro: 0.7075, F1 Macro: 0.509\n",
      "Epoch 7/10, Train Loss: 0.1422, Accuracy: 0.9084, F1 Micro: 0.7274, F1 Macro: 0.5509\n",
      "Epoch 8/10, Train Loss: 0.1214, Accuracy: 0.9092, F1 Micro: 0.735, F1 Macro: 0.5905\n",
      "Epoch 9/10, Train Loss: 0.106, Accuracy: 0.9125, F1 Micro: 0.7477, F1 Macro: 0.6149\n",
      "Epoch 10/10, Train Loss: 0.09, Accuracy: 0.9102, F1 Micro: 0.7503, F1 Macro: 0.6271\n",
      "Best result for 4704 samples: F1 Micro: 0.7503\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1190\n",
      "      Abusive       0.88      0.87      0.88      1018\n",
      "HS_Individual       0.69      0.74      0.72       768\n",
      "     HS_Group       0.67      0.62      0.65       422\n",
      "  HS_Religion       0.67      0.46      0.55       173\n",
      "      HS_Race       0.78      0.68      0.73       126\n",
      "  HS_Physical       0.62      0.17      0.26        60\n",
      "    HS_Gender       1.00      0.13      0.24        67\n",
      "     HS_Other       0.73      0.80      0.76       792\n",
      "      HS_Weak       0.67      0.72      0.69       725\n",
      "  HS_Moderate       0.57      0.54      0.56       352\n",
      "    HS_Strong       0.83      0.53      0.65       113\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5806\n",
      "    macro avg       0.75      0.60      0.63      5806\n",
      " weighted avg       0.75      0.75      0.74      5806\n",
      "  samples avg       0.44      0.43      0.42      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.0003802776336669922 seconds\n",
      "\n",
      "Fold 5 - New train size: 5288\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5288 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.431, Accuracy: 0.855, F1 Micro: 0.4589, F1 Macro: 0.1712\n",
      "Epoch 2/10, Train Loss: 0.3069, Accuracy: 0.8851, F1 Micro: 0.6389, F1 Macro: 0.3224\n",
      "Epoch 3/10, Train Loss: 0.2505, Accuracy: 0.8965, F1 Micro: 0.7091, F1 Macro: 0.4674\n",
      "Epoch 4/10, Train Loss: 0.2127, Accuracy: 0.9028, F1 Micro: 0.7303, F1 Macro: 0.5505\n",
      "Epoch 5/10, Train Loss: 0.1837, Accuracy: 0.909, F1 Micro: 0.7198, F1 Macro: 0.5429\n",
      "Epoch 6/10, Train Loss: 0.1545, Accuracy: 0.9069, F1 Micro: 0.7442, F1 Macro: 0.5849\n",
      "Epoch 7/10, Train Loss: 0.1274, Accuracy: 0.9108, F1 Micro: 0.7472, F1 Macro: 0.6062\n",
      "Epoch 8/10, Train Loss: 0.1128, Accuracy: 0.9078, F1 Micro: 0.7449, F1 Macro: 0.6217\n",
      "Epoch 9/10, Train Loss: 0.0964, Accuracy: 0.9123, F1 Micro: 0.7566, F1 Macro: 0.6376\n",
      "Epoch 10/10, Train Loss: 0.086, Accuracy: 0.9123, F1 Micro: 0.7584, F1 Macro: 0.6395\n",
      "Best result for 5288 samples: F1 Micro: 0.7584\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1190\n",
      "      Abusive       0.86      0.91      0.88      1018\n",
      "HS_Individual       0.67      0.79      0.73       768\n",
      "     HS_Group       0.74      0.55      0.63       422\n",
      "  HS_Religion       0.70      0.47      0.57       173\n",
      "      HS_Race       0.80      0.63      0.70       126\n",
      "  HS_Physical       0.69      0.15      0.25        60\n",
      "    HS_Gender       1.00      0.18      0.30        67\n",
      "     HS_Other       0.73      0.82      0.77       792\n",
      "      HS_Weak       0.65      0.78      0.71       725\n",
      "  HS_Moderate       0.65      0.45      0.53       352\n",
      "    HS_Strong       0.86      0.68      0.76       113\n",
      "\n",
      "    micro avg       0.75      0.76      0.76      5806\n",
      "    macro avg       0.76      0.61      0.64      5806\n",
      " weighted avg       0.75      0.76      0.75      5806\n",
      "  samples avg       0.45      0.45      0.43      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0003554821014404297 seconds\n",
      "\n",
      "Fold 5 - New train size: 5813\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5813 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4212, Accuracy: 0.8594, F1 Micro: 0.4433, F1 Macro: 0.1724\n",
      "Epoch 2/10, Train Loss: 0.3047, Accuracy: 0.8866, F1 Micro: 0.6227, F1 Macro: 0.2998\n",
      "Epoch 3/10, Train Loss: 0.2468, Accuracy: 0.9, F1 Micro: 0.7119, F1 Macro: 0.5114\n",
      "Epoch 4/10, Train Loss: 0.2144, Accuracy: 0.9067, F1 Micro: 0.7304, F1 Macro: 0.5514\n",
      "Epoch 5/10, Train Loss: 0.1786, Accuracy: 0.9102, F1 Micro: 0.7388, F1 Macro: 0.5674\n",
      "Epoch 6/10, Train Loss: 0.1542, Accuracy: 0.9123, F1 Micro: 0.7452, F1 Macro: 0.5881\n",
      "Epoch 7/10, Train Loss: 0.1338, Accuracy: 0.9126, F1 Micro: 0.7532, F1 Macro: 0.5984\n",
      "Epoch 8/10, Train Loss: 0.1087, Accuracy: 0.9133, F1 Micro: 0.757, F1 Macro: 0.6294\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.9134, F1 Micro: 0.7571, F1 Macro: 0.6352\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.918, F1 Micro: 0.7611, F1 Macro: 0.6622\n",
      "Best result for 5813 samples: F1 Micro: 0.7611\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1190\n",
      "      Abusive       0.90      0.86      0.88      1018\n",
      "HS_Individual       0.73      0.74      0.73       768\n",
      "     HS_Group       0.75      0.54      0.63       422\n",
      "  HS_Religion       0.75      0.53      0.62       173\n",
      "      HS_Race       0.82      0.68      0.74       126\n",
      "  HS_Physical       0.65      0.22      0.33        60\n",
      "    HS_Gender       0.94      0.22      0.36        67\n",
      "     HS_Other       0.77      0.74      0.76       792\n",
      "      HS_Weak       0.70      0.71      0.71       725\n",
      "  HS_Moderate       0.67      0.43      0.53       352\n",
      "    HS_Strong       0.84      0.77      0.80       113\n",
      "\n",
      "    micro avg       0.80      0.73      0.76      5806\n",
      "    macro avg       0.78      0.61      0.66      5806\n",
      " weighted avg       0.80      0.73      0.75      5806\n",
      "  samples avg       0.44      0.42      0.41      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.00034880638122558594 seconds\n",
      "\n",
      "Fold 5 - New train size: 6286\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6286 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4187, Accuracy: 0.861, F1 Micro: 0.4439, F1 Macro: 0.1772\n",
      "Epoch 2/10, Train Loss: 0.2959, Accuracy: 0.8901, F1 Micro: 0.6395, F1 Macro: 0.3374\n",
      "Epoch 3/10, Train Loss: 0.2382, Accuracy: 0.9034, F1 Micro: 0.7058, F1 Macro: 0.4844\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.9083, F1 Micro: 0.7273, F1 Macro: 0.5637\n",
      "Epoch 5/10, Train Loss: 0.1719, Accuracy: 0.911, F1 Micro: 0.7496, F1 Macro: 0.5828\n",
      "Epoch 6/10, Train Loss: 0.1485, Accuracy: 0.912, F1 Micro: 0.7568, F1 Macro: 0.6077\n",
      "Epoch 7/10, Train Loss: 0.1247, Accuracy: 0.9135, F1 Micro: 0.7486, F1 Macro: 0.6037\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9141, F1 Micro: 0.7575, F1 Macro: 0.6354\n",
      "Epoch 9/10, Train Loss: 0.0924, Accuracy: 0.9139, F1 Micro: 0.7549, F1 Macro: 0.648\n",
      "Epoch 10/10, Train Loss: 0.0806, Accuracy: 0.9109, F1 Micro: 0.7585, F1 Macro: 0.6687\n",
      "Best result for 6286 samples: F1 Micro: 0.7585\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1190\n",
      "      Abusive       0.85      0.92      0.88      1018\n",
      "HS_Individual       0.70      0.73      0.72       768\n",
      "     HS_Group       0.65      0.68      0.66       422\n",
      "  HS_Religion       0.62      0.60      0.61       173\n",
      "      HS_Race       0.70      0.74      0.72       126\n",
      "  HS_Physical       0.62      0.22      0.32        60\n",
      "    HS_Gender       0.75      0.31      0.44        67\n",
      "     HS_Other       0.75      0.78      0.76       792\n",
      "      HS_Weak       0.68      0.73      0.70       725\n",
      "  HS_Moderate       0.56      0.60      0.58       352\n",
      "    HS_Strong       0.83      0.74      0.79       113\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5806\n",
      "    macro avg       0.71      0.66      0.67      5806\n",
      " weighted avg       0.74      0.78      0.76      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 298\n",
      "Sampling duration: 0.22142791748046875 seconds\n",
      "\n",
      "Fold 5 - New train size: 6584\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6584 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4102, Accuracy: 0.866, F1 Micro: 0.5234, F1 Macro: 0.2317\n",
      "Epoch 2/10, Train Loss: 0.2902, Accuracy: 0.8925, F1 Micro: 0.6704, F1 Macro: 0.4107\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9049, F1 Micro: 0.7246, F1 Macro: 0.5187\n",
      "Epoch 4/10, Train Loss: 0.1985, Accuracy: 0.9103, F1 Micro: 0.7224, F1 Macro: 0.5375\n",
      "Epoch 5/10, Train Loss: 0.1697, Accuracy: 0.9149, F1 Micro: 0.7547, F1 Macro: 0.5939\n",
      "Epoch 6/10, Train Loss: 0.1427, Accuracy: 0.9168, F1 Micro: 0.7516, F1 Macro: 0.6199\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9127, F1 Micro: 0.7558, F1 Macro: 0.6261\n",
      "Epoch 8/10, Train Loss: 0.1008, Accuracy: 0.9163, F1 Micro: 0.7606, F1 Macro: 0.6458\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9161, F1 Micro: 0.7599, F1 Macro: 0.6569\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.9146, F1 Micro: 0.7662, F1 Macro: 0.6734\n",
      "Best result for 6584 samples: F1 Micro: 0.7662\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1190\n",
      "      Abusive       0.87      0.91      0.89      1018\n",
      "HS_Individual       0.71      0.75      0.73       768\n",
      "     HS_Group       0.67      0.66      0.67       422\n",
      "  HS_Religion       0.61      0.62      0.62       173\n",
      "      HS_Race       0.76      0.75      0.75       126\n",
      "  HS_Physical       0.86      0.20      0.32        60\n",
      "    HS_Gender       0.89      0.25      0.40        67\n",
      "     HS_Other       0.75      0.79      0.77       792\n",
      "      HS_Weak       0.68      0.72      0.70       725\n",
      "  HS_Moderate       0.59      0.57      0.58       352\n",
      "    HS_Strong       0.80      0.80      0.80       113\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5806\n",
      "    macro avg       0.75      0.66      0.67      5806\n",
      " weighted avg       0.76      0.78      0.76      5806\n",
      "  samples avg       0.47      0.46      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 0.0003592967987060547 seconds\n",
      "\n",
      "Fold 5 - New train size: 6980\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6980 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4055, Accuracy: 0.8697, F1 Micro: 0.5623, F1 Macro: 0.2562\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.89, F1 Micro: 0.6253, F1 Macro: 0.3554\n",
      "Epoch 3/10, Train Loss: 0.2381, Accuracy: 0.9052, F1 Micro: 0.7065, F1 Macro: 0.4866\n",
      "Epoch 4/10, Train Loss: 0.1972, Accuracy: 0.9038, F1 Micro: 0.7354, F1 Macro: 0.5416\n",
      "Epoch 5/10, Train Loss: 0.1692, Accuracy: 0.9153, F1 Micro: 0.7566, F1 Macro: 0.5881\n",
      "Epoch 6/10, Train Loss: 0.1404, Accuracy: 0.9169, F1 Micro: 0.7643, F1 Macro: 0.6138\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9159, F1 Micro: 0.7395, F1 Macro: 0.6101\n",
      "Epoch 8/10, Train Loss: 0.1043, Accuracy: 0.9184, F1 Micro: 0.7675, F1 Macro: 0.6455\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9188, F1 Micro: 0.7716, F1 Macro: 0.6808\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9168, F1 Micro: 0.7677, F1 Macro: 0.6883\n",
      "Best result for 6980 samples: F1 Micro: 0.7716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1190\n",
      "      Abusive       0.90      0.87      0.88      1018\n",
      "HS_Individual       0.73      0.75      0.74       768\n",
      "     HS_Group       0.72      0.65      0.68       422\n",
      "  HS_Religion       0.69      0.52      0.59       173\n",
      "      HS_Race       0.80      0.69      0.74       126\n",
      "  HS_Physical       0.93      0.23      0.37        60\n",
      "    HS_Gender       0.95      0.27      0.42        67\n",
      "     HS_Other       0.75      0.80      0.77       792\n",
      "      HS_Weak       0.71      0.73      0.72       725\n",
      "  HS_Moderate       0.63      0.54      0.58       352\n",
      "    HS_Strong       0.81      0.80      0.80       113\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5806\n",
      "    macro avg       0.79      0.64      0.68      5806\n",
      " weighted avg       0.78      0.76      0.77      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 0.0003514289855957031 seconds\n",
      "\n",
      "Fold 5 - New train size: 7336\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7336 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4026, Accuracy: 0.8716, F1 Micro: 0.5743, F1 Macro: 0.2626\n",
      "Epoch 2/10, Train Loss: 0.2788, Accuracy: 0.8954, F1 Micro: 0.6887, F1 Macro: 0.4403\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9062, F1 Micro: 0.7193, F1 Macro: 0.5282\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.9141, F1 Micro: 0.7391, F1 Macro: 0.5777\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9144, F1 Micro: 0.7584, F1 Macro: 0.6115\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9173, F1 Micro: 0.7613, F1 Macro: 0.6283\n",
      "Epoch 7/10, Train Loss: 0.1126, Accuracy: 0.9167, F1 Micro: 0.7643, F1 Macro: 0.6283\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.9177, F1 Micro: 0.7643, F1 Macro: 0.6611\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9212, F1 Micro: 0.7642, F1 Macro: 0.6613\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.9197, F1 Micro: 0.7769, F1 Macro: 0.6891\n",
      "Best result for 7336 samples: F1 Micro: 0.7769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1190\n",
      "      Abusive       0.86      0.91      0.89      1018\n",
      "HS_Individual       0.72      0.77      0.75       768\n",
      "     HS_Group       0.72      0.63      0.67       422\n",
      "  HS_Religion       0.68      0.57      0.62       173\n",
      "      HS_Race       0.77      0.70      0.73       126\n",
      "  HS_Physical       0.88      0.25      0.39        60\n",
      "    HS_Gender       0.91      0.31      0.47        67\n",
      "     HS_Other       0.77      0.78      0.78       792\n",
      "      HS_Weak       0.71      0.75      0.73       725\n",
      "  HS_Moderate       0.64      0.54      0.58       352\n",
      "    HS_Strong       0.83      0.79      0.81       113\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5806\n",
      "    macro avg       0.78      0.66      0.69      5806\n",
      " weighted avg       0.78      0.78      0.77      5806\n",
      "  samples avg       0.47      0.45      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 0.0004315376281738281 seconds\n",
      "\n",
      "Fold 5 - New train size: 7656\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7656 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.8727, F1 Micro: 0.5685, F1 Macro: 0.2587\n",
      "Epoch 2/10, Train Loss: 0.2725, Accuracy: 0.8968, F1 Micro: 0.7078, F1 Macro: 0.459\n",
      "Epoch 3/10, Train Loss: 0.226, Accuracy: 0.9044, F1 Micro: 0.7264, F1 Macro: 0.5134\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9137, F1 Micro: 0.7488, F1 Macro: 0.5777\n",
      "Epoch 5/10, Train Loss: 0.1569, Accuracy: 0.9145, F1 Micro: 0.7588, F1 Macro: 0.6152\n",
      "Epoch 6/10, Train Loss: 0.1388, Accuracy: 0.9187, F1 Micro: 0.7585, F1 Macro: 0.6338\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9177, F1 Micro: 0.7657, F1 Macro: 0.642\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9158, F1 Micro: 0.765, F1 Macro: 0.674\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9207, F1 Micro: 0.7766, F1 Macro: 0.6988\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9199, F1 Micro: 0.7767, F1 Macro: 0.6967\n",
      "Best result for 7656 samples: F1 Micro: 0.7767\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1190\n",
      "      Abusive       0.88      0.90      0.89      1018\n",
      "HS_Individual       0.73      0.76      0.74       768\n",
      "     HS_Group       0.70      0.65      0.67       422\n",
      "  HS_Religion       0.71      0.60      0.65       173\n",
      "      HS_Race       0.83      0.65      0.73       126\n",
      "  HS_Physical       0.94      0.28      0.44        60\n",
      "    HS_Gender       0.88      0.33      0.48        67\n",
      "     HS_Other       0.75      0.81      0.78       792\n",
      "      HS_Weak       0.71      0.72      0.72       725\n",
      "  HS_Moderate       0.63      0.54      0.58       352\n",
      "    HS_Strong       0.80      0.83      0.82       113\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5806\n",
      "    macro avg       0.78      0.66      0.70      5806\n",
      " weighted avg       0.78      0.77      0.77      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 0.22541308403015137 seconds\n",
      "\n",
      "Fold 5 - New train size: 7901\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 7901 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4008, Accuracy: 0.8739, F1 Micro: 0.5807, F1 Macro: 0.2688\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.8994, F1 Micro: 0.6937, F1 Macro: 0.4859\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9072, F1 Micro: 0.7155, F1 Macro: 0.5366\n",
      "Epoch 4/10, Train Loss: 0.1924, Accuracy: 0.912, F1 Micro: 0.7472, F1 Macro: 0.571\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9169, F1 Micro: 0.7582, F1 Macro: 0.601\n",
      "Epoch 6/10, Train Loss: 0.1379, Accuracy: 0.9185, F1 Micro: 0.7706, F1 Macro: 0.6453\n",
      "Epoch 7/10, Train Loss: 0.1116, Accuracy: 0.9205, F1 Micro: 0.7695, F1 Macro: 0.6587\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9171, F1 Micro: 0.7717, F1 Macro: 0.6898\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9156, F1 Micro: 0.7707, F1 Macro: 0.6759\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9202, F1 Micro: 0.7799, F1 Macro: 0.7113\n",
      "Best result for 7901 samples: F1 Micro: 0.7799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1190\n",
      "      Abusive       0.86      0.92      0.89      1018\n",
      "HS_Individual       0.70      0.79      0.74       768\n",
      "     HS_Group       0.74      0.60      0.66       422\n",
      "  HS_Religion       0.66      0.61      0.63       173\n",
      "      HS_Race       0.80      0.67      0.73       126\n",
      "  HS_Physical       0.69      0.42      0.52        60\n",
      "    HS_Gender       0.82      0.46      0.59        67\n",
      "     HS_Other       0.79      0.81      0.80       792\n",
      "      HS_Weak       0.68      0.78      0.73       725\n",
      "  HS_Moderate       0.66      0.51      0.57       352\n",
      "    HS_Strong       0.84      0.78      0.81       113\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5806\n",
      "    macro avg       0.76      0.69      0.71      5806\n",
      " weighted avg       0.77      0.79      0.78      5806\n",
      "  samples avg       0.46      0.46      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 0.0002536773681640625 seconds\n",
      "\n",
      "Fold 5 - New train size: 8165\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8165 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3904, Accuracy: 0.8711, F1 Micro: 0.5295, F1 Macro: 0.2435\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.899, F1 Micro: 0.6882, F1 Macro: 0.4583\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.9078, F1 Micro: 0.7101, F1 Macro: 0.4971\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.9156, F1 Micro: 0.7457, F1 Macro: 0.556\n",
      "Epoch 5/10, Train Loss: 0.163, Accuracy: 0.918, F1 Micro: 0.7507, F1 Macro: 0.5963\n",
      "Epoch 6/10, Train Loss: 0.1335, Accuracy: 0.9193, F1 Micro: 0.7592, F1 Macro: 0.6346\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.9203, F1 Micro: 0.7773, F1 Macro: 0.6717\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.92, F1 Micro: 0.77, F1 Macro: 0.6729\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9205, F1 Micro: 0.77, F1 Macro: 0.6942\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9204, F1 Micro: 0.7763, F1 Macro: 0.6985\n",
      "Best result for 8165 samples: F1 Micro: 0.7773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1190\n",
      "      Abusive       0.88      0.90      0.89      1018\n",
      "HS_Individual       0.73      0.77      0.75       768\n",
      "     HS_Group       0.73      0.66      0.69       422\n",
      "  HS_Religion       0.66      0.54      0.60       173\n",
      "      HS_Race       0.79      0.73      0.76       126\n",
      "  HS_Physical       0.90      0.15      0.26        60\n",
      "    HS_Gender       0.94      0.22      0.36        67\n",
      "     HS_Other       0.77      0.79      0.78       792\n",
      "      HS_Weak       0.71      0.74      0.73       725\n",
      "  HS_Moderate       0.65      0.55      0.60       352\n",
      "    HS_Strong       0.81      0.78      0.80       113\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5806\n",
      "    macro avg       0.78      0.64      0.67      5806\n",
      " weighted avg       0.78      0.77      0.77      5806\n",
      "  samples avg       0.45      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 238\n",
      "Sampling duration: 0.00034809112548828125 seconds\n",
      "\n",
      "Fold 5 - New train size: 8403\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8403 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8756, F1 Micro: 0.5767, F1 Macro: 0.2666\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.8982, F1 Micro: 0.7099, F1 Macro: 0.4669\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9076, F1 Micro: 0.7195, F1 Macro: 0.5068\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9127, F1 Micro: 0.7513, F1 Macro: 0.5949\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9188, F1 Micro: 0.7659, F1 Macro: 0.6366\n",
      "Epoch 6/10, Train Loss: 0.1279, Accuracy: 0.9207, F1 Micro: 0.7708, F1 Macro: 0.633\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.9208, F1 Micro: 0.7711, F1 Macro: 0.6597\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9223, F1 Micro: 0.7745, F1 Macro: 0.668\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9163, F1 Micro: 0.7735, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9213, F1 Micro: 0.7804, F1 Macro: 0.7092\n",
      "Best result for 8403 samples: F1 Micro: 0.7804\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1190\n",
      "      Abusive       0.88      0.91      0.90      1018\n",
      "HS_Individual       0.73      0.76      0.74       768\n",
      "     HS_Group       0.73      0.66      0.70       422\n",
      "  HS_Religion       0.66      0.66      0.66       173\n",
      "      HS_Race       0.73      0.75      0.74       126\n",
      "  HS_Physical       0.95      0.33      0.49        60\n",
      "    HS_Gender       0.96      0.34      0.51        67\n",
      "     HS_Other       0.78      0.75      0.77       792\n",
      "      HS_Weak       0.71      0.74      0.72       725\n",
      "  HS_Moderate       0.66      0.56      0.60       352\n",
      "    HS_Strong       0.84      0.79      0.81       113\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5806\n",
      "    macro avg       0.79      0.68      0.71      5806\n",
      " weighted avg       0.78      0.78      0.78      5806\n",
      "  samples avg       0.47      0.45      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 0.0003418922424316406 seconds\n",
      "\n",
      "Fold 5 - New train size: 8617\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8617 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3875, Accuracy: 0.878, F1 Micro: 0.6148, F1 Macro: 0.2885\n",
      "Epoch 2/10, Train Loss: 0.2686, Accuracy: 0.899, F1 Micro: 0.6827, F1 Macro: 0.4831\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.9046, F1 Micro: 0.7398, F1 Macro: 0.5581\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9152, F1 Micro: 0.7561, F1 Macro: 0.5963\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9192, F1 Micro: 0.7691, F1 Macro: 0.6158\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9184, F1 Micro: 0.7701, F1 Macro: 0.6656\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9216, F1 Micro: 0.7755, F1 Macro: 0.6701\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.921, F1 Micro: 0.7776, F1 Macro: 0.6989\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9205, F1 Micro: 0.7769, F1 Macro: 0.7023\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9242, F1 Micro: 0.7818, F1 Macro: 0.7009\n",
      "Best result for 8617 samples: F1 Micro: 0.7818\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.91      0.89      0.90      1018\n",
      "HS_Individual       0.77      0.72      0.74       768\n",
      "     HS_Group       0.73      0.66      0.69       422\n",
      "  HS_Religion       0.73      0.55      0.63       173\n",
      "      HS_Race       0.76      0.74      0.75       126\n",
      "  HS_Physical       0.94      0.28      0.44        60\n",
      "    HS_Gender       0.95      0.31      0.47        67\n",
      "     HS_Other       0.80      0.78      0.79       792\n",
      "      HS_Weak       0.76      0.69      0.72       725\n",
      "  HS_Moderate       0.65      0.57      0.61       352\n",
      "    HS_Strong       0.81      0.81      0.81       113\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5806\n",
      "    macro avg       0.81      0.65      0.70      5806\n",
      " weighted avg       0.81      0.75      0.78      5806\n",
      "  samples avg       0.46      0.44      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019073486328125 seconds\n",
      "\n",
      "Fold 5 - New train size: 8817\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 8817 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3864, Accuracy: 0.8773, F1 Micro: 0.5775, F1 Macro: 0.2656\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.8986, F1 Micro: 0.6616, F1 Macro: 0.4087\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9094, F1 Micro: 0.7212, F1 Macro: 0.5066\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9163, F1 Micro: 0.7525, F1 Macro: 0.5763\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.9192, F1 Micro: 0.7688, F1 Macro: 0.6344\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9206, F1 Micro: 0.7797, F1 Macro: 0.6722\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9221, F1 Micro: 0.7696, F1 Macro: 0.6771\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.919, F1 Micro: 0.7751, F1 Macro: 0.6822\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9187, F1 Micro: 0.7752, F1 Macro: 0.7123\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9245, F1 Micro: 0.7834, F1 Macro: 0.7101\n",
      "Best result for 8817 samples: F1 Micro: 0.7834\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1190\n",
      "      Abusive       0.90      0.89      0.90      1018\n",
      "HS_Individual       0.73      0.78      0.75       768\n",
      "     HS_Group       0.80      0.56      0.66       422\n",
      "  HS_Religion       0.76      0.46      0.57       173\n",
      "      HS_Race       0.88      0.66      0.75       126\n",
      "  HS_Physical       0.96      0.42      0.58        60\n",
      "    HS_Gender       0.87      0.39      0.54        67\n",
      "     HS_Other       0.78      0.81      0.80       792\n",
      "      HS_Weak       0.71      0.76      0.73       725\n",
      "  HS_Moderate       0.72      0.45      0.56       352\n",
      "    HS_Strong       0.86      0.78      0.82       113\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5806\n",
      "    macro avg       0.82      0.65      0.71      5806\n",
      " weighted avg       0.81      0.76      0.78      5806\n",
      "  samples avg       0.45      0.44      0.43      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017380714416503906 seconds\n",
      "\n",
      "Fold 5 - New train size: 9017\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9017 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.8736, F1 Micro: 0.5423, F1 Macro: 0.2461\n",
      "Epoch 2/10, Train Loss: 0.2584, Accuracy: 0.9013, F1 Micro: 0.7032, F1 Macro: 0.4739\n",
      "Epoch 3/10, Train Loss: 0.2173, Accuracy: 0.9121, F1 Micro: 0.7482, F1 Macro: 0.5747\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9168, F1 Micro: 0.7583, F1 Macro: 0.5897\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.9188, F1 Micro: 0.7627, F1 Macro: 0.6107\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.921, F1 Micro: 0.7757, F1 Macro: 0.6744\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9169, F1 Micro: 0.7706, F1 Macro: 0.6528\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9199, F1 Micro: 0.7782, F1 Macro: 0.7069\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.9239, F1 Micro: 0.7764, F1 Macro: 0.7098\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9225, F1 Micro: 0.7824, F1 Macro: 0.7199\n",
      "Best result for 9017 samples: F1 Micro: 0.7824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1190\n",
      "      Abusive       0.87      0.92      0.90      1018\n",
      "HS_Individual       0.74      0.74      0.74       768\n",
      "     HS_Group       0.72      0.65      0.69       422\n",
      "  HS_Religion       0.74      0.53      0.62       173\n",
      "      HS_Race       0.84      0.68      0.75       126\n",
      "  HS_Physical       1.00      0.37      0.54        60\n",
      "    HS_Gender       0.86      0.45      0.59        67\n",
      "     HS_Other       0.77      0.80      0.78       792\n",
      "      HS_Weak       0.72      0.73      0.73       725\n",
      "  HS_Moderate       0.64      0.58      0.61       352\n",
      "    HS_Strong       0.88      0.81      0.84       113\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5806\n",
      "    macro avg       0.80      0.68      0.72      5806\n",
      " weighted avg       0.79      0.77      0.78      5806\n",
      "  samples avg       0.47      0.46      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002903938293457031 seconds\n",
      "\n",
      "Fold 5 - New train size: 9217\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9217 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.88, F1 Micro: 0.6066, F1 Macro: 0.2847\n",
      "Epoch 2/10, Train Loss: 0.2563, Accuracy: 0.9022, F1 Micro: 0.707, F1 Macro: 0.5205\n",
      "Epoch 3/10, Train Loss: 0.2116, Accuracy: 0.9123, F1 Micro: 0.7366, F1 Macro: 0.5626\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.9164, F1 Micro: 0.7578, F1 Macro: 0.6021\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9199, F1 Micro: 0.7727, F1 Macro: 0.6589\n",
      "Epoch 6/10, Train Loss: 0.124, Accuracy: 0.9191, F1 Micro: 0.7766, F1 Macro: 0.6836\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9253, F1 Micro: 0.7819, F1 Macro: 0.6978\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9205, F1 Micro: 0.7797, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9232, F1 Micro: 0.7774, F1 Macro: 0.7088\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9239, F1 Micro: 0.7862, F1 Macro: 0.7326\n",
      "Best result for 9217 samples: F1 Micro: 0.7862\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1190\n",
      "      Abusive       0.89      0.91      0.90      1018\n",
      "HS_Individual       0.74      0.74      0.74       768\n",
      "     HS_Group       0.73      0.68      0.70       422\n",
      "  HS_Religion       0.68      0.61      0.64       173\n",
      "      HS_Race       0.80      0.75      0.77       126\n",
      "  HS_Physical       0.74      0.42      0.53        60\n",
      "    HS_Gender       0.81      0.57      0.67        67\n",
      "     HS_Other       0.80      0.78      0.79       792\n",
      "      HS_Weak       0.72      0.72      0.72       725\n",
      "  HS_Moderate       0.64      0.60      0.62       352\n",
      "    HS_Strong       0.88      0.81      0.85       113\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5806\n",
      "    macro avg       0.77      0.70      0.73      5806\n",
      " weighted avg       0.79      0.78      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 1\n",
      "Sampling duration: 0.2582368850708008 seconds\n",
      "\n",
      "Fold 5 - New train size: 9218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8793, F1 Micro: 0.6123, F1 Macro: 0.2949\n",
      "Epoch 2/10, Train Loss: 0.2562, Accuracy: 0.9036, F1 Micro: 0.717, F1 Macro: 0.5264\n",
      "Epoch 3/10, Train Loss: 0.2136, Accuracy: 0.913, F1 Micro: 0.7517, F1 Macro: 0.5872\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9169, F1 Micro: 0.752, F1 Macro: 0.5688\n",
      "Epoch 5/10, Train Loss: 0.1462, Accuracy: 0.9222, F1 Micro: 0.7686, F1 Macro: 0.6408\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9218, F1 Micro: 0.7787, F1 Macro: 0.6748\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9231, F1 Micro: 0.7735, F1 Macro: 0.6627\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9224, F1 Micro: 0.7804, F1 Macro: 0.7038\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9241, F1 Micro: 0.7849, F1 Macro: 0.7197\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.919, F1 Micro: 0.7787, F1 Macro: 0.723\n",
      "Best result for 9218 samples: F1 Micro: 0.7849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1190\n",
      "      Abusive       0.87      0.91      0.89      1018\n",
      "HS_Individual       0.75      0.76      0.75       768\n",
      "     HS_Group       0.76      0.64      0.70       422\n",
      "  HS_Religion       0.67      0.62      0.64       173\n",
      "      HS_Race       0.81      0.73      0.77       126\n",
      "  HS_Physical       0.79      0.37      0.50        60\n",
      "    HS_Gender       0.88      0.42      0.57        67\n",
      "     HS_Other       0.81      0.75      0.78       792\n",
      "      HS_Weak       0.73      0.74      0.74       725\n",
      "  HS_Moderate       0.66      0.56      0.61       352\n",
      "    HS_Strong       0.86      0.82      0.84       113\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5806\n",
      "    macro avg       0.79      0.68      0.72      5806\n",
      " weighted avg       0.80      0.77      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018453598022460938 seconds\n",
      "\n",
      "Fold 5 - New train size: 9418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.8805, F1 Micro: 0.6051, F1 Macro: 0.2848\n",
      "Epoch 2/10, Train Loss: 0.2611, Accuracy: 0.9029, F1 Micro: 0.7143, F1 Macro: 0.5143\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9101, F1 Micro: 0.7376, F1 Macro: 0.5357\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9144, F1 Micro: 0.7608, F1 Macro: 0.6025\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.9212, F1 Micro: 0.7655, F1 Macro: 0.6419\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.9208, F1 Micro: 0.7746, F1 Macro: 0.6697\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.922, F1 Micro: 0.77, F1 Macro: 0.6883\n",
      "Epoch 8/10, Train Loss: 0.0895, Accuracy: 0.9205, F1 Micro: 0.7768, F1 Macro: 0.6842\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9225, F1 Micro: 0.7835, F1 Macro: 0.7166\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9253, F1 Micro: 0.7804, F1 Macro: 0.7229\n",
      "Best result for 9418 samples: F1 Micro: 0.7835\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1190\n",
      "      Abusive       0.89      0.91      0.90      1018\n",
      "HS_Individual       0.74      0.74      0.74       768\n",
      "     HS_Group       0.72      0.67      0.69       422\n",
      "  HS_Religion       0.68      0.66      0.67       173\n",
      "      HS_Race       0.82      0.71      0.76       126\n",
      "  HS_Physical       1.00      0.30      0.46        60\n",
      "    HS_Gender       0.84      0.46      0.60        67\n",
      "     HS_Other       0.76      0.81      0.79       792\n",
      "      HS_Weak       0.72      0.73      0.72       725\n",
      "  HS_Moderate       0.63      0.60      0.61       352\n",
      "    HS_Strong       0.89      0.72      0.79       113\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5806\n",
      "    macro avg       0.80      0.68      0.72      5806\n",
      " weighted avg       0.79      0.78      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00030350685119628906 seconds\n",
      "\n",
      "Fold 5 - New train size: 9618\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9618 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3812, Accuracy: 0.8819, F1 Micro: 0.6173, F1 Macro: 0.2973\n",
      "Epoch 2/10, Train Loss: 0.2562, Accuracy: 0.9032, F1 Micro: 0.7004, F1 Macro: 0.4749\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.9117, F1 Micro: 0.7256, F1 Macro: 0.5286\n",
      "Epoch 4/10, Train Loss: 0.1791, Accuracy: 0.9155, F1 Micro: 0.7337, F1 Macro: 0.5727\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9203, F1 Micro: 0.7735, F1 Macro: 0.6506\n",
      "Epoch 6/10, Train Loss: 0.1219, Accuracy: 0.9213, F1 Micro: 0.766, F1 Macro: 0.6804\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9209, F1 Micro: 0.7764, F1 Macro: 0.6983\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9225, F1 Micro: 0.7802, F1 Macro: 0.7\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9203, F1 Micro: 0.7826, F1 Macro: 0.7226\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9209, F1 Micro: 0.7809, F1 Macro: 0.7128\n",
      "Best result for 9618 samples: F1 Micro: 0.7826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1190\n",
      "      Abusive       0.87      0.92      0.90      1018\n",
      "HS_Individual       0.72      0.78      0.75       768\n",
      "     HS_Group       0.69      0.68      0.69       422\n",
      "  HS_Religion       0.65      0.61      0.63       173\n",
      "      HS_Race       0.77      0.75      0.76       126\n",
      "  HS_Physical       0.92      0.37      0.52        60\n",
      "    HS_Gender       0.87      0.49      0.63        67\n",
      "     HS_Other       0.76      0.79      0.78       792\n",
      "      HS_Weak       0.71      0.76      0.73       725\n",
      "  HS_Moderate       0.62      0.58      0.60       352\n",
      "    HS_Strong       0.84      0.81      0.83       113\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5806\n",
      "    macro avg       0.77      0.70      0.72      5806\n",
      " weighted avg       0.77      0.80      0.78      5806\n",
      "  samples avg       0.47      0.47      0.46      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002951622009277344 seconds\n",
      "\n",
      "Fold 5 - New train size: 9818\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 9818 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3795, Accuracy: 0.8825, F1 Micro: 0.6385, F1 Macro: 0.3107\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9044, F1 Micro: 0.7124, F1 Macro: 0.5073\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.9073, F1 Micro: 0.7463, F1 Macro: 0.5589\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9172, F1 Micro: 0.7674, F1 Macro: 0.6083\n",
      "Epoch 5/10, Train Loss: 0.149, Accuracy: 0.9211, F1 Micro: 0.7663, F1 Macro: 0.6303\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9229, F1 Micro: 0.7803, F1 Macro: 0.6591\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9188, F1 Micro: 0.7706, F1 Macro: 0.6693\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9225, F1 Micro: 0.7833, F1 Macro: 0.7065\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9226, F1 Micro: 0.7778, F1 Macro: 0.7209\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9237, F1 Micro: 0.7857, F1 Macro: 0.725\n",
      "Best result for 9818 samples: F1 Micro: 0.7857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1190\n",
      "      Abusive       0.89      0.90      0.90      1018\n",
      "HS_Individual       0.71      0.80      0.75       768\n",
      "     HS_Group       0.80      0.59      0.68       422\n",
      "  HS_Religion       0.70      0.56      0.62       173\n",
      "      HS_Race       0.80      0.76      0.78       126\n",
      "  HS_Physical       0.86      0.40      0.55        60\n",
      "    HS_Gender       0.75      0.57      0.64        67\n",
      "     HS_Other       0.80      0.77      0.79       792\n",
      "      HS_Weak       0.69      0.78      0.73       725\n",
      "  HS_Moderate       0.74      0.49      0.59       352\n",
      "    HS_Strong       0.88      0.74      0.81       113\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5806\n",
      "    macro avg       0.79      0.69      0.73      5806\n",
      " weighted avg       0.80      0.78      0.78      5806\n",
      "  samples avg       0.46      0.45      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00020956993103027344 seconds\n",
      "\n",
      "Fold 5 - New train size: 10018\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10018 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3767, Accuracy: 0.8751, F1 Micro: 0.5497, F1 Macro: 0.2567\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.904, F1 Micro: 0.7132, F1 Macro: 0.5263\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9131, F1 Micro: 0.738, F1 Macro: 0.5585\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.9153, F1 Micro: 0.7661, F1 Macro: 0.6332\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9195, F1 Micro: 0.7708, F1 Macro: 0.6381\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.9222, F1 Micro: 0.7755, F1 Macro: 0.6821\n",
      "Epoch 7/10, Train Loss: 0.1011, Accuracy: 0.9213, F1 Micro: 0.7708, F1 Macro: 0.6844\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9241, F1 Micro: 0.7844, F1 Macro: 0.7136\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9247, F1 Micro: 0.7835, F1 Macro: 0.7187\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.9258, F1 Micro: 0.7845, F1 Macro: 0.7267\n",
      "Best result for 10018 samples: F1 Micro: 0.7845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.83      0.86      1190\n",
      "      Abusive       0.90      0.89      0.89      1018\n",
      "HS_Individual       0.79      0.70      0.74       768\n",
      "     HS_Group       0.73      0.68      0.70       422\n",
      "  HS_Religion       0.73      0.62      0.67       173\n",
      "      HS_Race       0.79      0.75      0.77       126\n",
      "  HS_Physical       1.00      0.35      0.52        60\n",
      "    HS_Gender       0.84      0.46      0.60        67\n",
      "     HS_Other       0.83      0.75      0.79       792\n",
      "      HS_Weak       0.77      0.68      0.72       725\n",
      "  HS_Moderate       0.64      0.60      0.62       352\n",
      "    HS_Strong       0.86      0.82      0.84       113\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      5806\n",
      "    macro avg       0.81      0.68      0.73      5806\n",
      " weighted avg       0.82      0.75      0.78      5806\n",
      "  samples avg       0.46      0.44      0.44      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001766681671142578 seconds\n",
      "\n",
      "Fold 5 - New train size: 10218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3747, Accuracy: 0.8834, F1 Micro: 0.6554, F1 Macro: 0.3319\n",
      "Epoch 2/10, Train Loss: 0.2531, Accuracy: 0.905, F1 Micro: 0.7179, F1 Macro: 0.5227\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.9097, F1 Micro: 0.7554, F1 Macro: 0.5822\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.916, F1 Micro: 0.7655, F1 Macro: 0.6215\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9244, F1 Micro: 0.778, F1 Macro: 0.6892\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9229, F1 Micro: 0.779, F1 Macro: 0.6637\n",
      "Epoch 7/10, Train Loss: 0.0952, Accuracy: 0.9239, F1 Micro: 0.7792, F1 Macro: 0.6794\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9243, F1 Micro: 0.7841, F1 Macro: 0.7092\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9234, F1 Micro: 0.7854, F1 Macro: 0.7155\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9245, F1 Micro: 0.7852, F1 Macro: 0.7183\n",
      "Best result for 10218 samples: F1 Micro: 0.7854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1190\n",
      "      Abusive       0.89      0.90      0.89      1018\n",
      "HS_Individual       0.73      0.78      0.75       768\n",
      "     HS_Group       0.74      0.64      0.69       422\n",
      "  HS_Religion       0.71      0.60      0.65       173\n",
      "      HS_Race       0.83      0.71      0.76       126\n",
      "  HS_Physical       1.00      0.37      0.54        60\n",
      "    HS_Gender       0.92      0.36      0.52        67\n",
      "     HS_Other       0.79      0.80      0.79       792\n",
      "      HS_Weak       0.71      0.76      0.73       725\n",
      "  HS_Moderate       0.66      0.57      0.61       352\n",
      "    HS_Strong       0.89      0.71      0.79       113\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5806\n",
      "    macro avg       0.81      0.67      0.72      5806\n",
      " weighted avg       0.79      0.78      0.78      5806\n",
      "  samples avg       0.46      0.45      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00016307830810546875 seconds\n",
      "\n",
      "Fold 5 - New train size: 10418\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10418 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3727, Accuracy: 0.8809, F1 Micro: 0.5947, F1 Macro: 0.2878\n",
      "Epoch 2/10, Train Loss: 0.2534, Accuracy: 0.9018, F1 Micro: 0.6727, F1 Macro: 0.4645\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9158, F1 Micro: 0.7479, F1 Macro: 0.5875\n",
      "Epoch 4/10, Train Loss: 0.1683, Accuracy: 0.9203, F1 Micro: 0.7579, F1 Macro: 0.6162\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9195, F1 Micro: 0.7745, F1 Macro: 0.6636\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9251, F1 Micro: 0.779, F1 Macro: 0.6846\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9238, F1 Micro: 0.7839, F1 Macro: 0.7188\n",
      "Epoch 8/10, Train Loss: 0.0817, Accuracy: 0.9222, F1 Micro: 0.7822, F1 Macro: 0.7121\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9214, F1 Micro: 0.7823, F1 Macro: 0.7247\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9208, F1 Micro: 0.7847, F1 Macro: 0.7285\n",
      "Best result for 10418 samples: F1 Micro: 0.7847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1190\n",
      "      Abusive       0.88      0.91      0.90      1018\n",
      "HS_Individual       0.70      0.79      0.74       768\n",
      "     HS_Group       0.72      0.66      0.69       422\n",
      "  HS_Religion       0.73      0.57      0.64       173\n",
      "      HS_Race       0.82      0.74      0.78       126\n",
      "  HS_Physical       0.82      0.45      0.58        60\n",
      "    HS_Gender       0.71      0.52      0.60        67\n",
      "     HS_Other       0.75      0.84      0.79       792\n",
      "      HS_Weak       0.68      0.78      0.73       725\n",
      "  HS_Moderate       0.65      0.59      0.62       352\n",
      "    HS_Strong       0.86      0.78      0.82       113\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5806\n",
      "    macro avg       0.76      0.71      0.73      5806\n",
      " weighted avg       0.77      0.80      0.78      5806\n",
      "  samples avg       0.46      0.47      0.45      5806\n",
      "\n",
      "Nearest checkpoint: 10536\n",
      "Acquired samples: 118\n",
      "Sampling duration: 0.30030155181884766 seconds\n",
      "\n",
      "Fold 5 - New train size: 10536\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 10536 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3697, Accuracy: 0.8841, F1 Micro: 0.653, F1 Macro: 0.3414\n",
      "Epoch 2/10, Train Loss: 0.2482, Accuracy: 0.9061, F1 Micro: 0.7119, F1 Macro: 0.5127\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9147, F1 Micro: 0.744, F1 Macro: 0.5614\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9195, F1 Micro: 0.7696, F1 Macro: 0.6192\n",
      "Epoch 5/10, Train Loss: 0.1418, Accuracy: 0.9229, F1 Micro: 0.772, F1 Macro: 0.6579\n",
      "Epoch 6/10, Train Loss: 0.1214, Accuracy: 0.9222, F1 Micro: 0.776, F1 Macro: 0.6674\n",
      "Epoch 7/10, Train Loss: 0.0994, Accuracy: 0.9229, F1 Micro: 0.7787, F1 Macro: 0.6953\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9255, F1 Micro: 0.7833, F1 Macro: 0.713\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9231, F1 Micro: 0.7863, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9251, F1 Micro: 0.7858, F1 Macro: 0.7276\n",
      "Best result for 10536 samples: F1 Micro: 0.7863\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1190\n",
      "      Abusive       0.88      0.92      0.90      1018\n",
      "HS_Individual       0.73      0.76      0.75       768\n",
      "     HS_Group       0.72      0.67      0.69       422\n",
      "  HS_Religion       0.74      0.57      0.64       173\n",
      "      HS_Race       0.81      0.72      0.76       126\n",
      "  HS_Physical       0.95      0.32      0.47        60\n",
      "    HS_Gender       0.75      0.45      0.56        67\n",
      "     HS_Other       0.78      0.81      0.79       792\n",
      "      HS_Weak       0.70      0.75      0.73       725\n",
      "  HS_Moderate       0.65      0.59      0.61       352\n",
      "    HS_Strong       0.90      0.65      0.76       113\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5806\n",
      "    macro avg       0.79      0.67      0.71      5806\n",
      " weighted avg       0.79      0.79      0.78      5806\n",
      "  samples avg       0.47      0.46      0.45      5806\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 5813.66 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5 \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[1:]\n",
    "X = data['Tweet'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Each element in these lists will be a list of metrics for one fold's learning curve\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    # Adjust checkpoints based on the current fold's training size\n",
    "    checkpoints = [\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # 1. Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "        \n",
    "        # 3. Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        random_sampling(current_train_size, X_pool, train_indices, remaining_indices, sampling_dur, new_samples_shared, fold, X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 4. Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004ef84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T21:41:45.880299Z",
     "iopub.status.busy": "2025-06-25T21:41:45.879901Z",
     "iopub.status.idle": "2025-06-25T21:41:46.678830Z",
     "shell.execute_reply": "2025-06-25T21:41:46.677913Z"
    },
    "papermill": {
     "duration": 0.988326,
     "end_time": "2025-06-25T21:41:46.680443",
     "exception": false,
     "start_time": "2025-06-25T21:41:45.692117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6473829,
     "sourceId": 10457689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29072.790566,
   "end_time": "2025-06-25T21:41:50.028062",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-25T13:37:17.237496",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "061c2e4ddb5b4b71a9137d996c607c7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11750f821b5a4f809a45b5c50a247913": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "13430b35bfe648a0bf198a46956cf2b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5483e85146e545dba3fddd17ebe6c9d9",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_68aaac306299485aada5c60bb1f9c72d",
       "tabbable": null,
       "tooltip": null,
       "value": 497810400.0
      }
     },
     "17394c71fe4f4f368d04afa2bf6b55d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f17b5e30b6b1409fa751b0e32ea6a7e0",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eaadc4ad5a5346f7b2b3a05dad1f4e46",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "1ad558bd214b481e90ce97312f964ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c207d7e5ea9d4fd9877925cef600295a",
        "IPY_MODEL_1fb21fadd7e648a5bd82b54c53e41fd6",
        "IPY_MODEL_c333a5c9a61240a293e179ed312ff8aa"
       ],
       "layout": "IPY_MODEL_65356b6a394942698fd7dad2e58b8d53",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1fb21fadd7e648a5bd82b54c53e41fd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e873a50e483b48eeae7c5585189a3efe",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f4eda8357fb6436089e73a7b7c1f75b1",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "1fd33633b9c2408db73c958f2508db04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "236912e29eb947d09f7c1df2eaca14a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a133babe98c4b8cb253fee90b2f40e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2bc509b621b84233828211e3e5e422a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31191fab0d4244f199f668dd0e470dfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_998f7d99613e4aa39e10c8c77a6aa0b7",
       "placeholder": "",
       "style": "IPY_MODEL_8f4e70c7a34240f78299fda4adf953e3",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "31eff77ed5e2436fb9c6528204b4c514": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "44eddd42909545e9bcf57adb38f3b27b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dea7f08c117646d798d11279dc4702d7",
        "IPY_MODEL_cc3753eb03694483aed7812f7803b017",
        "IPY_MODEL_6d630abaf1044bc8b4e31d7ddf4849f0"
       ],
       "layout": "IPY_MODEL_f69ba5d495e24ed8b13d7789da68e40c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d2792f997064352a84ce2d52ba85e2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d2f3b2e9ecb4c45a6e142f3a5c4295e",
       "placeholder": "",
       "style": "IPY_MODEL_31eff77ed5e2436fb9c6528204b4c514",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:"
      }
     },
     "4e325ad2959c436e9238837d32a4948a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_70b99e1aed9545738f4f4ebb437a1b46",
        "IPY_MODEL_13430b35bfe648a0bf198a46956cf2b0",
        "IPY_MODEL_d3e91128e6ce4470939c83f3b9309b88"
       ],
       "layout": "IPY_MODEL_e3b9626aa01c481ebff4d490d89b29d6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "505aebfd77e64debbdf226b9622cf155": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5483e85146e545dba3fddd17ebe6c9d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d2f3b2e9ecb4c45a6e142f3a5c4295e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65356b6a394942698fd7dad2e58b8d53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68aaac306299485aada5c60bb1f9c72d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b23d4ea92374beabfb4566c8036a17b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6d630abaf1044bc8b4e31d7ddf4849f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b5a6e61d263347d0852902125af9b476",
       "placeholder": "",
       "style": "IPY_MODEL_fb5ce4708c004dada06ec732f65bd29e",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/?[00:00&lt;00:00,10.1MB/s]"
      }
     },
     "70b99e1aed9545738f4f4ebb437a1b46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cfd0e40438694697a69b3a26e75fdcfc",
       "placeholder": "",
       "style": "IPY_MODEL_7ab8d00bd7f7494cbe60cae7f69e720b",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin:100%"
      }
     },
     "7ab8d00bd7f7494cbe60cae7f69e720b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "839c2f577c93401591f89df97ae60ef6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8801372936ee4d03ae816e4b1cafc856": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4d2792f997064352a84ce2d52ba85e2c",
        "IPY_MODEL_17394c71fe4f4f368d04afa2bf6b55d2",
        "IPY_MODEL_a72df45e93ff4e77af7048232858d836"
       ],
       "layout": "IPY_MODEL_d0b3d3c3dee6496a82ec86adc8cd148b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "897c4eca355748a5b3a6493d056c1c15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2a02bb5813f44059e3e94dbc0905b22",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1fd33633b9c2408db73c958f2508db04",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "8f4e70c7a34240f78299fda4adf953e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "967d784f80e44958a9512f62eca2f407": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "998f7d99613e4aa39e10c8c77a6aa0b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ee3510ab6744bf9bd6d4d3f886799fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a72df45e93ff4e77af7048232858d836": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_236912e29eb947d09f7c1df2eaca14a8",
       "placeholder": "",
       "style": "IPY_MODEL_2bc509b621b84233828211e3e5e422a3",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/?[00:00&lt;00:00,127kB/s]"
      }
     },
     "af5dad4717034eaa9fb31a05d7ac156d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_967d784f80e44958a9512f62eca2f407",
       "placeholder": "",
       "style": "IPY_MODEL_e7e4d6958b6c4b52a8d920f799b23161",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,161B/s]"
      }
     },
     "b5a6e61d263347d0852902125af9b476": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf492e89ae71461faa1fbd233300459f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c207d7e5ea9d4fd9877925cef600295a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_de9e9ed64b6c4d0e9cfba7ecec11e64f",
       "placeholder": "",
       "style": "IPY_MODEL_bf492e89ae71461faa1fbd233300459f",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "c333a5c9a61240a293e179ed312ff8aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ee3510ab6744bf9bd6d4d3f886799fa",
       "placeholder": "",
       "style": "IPY_MODEL_505aebfd77e64debbdf226b9622cf155",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,11.0kB/s]"
      }
     },
     "cbee567db00749a38e6b76f1222c23fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31191fab0d4244f199f668dd0e470dfc",
        "IPY_MODEL_897c4eca355748a5b3a6493d056c1c15",
        "IPY_MODEL_af5dad4717034eaa9fb31a05d7ac156d"
       ],
       "layout": "IPY_MODEL_ef21f5a7519645f386dbcc14406620a7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cc3753eb03694483aed7812f7803b017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_11750f821b5a4f809a45b5c50a247913",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2a133babe98c4b8cb253fee90b2f40e2",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "cfd0e40438694697a69b3a26e75fdcfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0b3d3c3dee6496a82ec86adc8cd148b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3e91128e6ce4470939c83f3b9309b88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_061c2e4ddb5b4b71a9137d996c607c7f",
       "placeholder": "",
       "style": "IPY_MODEL_6b23d4ea92374beabfb4566c8036a17b",
       "tabbable": null,
       "tooltip": null,
       "value": "498M/498M[00:02&lt;00:00,250MB/s]"
      }
     },
     "d425996f4a7842fc82e8fab5a7697600": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "de9e9ed64b6c4d0e9cfba7ecec11e64f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dea7f08c117646d798d11279dc4702d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_839c2f577c93401591f89df97ae60ef6",
       "placeholder": "",
       "style": "IPY_MODEL_d425996f4a7842fc82e8fab5a7697600",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:"
      }
     },
     "e3b9626aa01c481ebff4d490d89b29d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7e4d6958b6c4b52a8d920f799b23161": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e873a50e483b48eeae7c5585189a3efe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eaadc4ad5a5346f7b2b3a05dad1f4e46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ef21f5a7519645f386dbcc14406620a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f17b5e30b6b1409fa751b0e32ea6a7e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f2a02bb5813f44059e3e94dbc0905b22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4eda8357fb6436089e73a7b7c1f75b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f69ba5d495e24ed8b13d7789da68e40c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb5ce4708c004dada06ec732f65bd29e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
