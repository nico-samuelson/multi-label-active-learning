{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../../dataset/id-multi-label-hate-speech-and-abusive-language-detection/re_dataset.csv', encoding='latin-1')\n",
    "alay_dict = pd.read_csv('../../../../dataset/id-multi-label-hate-speech-and-abusive-language-detection/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', 1: 'replacement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535,) (10535, 12)\n",
      "(2634,) (2634, 12)\n",
      "['yusril kelompok islam te indas di era jokowi fakta 2019 ganti presiden'\n",
      " 'zaman now segalanya jelas dan data sudah jelas jika klaimnya benar kasih jika klaimnya tidak benar tidak usah repot jangan ganti presiden 2019 nanti juga ketahuan makhluk makhluk yang jago melintir fakta haiz hidup kok mencari penderitaan kerja woi biar dapur bisa dan'\n",
      " 'zaman susilo bambang yudhoyono biaya makan sekeluarga dalam sehari cukup rupiah 50 000 zaman sekarang rupiah 120 000 sekolah dasar rupiah 150 000 baru cukup gaji pegawai negeri sipil sudah 4 tahun tidak naik kasihan nasib rakyat yang gajinya rata rata dua juta sekolah dasar 3 juta mumet mereka mungkin mereka berdoa oh lamanya']\n",
      "(10535,) (10535, 12)\n",
      "['yusril halang halang i pembubaran hati mau lengserkan jokowi '\n",
      " 'yusril kelompok islam te indas di era jokowi fakta 2019 ganti presiden'\n",
      " 'zaman now segalanya jelas dan data sudah jelas jika klaimnya benar kasih jika klaimnya tidak benar tidak usah repot jangan ganti presiden 2019 nanti juga ketahuan makhluk makhluk yang jago melintir fakta haiz hidup kok mencari penderitaan kerja woi biar dapur bisa dan']\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data.sort_values(by=['Tweet'], inplace=True)\n",
    "\n",
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "print(X_train[-3:])\n",
    "\n",
    "acquired_data = pd.read_csv('../acquired_data/hsd-coreset-1-data-10535.csv')\n",
    "acquired_data.sort_values(by=['processed_text'], inplace=True)\n",
    "\n",
    "acq_X_train = acquired_data['processed_text'].values\n",
    "acq_y_train = acquired_data[acquired_data.columns[1:]].values\n",
    "\n",
    "print(acq_X_train.shape, acq_y_train.shape)\n",
    "print(acq_X_train[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(ori_x, acquired_x):\n",
    "    ori_x_sorted = np.sort(ori_x)\n",
    "    acquired_x_sorted = np.sort(acquired_x)\n",
    "\n",
    "    if np.array_equal(ori_x_sorted, acquired_x_sorted):\n",
    "        print(\"ori_x and acquired_x contain the same elements (ignoring order).\")\n",
    "    else:\n",
    "        print(\"ori_x and acquired_x have different elements.\")\n",
    "\n",
    "    set_ori_x = set(ori_x)\n",
    "    set_acquired_x = set(acquired_x)\n",
    "\n",
    "    print(\"Elements in ori_x but not in acquired_x:\", len(set_ori_x - set_acquired_x))\n",
    "    print(\"Elements in acquired_x but not in ori_x:\", len(set_acquired_x - set_ori_x))\n",
    "\n",
    "    diff_indices = np.where(ori_x != acquired_x)[0]  # Get indices where elements differ\n",
    "    print(\"Mismatched indices:\", diff_indices)\n",
    "    print(\"ori_x mismatches:\", ori_x[diff_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori_x and acquired_x have different elements.\n",
      "Elements in ori_x but not in acquired_x: 110\n",
      "Elements in acquired_x but not in ori_x: 0\n",
      "Mismatched indices: [   99   100   101 ... 10532 10533 10534]\n",
      "ori_x mismatches: [' abu gosok janda sudah pak dungu lagi yang begini jadi pengikutnya presiden malu memalukan saja jokowi saja '\n",
      " ' acara tidak mutu ini ulah mantan aktivis bagaimana nih organisasi masyarakat mahasiswa gembong partai komunis indonesia yang jadi musuh bebuyutan himpunan mahasiswa islam '\n",
      " ' acting nya murahan jadi enggak laku di pasaran cebong ya begitu dungu nya permanen haha kebanyakan minum air kencing bagong tumpul itu otak '\n",
      " ...\n",
      " 'yusril kelompok islam te indas di era jokowi fakta 2019 ganti presiden'\n",
      " 'zaman now segalanya jelas dan data sudah jelas jika klaimnya benar kasih jika klaimnya tidak benar tidak usah repot jangan ganti presiden 2019 nanti juga ketahuan makhluk makhluk yang jago melintir fakta haiz hidup kok mencari penderitaan kerja woi biar dapur bisa dan'\n",
      " 'zaman susilo bambang yudhoyono biaya makan sekeluarga dalam sehari cukup rupiah 50 000 zaman sekarang rupiah 120 000 sekolah dasar rupiah 150 000 baru cukup gaji pegawai negeri sipil sudah 4 tahun tidak naik kasihan nasib rakyat yang gajinya rata rata dua juta sekolah dasar 3 juta mumet mereka mungkin mereka berdoa oh lamanya']\n"
     ]
    }
   ],
   "source": [
    "verify(X_train, acq_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
