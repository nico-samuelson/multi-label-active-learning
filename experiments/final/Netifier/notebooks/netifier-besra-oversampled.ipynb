{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7214908b",
   "metadata": {
    "papermill": {
     "duration": 0.004518,
     "end_time": "2025-05-11T06:25:57.428901",
     "exception": false,
     "start_time": "2025-05-11T06:25:57.424383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d88c616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:25:57.438082Z",
     "iopub.status.busy": "2025-05-11T06:25:57.437710Z",
     "iopub.status.idle": "2025-05-11T06:26:32.731107Z",
     "shell.execute_reply": "2025-05-11T06:26:32.730192Z"
    },
    "papermill": {
     "duration": 35.299795,
     "end_time": "2025-05-11T06:26:32.732824",
     "exception": false,
     "start_time": "2025-05-11T06:25:57.433029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d7317",
   "metadata": {
    "papermill": {
     "duration": 0.003658,
     "end_time": "2025-05-11T06:26:32.740791",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.737133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586885ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:32.750141Z",
     "iopub.status.busy": "2025-05-11T06:26:32.749573Z",
     "iopub.status.idle": "2025-05-11T06:26:32.753381Z",
     "shell.execute_reply": "2025-05-11T06:26:32.752502Z"
    },
    "papermill": {
     "duration": 0.009842,
     "end_time": "2025-05-11T06:26:32.754706",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.744864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7367a341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:32.763345Z",
     "iopub.status.busy": "2025-05-11T06:26:32.763119Z",
     "iopub.status.idle": "2025-05-11T06:26:32.766893Z",
     "shell.execute_reply": "2025-05-11T06:26:32.766113Z"
    },
    "papermill": {
     "duration": 0.009535,
     "end_time": "2025-05-11T06:26:32.768145",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.758610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f83455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:32.776661Z",
     "iopub.status.busy": "2025-05-11T06:26:32.776462Z",
     "iopub.status.idle": "2025-05-11T06:26:32.788567Z",
     "shell.execute_reply": "2025-05-11T06:26:32.787962Z"
    },
    "papermill": {
     "duration": 0.017751,
     "end_time": "2025-05-11T06:26:32.789805",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.772054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134c07f",
   "metadata": {
    "papermill": {
     "duration": 0.003677,
     "end_time": "2025-05-11T06:26:32.797631",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.793954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2879112d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:32.806202Z",
     "iopub.status.busy": "2025-05-11T06:26:32.805958Z",
     "iopub.status.idle": "2025-05-11T06:26:32.862041Z",
     "shell.execute_reply": "2025-05-11T06:26:32.860483Z"
    },
    "papermill": {
     "duration": 0.062589,
     "end_time": "2025-05-11T06:26:32.864143",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.801554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-undersampled-besra'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa58b12",
   "metadata": {
    "papermill": {
     "duration": 0.003689,
     "end_time": "2025-05-11T06:26:32.871964",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.868275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46344966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:32.880956Z",
     "iopub.status.busy": "2025-05-11T06:26:32.880660Z",
     "iopub.status.idle": "2025-05-11T06:26:32.997245Z",
     "shell.execute_reply": "2025-05-11T06:26:32.996370Z"
    },
    "papermill": {
     "duration": 0.122624,
     "end_time": "2025-05-11T06:26:32.998575",
     "exception": false,
     "start_time": "2025-05-11T06:26:32.875951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9800, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/netifier/oversampled.csv', encoding='latin-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae34b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.008044Z",
     "iopub.status.busy": "2025-05-11T06:26:33.007812Z",
     "iopub.status.idle": "2025-05-11T06:26:33.034890Z",
     "shell.execute_reply": "2025-05-11T06:26:33.034037Z"
    },
    "papermill": {
     "duration": 0.033207,
     "end_time": "2025-05-11T06:26:33.036325",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.003118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>itu membuktikan bahwa rakyat malaysia anti cin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heh kontol jan sok pemes kamu di rp muka seper...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eh memek diam kamu kepala kamu kaya kontol muk...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text  pornografi  sara  \\\n",
       "0  jabar memang provinsi barokah boleh juga dan n...           0     0   \n",
       "1  sidangahok semoga sipenista agama dan ateknya ...           0     1   \n",
       "2  itu membuktikan bahwa rakyat malaysia anti cin...           0     1   \n",
       "3  heh kontol jan sok pemes kamu di rp muka seper...           1     0   \n",
       "4  eh memek diam kamu kepala kamu kaya kontol muk...           1     0   \n",
       "\n",
       "   radikalisme  pencemaran_nama_baik  \n",
       "0            0                     1  \n",
       "1            1                     1  \n",
       "2            0                     1  \n",
       "3            0                     1  \n",
       "4            0                     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9101f490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.045928Z",
     "iopub.status.busy": "2025-05-11T06:26:33.045722Z",
     "iopub.status.idle": "2025-05-11T06:26:33.068950Z",
     "shell.execute_reply": "2025-05-11T06:26:33.068064Z"
    },
    "papermill": {
     "duration": 0.0295,
     "end_time": "2025-05-11T06:26:33.070278",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.040778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7840,) (7840, 4)\n",
      "(1960,) (1960, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['processed_text'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['processed_text'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024a226",
   "metadata": {
    "papermill": {
     "duration": 0.004237,
     "end_time": "2025-05-11T06:26:33.078899",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.074662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a886000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.088470Z",
     "iopub.status.busy": "2025-05-11T06:26:33.088255Z",
     "iopub.status.idle": "2025-05-11T06:26:33.867965Z",
     "shell.execute_reply": "2025-05-11T06:26:33.867271Z"
    },
    "papermill": {
     "duration": 0.786121,
     "end_time": "2025-05-11T06:26:33.869446",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.083325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa7ff9363384a7db3657b32ca4ae831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca1ab39bc774385b99dad898a137843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa3f77a72d4b418bcf0e4a9fd4ed5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73347f57528a49db80d0377112fc7c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb33d5f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.880440Z",
     "iopub.status.busy": "2025-05-11T06:26:33.880198Z",
     "iopub.status.idle": "2025-05-11T06:26:33.884536Z",
     "shell.execute_reply": "2025-05-11T06:26:33.883890Z"
    },
    "papermill": {
     "duration": 0.0108,
     "end_time": "2025-05-11T06:26:33.885623",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.874823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a129ae",
   "metadata": {
    "papermill": {
     "duration": 0.004611,
     "end_time": "2025-05-11T06:26:33.894943",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.890332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a888153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.905198Z",
     "iopub.status.busy": "2025-05-11T06:26:33.904939Z",
     "iopub.status.idle": "2025-05-11T06:26:33.908563Z",
     "shell.execute_reply": "2025-05-11T06:26:33.907937Z"
    },
    "papermill": {
     "duration": 0.009934,
     "end_time": "2025-05-11T06:26:33.909655",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.899721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a50c2c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.919862Z",
     "iopub.status.busy": "2025-05-11T06:26:33.919654Z",
     "iopub.status.idle": "2025-05-11T06:26:33.924181Z",
     "shell.execute_reply": "2025-05-11T06:26:33.923557Z"
    },
    "papermill": {
     "duration": 0.010947,
     "end_time": "2025-05-11T06:26:33.925414",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.914467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    )  \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0b478e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.936199Z",
     "iopub.status.busy": "2025-05-11T06:26:33.935957Z",
     "iopub.status.idle": "2025-05-11T06:26:33.947985Z",
     "shell.execute_reply": "2025-05-11T06:26:33.947373Z"
    },
    "papermill": {
     "duration": 0.018397,
     "end_time": "2025-05-11T06:26:33.949077",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.930680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(train_labels),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "\n",
    "            nearest_cp = current_train_size\n",
    "            if nearest_cp not in checkpoints:\n",
    "                for cp in checkpoints:\n",
    "                    if cp > current_train_size:\n",
    "                        nearest_cp = cp\n",
    "                        break\n",
    "            percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a8da4",
   "metadata": {
    "papermill": {
     "duration": 0.004708,
     "end_time": "2025-05-11T06:26:33.958608",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.953900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1868e741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.969219Z",
     "iopub.status.busy": "2025-05-11T06:26:33.968936Z",
     "iopub.status.idle": "2025-05-11T06:26:33.974150Z",
     "shell.execute_reply": "2025-05-11T06:26:33.973498Z"
    },
    "papermill": {
     "duration": 0.012,
     "end_time": "2025-05-11T06:26:33.975464",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.963464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2668f",
   "metadata": {
    "papermill": {
     "duration": 0.004881,
     "end_time": "2025-05-11T06:26:33.985488",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.980607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee12de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:33.996560Z",
     "iopub.status.busy": "2025-05-11T06:26:33.996347Z",
     "iopub.status.idle": "2025-05-11T06:26:34.016686Z",
     "shell.execute_reply": "2025-05-11T06:26:34.015831Z"
    },
    "papermill": {
     "duration": 0.027322,
     "end_time": "2025-05-11T06:26:34.017887",
     "exception": false,
     "start_time": "2025-05-11T06:26:33.990565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (âˆ†Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = NetifierDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "    \n",
    "    accelerator.wait_for_everyone()    \n",
    "    if accelerator.is_local_main_process:\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'pornografi': [y_train[i][0] for i in temp],\n",
    "                'sara': [y_train[i][1] for i in temp],\n",
    "                'radikalisme': [y_train[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "    \n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'pornografi': [y_train[i][0] for i in temp],\n",
    "                    'sara': [y_train[i][1] for i in temp],\n",
    "                    'radikalisme': [y_train[i][2] for i in temp],\n",
    "                    'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545a6ee",
   "metadata": {
    "papermill": {
     "duration": 0.005151,
     "end_time": "2025-05-11T06:26:34.028000",
     "exception": false,
     "start_time": "2025-05-11T06:26:34.022849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb16909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:34.038517Z",
     "iopub.status.busy": "2025-05-11T06:26:34.038291Z",
     "iopub.status.idle": "2025-05-11T06:26:34.049153Z",
     "shell.execute_reply": "2025-05-11T06:26:34.048545Z"
    },
    "papermill": {
     "duration": 0.017619,
     "end_time": "2025-05-11T06:26:34.050528",
     "exception": false,
     "start_time": "2025-05-11T06:26:34.032909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        nearest_cp = current_train_size\n",
    "        if nearest_cp not in checkpoints:\n",
    "            for cp in checkpoints:\n",
    "                if cp > current_train_size:\n",
    "                    nearest_cp = cp\n",
    "                    break\n",
    "        percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "        \n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}-{percentage}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fa4339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:26:34.062166Z",
     "iopub.status.busy": "2025-05-11T06:26:34.061916Z",
     "iopub.status.idle": "2025-05-11T10:49:20.329602Z",
     "shell.execute_reply": "2025-05-11T10:49:20.328641Z"
    },
    "papermill": {
     "duration": 15766.274606,
     "end_time": "2025-05-11T10:49:20.330952",
     "exception": false,
     "start_time": "2025-05-11T06:26:34.056346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5909, Accuracy: 0.7523, F1 Micro: 0.003, F1 Macro: 0.003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5225, Accuracy: 0.7747, F1 Micro: 0.1691, F1 Macro: 0.1343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4368, Accuracy: 0.8119, F1 Micro: 0.462, F1 Macro: 0.3593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3851, Accuracy: 0.8191, F1 Micro: 0.5089, F1 Macro: 0.3984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3429, Accuracy: 0.8252, F1 Micro: 0.5415, F1 Macro: 0.4247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3034, Accuracy: 0.8421, F1 Micro: 0.6073, F1 Macro: 0.5383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2655, Accuracy: 0.8575, F1 Micro: 0.6647, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2399, Accuracy: 0.8643, F1 Micro: 0.7021, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1908, Accuracy: 0.8725, F1 Micro: 0.7143, F1 Macro: 0.6995\n",
      "Epoch 10/10, Train Loss: 0.1687, Accuracy: 0.8736, F1 Micro: 0.7117, F1 Macro: 0.6953\n",
      "Model 1 - Iteration 490: Accuracy: 0.8725, F1 Micro: 0.7143, F1 Macro: 0.6995\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       502\n",
      "                sara       0.75      0.49      0.59       504\n",
      "         radikalisme       0.75      0.80      0.77       481\n",
      "pencemaran_nama_baik       0.78      0.41      0.54       482\n",
      "\n",
      "           micro avg       0.80      0.64      0.71      1969\n",
      "           macro avg       0.80      0.64      0.70      1969\n",
      "        weighted avg       0.80      0.64      0.70      1969\n",
      "         samples avg       0.52      0.51      0.51      1969\n",
      "\n",
      "Training completed in 62.74121713638306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5534, Accuracy: 0.7519, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4994, Accuracy: 0.7888, F1 Micro: 0.2751, F1 Macro: 0.207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4097, Accuracy: 0.8165, F1 Micro: 0.5084, F1 Macro: 0.3823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3502, Accuracy: 0.8254, F1 Micro: 0.5321, F1 Macro: 0.4154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.307, Accuracy: 0.8337, F1 Micro: 0.5692, F1 Macro: 0.4729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.273, Accuracy: 0.8541, F1 Micro: 0.6638, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2282, Accuracy: 0.8603, F1 Micro: 0.6745, F1 Macro: 0.6435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1984, Accuracy: 0.8703, F1 Micro: 0.7273, F1 Macro: 0.7251\n",
      "Epoch 9/10, Train Loss: 0.1527, Accuracy: 0.8713, F1 Micro: 0.7211, F1 Macro: 0.711\n",
      "Epoch 10/10, Train Loss: 0.133, Accuracy: 0.8759, F1 Micro: 0.7262, F1 Macro: 0.7163\n",
      "Model 2 - Iteration 490: Accuracy: 0.8703, F1 Micro: 0.7273, F1 Macro: 0.7251\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.84      0.89       502\n",
      "                sara       0.67      0.58      0.62       504\n",
      "         radikalisme       0.76      0.79      0.78       481\n",
      "pencemaran_nama_baik       0.66      0.57      0.61       482\n",
      "\n",
      "           micro avg       0.76      0.70      0.73      1969\n",
      "           macro avg       0.76      0.70      0.73      1969\n",
      "        weighted avg       0.76      0.70      0.73      1969\n",
      "         samples avg       0.54      0.54      0.53      1969\n",
      "\n",
      "Training completed in 59.67392659187317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5825, Accuracy: 0.7519, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5327, Accuracy: 0.7824, F1 Micro: 0.2355, F1 Macro: 0.1698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4418, Accuracy: 0.8053, F1 Micro: 0.4476, F1 Macro: 0.3799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3815, Accuracy: 0.8144, F1 Micro: 0.5244, F1 Macro: 0.4272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3367, Accuracy: 0.825, F1 Micro: 0.5402, F1 Macro: 0.439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3027, Accuracy: 0.8426, F1 Micro: 0.6165, F1 Macro: 0.5579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2606, Accuracy: 0.8581, F1 Micro: 0.6627, F1 Macro: 0.6291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2308, Accuracy: 0.864, F1 Micro: 0.7094, F1 Macro: 0.7011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1803, Accuracy: 0.8734, F1 Micro: 0.7124, F1 Macro: 0.6964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1624, Accuracy: 0.8749, F1 Micro: 0.7202, F1 Macro: 0.7075\n",
      "Model 3 - Iteration 490: Accuracy: 0.8749, F1 Micro: 0.7202, F1 Macro: 0.7075\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.85      0.87       502\n",
      "                sara       0.75      0.52      0.61       504\n",
      "         radikalisme       0.80      0.78      0.79       481\n",
      "pencemaran_nama_baik       0.77      0.44      0.56       482\n",
      "\n",
      "           micro avg       0.81      0.65      0.72      1969\n",
      "           macro avg       0.80      0.65      0.71      1969\n",
      "        weighted avg       0.80      0.65      0.71      1969\n",
      "         samples avg       0.53      0.51      0.51      1969\n",
      "\n",
      "Training completed in 64.90351796150208 s\n",
      "Averaged - Iteration 490: Accuracy: 0.8726, F1 Micro: 0.7206, F1 Macro: 0.7107\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 735\n",
      "Sampling duration: 139.20350980758667 seconds\n",
      "New train size: 1225\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5265, Accuracy: 0.8063, F1 Micro: 0.3937, F1 Macro: 0.3298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3765, Accuracy: 0.8272, F1 Micro: 0.5916, F1 Macro: 0.539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3041, Accuracy: 0.8561, F1 Micro: 0.6875, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.243, Accuracy: 0.8706, F1 Micro: 0.7065, F1 Macro: 0.692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2012, Accuracy: 0.8747, F1 Micro: 0.7162, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1627, Accuracy: 0.8799, F1 Micro: 0.741, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1264, Accuracy: 0.8832, F1 Micro: 0.7653, F1 Macro: 0.7645\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.8798, F1 Micro: 0.7451, F1 Macro: 0.7396\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.8836, F1 Micro: 0.7594, F1 Macro: 0.7579\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.8805, F1 Micro: 0.7577, F1 Macro: 0.7546\n",
      "Model 1 - Iteration 1225: Accuracy: 0.8832, F1 Micro: 0.7653, F1 Macro: 0.7645\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       502\n",
      "                sara       0.72      0.70      0.71       504\n",
      "         radikalisme       0.72      0.86      0.78       481\n",
      "pencemaran_nama_baik       0.68      0.64      0.66       482\n",
      "\n",
      "           micro avg       0.76      0.77      0.77      1969\n",
      "           macro avg       0.77      0.77      0.76      1969\n",
      "        weighted avg       0.77      0.77      0.77      1969\n",
      "         samples avg       0.59      0.60      0.58      1969\n",
      "\n",
      "Training completed in 81.3169162273407 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5066, Accuracy: 0.8102, F1 Micro: 0.4355, F1 Macro: 0.3587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3633, Accuracy: 0.8366, F1 Micro: 0.628, F1 Macro: 0.5902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.288, Accuracy: 0.8575, F1 Micro: 0.6957, F1 Macro: 0.6726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2239, Accuracy: 0.8753, F1 Micro: 0.7324, F1 Macro: 0.7247\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.8779, F1 Micro: 0.726, F1 Macro: 0.7124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.8833, F1 Micro: 0.7474, F1 Macro: 0.7401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.8805, F1 Micro: 0.7518, F1 Macro: 0.749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.8809, F1 Micro: 0.7554, F1 Macro: 0.752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.8824, F1 Micro: 0.7637, F1 Macro: 0.7648\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.8803, F1 Micro: 0.75, F1 Macro: 0.7448\n",
      "Model 2 - Iteration 1225: Accuracy: 0.8824, F1 Micro: 0.7637, F1 Macro: 0.7648\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.85      0.90       502\n",
      "                sara       0.71      0.66      0.69       504\n",
      "         radikalisme       0.74      0.86      0.79       481\n",
      "pencemaran_nama_baik       0.66      0.70      0.68       482\n",
      "\n",
      "           micro avg       0.76      0.77      0.76      1969\n",
      "           macro avg       0.77      0.77      0.76      1969\n",
      "        weighted avg       0.77      0.77      0.77      1969\n",
      "         samples avg       0.59      0.60      0.58      1969\n",
      "\n",
      "Training completed in 83.891756772995 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5313, Accuracy: 0.7932, F1 Micro: 0.3772, F1 Macro: 0.3125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3896, Accuracy: 0.8207, F1 Micro: 0.5826, F1 Macro: 0.5394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3134, Accuracy: 0.8599, F1 Micro: 0.7092, F1 Macro: 0.6999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2416, Accuracy: 0.8759, F1 Micro: 0.7251, F1 Macro: 0.7135\n",
      "Epoch 5/10, Train Loss: 0.2038, Accuracy: 0.877, F1 Micro: 0.7229, F1 Macro: 0.708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1608, Accuracy: 0.8818, F1 Micro: 0.7539, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1229, Accuracy: 0.8848, F1 Micro: 0.7646, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.8867, F1 Micro: 0.7648, F1 Macro: 0.7641\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.8817, F1 Micro: 0.7577, F1 Macro: 0.7545\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.8793, F1 Micro: 0.7549, F1 Macro: 0.7531\n",
      "Model 3 - Iteration 1225: Accuracy: 0.8867, F1 Micro: 0.7648, F1 Macro: 0.7641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.91       502\n",
      "                sara       0.73      0.63      0.68       504\n",
      "         radikalisme       0.80      0.82      0.81       481\n",
      "pencemaran_nama_baik       0.67      0.65      0.66       482\n",
      "\n",
      "           micro avg       0.79      0.74      0.76      1969\n",
      "           macro avg       0.79      0.74      0.76      1969\n",
      "        weighted avg       0.79      0.74      0.76      1969\n",
      "         samples avg       0.58      0.58      0.57      1969\n",
      "\n",
      "Training completed in 81.08020520210266 s\n",
      "Averaged - Iteration 1225: Accuracy: 0.8783, F1 Micro: 0.7426, F1 Macro: 0.7376\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 662\n",
      "Sampling duration: 125.66943430900574 seconds\n",
      "New train size: 1887\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5011, Accuracy: 0.8235, F1 Micro: 0.5576, F1 Macro: 0.472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3403, Accuracy: 0.8567, F1 Micro: 0.6715, F1 Macro: 0.6326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2691, Accuracy: 0.8845, F1 Micro: 0.7494, F1 Macro: 0.7422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2087, Accuracy: 0.8855, F1 Micro: 0.7531, F1 Macro: 0.7429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.8921, F1 Micro: 0.7744, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.8942, F1 Micro: 0.7786, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0894, Accuracy: 0.8892, F1 Micro: 0.7717, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.8909, F1 Micro: 0.7774, F1 Macro: 0.7746\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.8901, F1 Micro: 0.7726, F1 Macro: 0.7685\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.89, F1 Micro: 0.7686, F1 Macro: 0.7638\n",
      "Model 1 - Iteration 1887: Accuracy: 0.8942, F1 Micro: 0.7786, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       502\n",
      "                sara       0.74      0.69      0.72       504\n",
      "         radikalisme       0.81      0.82      0.81       481\n",
      "pencemaran_nama_baik       0.74      0.61      0.67       482\n",
      "\n",
      "           micro avg       0.81      0.75      0.78      1969\n",
      "           macro avg       0.81      0.75      0.78      1969\n",
      "        weighted avg       0.81      0.75      0.78      1969\n",
      "         samples avg       0.59      0.59      0.58      1969\n",
      "\n",
      "Training completed in 98.20959496498108 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4819, Accuracy: 0.8245, F1 Micro: 0.5461, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3259, Accuracy: 0.8629, F1 Micro: 0.6946, F1 Macro: 0.6667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2495, Accuracy: 0.8857, F1 Micro: 0.761, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1995, Accuracy: 0.8919, F1 Micro: 0.7661, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.149, Accuracy: 0.8896, F1 Micro: 0.7817, F1 Macro: 0.7829\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.8944, F1 Micro: 0.7811, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.8837, F1 Micro: 0.7789, F1 Macro: 0.7803\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.8823, F1 Micro: 0.7692, F1 Macro: 0.7671\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.8896, F1 Micro: 0.7769, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.888, F1 Micro: 0.7779, F1 Macro: 0.7784\n",
      "Model 2 - Iteration 1887: Accuracy: 0.8896, F1 Micro: 0.7817, F1 Macro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       502\n",
      "                sara       0.73      0.68      0.70       504\n",
      "         radikalisme       0.81      0.82      0.82       481\n",
      "pencemaran_nama_baik       0.64      0.79      0.71       482\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1969\n",
      "           macro avg       0.77      0.80      0.78      1969\n",
      "        weighted avg       0.77      0.80      0.78      1969\n",
      "         samples avg       0.61      0.62      0.60      1969\n",
      "\n",
      "Training completed in 97.16620993614197 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5098, Accuracy: 0.8177, F1 Micro: 0.5417, F1 Macro: 0.4714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3448, Accuracy: 0.856, F1 Micro: 0.6796, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2664, Accuracy: 0.8839, F1 Micro: 0.7549, F1 Macro: 0.752\n",
      "Epoch 4/10, Train Loss: 0.2072, Accuracy: 0.8828, F1 Micro: 0.7422, F1 Macro: 0.7289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.8897, F1 Micro: 0.7638, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.8945, F1 Micro: 0.7824, F1 Macro: 0.7808\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.8925, F1 Micro: 0.7765, F1 Macro: 0.7738\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.8906, F1 Micro: 0.7741, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.8925, F1 Micro: 0.7783, F1 Macro: 0.775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0379, Accuracy: 0.894, F1 Micro: 0.7841, F1 Macro: 0.7817\n",
      "Model 3 - Iteration 1887: Accuracy: 0.894, F1 Micro: 0.7841, F1 Macro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       502\n",
      "                sara       0.76      0.65      0.70       504\n",
      "         radikalisme       0.79      0.89      0.83       481\n",
      "pencemaran_nama_baik       0.70      0.68      0.69       482\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1969\n",
      "           macro avg       0.79      0.78      0.78      1969\n",
      "        weighted avg       0.79      0.78      0.78      1969\n",
      "         samples avg       0.61      0.60      0.59      1969\n",
      "\n",
      "Training completed in 98.8140287399292 s\n",
      "Averaged - Iteration 1887: Accuracy: 0.8831, F1 Micro: 0.7555, F1 Macro: 0.7518\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 596\n",
      "Sampling duration: 113.09215569496155 seconds\n",
      "New train size: 2483\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4781, Accuracy: 0.8281, F1 Micro: 0.5721, F1 Macro: 0.4821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3177, Accuracy: 0.8766, F1 Micro: 0.7467, F1 Macro: 0.7422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.244, Accuracy: 0.8914, F1 Micro: 0.7598, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.8962, F1 Micro: 0.783, F1 Macro: 0.779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.8948, F1 Micro: 0.7853, F1 Macro: 0.783\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.8959, F1 Micro: 0.7825, F1 Macro: 0.7771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.8964, F1 Micro: 0.7865, F1 Macro: 0.7823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0555, Accuracy: 0.8984, F1 Micro: 0.7927, F1 Macro: 0.7909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0398, Accuracy: 0.8983, F1 Micro: 0.7979, F1 Macro: 0.7979\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.8964, F1 Micro: 0.787, F1 Macro: 0.7843\n",
      "Model 1 - Iteration 2483: Accuracy: 0.8983, F1 Micro: 0.7979, F1 Macro: 0.7979\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       502\n",
      "                sara       0.75      0.72      0.73       504\n",
      "         radikalisme       0.78      0.88      0.83       481\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       482\n",
      "\n",
      "           micro avg       0.79      0.81      0.80      1969\n",
      "           macro avg       0.79      0.81      0.80      1969\n",
      "        weighted avg       0.79      0.81      0.80      1969\n",
      "         samples avg       0.62      0.62      0.61      1969\n",
      "\n",
      "Training completed in 118.39120936393738 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4612, Accuracy: 0.8382, F1 Micro: 0.603, F1 Macro: 0.525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3061, Accuracy: 0.874, F1 Micro: 0.7202, F1 Macro: 0.7017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2364, Accuracy: 0.8826, F1 Micro: 0.7288, F1 Macro: 0.7067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1966, Accuracy: 0.8981, F1 Micro: 0.7844, F1 Macro: 0.7787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.8994, F1 Micro: 0.792, F1 Macro: 0.7904\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.8939, F1 Micro: 0.7727, F1 Macro: 0.7646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.8974, F1 Micro: 0.7966, F1 Macro: 0.7957\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.8976, F1 Micro: 0.7842, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.8962, F1 Micro: 0.7833, F1 Macro: 0.7799\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.8945, F1 Micro: 0.7731, F1 Macro: 0.7674\n",
      "Model 2 - Iteration 2483: Accuracy: 0.8974, F1 Micro: 0.7966, F1 Macro: 0.7957\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       502\n",
      "                sara       0.77      0.66      0.71       504\n",
      "         radikalisme       0.77      0.91      0.84       481\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       482\n",
      "\n",
      "           micro avg       0.78      0.81      0.80      1969\n",
      "           macro avg       0.79      0.81      0.80      1969\n",
      "        weighted avg       0.79      0.81      0.80      1969\n",
      "         samples avg       0.62      0.63      0.61      1969\n",
      "\n",
      "Training completed in 114.73919415473938 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4829, Accuracy: 0.8298, F1 Micro: 0.5824, F1 Macro: 0.5086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3203, Accuracy: 0.8734, F1 Micro: 0.7226, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2445, Accuracy: 0.8906, F1 Micro: 0.7627, F1 Macro: 0.7566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2009, Accuracy: 0.8955, F1 Micro: 0.776, F1 Macro: 0.768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.8996, F1 Micro: 0.793, F1 Macro: 0.7908\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.8977, F1 Micro: 0.7869, F1 Macro: 0.7815\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9012, F1 Micro: 0.7922, F1 Macro: 0.7885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.8979, F1 Micro: 0.7932, F1 Macro: 0.7914\n",
      "Epoch 9/10, Train Loss: 0.0373, Accuracy: 0.8947, F1 Micro: 0.782, F1 Macro: 0.7791\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.8981, F1 Micro: 0.7887, F1 Macro: 0.787\n",
      "Model 3 - Iteration 2483: Accuracy: 0.8979, F1 Micro: 0.7932, F1 Macro: 0.7914\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       502\n",
      "                sara       0.77      0.68      0.72       504\n",
      "         radikalisme       0.81      0.87      0.84       481\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       482\n",
      "\n",
      "           micro avg       0.80      0.79      0.79      1969\n",
      "           macro avg       0.80      0.79      0.79      1969\n",
      "        weighted avg       0.80      0.79      0.79      1969\n",
      "         samples avg       0.61      0.61      0.60      1969\n",
      "\n",
      "Training completed in 114.19292140007019 s\n",
      "Averaged - Iteration 2483: Accuracy: 0.8868, F1 Micro: 0.7656, F1 Macro: 0.7626\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 536\n",
      "Sampling duration: 102.75540518760681 seconds\n",
      "New train size: 3019\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4576, Accuracy: 0.8421, F1 Micro: 0.6222, F1 Macro: 0.5599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3001, Accuracy: 0.8891, F1 Micro: 0.7718, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2245, Accuracy: 0.8896, F1 Micro: 0.7914, F1 Macro: 0.7951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9016, F1 Micro: 0.7942, F1 Macro: 0.7897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.901, F1 Micro: 0.7945, F1 Macro: 0.7909\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9015, F1 Micro: 0.7912, F1 Macro: 0.7868\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9016, F1 Micro: 0.7938, F1 Macro: 0.7907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0487, Accuracy: 0.9023, F1 Micro: 0.8035, F1 Macro: 0.8033\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.8986, F1 Micro: 0.7941, F1 Macro: 0.7925\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.9005, F1 Micro: 0.7954, F1 Macro: 0.7937\n",
      "Model 1 - Iteration 3019: Accuracy: 0.9023, F1 Micro: 0.8035, F1 Macro: 0.8033\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       502\n",
      "                sara       0.76      0.76      0.76       504\n",
      "         radikalisme       0.80      0.87      0.84       481\n",
      "pencemaran_nama_baik       0.71      0.71      0.71       482\n",
      "\n",
      "           micro avg       0.80      0.80      0.80      1969\n",
      "           macro avg       0.80      0.80      0.80      1969\n",
      "        weighted avg       0.80      0.80      0.80      1969\n",
      "         samples avg       0.63      0.62      0.62      1969\n",
      "\n",
      "Training completed in 132.0321261882782 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4435, Accuracy: 0.8558, F1 Micro: 0.6533, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2858, Accuracy: 0.8958, F1 Micro: 0.784, F1 Macro: 0.7833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.8962, F1 Micro: 0.7953, F1 Macro: 0.7964\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.8998, F1 Micro: 0.7898, F1 Macro: 0.7858\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.8994, F1 Micro: 0.7914, F1 Macro: 0.7878\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.8999, F1 Micro: 0.7924, F1 Macro: 0.7905\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9001, F1 Micro: 0.7883, F1 Macro: 0.7829\n",
      "Epoch 8/10, Train Loss: 0.0472, Accuracy: 0.8984, F1 Micro: 0.7913, F1 Macro: 0.789\n",
      "Epoch 9/10, Train Loss: 0.0367, Accuracy: 0.8974, F1 Micro: 0.7942, F1 Macro: 0.7928\n",
      "Epoch 10/10, Train Loss: 0.0273, Accuracy: 0.8987, F1 Micro: 0.7924, F1 Macro: 0.7905\n",
      "Model 2 - Iteration 3019: Accuracy: 0.8962, F1 Micro: 0.7953, F1 Macro: 0.7964\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       502\n",
      "                sara       0.75      0.74      0.75       504\n",
      "         radikalisme       0.77      0.89      0.82       481\n",
      "pencemaran_nama_baik       0.67      0.75      0.71       482\n",
      "\n",
      "           micro avg       0.78      0.81      0.80      1969\n",
      "           macro avg       0.78      0.81      0.80      1969\n",
      "        weighted avg       0.78      0.81      0.80      1969\n",
      "         samples avg       0.62      0.63      0.61      1969\n",
      "\n",
      "Training completed in 127.62753772735596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4594, Accuracy: 0.8436, F1 Micro: 0.6178, F1 Macro: 0.5613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2982, Accuracy: 0.8915, F1 Micro: 0.7792, F1 Macro: 0.778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.222, Accuracy: 0.8933, F1 Micro: 0.788, F1 Macro: 0.7886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.9011, F1 Micro: 0.7985, F1 Macro: 0.7965\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9021, F1 Micro: 0.7922, F1 Macro: 0.7876\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9007, F1 Micro: 0.7866, F1 Macro: 0.7814\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9031, F1 Micro: 0.7973, F1 Macro: 0.7942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0481, Accuracy: 0.9032, F1 Micro: 0.8045, F1 Macro: 0.804\n",
      "Epoch 9/10, Train Loss: 0.0382, Accuracy: 0.8997, F1 Micro: 0.7948, F1 Macro: 0.7931\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9023, F1 Micro: 0.7977, F1 Macro: 0.7934\n",
      "Model 3 - Iteration 3019: Accuracy: 0.9032, F1 Micro: 0.8045, F1 Macro: 0.804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       502\n",
      "                sara       0.76      0.75      0.75       504\n",
      "         radikalisme       0.81      0.88      0.84       481\n",
      "pencemaran_nama_baik       0.73      0.70      0.72       482\n",
      "\n",
      "           micro avg       0.81      0.80      0.80      1969\n",
      "           macro avg       0.81      0.80      0.80      1969\n",
      "        weighted avg       0.81      0.80      0.80      1969\n",
      "         samples avg       0.62      0.62      0.61      1969\n",
      "\n",
      "Training completed in 130.57304787635803 s\n",
      "Averaged - Iteration 3019: Accuracy: 0.8895, F1 Micro: 0.7727, F1 Macro: 0.7703\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 483\n",
      "Sampling duration: 92.11105799674988 seconds\n",
      "New train size: 3502\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4508, Accuracy: 0.8587, F1 Micro: 0.6953, F1 Macro: 0.6782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2832, Accuracy: 0.8905, F1 Micro: 0.7696, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.8981, F1 Micro: 0.7975, F1 Macro: 0.7977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9015, F1 Micro: 0.8075, F1 Macro: 0.8094\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9054, F1 Micro: 0.8071, F1 Macro: 0.805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.906, F1 Micro: 0.81, F1 Macro: 0.8099\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9044, F1 Micro: 0.8004, F1 Macro: 0.7965\n",
      "Epoch 8/10, Train Loss: 0.0481, Accuracy: 0.9031, F1 Micro: 0.8072, F1 Macro: 0.8062\n",
      "Epoch 9/10, Train Loss: 0.0364, Accuracy: 0.9051, F1 Micro: 0.8093, F1 Macro: 0.8083\n",
      "Epoch 10/10, Train Loss: 0.0244, Accuracy: 0.9051, F1 Micro: 0.8071, F1 Macro: 0.806\n",
      "Model 1 - Iteration 3502: Accuracy: 0.906, F1 Micro: 0.81, F1 Macro: 0.8099\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       502\n",
      "                sara       0.76      0.74      0.75       504\n",
      "         radikalisme       0.82      0.87      0.84       481\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       482\n",
      "\n",
      "           micro avg       0.81      0.81      0.81      1969\n",
      "           macro avg       0.81      0.81      0.81      1969\n",
      "        weighted avg       0.81      0.81      0.81      1969\n",
      "         samples avg       0.62      0.62      0.61      1969\n",
      "\n",
      "Training completed in 142.83345675468445 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4315, Accuracy: 0.8693, F1 Micro: 0.727, F1 Macro: 0.7217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.8919, F1 Micro: 0.7767, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9021, F1 Micro: 0.8035, F1 Macro: 0.803\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.902, F1 Micro: 0.8017, F1 Macro: 0.8019\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9039, F1 Micro: 0.8017, F1 Macro: 0.7993\n",
      "Epoch 6/10, Train Loss: 0.0831, Accuracy: 0.9023, F1 Micro: 0.8001, F1 Macro: 0.7989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0619, Accuracy: 0.9052, F1 Micro: 0.8047, F1 Macro: 0.8029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0437, Accuracy: 0.907, F1 Micro: 0.8143, F1 Macro: 0.8136\n",
      "Epoch 9/10, Train Loss: 0.034, Accuracy: 0.9031, F1 Micro: 0.8037, F1 Macro: 0.8013\n",
      "Epoch 10/10, Train Loss: 0.0237, Accuracy: 0.9037, F1 Micro: 0.8067, F1 Macro: 0.8058\n",
      "Model 2 - Iteration 3502: Accuracy: 0.907, F1 Micro: 0.8143, F1 Macro: 0.8136\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       502\n",
      "                sara       0.77      0.76      0.77       504\n",
      "         radikalisme       0.81      0.88      0.84       481\n",
      "pencemaran_nama_baik       0.72      0.73      0.73       482\n",
      "\n",
      "           micro avg       0.81      0.82      0.81      1969\n",
      "           macro avg       0.81      0.82      0.81      1969\n",
      "        weighted avg       0.81      0.82      0.81      1969\n",
      "         samples avg       0.64      0.64      0.63      1969\n",
      "\n",
      "Training completed in 143.32253003120422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4539, Accuracy: 0.8576, F1 Micro: 0.6926, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2851, Accuracy: 0.893, F1 Micro: 0.7775, F1 Macro: 0.7752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2162, Accuracy: 0.8978, F1 Micro: 0.7976, F1 Macro: 0.7979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9056, F1 Micro: 0.809, F1 Macro: 0.8105\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9027, F1 Micro: 0.8018, F1 Macro: 0.7988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.906, F1 Micro: 0.8145, F1 Macro: 0.8154\n",
      "Epoch 7/10, Train Loss: 0.0611, Accuracy: 0.9052, F1 Micro: 0.8047, F1 Macro: 0.8032\n",
      "Epoch 8/10, Train Loss: 0.0456, Accuracy: 0.9056, F1 Micro: 0.811, F1 Macro: 0.8106\n",
      "Epoch 9/10, Train Loss: 0.0354, Accuracy: 0.9069, F1 Micro: 0.8115, F1 Macro: 0.8094\n",
      "Epoch 10/10, Train Loss: 0.0247, Accuracy: 0.9055, F1 Micro: 0.8089, F1 Macro: 0.8079\n",
      "Model 3 - Iteration 3502: Accuracy: 0.906, F1 Micro: 0.8145, F1 Macro: 0.8154\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       502\n",
      "                sara       0.76      0.75      0.76       504\n",
      "         radikalisme       0.82      0.89      0.85       481\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       482\n",
      "\n",
      "           micro avg       0.80      0.83      0.81      1969\n",
      "           macro avg       0.80      0.83      0.82      1969\n",
      "        weighted avg       0.80      0.83      0.82      1969\n",
      "         samples avg       0.63      0.64      0.63      1969\n",
      "\n",
      "Training completed in 143.70158576965332 s\n",
      "Averaged - Iteration 3502: Accuracy: 0.8923, F1 Micro: 0.7794, F1 Macro: 0.7774\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 434\n",
      "Sampling duration: 83.22602772712708 seconds\n",
      "New train size: 3936\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4439, Accuracy: 0.8598, F1 Micro: 0.6714, F1 Macro: 0.6346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.8962, F1 Micro: 0.7914, F1 Macro: 0.7929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.8993, F1 Micro: 0.8023, F1 Macro: 0.8027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9049, F1 Micro: 0.8056, F1 Macro: 0.803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9015, F1 Micro: 0.8114, F1 Macro: 0.8146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0882, Accuracy: 0.9056, F1 Micro: 0.8188, F1 Macro: 0.8201\n",
      "Epoch 7/10, Train Loss: 0.0612, Accuracy: 0.9069, F1 Micro: 0.8031, F1 Macro: 0.7984\n",
      "Epoch 8/10, Train Loss: 0.0437, Accuracy: 0.9088, F1 Micro: 0.8126, F1 Macro: 0.8118\n",
      "Epoch 9/10, Train Loss: 0.0337, Accuracy: 0.9065, F1 Micro: 0.8082, F1 Macro: 0.8055\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.9039, F1 Micro: 0.8076, F1 Macro: 0.8046\n",
      "Model 1 - Iteration 3936: Accuracy: 0.9056, F1 Micro: 0.8188, F1 Macro: 0.8201\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       502\n",
      "                sara       0.75      0.85      0.79       504\n",
      "         radikalisme       0.78      0.90      0.84       481\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       482\n",
      "\n",
      "           micro avg       0.78      0.86      0.82      1969\n",
      "           macro avg       0.79      0.86      0.82      1969\n",
      "        weighted avg       0.79      0.86      0.82      1969\n",
      "         samples avg       0.65      0.66      0.64      1969\n",
      "\n",
      "Training completed in 157.7274887561798 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4286, Accuracy: 0.8696, F1 Micro: 0.7023, F1 Macro: 0.6835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.8948, F1 Micro: 0.7879, F1 Macro: 0.7887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1953, Accuracy: 0.9015, F1 Micro: 0.8005, F1 Macro: 0.8\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9069, F1 Micro: 0.8063, F1 Macro: 0.8028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9036, F1 Micro: 0.8114, F1 Macro: 0.8132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0803, Accuracy: 0.908, F1 Micro: 0.8158, F1 Macro: 0.8159\n",
      "Epoch 7/10, Train Loss: 0.0591, Accuracy: 0.9071, F1 Micro: 0.8118, F1 Macro: 0.8103\n",
      "Epoch 8/10, Train Loss: 0.0411, Accuracy: 0.907, F1 Micro: 0.8149, F1 Macro: 0.8154\n",
      "Epoch 9/10, Train Loss: 0.0307, Accuracy: 0.9081, F1 Micro: 0.8083, F1 Macro: 0.8067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0262, Accuracy: 0.9097, F1 Micro: 0.8205, F1 Macro: 0.8208\n",
      "Model 2 - Iteration 3936: Accuracy: 0.9097, F1 Micro: 0.8205, F1 Macro: 0.8208\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       502\n",
      "                sara       0.80      0.76      0.78       504\n",
      "         radikalisme       0.85      0.86      0.86       481\n",
      "pencemaran_nama_baik       0.69      0.78      0.74       482\n",
      "\n",
      "           micro avg       0.81      0.83      0.82      1969\n",
      "           macro avg       0.81      0.83      0.82      1969\n",
      "        weighted avg       0.81      0.83      0.82      1969\n",
      "         samples avg       0.64      0.65      0.63      1969\n",
      "\n",
      "Training completed in 159.1879243850708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4461, Accuracy: 0.8619, F1 Micro: 0.6803, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2727, Accuracy: 0.8981, F1 Micro: 0.7939, F1 Macro: 0.7955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9025, F1 Micro: 0.8082, F1 Macro: 0.8087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1664, Accuracy: 0.9079, F1 Micro: 0.8091, F1 Macro: 0.8043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.9021, F1 Micro: 0.8114, F1 Macro: 0.8135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9079, F1 Micro: 0.823, F1 Macro: 0.8248\n",
      "Epoch 7/10, Train Loss: 0.0589, Accuracy: 0.9085, F1 Micro: 0.8179, F1 Macro: 0.8184\n",
      "Epoch 8/10, Train Loss: 0.0426, Accuracy: 0.91, F1 Micro: 0.8207, F1 Macro: 0.8207\n",
      "Epoch 9/10, Train Loss: 0.0314, Accuracy: 0.9108, F1 Micro: 0.8188, F1 Macro: 0.8183\n",
      "Epoch 10/10, Train Loss: 0.0228, Accuracy: 0.9094, F1 Micro: 0.8216, F1 Macro: 0.8207\n",
      "Model 3 - Iteration 3936: Accuracy: 0.9079, F1 Micro: 0.823, F1 Macro: 0.8248\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       502\n",
      "                sara       0.75      0.82      0.78       504\n",
      "         radikalisme       0.80      0.90      0.85       481\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       482\n",
      "\n",
      "           micro avg       0.79      0.86      0.82      1969\n",
      "           macro avg       0.79      0.86      0.82      1969\n",
      "        weighted avg       0.79      0.86      0.83      1969\n",
      "         samples avg       0.65      0.66      0.65      1969\n",
      "\n",
      "Training completed in 157.46642351150513 s\n",
      "Averaged - Iteration 3936: Accuracy: 0.8945, F1 Micro: 0.7853, F1 Macro: 0.7838\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 391\n",
      "Sampling duration: 75.46602439880371 seconds\n",
      "New train size: 4327\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4241, Accuracy: 0.8736, F1 Micro: 0.7155, F1 Macro: 0.7053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.8993, F1 Micro: 0.7848, F1 Macro: 0.7825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2036, Accuracy: 0.9068, F1 Micro: 0.8114, F1 Macro: 0.8124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1535, Accuracy: 0.9085, F1 Micro: 0.8139, F1 Macro: 0.8123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9088, F1 Micro: 0.8177, F1 Macro: 0.8182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.079, Accuracy: 0.9102, F1 Micro: 0.821, F1 Macro: 0.8222\n",
      "Epoch 7/10, Train Loss: 0.0597, Accuracy: 0.91, F1 Micro: 0.818, F1 Macro: 0.8167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0411, Accuracy: 0.9131, F1 Micro: 0.8247, F1 Macro: 0.8243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0342, Accuracy: 0.9122, F1 Micro: 0.8278, F1 Macro: 0.8282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0264, Accuracy: 0.912, F1 Micro: 0.83, F1 Macro: 0.8306\n",
      "Model 1 - Iteration 4327: Accuracy: 0.912, F1 Micro: 0.83, F1 Macro: 0.8306\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       502\n",
      "                sara       0.78      0.81      0.79       504\n",
      "         radikalisme       0.80      0.91      0.85       481\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       482\n",
      "\n",
      "           micro avg       0.80      0.87      0.83      1969\n",
      "           macro avg       0.80      0.87      0.83      1969\n",
      "        weighted avg       0.80      0.87      0.83      1969\n",
      "         samples avg       0.65      0.67      0.65      1969\n",
      "\n",
      "Training completed in 173.76910161972046 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4079, Accuracy: 0.8836, F1 Micro: 0.7439, F1 Macro: 0.7389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.8997, F1 Micro: 0.7853, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1982, Accuracy: 0.9102, F1 Micro: 0.8157, F1 Macro: 0.816\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.91, F1 Micro: 0.8133, F1 Macro: 0.81\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1121, Accuracy: 0.911, F1 Micro: 0.8186, F1 Macro: 0.819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0798, Accuracy: 0.9109, F1 Micro: 0.8266, F1 Macro: 0.8291\n",
      "Epoch 7/10, Train Loss: 0.0563, Accuracy: 0.9091, F1 Micro: 0.8219, F1 Macro: 0.8223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0419, Accuracy: 0.9133, F1 Micro: 0.8301, F1 Macro: 0.8309\n",
      "Epoch 9/10, Train Loss: 0.0305, Accuracy: 0.9123, F1 Micro: 0.8241, F1 Macro: 0.8242\n",
      "Epoch 10/10, Train Loss: 0.0253, Accuracy: 0.9128, F1 Micro: 0.8292, F1 Macro: 0.831\n",
      "Model 2 - Iteration 4327: Accuracy: 0.9133, F1 Micro: 0.8301, F1 Macro: 0.8309\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       502\n",
      "                sara       0.78      0.82      0.80       504\n",
      "         radikalisme       0.81      0.89      0.85       481\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       482\n",
      "\n",
      "           micro avg       0.81      0.85      0.83      1969\n",
      "           macro avg       0.81      0.85      0.83      1969\n",
      "        weighted avg       0.81      0.85      0.83      1969\n",
      "         samples avg       0.65      0.66      0.64      1969\n",
      "\n",
      "Training completed in 169.35286593437195 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4235, Accuracy: 0.8824, F1 Micro: 0.746, F1 Macro: 0.7417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.9002, F1 Micro: 0.7877, F1 Macro: 0.7861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9083, F1 Micro: 0.8131, F1 Macro: 0.8132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.9105, F1 Micro: 0.8205, F1 Macro: 0.819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1125, Accuracy: 0.9128, F1 Micro: 0.8253, F1 Macro: 0.8251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0779, Accuracy: 0.9123, F1 Micro: 0.8312, F1 Macro: 0.8342\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.9115, F1 Micro: 0.8207, F1 Macro: 0.8194\n",
      "Epoch 8/10, Train Loss: 0.0407, Accuracy: 0.9143, F1 Micro: 0.8259, F1 Macro: 0.8246\n",
      "Epoch 9/10, Train Loss: 0.032, Accuracy: 0.9148, F1 Micro: 0.8311, F1 Macro: 0.831\n",
      "Epoch 10/10, Train Loss: 0.0261, Accuracy: 0.9149, F1 Micro: 0.8307, F1 Macro: 0.831\n",
      "Model 3 - Iteration 4327: Accuracy: 0.9123, F1 Micro: 0.8312, F1 Macro: 0.8342\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       502\n",
      "                sara       0.79      0.83      0.81       504\n",
      "         radikalisme       0.83      0.88      0.85       481\n",
      "pencemaran_nama_baik       0.66      0.86      0.75       482\n",
      "\n",
      "           micro avg       0.80      0.87      0.83      1969\n",
      "           macro avg       0.81      0.87      0.83      1969\n",
      "        weighted avg       0.81      0.87      0.83      1969\n",
      "         samples avg       0.66      0.67      0.66      1969\n",
      "\n",
      "Training completed in 169.7695369720459 s\n",
      "Averaged - Iteration 4327: Accuracy: 0.8968, F1 Micro: 0.791, F1 Macro: 0.7898\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 352\n",
      "Sampling duration: 68.34078025817871 seconds\n",
      "New train size: 4679\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4218, Accuracy: 0.8701, F1 Micro: 0.7228, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.273, Accuracy: 0.8984, F1 Micro: 0.7963, F1 Macro: 0.7971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2055, Accuracy: 0.911, F1 Micro: 0.8133, F1 Macro: 0.8118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9132, F1 Micro: 0.8275, F1 Macro: 0.8264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1096, Accuracy: 0.9134, F1 Micro: 0.8302, F1 Macro: 0.832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0789, Accuracy: 0.9163, F1 Micro: 0.8319, F1 Macro: 0.8323\n",
      "Epoch 7/10, Train Loss: 0.0557, Accuracy: 0.9115, F1 Micro: 0.8278, F1 Macro: 0.8297\n",
      "Epoch 8/10, Train Loss: 0.0393, Accuracy: 0.9165, F1 Micro: 0.8296, F1 Macro: 0.8275\n",
      "Epoch 9/10, Train Loss: 0.0308, Accuracy: 0.9153, F1 Micro: 0.83, F1 Macro: 0.8284\n",
      "Epoch 10/10, Train Loss: 0.024, Accuracy: 0.9152, F1 Micro: 0.8294, F1 Macro: 0.8287\n",
      "Model 1 - Iteration 4679: Accuracy: 0.9163, F1 Micro: 0.8319, F1 Macro: 0.8323\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       502\n",
      "                sara       0.77      0.82      0.79       504\n",
      "         radikalisme       0.85      0.88      0.86       481\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       482\n",
      "\n",
      "           micro avg       0.83      0.83      0.83      1969\n",
      "           macro avg       0.83      0.83      0.83      1969\n",
      "        weighted avg       0.83      0.83      0.83      1969\n",
      "         samples avg       0.65      0.65      0.64      1969\n",
      "\n",
      "Training completed in 180.66635012626648 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4036, Accuracy: 0.8787, F1 Micro: 0.7494, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.259, Accuracy: 0.9025, F1 Micro: 0.8043, F1 Macro: 0.8055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1971, Accuracy: 0.9094, F1 Micro: 0.8093, F1 Macro: 0.807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1515, Accuracy: 0.9142, F1 Micro: 0.8276, F1 Macro: 0.8267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.106, Accuracy: 0.9132, F1 Micro: 0.8305, F1 Macro: 0.8318\n",
      "Epoch 6/10, Train Loss: 0.0724, Accuracy: 0.9137, F1 Micro: 0.8289, F1 Macro: 0.8304\n",
      "Epoch 7/10, Train Loss: 0.0527, Accuracy: 0.9124, F1 Micro: 0.8252, F1 Macro: 0.8252\n",
      "Epoch 8/10, Train Loss: 0.042, Accuracy: 0.9131, F1 Micro: 0.8243, F1 Macro: 0.8227\n",
      "Epoch 9/10, Train Loss: 0.0293, Accuracy: 0.9152, F1 Micro: 0.8288, F1 Macro: 0.8287\n",
      "Epoch 10/10, Train Loss: 0.0221, Accuracy: 0.9143, F1 Micro: 0.8213, F1 Macro: 0.8179\n",
      "Model 2 - Iteration 4679: Accuracy: 0.9132, F1 Micro: 0.8305, F1 Macro: 0.8318\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       502\n",
      "                sara       0.76      0.85      0.80       504\n",
      "         radikalisme       0.83      0.90      0.86       481\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       482\n",
      "\n",
      "           micro avg       0.81      0.86      0.83      1969\n",
      "           macro avg       0.81      0.86      0.83      1969\n",
      "        weighted avg       0.81      0.86      0.83      1969\n",
      "         samples avg       0.65      0.66      0.65      1969\n",
      "\n",
      "Training completed in 179.9390411376953 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4243, Accuracy: 0.8712, F1 Micro: 0.728, F1 Macro: 0.7209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2705, Accuracy: 0.8981, F1 Micro: 0.7952, F1 Macro: 0.7969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9097, F1 Micro: 0.8109, F1 Macro: 0.8094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9115, F1 Micro: 0.8197, F1 Macro: 0.8181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1113, Accuracy: 0.9149, F1 Micro: 0.83, F1 Macro: 0.8313\n",
      "Epoch 6/10, Train Loss: 0.0746, Accuracy: 0.9142, F1 Micro: 0.8275, F1 Macro: 0.8283\n",
      "Epoch 7/10, Train Loss: 0.0539, Accuracy: 0.9127, F1 Micro: 0.8288, F1 Macro: 0.8304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0436, Accuracy: 0.9163, F1 Micro: 0.8304, F1 Macro: 0.8275\n",
      "Epoch 9/10, Train Loss: 0.0299, Accuracy: 0.9134, F1 Micro: 0.8264, F1 Macro: 0.8261\n",
      "Epoch 10/10, Train Loss: 0.024, Accuracy: 0.9165, F1 Micro: 0.8243, F1 Macro: 0.8216\n",
      "Model 3 - Iteration 4679: Accuracy: 0.9163, F1 Micro: 0.8304, F1 Macro: 0.8275\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       502\n",
      "                sara       0.79      0.81      0.80       504\n",
      "         radikalisme       0.84      0.91      0.87       481\n",
      "pencemaran_nama_baik       0.77      0.66      0.71       482\n",
      "\n",
      "           micro avg       0.84      0.83      0.83      1969\n",
      "           macro avg       0.83      0.83      0.83      1969\n",
      "        weighted avg       0.83      0.83      0.83      1969\n",
      "         samples avg       0.65      0.64      0.64      1969\n",
      "\n",
      "Training completed in 181.02541637420654 s\n",
      "Averaged - Iteration 4679: Accuracy: 0.8988, F1 Micro: 0.7954, F1 Macro: 0.7943\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4900\n",
      "Acquired samples: 221\n",
      "Sampling duration: 61.067893981933594 seconds\n",
      "New train size: 4900\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4138, Accuracy: 0.8793, F1 Micro: 0.7327, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2652, Accuracy: 0.904, F1 Micro: 0.791, F1 Macro: 0.789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1972, Accuracy: 0.908, F1 Micro: 0.8199, F1 Macro: 0.8217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1571, Accuracy: 0.9177, F1 Micro: 0.8327, F1 Macro: 0.8329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1091, Accuracy: 0.9146, F1 Micro: 0.8332, F1 Macro: 0.8337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0739, Accuracy: 0.9186, F1 Micro: 0.8399, F1 Macro: 0.84\n",
      "Epoch 7/10, Train Loss: 0.054, Accuracy: 0.9175, F1 Micro: 0.8341, F1 Macro: 0.8334\n",
      "Epoch 8/10, Train Loss: 0.0386, Accuracy: 0.9178, F1 Micro: 0.8378, F1 Macro: 0.8384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0307, Accuracy: 0.9206, F1 Micro: 0.8399, F1 Macro: 0.8398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0232, Accuracy: 0.9199, F1 Micro: 0.8419, F1 Macro: 0.8419\n",
      "Model 1 - Iteration 4900: Accuracy: 0.9199, F1 Micro: 0.8419, F1 Macro: 0.8419\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       502\n",
      "                sara       0.78      0.85      0.81       504\n",
      "         radikalisme       0.83      0.91      0.87       481\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       482\n",
      "\n",
      "           micro avg       0.82      0.86      0.84      1969\n",
      "           macro avg       0.83      0.86      0.84      1969\n",
      "        weighted avg       0.83      0.86      0.84      1969\n",
      "         samples avg       0.66      0.66      0.65      1969\n",
      "\n",
      "Training completed in 189.45577263832092 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3956, Accuracy: 0.8843, F1 Micro: 0.7484, F1 Macro: 0.7415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9085, F1 Micro: 0.8072, F1 Macro: 0.8076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1901, Accuracy: 0.9109, F1 Micro: 0.8281, F1 Macro: 0.8302\n",
      "Epoch 4/10, Train Loss: 0.1518, Accuracy: 0.9168, F1 Micro: 0.8254, F1 Macro: 0.8237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1052, Accuracy: 0.9143, F1 Micro: 0.8313, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0693, Accuracy: 0.918, F1 Micro: 0.8372, F1 Macro: 0.8373\n",
      "Epoch 7/10, Train Loss: 0.0503, Accuracy: 0.9149, F1 Micro: 0.8283, F1 Macro: 0.8281\n",
      "Epoch 8/10, Train Loss: 0.035, Accuracy: 0.9172, F1 Micro: 0.8372, F1 Macro: 0.8377\n",
      "Epoch 9/10, Train Loss: 0.0291, Accuracy: 0.9195, F1 Micro: 0.8333, F1 Macro: 0.8317\n",
      "Epoch 10/10, Train Loss: 0.0196, Accuracy: 0.916, F1 Micro: 0.834, F1 Macro: 0.835\n",
      "Model 2 - Iteration 4900: Accuracy: 0.918, F1 Micro: 0.8372, F1 Macro: 0.8373\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       502\n",
      "                sara       0.78      0.85      0.81       504\n",
      "         radikalisme       0.86      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.74      0.75      0.74       482\n",
      "\n",
      "           micro avg       0.82      0.85      0.84      1969\n",
      "           macro avg       0.83      0.85      0.84      1969\n",
      "        weighted avg       0.83      0.85      0.84      1969\n",
      "         samples avg       0.66      0.66      0.65      1969\n",
      "\n",
      "Training completed in 184.59291052818298 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4128, Accuracy: 0.8779, F1 Micro: 0.7369, F1 Macro: 0.726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.9051, F1 Micro: 0.7974, F1 Macro: 0.7959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1916, Accuracy: 0.9103, F1 Micro: 0.8217, F1 Macro: 0.8228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1556, Accuracy: 0.9157, F1 Micro: 0.8272, F1 Macro: 0.825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1082, Accuracy: 0.9139, F1 Micro: 0.8306, F1 Macro: 0.8313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0717, Accuracy: 0.9175, F1 Micro: 0.8326, F1 Macro: 0.8314\n",
      "Epoch 7/10, Train Loss: 0.0516, Accuracy: 0.9168, F1 Micro: 0.832, F1 Macro: 0.8317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.035, Accuracy: 0.9156, F1 Micro: 0.8336, F1 Macro: 0.8348\n",
      "Epoch 9/10, Train Loss: 0.0276, Accuracy: 0.9178, F1 Micro: 0.832, F1 Macro: 0.8306\n",
      "Epoch 10/10, Train Loss: 0.022, Accuracy: 0.9152, F1 Micro: 0.8331, F1 Macro: 0.8322\n",
      "Model 3 - Iteration 4900: Accuracy: 0.9156, F1 Micro: 0.8336, F1 Macro: 0.8348\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       502\n",
      "                sara       0.81      0.79      0.80       504\n",
      "         radikalisme       0.85      0.90      0.87       481\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       482\n",
      "\n",
      "           micro avg       0.82      0.85      0.83      1969\n",
      "           macro avg       0.82      0.85      0.83      1969\n",
      "        weighted avg       0.82      0.85      0.84      1969\n",
      "         samples avg       0.66      0.66      0.65      1969\n",
      "\n",
      "Training completed in 187.89977073669434 s\n",
      "Averaged - Iteration 4900: Accuracy: 0.9007, F1 Micro: 0.7996, F1 Macro: 0.7987\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5880\n",
      "Acquired samples: 294\n",
      "Sampling duration: 57.483136892318726 seconds\n",
      "New train size: 5194\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4079, Accuracy: 0.8846, F1 Micro: 0.7617, F1 Macro: 0.7589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.9066, F1 Micro: 0.8101, F1 Macro: 0.8083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9138, F1 Micro: 0.829, F1 Macro: 0.8295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1493, Accuracy: 0.918, F1 Micro: 0.833, F1 Macro: 0.8328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1102, Accuracy: 0.9162, F1 Micro: 0.8367, F1 Macro: 0.8385\n",
      "Epoch 6/10, Train Loss: 0.0691, Accuracy: 0.9209, F1 Micro: 0.834, F1 Macro: 0.8302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0534, Accuracy: 0.9212, F1 Micro: 0.8423, F1 Macro: 0.8422\n",
      "Epoch 8/10, Train Loss: 0.0363, Accuracy: 0.9187, F1 Micro: 0.8375, F1 Macro: 0.8382\n",
      "Epoch 9/10, Train Loss: 0.0275, Accuracy: 0.9186, F1 Micro: 0.8366, F1 Macro: 0.8357\n",
      "Epoch 10/10, Train Loss: 0.0246, Accuracy: 0.9186, F1 Micro: 0.8332, F1 Macro: 0.8296\n",
      "Model 1 - Iteration 5194: Accuracy: 0.9212, F1 Micro: 0.8423, F1 Macro: 0.8422\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       502\n",
      "                sara       0.80      0.82      0.81       504\n",
      "         radikalisme       0.86      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       482\n",
      "\n",
      "           micro avg       0.84      0.85      0.84      1969\n",
      "           macro avg       0.84      0.85      0.84      1969\n",
      "        weighted avg       0.84      0.85      0.84      1969\n",
      "         samples avg       0.66      0.65      0.65      1969\n",
      "\n",
      "Training completed in 195.8295259475708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3927, Accuracy: 0.8884, F1 Micro: 0.7713, F1 Macro: 0.7672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.9069, F1 Micro: 0.8147, F1 Macro: 0.8156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9156, F1 Micro: 0.8269, F1 Macro: 0.8264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1469, Accuracy: 0.9192, F1 Micro: 0.8353, F1 Macro: 0.8346\n",
      "Epoch 5/10, Train Loss: 0.104, Accuracy: 0.9149, F1 Micro: 0.8324, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0679, Accuracy: 0.923, F1 Micro: 0.8393, F1 Macro: 0.837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0543, Accuracy: 0.9181, F1 Micro: 0.8397, F1 Macro: 0.8407\n",
      "Epoch 8/10, Train Loss: 0.0365, Accuracy: 0.9195, F1 Micro: 0.8341, F1 Macro: 0.8332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0278, Accuracy: 0.9212, F1 Micro: 0.8422, F1 Macro: 0.8417\n",
      "Epoch 10/10, Train Loss: 0.0218, Accuracy: 0.9189, F1 Micro: 0.837, F1 Macro: 0.8368\n",
      "Model 2 - Iteration 5194: Accuracy: 0.9212, F1 Micro: 0.8422, F1 Macro: 0.8417\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       502\n",
      "                sara       0.81      0.82      0.81       504\n",
      "         radikalisme       0.85      0.91      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       482\n",
      "\n",
      "           micro avg       0.84      0.85      0.84      1969\n",
      "           macro avg       0.84      0.85      0.84      1969\n",
      "        weighted avg       0.84      0.85      0.84      1969\n",
      "         samples avg       0.66      0.66      0.65      1969\n",
      "\n",
      "Training completed in 197.07234954833984 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4106, Accuracy: 0.8894, F1 Micro: 0.7769, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2617, Accuracy: 0.9064, F1 Micro: 0.8125, F1 Macro: 0.8126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9167, F1 Micro: 0.8334, F1 Macro: 0.834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9183, F1 Micro: 0.8392, F1 Macro: 0.8404\n",
      "Epoch 5/10, Train Loss: 0.1063, Accuracy: 0.9194, F1 Micro: 0.8392, F1 Macro: 0.8397\n",
      "Epoch 6/10, Train Loss: 0.0658, Accuracy: 0.9191, F1 Micro: 0.8262, F1 Macro: 0.8209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0534, Accuracy: 0.9187, F1 Micro: 0.8393, F1 Macro: 0.8389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0358, Accuracy: 0.9187, F1 Micro: 0.8416, F1 Macro: 0.8432\n",
      "Epoch 9/10, Train Loss: 0.0286, Accuracy: 0.9187, F1 Micro: 0.8333, F1 Macro: 0.8322\n",
      "Epoch 10/10, Train Loss: 0.0221, Accuracy: 0.9195, F1 Micro: 0.8408, F1 Macro: 0.841\n",
      "Model 3 - Iteration 5194: Accuracy: 0.9187, F1 Micro: 0.8416, F1 Macro: 0.8432\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       502\n",
      "                sara       0.81      0.84      0.82       504\n",
      "         radikalisme       0.85      0.90      0.87       481\n",
      "pencemaran_nama_baik       0.70      0.84      0.76       482\n",
      "\n",
      "           micro avg       0.81      0.87      0.84      1969\n",
      "           macro avg       0.82      0.87      0.84      1969\n",
      "        weighted avg       0.82      0.87      0.84      1969\n",
      "         samples avg       0.67      0.67      0.66      1969\n",
      "\n",
      "Training completed in 196.15601420402527 s\n",
      "Averaged - Iteration 5194: Accuracy: 0.9025, F1 Micro: 0.8035, F1 Macro: 0.8027\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5880\n",
      "Acquired samples: 265\n",
      "Sampling duration: 51.259729862213135 seconds\n",
      "New train size: 5459\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8753, F1 Micro: 0.7676, F1 Macro: 0.7754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.906, F1 Micro: 0.8098, F1 Macro: 0.8104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.9156, F1 Micro: 0.8261, F1 Macro: 0.8247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1497, Accuracy: 0.9199, F1 Micro: 0.8382, F1 Macro: 0.838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1098, Accuracy: 0.9231, F1 Micro: 0.8458, F1 Macro: 0.8467\n",
      "Epoch 6/10, Train Loss: 0.0683, Accuracy: 0.9189, F1 Micro: 0.8356, F1 Macro: 0.8334\n",
      "Epoch 7/10, Train Loss: 0.0543, Accuracy: 0.92, F1 Micro: 0.8438, F1 Macro: 0.8458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0374, Accuracy: 0.9243, F1 Micro: 0.8496, F1 Macro: 0.8497\n",
      "Epoch 9/10, Train Loss: 0.0316, Accuracy: 0.9215, F1 Micro: 0.8436, F1 Macro: 0.8447\n",
      "Epoch 10/10, Train Loss: 0.0239, Accuracy: 0.9224, F1 Micro: 0.8484, F1 Macro: 0.8496\n",
      "Model 1 - Iteration 5459: Accuracy: 0.9243, F1 Micro: 0.8496, F1 Macro: 0.8497\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       502\n",
      "                sara       0.80      0.84      0.82       504\n",
      "         radikalisme       0.86      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.78      0.77       482\n",
      "\n",
      "           micro avg       0.84      0.86      0.85      1969\n",
      "           macro avg       0.84      0.86      0.85      1969\n",
      "        weighted avg       0.84      0.86      0.85      1969\n",
      "         samples avg       0.68      0.67      0.66      1969\n",
      "\n",
      "Training completed in 203.45200395584106 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3973, Accuracy: 0.8841, F1 Micro: 0.779, F1 Macro: 0.7853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2543, Accuracy: 0.9119, F1 Micro: 0.8186, F1 Macro: 0.818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1906, Accuracy: 0.9175, F1 Micro: 0.8292, F1 Macro: 0.8274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1437, Accuracy: 0.9233, F1 Micro: 0.8428, F1 Macro: 0.8418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1034, Accuracy: 0.9199, F1 Micro: 0.844, F1 Macro: 0.8464\n",
      "Epoch 6/10, Train Loss: 0.0666, Accuracy: 0.9202, F1 Micro: 0.8344, F1 Macro: 0.8318\n",
      "Epoch 7/10, Train Loss: 0.0491, Accuracy: 0.9182, F1 Micro: 0.8417, F1 Macro: 0.8448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.033, Accuracy: 0.9229, F1 Micro: 0.8462, F1 Macro: 0.8461\n",
      "Epoch 9/10, Train Loss: 0.0287, Accuracy: 0.9206, F1 Micro: 0.8394, F1 Macro: 0.8393\n",
      "Epoch 10/10, Train Loss: 0.0206, Accuracy: 0.9204, F1 Micro: 0.8415, F1 Macro: 0.8398\n",
      "Model 2 - Iteration 5459: Accuracy: 0.9229, F1 Micro: 0.8462, F1 Macro: 0.8461\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       502\n",
      "                sara       0.82      0.83      0.83       504\n",
      "         radikalisme       0.85      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.78      0.76       482\n",
      "\n",
      "           micro avg       0.84      0.85      0.85      1969\n",
      "           macro avg       0.84      0.85      0.85      1969\n",
      "        weighted avg       0.84      0.85      0.85      1969\n",
      "         samples avg       0.67      0.66      0.66      1969\n",
      "\n",
      "Training completed in 203.28403878211975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4121, Accuracy: 0.879, F1 Micro: 0.7686, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9104, F1 Micro: 0.8193, F1 Macro: 0.8201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9191, F1 Micro: 0.8355, F1 Macro: 0.8353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9214, F1 Micro: 0.8387, F1 Macro: 0.8365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1077, Accuracy: 0.92, F1 Micro: 0.8427, F1 Macro: 0.8446\n",
      "Epoch 6/10, Train Loss: 0.067, Accuracy: 0.919, F1 Micro: 0.8305, F1 Macro: 0.827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0505, Accuracy: 0.9235, F1 Micro: 0.8452, F1 Macro: 0.8452\n",
      "Epoch 8/10, Train Loss: 0.0345, Accuracy: 0.9221, F1 Micro: 0.8435, F1 Macro: 0.844\n",
      "Epoch 9/10, Train Loss: 0.0274, Accuracy: 0.9202, F1 Micro: 0.8381, F1 Macro: 0.8387\n",
      "Epoch 10/10, Train Loss: 0.0227, Accuracy: 0.9199, F1 Micro: 0.8419, F1 Macro: 0.8421\n",
      "Model 3 - Iteration 5459: Accuracy: 0.9235, F1 Micro: 0.8452, F1 Macro: 0.8452\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       502\n",
      "                sara       0.83      0.82      0.82       504\n",
      "         radikalisme       0.88      0.89      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.76      0.75       482\n",
      "\n",
      "           micro avg       0.85      0.84      0.85      1969\n",
      "           macro avg       0.85      0.84      0.85      1969\n",
      "        weighted avg       0.85      0.84      0.85      1969\n",
      "         samples avg       0.66      0.65      0.65      1969\n",
      "\n",
      "Training completed in 203.73393154144287 s\n",
      "Averaged - Iteration 5459: Accuracy: 0.9043, F1 Micro: 0.8071, F1 Macro: 0.8064\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5880\n",
      "Acquired samples: 239\n",
      "Sampling duration: 48.2285258769989 seconds\n",
      "New train size: 5698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3982, Accuracy: 0.8939, F1 Micro: 0.7752, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2546, Accuracy: 0.9137, F1 Micro: 0.8201, F1 Macro: 0.8175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1882, Accuracy: 0.9165, F1 Micro: 0.8357, F1 Macro: 0.8365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1419, Accuracy: 0.9197, F1 Micro: 0.8453, F1 Macro: 0.846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1046, Accuracy: 0.9241, F1 Micro: 0.8481, F1 Macro: 0.8481\n",
      "Epoch 6/10, Train Loss: 0.0781, Accuracy: 0.9223, F1 Micro: 0.8419, F1 Macro: 0.8398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.055, Accuracy: 0.9241, F1 Micro: 0.8492, F1 Macro: 0.8506\n",
      "Epoch 8/10, Train Loss: 0.037, Accuracy: 0.9233, F1 Micro: 0.8463, F1 Macro: 0.8461\n",
      "Epoch 9/10, Train Loss: 0.0301, Accuracy: 0.922, F1 Micro: 0.8447, F1 Macro: 0.8448\n",
      "Epoch 10/10, Train Loss: 0.0234, Accuracy: 0.9239, F1 Micro: 0.8452, F1 Macro: 0.8449\n",
      "Model 1 - Iteration 5698: Accuracy: 0.9241, F1 Micro: 0.8492, F1 Macro: 0.8506\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.79      0.86      0.82       504\n",
      "         radikalisme       0.89      0.87      0.88       481\n",
      "pencemaran_nama_baik       0.74      0.81      0.77       482\n",
      "\n",
      "           micro avg       0.84      0.86      0.85      1969\n",
      "           macro avg       0.84      0.86      0.85      1969\n",
      "        weighted avg       0.84      0.86      0.85      1969\n",
      "         samples avg       0.67      0.67      0.66      1969\n",
      "\n",
      "Training completed in 214.05009603500366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3859, Accuracy: 0.8924, F1 Micro: 0.7732, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2499, Accuracy: 0.9137, F1 Micro: 0.8238, F1 Macro: 0.8227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1851, Accuracy: 0.9189, F1 Micro: 0.8374, F1 Macro: 0.8374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1337, Accuracy: 0.921, F1 Micro: 0.8437, F1 Macro: 0.8427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1033, Accuracy: 0.9217, F1 Micro: 0.8459, F1 Macro: 0.8474\n",
      "Epoch 6/10, Train Loss: 0.0722, Accuracy: 0.919, F1 Micro: 0.8373, F1 Macro: 0.8362\n",
      "Epoch 7/10, Train Loss: 0.0493, Accuracy: 0.923, F1 Micro: 0.8453, F1 Macro: 0.8462\n",
      "Epoch 8/10, Train Loss: 0.036, Accuracy: 0.9245, F1 Micro: 0.8445, F1 Macro: 0.843\n",
      "Epoch 9/10, Train Loss: 0.0302, Accuracy: 0.922, F1 Micro: 0.8412, F1 Macro: 0.8392\n",
      "Epoch 10/10, Train Loss: 0.0256, Accuracy: 0.9235, F1 Micro: 0.8441, F1 Macro: 0.8434\n",
      "Model 2 - Iteration 5698: Accuracy: 0.9217, F1 Micro: 0.8459, F1 Macro: 0.8474\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.88      0.92       502\n",
      "                sara       0.77      0.88      0.82       504\n",
      "         radikalisme       0.86      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       482\n",
      "\n",
      "           micro avg       0.83      0.87      0.85      1969\n",
      "           macro avg       0.83      0.87      0.85      1969\n",
      "        weighted avg       0.83      0.87      0.85      1969\n",
      "         samples avg       0.67      0.67      0.66      1969\n",
      "\n",
      "Training completed in 211.35771536827087 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4016, Accuracy: 0.8911, F1 Micro: 0.7676, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2563, Accuracy: 0.9123, F1 Micro: 0.8206, F1 Macro: 0.8202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9147, F1 Micro: 0.8311, F1 Macro: 0.8331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9207, F1 Micro: 0.8434, F1 Macro: 0.8436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1056, Accuracy: 0.9246, F1 Micro: 0.8462, F1 Macro: 0.8443\n",
      "Epoch 6/10, Train Loss: 0.0739, Accuracy: 0.923, F1 Micro: 0.8432, F1 Macro: 0.8415\n",
      "Epoch 7/10, Train Loss: 0.0503, Accuracy: 0.9207, F1 Micro: 0.8379, F1 Macro: 0.8358\n",
      "Epoch 8/10, Train Loss: 0.0372, Accuracy: 0.9183, F1 Micro: 0.8257, F1 Macro: 0.8202\n",
      "Epoch 9/10, Train Loss: 0.0334, Accuracy: 0.9202, F1 Micro: 0.8414, F1 Macro: 0.839\n",
      "Epoch 10/10, Train Loss: 0.0237, Accuracy: 0.9202, F1 Micro: 0.8387, F1 Macro: 0.8374\n",
      "Model 3 - Iteration 5698: Accuracy: 0.9246, F1 Micro: 0.8462, F1 Macro: 0.8443\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.89      0.92       502\n",
      "                sara       0.81      0.87      0.84       504\n",
      "         radikalisme       0.87      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.79      0.69      0.73       482\n",
      "\n",
      "           micro avg       0.86      0.84      0.85      1969\n",
      "           macro avg       0.86      0.83      0.84      1969\n",
      "        weighted avg       0.86      0.84      0.85      1969\n",
      "         samples avg       0.66      0.65      0.65      1969\n",
      "\n",
      "Training completed in 212.0143964290619 s\n",
      "Averaged - Iteration 5698: Accuracy: 0.9058, F1 Micro: 0.8102, F1 Macro: 0.8095\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5880\n",
      "Acquired samples: 182\n",
      "Sampling duration: 43.796196937561035 seconds\n",
      "New train size: 5880\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3926, Accuracy: 0.8877, F1 Micro: 0.7587, F1 Macro: 0.7543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9122, F1 Micro: 0.8175, F1 Macro: 0.8162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.196, Accuracy: 0.9187, F1 Micro: 0.8332, F1 Macro: 0.8313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.921, F1 Micro: 0.8439, F1 Macro: 0.8448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1072, Accuracy: 0.9239, F1 Micro: 0.846, F1 Macro: 0.8449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0714, Accuracy: 0.9243, F1 Micro: 0.8461, F1 Macro: 0.8469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0561, Accuracy: 0.9262, F1 Micro: 0.8504, F1 Macro: 0.8509\n",
      "Epoch 8/10, Train Loss: 0.0407, Accuracy: 0.922, F1 Micro: 0.8448, F1 Macro: 0.844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0339, Accuracy: 0.9255, F1 Micro: 0.8509, F1 Macro: 0.852\n",
      "Epoch 10/10, Train Loss: 0.0231, Accuracy: 0.922, F1 Micro: 0.8405, F1 Macro: 0.839\n",
      "Model 1 - Iteration 5880: Accuracy: 0.9255, F1 Micro: 0.8509, F1 Macro: 0.852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.88      0.92       502\n",
      "                sara       0.80      0.85      0.82       504\n",
      "         radikalisme       0.87      0.89      0.88       481\n",
      "pencemaran_nama_baik       0.76      0.80      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.86      0.85      1969\n",
      "           macro avg       0.85      0.86      0.85      1969\n",
      "        weighted avg       0.85      0.86      0.85      1969\n",
      "         samples avg       0.67      0.67      0.66      1969\n",
      "\n",
      "Training completed in 218.38540697097778 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.8913, F1 Micro: 0.7649, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9133, F1 Micro: 0.8206, F1 Macro: 0.8189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9211, F1 Micro: 0.8431, F1 Macro: 0.843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.142, Accuracy: 0.9196, F1 Micro: 0.8436, F1 Macro: 0.8445\n",
      "Epoch 5/10, Train Loss: 0.1044, Accuracy: 0.922, F1 Micro: 0.8432, F1 Macro: 0.8414\n",
      "Epoch 6/10, Train Loss: 0.0709, Accuracy: 0.9226, F1 Micro: 0.8397, F1 Macro: 0.8387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0515, Accuracy: 0.9253, F1 Micro: 0.8463, F1 Macro: 0.8457\n",
      "Epoch 8/10, Train Loss: 0.037, Accuracy: 0.9234, F1 Micro: 0.844, F1 Macro: 0.8423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0305, Accuracy: 0.9245, F1 Micro: 0.8477, F1 Macro: 0.8483\n",
      "Epoch 10/10, Train Loss: 0.0219, Accuracy: 0.9254, F1 Micro: 0.8473, F1 Macro: 0.8455\n",
      "Model 2 - Iteration 5880: Accuracy: 0.9245, F1 Micro: 0.8477, F1 Macro: 0.8483\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.88      0.92       502\n",
      "                sara       0.82      0.83      0.82       504\n",
      "         radikalisme       0.87      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       482\n",
      "\n",
      "           micro avg       0.85      0.85      0.85      1969\n",
      "           macro avg       0.85      0.85      0.85      1969\n",
      "        weighted avg       0.85      0.85      0.85      1969\n",
      "         samples avg       0.67      0.66      0.66      1969\n",
      "\n",
      "Training completed in 215.0231966972351 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.889, F1 Micro: 0.7637, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9142, F1 Micro: 0.8235, F1 Macro: 0.8235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1941, Accuracy: 0.9199, F1 Micro: 0.8388, F1 Macro: 0.8391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1452, Accuracy: 0.9225, F1 Micro: 0.8449, F1 Macro: 0.8454\n",
      "Epoch 5/10, Train Loss: 0.1058, Accuracy: 0.922, F1 Micro: 0.8424, F1 Macro: 0.84\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.074, Accuracy: 0.924, F1 Micro: 0.8463, F1 Macro: 0.847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0486, Accuracy: 0.9241, F1 Micro: 0.8495, F1 Macro: 0.8508\n",
      "Epoch 8/10, Train Loss: 0.0395, Accuracy: 0.9219, F1 Micro: 0.8458, F1 Macro: 0.8445\n",
      "Epoch 9/10, Train Loss: 0.0316, Accuracy: 0.9235, F1 Micro: 0.8466, F1 Macro: 0.8464\n",
      "Epoch 10/10, Train Loss: 0.0229, Accuracy: 0.924, F1 Micro: 0.8494, F1 Macro: 0.8493\n",
      "Model 3 - Iteration 5880: Accuracy: 0.9241, F1 Micro: 0.8495, F1 Macro: 0.8508\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.93       502\n",
      "                sara       0.82      0.85      0.84       504\n",
      "         radikalisme       0.88      0.89      0.88       481\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       482\n",
      "\n",
      "           micro avg       0.84      0.86      0.85      1969\n",
      "           macro avg       0.84      0.86      0.85      1969\n",
      "        weighted avg       0.84      0.86      0.85      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 216.43823766708374 s\n",
      "Averaged - Iteration 5880: Accuracy: 0.9071, F1 Micro: 0.813, F1 Macro: 0.8124\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6860\n",
      "Acquired samples: 200\n",
      "Sampling duration: 38.656984090805054 seconds\n",
      "New train size: 6080\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.8857, F1 Micro: 0.7825, F1 Macro: 0.7867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2455, Accuracy: 0.9069, F1 Micro: 0.8202, F1 Macro: 0.8231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1924, Accuracy: 0.9212, F1 Micro: 0.8408, F1 Macro: 0.8397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1441, Accuracy: 0.9223, F1 Micro: 0.8426, F1 Macro: 0.8425\n",
      "Epoch 5/10, Train Loss: 0.1058, Accuracy: 0.924, F1 Micro: 0.8375, F1 Macro: 0.8325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0689, Accuracy: 0.9259, F1 Micro: 0.8533, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0473, Accuracy: 0.9277, F1 Micro: 0.8546, F1 Macro: 0.8551\n",
      "Epoch 8/10, Train Loss: 0.0398, Accuracy: 0.925, F1 Micro: 0.8468, F1 Macro: 0.8448\n",
      "Epoch 9/10, Train Loss: 0.0308, Accuracy: 0.9236, F1 Micro: 0.8474, F1 Macro: 0.8469\n",
      "Epoch 10/10, Train Loss: 0.0242, Accuracy: 0.9254, F1 Micro: 0.8509, F1 Macro: 0.8496\n",
      "Model 1 - Iteration 6080: Accuracy: 0.9277, F1 Micro: 0.8546, F1 Macro: 0.8551\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.89      0.93       502\n",
      "                sara       0.85      0.82      0.84       504\n",
      "         radikalisme       0.86      0.91      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.80      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.86      0.85      1969\n",
      "           macro avg       0.85      0.86      0.86      1969\n",
      "        weighted avg       0.86      0.86      0.86      1969\n",
      "         samples avg       0.68      0.67      0.66      1969\n",
      "\n",
      "Training completed in 221.37286567687988 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.8868, F1 Micro: 0.7789, F1 Macro: 0.7821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2385, Accuracy: 0.91, F1 Micro: 0.8242, F1 Macro: 0.826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1881, Accuracy: 0.922, F1 Micro: 0.8436, F1 Macro: 0.8424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1397, Accuracy: 0.9257, F1 Micro: 0.8504, F1 Macro: 0.8493\n",
      "Epoch 5/10, Train Loss: 0.1018, Accuracy: 0.9244, F1 Micro: 0.8395, F1 Macro: 0.8384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.07, Accuracy: 0.9263, F1 Micro: 0.8525, F1 Macro: 0.853\n",
      "Epoch 7/10, Train Loss: 0.0437, Accuracy: 0.9263, F1 Micro: 0.8505, F1 Macro: 0.8508\n",
      "Epoch 8/10, Train Loss: 0.0372, Accuracy: 0.9238, F1 Micro: 0.8449, F1 Macro: 0.8428\n",
      "Epoch 9/10, Train Loss: 0.0315, Accuracy: 0.925, F1 Micro: 0.8459, F1 Macro: 0.8445\n",
      "Epoch 10/10, Train Loss: 0.0233, Accuracy: 0.9236, F1 Micro: 0.8491, F1 Macro: 0.8499\n",
      "Model 2 - Iteration 6080: Accuracy: 0.9263, F1 Micro: 0.8525, F1 Macro: 0.853\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.82      0.84      0.83       504\n",
      "         radikalisme       0.87      0.90      0.89       481\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       482\n",
      "\n",
      "           micro avg       0.85      0.86      0.85      1969\n",
      "           macro avg       0.85      0.86      0.85      1969\n",
      "        weighted avg       0.85      0.86      0.85      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 219.43926310539246 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3988, Accuracy: 0.8839, F1 Micro: 0.7742, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2425, Accuracy: 0.9118, F1 Micro: 0.8273, F1 Macro: 0.8287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9224, F1 Micro: 0.8453, F1 Macro: 0.8444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1406, Accuracy: 0.9254, F1 Micro: 0.8483, F1 Macro: 0.8484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.101, Accuracy: 0.9278, F1 Micro: 0.8514, F1 Macro: 0.8512\n",
      "Epoch 6/10, Train Loss: 0.0674, Accuracy: 0.926, F1 Micro: 0.8493, F1 Macro: 0.8488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.044, Accuracy: 0.9258, F1 Micro: 0.852, F1 Macro: 0.852\n",
      "Epoch 8/10, Train Loss: 0.0334, Accuracy: 0.9212, F1 Micro: 0.841, F1 Macro: 0.8401\n",
      "Epoch 9/10, Train Loss: 0.0298, Accuracy: 0.9236, F1 Micro: 0.8469, F1 Macro: 0.8466\n",
      "Epoch 10/10, Train Loss: 0.0236, Accuracy: 0.9249, F1 Micro: 0.8445, F1 Macro: 0.8425\n",
      "Model 3 - Iteration 6080: Accuracy: 0.9258, F1 Micro: 0.852, F1 Macro: 0.852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       502\n",
      "                sara       0.81      0.87      0.84       504\n",
      "         radikalisme       0.86      0.91      0.88       481\n",
      "pencemaran_nama_baik       0.76      0.77      0.76       482\n",
      "\n",
      "           micro avg       0.84      0.86      0.85      1969\n",
      "           macro avg       0.84      0.86      0.85      1969\n",
      "        weighted avg       0.85      0.86      0.85      1969\n",
      "         samples avg       0.67      0.67      0.66      1969\n",
      "\n",
      "Training completed in 221.76611614227295 s\n",
      "Averaged - Iteration 6080: Accuracy: 0.9084, F1 Micro: 0.8157, F1 Macro: 0.8152\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6860\n",
      "Acquired samples: 200\n",
      "Sampling duration: 34.764904260635376 seconds\n",
      "New train size: 6280\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.893, F1 Micro: 0.7658, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9139, F1 Micro: 0.8196, F1 Macro: 0.8167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1816, Accuracy: 0.9186, F1 Micro: 0.8403, F1 Macro: 0.8401\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9224, F1 Micro: 0.8398, F1 Macro: 0.8362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0933, Accuracy: 0.9254, F1 Micro: 0.8408, F1 Macro: 0.8369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0683, Accuracy: 0.9245, F1 Micro: 0.848, F1 Macro: 0.8475\n",
      "Epoch 7/10, Train Loss: 0.0495, Accuracy: 0.9233, F1 Micro: 0.8422, F1 Macro: 0.8399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0386, Accuracy: 0.9299, F1 Micro: 0.8599, F1 Macro: 0.86\n",
      "Epoch 9/10, Train Loss: 0.0289, Accuracy: 0.928, F1 Micro: 0.8555, F1 Macro: 0.8558\n",
      "Epoch 10/10, Train Loss: 0.0229, Accuracy: 0.9272, F1 Micro: 0.8558, F1 Macro: 0.8569\n",
      "Model 1 - Iteration 6280: Accuracy: 0.9299, F1 Micro: 0.8599, F1 Macro: 0.86\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.93       502\n",
      "                sara       0.83      0.83      0.83       504\n",
      "         radikalisme       0.87      0.90      0.89       481\n",
      "pencemaran_nama_baik       0.78      0.81      0.79       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 228.79119062423706 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3689, Accuracy: 0.8957, F1 Micro: 0.7777, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2412, Accuracy: 0.9147, F1 Micro: 0.8211, F1 Macro: 0.818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1768, Accuracy: 0.9161, F1 Micro: 0.8355, F1 Macro: 0.8346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1379, Accuracy: 0.9214, F1 Micro: 0.8387, F1 Macro: 0.836\n",
      "Epoch 5/10, Train Loss: 0.0888, Accuracy: 0.9166, F1 Micro: 0.8195, F1 Macro: 0.8108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0676, Accuracy: 0.9272, F1 Micro: 0.8555, F1 Macro: 0.8549\n",
      "Epoch 7/10, Train Loss: 0.0443, Accuracy: 0.9274, F1 Micro: 0.8515, F1 Macro: 0.8509\n",
      "Epoch 8/10, Train Loss: 0.0311, Accuracy: 0.9264, F1 Micro: 0.8539, F1 Macro: 0.8536\n",
      "Epoch 9/10, Train Loss: 0.0309, Accuracy: 0.9267, F1 Micro: 0.8494, F1 Macro: 0.8493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0224, Accuracy: 0.9283, F1 Micro: 0.8572, F1 Macro: 0.8576\n",
      "Model 2 - Iteration 6280: Accuracy: 0.9283, F1 Micro: 0.8572, F1 Macro: 0.8576\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.93       502\n",
      "                sara       0.82      0.85      0.84       504\n",
      "         radikalisme       0.86      0.91      0.88       481\n",
      "pencemaran_nama_baik       0.76      0.81      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.68      0.68      0.67      1969\n",
      "\n",
      "Training completed in 228.4451196193695 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3884, Accuracy: 0.889, F1 Micro: 0.756, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2458, Accuracy: 0.9115, F1 Micro: 0.8167, F1 Macro: 0.8137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1796, Accuracy: 0.9186, F1 Micro: 0.8399, F1 Macro: 0.8393\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9212, F1 Micro: 0.8375, F1 Macro: 0.8344\n",
      "Epoch 5/10, Train Loss: 0.0928, Accuracy: 0.9239, F1 Micro: 0.8375, F1 Macro: 0.832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0668, Accuracy: 0.9255, F1 Micro: 0.8522, F1 Macro: 0.8528\n",
      "Epoch 7/10, Train Loss: 0.0445, Accuracy: 0.9249, F1 Micro: 0.8495, F1 Macro: 0.8489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0352, Accuracy: 0.9255, F1 Micro: 0.8523, F1 Macro: 0.8522\n",
      "Epoch 9/10, Train Loss: 0.0282, Accuracy: 0.928, F1 Micro: 0.8503, F1 Macro: 0.8495\n",
      "Epoch 10/10, Train Loss: 0.0232, Accuracy: 0.9269, F1 Micro: 0.8514, F1 Macro: 0.8508\n",
      "Model 3 - Iteration 6280: Accuracy: 0.9255, F1 Micro: 0.8523, F1 Macro: 0.8522\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       502\n",
      "                sara       0.81      0.87      0.84       504\n",
      "         radikalisme       0.86      0.90      0.88       481\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       482\n",
      "\n",
      "           micro avg       0.84      0.87      0.85      1969\n",
      "           macro avg       0.84      0.87      0.85      1969\n",
      "        weighted avg       0.84      0.87      0.85      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 228.3887858390808 s\n",
      "Averaged - Iteration 6280: Accuracy: 0.9096, F1 Micro: 0.8182, F1 Macro: 0.8178\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6860\n",
      "Acquired samples: 200\n",
      "Sampling duration: 31.18062949180603 seconds\n",
      "New train size: 6480\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3882, Accuracy: 0.8948, F1 Micro: 0.7693, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.9122, F1 Micro: 0.8266, F1 Macro: 0.8269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1835, Accuracy: 0.918, F1 Micro: 0.8284, F1 Macro: 0.8256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9205, F1 Micro: 0.8442, F1 Macro: 0.8464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0982, Accuracy: 0.9278, F1 Micro: 0.8551, F1 Macro: 0.8541\n",
      "Epoch 6/10, Train Loss: 0.0724, Accuracy: 0.9255, F1 Micro: 0.8508, F1 Macro: 0.8504\n",
      "Epoch 7/10, Train Loss: 0.0462, Accuracy: 0.9253, F1 Micro: 0.8513, F1 Macro: 0.8504\n",
      "Epoch 8/10, Train Loss: 0.0356, Accuracy: 0.926, F1 Micro: 0.8521, F1 Macro: 0.852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0284, Accuracy: 0.9294, F1 Micro: 0.8572, F1 Macro: 0.8571\n",
      "Epoch 10/10, Train Loss: 0.0235, Accuracy: 0.9275, F1 Micro: 0.854, F1 Macro: 0.8529\n",
      "Model 1 - Iteration 6480: Accuracy: 0.9294, F1 Micro: 0.8572, F1 Macro: 0.8571\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.89      0.92       502\n",
      "                sara       0.82      0.86      0.84       504\n",
      "         radikalisme       0.88      0.90      0.89       481\n",
      "pencemaran_nama_baik       0.79      0.76      0.78       482\n",
      "\n",
      "           micro avg       0.86      0.85      0.86      1969\n",
      "           macro avg       0.86      0.85      0.86      1969\n",
      "        weighted avg       0.86      0.85      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 234.33432030677795 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3756, Accuracy: 0.892, F1 Micro: 0.7555, F1 Macro: 0.7412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2363, Accuracy: 0.9176, F1 Micro: 0.836, F1 Macro: 0.8366\n",
      "Epoch 3/10, Train Loss: 0.1753, Accuracy: 0.921, F1 Micro: 0.8348, F1 Macro: 0.8309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.141, Accuracy: 0.924, F1 Micro: 0.8468, F1 Macro: 0.8474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0928, Accuracy: 0.9255, F1 Micro: 0.8503, F1 Macro: 0.8488\n",
      "Epoch 6/10, Train Loss: 0.0668, Accuracy: 0.9241, F1 Micro: 0.8487, F1 Macro: 0.8486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0448, Accuracy: 0.9284, F1 Micro: 0.8555, F1 Macro: 0.8547\n",
      "Epoch 8/10, Train Loss: 0.0327, Accuracy: 0.9263, F1 Micro: 0.8525, F1 Macro: 0.852\n",
      "Epoch 9/10, Train Loss: 0.0273, Accuracy: 0.9233, F1 Micro: 0.8448, F1 Macro: 0.8431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0216, Accuracy: 0.928, F1 Micro: 0.8569, F1 Macro: 0.857\n",
      "Model 2 - Iteration 6480: Accuracy: 0.928, F1 Micro: 0.8569, F1 Macro: 0.857\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       502\n",
      "                sara       0.83      0.87      0.85       504\n",
      "         radikalisme       0.87      0.91      0.89       481\n",
      "pencemaran_nama_baik       0.75      0.79      0.77       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 234.0576457977295 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8906, F1 Micro: 0.7597, F1 Macro: 0.7461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2418, Accuracy: 0.9118, F1 Micro: 0.8278, F1 Macro: 0.8298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1823, Accuracy: 0.9217, F1 Micro: 0.8363, F1 Macro: 0.8334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.927, F1 Micro: 0.8485, F1 Macro: 0.8478\n",
      "Epoch 5/10, Train Loss: 0.0967, Accuracy: 0.9234, F1 Micro: 0.8384, F1 Macro: 0.8341\n",
      "Epoch 6/10, Train Loss: 0.0739, Accuracy: 0.9238, F1 Micro: 0.8472, F1 Macro: 0.8467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0453, Accuracy: 0.9257, F1 Micro: 0.8515, F1 Macro: 0.8522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0347, Accuracy: 0.9284, F1 Micro: 0.8563, F1 Macro: 0.8566\n",
      "Epoch 9/10, Train Loss: 0.0272, Accuracy: 0.9258, F1 Micro: 0.8505, F1 Macro: 0.8491\n",
      "Epoch 10/10, Train Loss: 0.0212, Accuracy: 0.9243, F1 Micro: 0.8407, F1 Macro: 0.8364\n",
      "Model 3 - Iteration 6480: Accuracy: 0.9284, F1 Micro: 0.8563, F1 Macro: 0.8566\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       502\n",
      "                sara       0.82      0.87      0.85       504\n",
      "         radikalisme       0.88      0.89      0.88       481\n",
      "pencemaran_nama_baik       0.76      0.77      0.77       482\n",
      "\n",
      "           micro avg       0.85      0.86      0.86      1969\n",
      "           macro avg       0.85      0.86      0.86      1969\n",
      "        weighted avg       0.86      0.86      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 234.5019872188568 s\n",
      "Averaged - Iteration 6480: Accuracy: 0.9108, F1 Micro: 0.8205, F1 Macro: 0.8201\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6860\n",
      "Acquired samples: 200\n",
      "Sampling duration: 28.049073219299316 seconds\n",
      "New train size: 6680\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8972, F1 Micro: 0.7854, F1 Macro: 0.7848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2395, Accuracy: 0.9148, F1 Micro: 0.8303, F1 Macro: 0.831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1846, Accuracy: 0.923, F1 Micro: 0.8466, F1 Macro: 0.8477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1447, Accuracy: 0.9216, F1 Micro: 0.8478, F1 Macro: 0.8491\n",
      "Epoch 5/10, Train Loss: 0.097, Accuracy: 0.9269, F1 Micro: 0.8474, F1 Macro: 0.8431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0657, Accuracy: 0.9274, F1 Micro: 0.8548, F1 Macro: 0.855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0439, Accuracy: 0.9284, F1 Micro: 0.8554, F1 Macro: 0.8548\n",
      "Epoch 8/10, Train Loss: 0.0341, Accuracy: 0.9282, F1 Micro: 0.8539, F1 Macro: 0.8533\n",
      "Epoch 9/10, Train Loss: 0.0271, Accuracy: 0.9255, F1 Micro: 0.8521, F1 Macro: 0.852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0217, Accuracy: 0.9275, F1 Micro: 0.8565, F1 Macro: 0.8562\n",
      "Model 1 - Iteration 6680: Accuracy: 0.9275, F1 Micro: 0.8565, F1 Macro: 0.8562\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.92      0.94       502\n",
      "                sara       0.81      0.87      0.84       504\n",
      "         radikalisme       0.83      0.92      0.88       481\n",
      "pencemaran_nama_baik       0.77      0.77      0.77       482\n",
      "\n",
      "           micro avg       0.84      0.87      0.86      1969\n",
      "           macro avg       0.84      0.87      0.86      1969\n",
      "        weighted avg       0.84      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 242.2630672454834 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.371, Accuracy: 0.8969, F1 Micro: 0.7822, F1 Macro: 0.781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.232, Accuracy: 0.9147, F1 Micro: 0.8313, F1 Macro: 0.8331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1763, Accuracy: 0.9236, F1 Micro: 0.846, F1 Macro: 0.8456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9217, F1 Micro: 0.8475, F1 Macro: 0.8488\n",
      "Epoch 5/10, Train Loss: 0.0932, Accuracy: 0.9264, F1 Micro: 0.8456, F1 Macro: 0.8406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0616, Accuracy: 0.9288, F1 Micro: 0.8521, F1 Macro: 0.8499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0456, Accuracy: 0.9257, F1 Micro: 0.8531, F1 Macro: 0.8537\n",
      "Epoch 8/10, Train Loss: 0.0307, Accuracy: 0.9257, F1 Micro: 0.8508, F1 Macro: 0.8503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0251, Accuracy: 0.9268, F1 Micro: 0.8547, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0223, Accuracy: 0.9287, F1 Micro: 0.8581, F1 Macro: 0.8575\n",
      "Model 2 - Iteration 6680: Accuracy: 0.9287, F1 Micro: 0.8581, F1 Macro: 0.8575\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       502\n",
      "                sara       0.82      0.86      0.84       504\n",
      "         radikalisme       0.86      0.92      0.89       481\n",
      "pencemaran_nama_baik       0.78      0.77      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 243.9076554775238 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3851, Accuracy: 0.8965, F1 Micro: 0.7808, F1 Macro: 0.7787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2388, Accuracy: 0.9162, F1 Micro: 0.8296, F1 Macro: 0.8292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1798, Accuracy: 0.9233, F1 Micro: 0.8491, F1 Macro: 0.8505\n",
      "Epoch 4/10, Train Loss: 0.1401, Accuracy: 0.9199, F1 Micro: 0.8434, F1 Macro: 0.8443\n",
      "Epoch 5/10, Train Loss: 0.094, Accuracy: 0.9257, F1 Micro: 0.8483, F1 Macro: 0.8448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0641, Accuracy: 0.9279, F1 Micro: 0.8547, F1 Macro: 0.8547\n",
      "Epoch 7/10, Train Loss: 0.0418, Accuracy: 0.9249, F1 Micro: 0.852, F1 Macro: 0.8516\n",
      "Epoch 8/10, Train Loss: 0.0334, Accuracy: 0.928, F1 Micro: 0.8532, F1 Macro: 0.8523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0273, Accuracy: 0.9296, F1 Micro: 0.8581, F1 Macro: 0.8582\n",
      "Epoch 10/10, Train Loss: 0.0204, Accuracy: 0.9288, F1 Micro: 0.8558, F1 Macro: 0.8547\n",
      "Model 3 - Iteration 6680: Accuracy: 0.9296, F1 Micro: 0.8581, F1 Macro: 0.8582\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.94       502\n",
      "                sara       0.84      0.83      0.83       504\n",
      "         radikalisme       0.88      0.90      0.89       481\n",
      "pencemaran_nama_baik       0.76      0.79      0.78       482\n",
      "\n",
      "           micro avg       0.86      0.86      0.86      1969\n",
      "           macro avg       0.86      0.86      0.86      1969\n",
      "        weighted avg       0.86      0.86      0.86      1969\n",
      "         samples avg       0.69      0.67      0.67      1969\n",
      "\n",
      "Training completed in 238.69824361801147 s\n",
      "Averaged - Iteration 6680: Accuracy: 0.9117, F1 Micro: 0.8225, F1 Macro: 0.8221\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6860\n",
      "Acquired samples: 180\n",
      "Sampling duration: 23.900453805923462 seconds\n",
      "New train size: 6860\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3821, Accuracy: 0.8971, F1 Micro: 0.7869, F1 Macro: 0.7871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2374, Accuracy: 0.9098, F1 Micro: 0.8261, F1 Macro: 0.8275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1777, Accuracy: 0.9241, F1 Micro: 0.8467, F1 Macro: 0.8443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1305, Accuracy: 0.9262, F1 Micro: 0.8535, F1 Macro: 0.8539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0886, Accuracy: 0.9283, F1 Micro: 0.8544, F1 Macro: 0.8534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0625, Accuracy: 0.9296, F1 Micro: 0.8556, F1 Macro: 0.8536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0433, Accuracy: 0.9296, F1 Micro: 0.8567, F1 Macro: 0.8552\n",
      "Epoch 8/10, Train Loss: 0.0387, Accuracy: 0.9282, F1 Micro: 0.8559, F1 Macro: 0.8546\n",
      "Epoch 9/10, Train Loss: 0.0256, Accuracy: 0.9303, F1 Micro: 0.8549, F1 Macro: 0.8525\n",
      "Epoch 10/10, Train Loss: 0.0206, Accuracy: 0.9277, F1 Micro: 0.856, F1 Macro: 0.8575\n",
      "Model 1 - Iteration 6860: Accuracy: 0.9296, F1 Micro: 0.8567, F1 Macro: 0.8552\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.94       502\n",
      "                sara       0.83      0.83      0.83       504\n",
      "         radikalisme       0.88      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.80      0.73      0.76       482\n",
      "\n",
      "           micro avg       0.86      0.85      0.86      1969\n",
      "           macro avg       0.86      0.85      0.86      1969\n",
      "        weighted avg       0.86      0.85      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 247.57935237884521 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.372, Accuracy: 0.8987, F1 Micro: 0.7895, F1 Macro: 0.7893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2281, Accuracy: 0.9133, F1 Micro: 0.8315, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1731, Accuracy: 0.9235, F1 Micro: 0.8435, F1 Macro: 0.8397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1237, Accuracy: 0.9255, F1 Micro: 0.8483, F1 Macro: 0.8459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0879, Accuracy: 0.9289, F1 Micro: 0.8568, F1 Macro: 0.8564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0618, Accuracy: 0.9302, F1 Micro: 0.8581, F1 Macro: 0.8577\n",
      "Epoch 7/10, Train Loss: 0.0436, Accuracy: 0.9275, F1 Micro: 0.8519, F1 Macro: 0.8507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0362, Accuracy: 0.9282, F1 Micro: 0.8586, F1 Macro: 0.8591\n",
      "Epoch 9/10, Train Loss: 0.0257, Accuracy: 0.9292, F1 Micro: 0.854, F1 Macro: 0.8522\n",
      "Epoch 10/10, Train Loss: 0.0184, Accuracy: 0.9297, F1 Micro: 0.8558, F1 Macro: 0.8545\n",
      "Model 2 - Iteration 6860: Accuracy: 0.9282, F1 Micro: 0.8586, F1 Macro: 0.8591\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       502\n",
      "                sara       0.78      0.89      0.83       504\n",
      "         radikalisme       0.89      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.76      0.79      0.78       482\n",
      "\n",
      "           micro avg       0.84      0.88      0.86      1969\n",
      "           macro avg       0.84      0.88      0.86      1969\n",
      "        weighted avg       0.84      0.88      0.86      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 247.4624297618866 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8968, F1 Micro: 0.7919, F1 Macro: 0.7941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2353, Accuracy: 0.909, F1 Micro: 0.8271, F1 Macro: 0.8292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9248, F1 Micro: 0.8468, F1 Macro: 0.8445\n",
      "Epoch 4/10, Train Loss: 0.1269, Accuracy: 0.9243, F1 Micro: 0.8468, F1 Macro: 0.8461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0908, Accuracy: 0.9296, F1 Micro: 0.8571, F1 Macro: 0.857\n",
      "Epoch 6/10, Train Loss: 0.061, Accuracy: 0.9284, F1 Micro: 0.8537, F1 Macro: 0.8533\n",
      "Epoch 7/10, Train Loss: 0.0421, Accuracy: 0.9273, F1 Micro: 0.85, F1 Macro: 0.8485\n",
      "Epoch 8/10, Train Loss: 0.0362, Accuracy: 0.9298, F1 Micro: 0.8565, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0245, Accuracy: 0.9308, F1 Micro: 0.8577, F1 Macro: 0.8562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0207, Accuracy: 0.9292, F1 Micro: 0.8606, F1 Macro: 0.8618\n",
      "Model 3 - Iteration 6860: Accuracy: 0.9292, F1 Micro: 0.8606, F1 Macro: 0.8618\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.93       502\n",
      "                sara       0.80      0.88      0.84       504\n",
      "         radikalisme       0.88      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.74      0.82      0.78       482\n",
      "\n",
      "           micro avg       0.84      0.88      0.86      1969\n",
      "           macro avg       0.84      0.88      0.86      1969\n",
      "        weighted avg       0.85      0.88      0.86      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 245.75267219543457 s\n",
      "Averaged - Iteration 6860: Accuracy: 0.9126, F1 Micro: 0.8244, F1 Macro: 0.8241\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7840\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.322033405303955 seconds\n",
      "New train size: 7060\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3714, Accuracy: 0.8939, F1 Micro: 0.7811, F1 Macro: 0.7772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2355, Accuracy: 0.9127, F1 Micro: 0.8231, F1 Macro: 0.8243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.9215, F1 Micro: 0.8482, F1 Macro: 0.8496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1321, Accuracy: 0.9258, F1 Micro: 0.8519, F1 Macro: 0.8513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0869, Accuracy: 0.9287, F1 Micro: 0.8568, F1 Macro: 0.8565\n",
      "Epoch 6/10, Train Loss: 0.0631, Accuracy: 0.9298, F1 Micro: 0.8531, F1 Macro: 0.8499\n",
      "Epoch 7/10, Train Loss: 0.043, Accuracy: 0.9282, F1 Micro: 0.8527, F1 Macro: 0.8508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.033, Accuracy: 0.9303, F1 Micro: 0.8625, F1 Macro: 0.8629\n",
      "Epoch 9/10, Train Loss: 0.0249, Accuracy: 0.9294, F1 Micro: 0.857, F1 Macro: 0.8559\n",
      "Epoch 10/10, Train Loss: 0.0214, Accuracy: 0.9287, F1 Micro: 0.856, F1 Macro: 0.8551\n",
      "Model 1 - Iteration 7060: Accuracy: 0.9303, F1 Micro: 0.8625, F1 Macro: 0.8629\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.93       502\n",
      "                sara       0.81      0.86      0.83       504\n",
      "         radikalisme       0.87      0.91      0.89       481\n",
      "pencemaran_nama_baik       0.77      0.83      0.80       482\n",
      "\n",
      "           micro avg       0.85      0.88      0.86      1969\n",
      "           macro avg       0.85      0.88      0.86      1969\n",
      "        weighted avg       0.85      0.88      0.86      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 253.13816952705383 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3613, Accuracy: 0.8953, F1 Micro: 0.7847, F1 Macro: 0.7815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2291, Accuracy: 0.919, F1 Micro: 0.836, F1 Macro: 0.8366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1741, Accuracy: 0.9226, F1 Micro: 0.8489, F1 Macro: 0.8498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.131, Accuracy: 0.9269, F1 Micro: 0.8569, F1 Macro: 0.8574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0902, Accuracy: 0.9304, F1 Micro: 0.8604, F1 Macro: 0.8602\n",
      "Epoch 6/10, Train Loss: 0.0619, Accuracy: 0.9313, F1 Micro: 0.8584, F1 Macro: 0.8566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0402, Accuracy: 0.933, F1 Micro: 0.8645, F1 Macro: 0.8644\n",
      "Epoch 8/10, Train Loss: 0.0329, Accuracy: 0.9326, F1 Micro: 0.8636, F1 Macro: 0.863\n",
      "Epoch 9/10, Train Loss: 0.0224, Accuracy: 0.9315, F1 Micro: 0.8629, F1 Macro: 0.8625\n",
      "Epoch 10/10, Train Loss: 0.0207, Accuracy: 0.932, F1 Micro: 0.8637, F1 Macro: 0.8636\n",
      "Model 2 - Iteration 7060: Accuracy: 0.933, F1 Micro: 0.8645, F1 Macro: 0.8644\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.83      0.86      0.85       504\n",
      "         radikalisme       0.91      0.90      0.90       481\n",
      "pencemaran_nama_baik       0.78      0.78      0.78       482\n",
      "\n",
      "           micro avg       0.87      0.86      0.86      1969\n",
      "           macro avg       0.87      0.86      0.86      1969\n",
      "        weighted avg       0.87      0.86      0.86      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 251.99372172355652 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3717, Accuracy: 0.8931, F1 Micro: 0.7741, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2333, Accuracy: 0.9192, F1 Micro: 0.8353, F1 Macro: 0.8354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9197, F1 Micro: 0.8441, F1 Macro: 0.8467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.128, Accuracy: 0.9267, F1 Micro: 0.8524, F1 Macro: 0.8497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0897, Accuracy: 0.9283, F1 Micro: 0.8578, F1 Macro: 0.8585\n",
      "Epoch 6/10, Train Loss: 0.0612, Accuracy: 0.9278, F1 Micro: 0.8532, F1 Macro: 0.852\n",
      "Epoch 7/10, Train Loss: 0.0394, Accuracy: 0.9283, F1 Micro: 0.8515, F1 Macro: 0.8501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0327, Accuracy: 0.9292, F1 Micro: 0.8594, F1 Macro: 0.8586\n",
      "Epoch 9/10, Train Loss: 0.0222, Accuracy: 0.9294, F1 Micro: 0.855, F1 Macro: 0.8536\n",
      "Epoch 10/10, Train Loss: 0.0204, Accuracy: 0.9294, F1 Micro: 0.8572, F1 Macro: 0.8569\n",
      "Model 3 - Iteration 7060: Accuracy: 0.9292, F1 Micro: 0.8594, F1 Macro: 0.8586\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.93       502\n",
      "                sara       0.81      0.89      0.84       504\n",
      "         radikalisme       0.88      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.78      0.76      0.77       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 251.42014050483704 s\n",
      "Averaged - Iteration 7060: Accuracy: 0.9136, F1 Micro: 0.8263, F1 Macro: 0.826\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7840\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.967576742172241 seconds\n",
      "New train size: 7260\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3672, Accuracy: 0.8958, F1 Micro: 0.7942, F1 Macro: 0.7952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2344, Accuracy: 0.9173, F1 Micro: 0.8351, F1 Macro: 0.8359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1703, Accuracy: 0.9265, F1 Micro: 0.8533, F1 Macro: 0.8536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1322, Accuracy: 0.928, F1 Micro: 0.8535, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0885, Accuracy: 0.9296, F1 Micro: 0.8611, F1 Macro: 0.8612\n",
      "Epoch 6/10, Train Loss: 0.0621, Accuracy: 0.9279, F1 Micro: 0.8527, F1 Macro: 0.8515\n",
      "Epoch 7/10, Train Loss: 0.0464, Accuracy: 0.9249, F1 Micro: 0.8483, F1 Macro: 0.8482\n",
      "Epoch 8/10, Train Loss: 0.0346, Accuracy: 0.9288, F1 Micro: 0.8521, F1 Macro: 0.8509\n",
      "Epoch 9/10, Train Loss: 0.0285, Accuracy: 0.9286, F1 Micro: 0.8562, F1 Macro: 0.8565\n",
      "Epoch 10/10, Train Loss: 0.0224, Accuracy: 0.9278, F1 Micro: 0.8492, F1 Macro: 0.849\n",
      "Model 1 - Iteration 7260: Accuracy: 0.9296, F1 Micro: 0.8611, F1 Macro: 0.8612\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.93       502\n",
      "                sara       0.81      0.87      0.84       504\n",
      "         radikalisme       0.85      0.93      0.89       481\n",
      "pencemaran_nama_baik       0.77      0.80      0.78       482\n",
      "\n",
      "           micro avg       0.84      0.88      0.86      1969\n",
      "           macro avg       0.84      0.88      0.86      1969\n",
      "        weighted avg       0.85      0.88      0.86      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 256.23715806007385 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3567, Accuracy: 0.8949, F1 Micro: 0.7888, F1 Macro: 0.7887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2246, Accuracy: 0.9201, F1 Micro: 0.8409, F1 Macro: 0.8419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1647, Accuracy: 0.926, F1 Micro: 0.8484, F1 Macro: 0.8483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1307, Accuracy: 0.9302, F1 Micro: 0.8557, F1 Macro: 0.8556\n",
      "Epoch 5/10, Train Loss: 0.085, Accuracy: 0.9289, F1 Micro: 0.854, F1 Macro: 0.8525\n",
      "Epoch 6/10, Train Loss: 0.0589, Accuracy: 0.9249, F1 Micro: 0.8523, F1 Macro: 0.8536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0416, Accuracy: 0.9296, F1 Micro: 0.8563, F1 Macro: 0.8542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0323, Accuracy: 0.9303, F1 Micro: 0.8605, F1 Macro: 0.8612\n",
      "Epoch 9/10, Train Loss: 0.0239, Accuracy: 0.9301, F1 Micro: 0.86, F1 Macro: 0.86\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0222, Accuracy: 0.9323, F1 Micro: 0.863, F1 Macro: 0.8622\n",
      "Model 2 - Iteration 7260: Accuracy: 0.9323, F1 Micro: 0.863, F1 Macro: 0.8622\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.91      0.93       502\n",
      "                sara       0.84      0.85      0.84       504\n",
      "         radikalisme       0.87      0.92      0.90       481\n",
      "pencemaran_nama_baik       0.80      0.76      0.78       482\n",
      "\n",
      "           micro avg       0.87      0.86      0.86      1969\n",
      "           macro avg       0.87      0.86      0.86      1969\n",
      "        weighted avg       0.87      0.86      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 259.53735160827637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3725, Accuracy: 0.895, F1 Micro: 0.7925, F1 Macro: 0.7938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.9189, F1 Micro: 0.8375, F1 Macro: 0.8377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1652, Accuracy: 0.9243, F1 Micro: 0.8451, F1 Macro: 0.8453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.9283, F1 Micro: 0.8514, F1 Macro: 0.851\n",
      "Epoch 5/10, Train Loss: 0.0875, Accuracy: 0.9265, F1 Micro: 0.8474, F1 Macro: 0.8447\n",
      "Epoch 6/10, Train Loss: 0.0584, Accuracy: 0.9243, F1 Micro: 0.8513, F1 Macro: 0.852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0413, Accuracy: 0.9286, F1 Micro: 0.8535, F1 Macro: 0.8524\n",
      "Epoch 8/10, Train Loss: 0.0327, Accuracy: 0.9275, F1 Micro: 0.849, F1 Macro: 0.8475\n",
      "Epoch 9/10, Train Loss: 0.0269, Accuracy: 0.9253, F1 Micro: 0.8468, F1 Macro: 0.8445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0226, Accuracy: 0.9284, F1 Micro: 0.8548, F1 Macro: 0.8545\n",
      "Model 3 - Iteration 7260: Accuracy: 0.9284, F1 Micro: 0.8548, F1 Macro: 0.8545\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       502\n",
      "                sara       0.82      0.86      0.84       504\n",
      "         radikalisme       0.90      0.88      0.89       481\n",
      "pencemaran_nama_baik       0.78      0.76      0.77       482\n",
      "\n",
      "           micro avg       0.86      0.85      0.85      1969\n",
      "           macro avg       0.86      0.85      0.85      1969\n",
      "        weighted avg       0.86      0.85      0.86      1969\n",
      "         samples avg       0.68      0.67      0.67      1969\n",
      "\n",
      "Training completed in 258.40553283691406 s\n",
      "Averaged - Iteration 7260: Accuracy: 0.9143, F1 Micro: 0.8279, F1 Macro: 0.8275\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7840\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.512884616851807 seconds\n",
      "New train size: 7460\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3601, Accuracy: 0.8954, F1 Micro: 0.7951, F1 Macro: 0.7988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2238, Accuracy: 0.9136, F1 Micro: 0.8144, F1 Macro: 0.8068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1681, Accuracy: 0.9263, F1 Micro: 0.8438, F1 Macro: 0.8423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1233, Accuracy: 0.9267, F1 Micro: 0.8534, F1 Macro: 0.8531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0933, Accuracy: 0.9302, F1 Micro: 0.8599, F1 Macro: 0.8608\n",
      "Epoch 6/10, Train Loss: 0.058, Accuracy: 0.9291, F1 Micro: 0.8512, F1 Macro: 0.8484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0494, Accuracy: 0.9323, F1 Micro: 0.8647, F1 Macro: 0.8647\n",
      "Epoch 8/10, Train Loss: 0.0334, Accuracy: 0.9298, F1 Micro: 0.857, F1 Macro: 0.8563\n",
      "Epoch 9/10, Train Loss: 0.0244, Accuracy: 0.9306, F1 Micro: 0.8617, F1 Macro: 0.8628\n",
      "Epoch 10/10, Train Loss: 0.0201, Accuracy: 0.934, F1 Micro: 0.8645, F1 Macro: 0.8639\n",
      "Model 1 - Iteration 7460: Accuracy: 0.9323, F1 Micro: 0.8647, F1 Macro: 0.8647\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.91      0.93       502\n",
      "                sara       0.80      0.88      0.84       504\n",
      "         radikalisme       0.89      0.92      0.90       481\n",
      "pencemaran_nama_baik       0.79      0.77      0.78       482\n",
      "\n",
      "           micro avg       0.86      0.87      0.86      1969\n",
      "           macro avg       0.86      0.87      0.86      1969\n",
      "        weighted avg       0.86      0.87      0.87      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 263.62576508522034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3494, Accuracy: 0.8988, F1 Micro: 0.799, F1 Macro: 0.803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2139, Accuracy: 0.9123, F1 Micro: 0.8127, F1 Macro: 0.8061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.9264, F1 Micro: 0.8454, F1 Macro: 0.8439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1179, Accuracy: 0.9277, F1 Micro: 0.8521, F1 Macro: 0.8512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0888, Accuracy: 0.9294, F1 Micro: 0.8569, F1 Macro: 0.8581\n",
      "Epoch 6/10, Train Loss: 0.0572, Accuracy: 0.9283, F1 Micro: 0.8544, F1 Macro: 0.8534\n",
      "Epoch 7/10, Train Loss: 0.0457, Accuracy: 0.925, F1 Micro: 0.8537, F1 Macro: 0.8535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0295, Accuracy: 0.9301, F1 Micro: 0.8593, F1 Macro: 0.8592\n",
      "Epoch 9/10, Train Loss: 0.0244, Accuracy: 0.927, F1 Micro: 0.8541, F1 Macro: 0.854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0192, Accuracy: 0.9307, F1 Micro: 0.8613, F1 Macro: 0.8614\n",
      "Model 2 - Iteration 7460: Accuracy: 0.9307, F1 Micro: 0.8613, F1 Macro: 0.8614\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.83      0.85      0.84       504\n",
      "         radikalisme       0.88      0.92      0.90       481\n",
      "pencemaran_nama_baik       0.77      0.79      0.78       482\n",
      "\n",
      "           micro avg       0.86      0.87      0.86      1969\n",
      "           macro avg       0.86      0.87      0.86      1969\n",
      "        weighted avg       0.86      0.87      0.86      1969\n",
      "         samples avg       0.68      0.68      0.67      1969\n",
      "\n",
      "Training completed in 264.25310587882996 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3626, Accuracy: 0.8967, F1 Micro: 0.7984, F1 Macro: 0.8025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2213, Accuracy: 0.9149, F1 Micro: 0.82, F1 Macro: 0.8142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1646, Accuracy: 0.9235, F1 Micro: 0.8395, F1 Macro: 0.839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1189, Accuracy: 0.9272, F1 Micro: 0.8514, F1 Macro: 0.8498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0911, Accuracy: 0.9284, F1 Micro: 0.8519, F1 Macro: 0.8512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0561, Accuracy: 0.9297, F1 Micro: 0.8563, F1 Macro: 0.8545\n",
      "Epoch 7/10, Train Loss: 0.0454, Accuracy: 0.9268, F1 Micro: 0.8555, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0285, Accuracy: 0.9317, F1 Micro: 0.8601, F1 Macro: 0.8588\n",
      "Epoch 9/10, Train Loss: 0.0222, Accuracy: 0.9327, F1 Micro: 0.8595, F1 Macro: 0.858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0207, Accuracy: 0.9318, F1 Micro: 0.8649, F1 Macro: 0.8648\n",
      "Model 3 - Iteration 7460: Accuracy: 0.9318, F1 Micro: 0.8649, F1 Macro: 0.8648\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.83      0.88      0.86       504\n",
      "         radikalisme       0.86      0.93      0.89       481\n",
      "pencemaran_nama_baik       0.77      0.79      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.88      0.86      1969\n",
      "           macro avg       0.85      0.88      0.86      1969\n",
      "        weighted avg       0.85      0.88      0.87      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 266.74354577064514 s\n",
      "Averaged - Iteration 7460: Accuracy: 0.9151, F1 Micro: 0.8295, F1 Macro: 0.8292\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7840\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.719556331634521 seconds\n",
      "New train size: 7660\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3622, Accuracy: 0.8996, F1 Micro: 0.7866, F1 Macro: 0.7831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.22, Accuracy: 0.916, F1 Micro: 0.8345, F1 Macro: 0.8337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1594, Accuracy: 0.9216, F1 Micro: 0.8436, F1 Macro: 0.8431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1213, Accuracy: 0.9262, F1 Micro: 0.8501, F1 Macro: 0.8494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.085, Accuracy: 0.9268, F1 Micro: 0.8521, F1 Macro: 0.8507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0589, Accuracy: 0.9298, F1 Micro: 0.8597, F1 Macro: 0.8586\n",
      "Epoch 7/10, Train Loss: 0.0414, Accuracy: 0.9299, F1 Micro: 0.8585, F1 Macro: 0.8575\n",
      "Epoch 8/10, Train Loss: 0.0287, Accuracy: 0.9292, F1 Micro: 0.8588, F1 Macro: 0.8597\n",
      "Epoch 9/10, Train Loss: 0.0251, Accuracy: 0.9284, F1 Micro: 0.8593, F1 Macro: 0.8599\n",
      "Epoch 10/10, Train Loss: 0.0177, Accuracy: 0.9274, F1 Micro: 0.8516, F1 Macro: 0.8495\n",
      "Model 1 - Iteration 7660: Accuracy: 0.9298, F1 Micro: 0.8597, F1 Macro: 0.8586\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       502\n",
      "                sara       0.79      0.88      0.83       504\n",
      "         radikalisme       0.88      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.81      0.74      0.77       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.67      1969\n",
      "\n",
      "Training completed in 269.93665385246277 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3497, Accuracy: 0.902, F1 Micro: 0.7942, F1 Macro: 0.7928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2114, Accuracy: 0.9205, F1 Micro: 0.8369, F1 Macro: 0.8332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.153, Accuracy: 0.9253, F1 Micro: 0.8516, F1 Macro: 0.8521\n",
      "Epoch 4/10, Train Loss: 0.1188, Accuracy: 0.924, F1 Micro: 0.8471, F1 Macro: 0.8455\n",
      "Epoch 5/10, Train Loss: 0.0837, Accuracy: 0.927, F1 Micro: 0.8492, F1 Macro: 0.8456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0577, Accuracy: 0.9302, F1 Micro: 0.8613, F1 Macro: 0.8619\n",
      "Epoch 7/10, Train Loss: 0.0395, Accuracy: 0.928, F1 Micro: 0.8562, F1 Macro: 0.8557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0305, Accuracy: 0.9318, F1 Micro: 0.8631, F1 Macro: 0.8639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0241, Accuracy: 0.932, F1 Micro: 0.8661, F1 Macro: 0.866\n",
      "Epoch 10/10, Train Loss: 0.0203, Accuracy: 0.9316, F1 Micro: 0.8597, F1 Macro: 0.8584\n",
      "Model 2 - Iteration 7660: Accuracy: 0.932, F1 Micro: 0.8661, F1 Macro: 0.866\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.94       502\n",
      "                sara       0.80      0.89      0.85       504\n",
      "         radikalisme       0.87      0.93      0.90       481\n",
      "pencemaran_nama_baik       0.78      0.79      0.78       482\n",
      "\n",
      "           micro avg       0.85      0.89      0.87      1969\n",
      "           macro avg       0.85      0.89      0.87      1969\n",
      "        weighted avg       0.85      0.89      0.87      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 268.74343061447144 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3605, Accuracy: 0.8983, F1 Micro: 0.7812, F1 Macro: 0.7786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2186, Accuracy: 0.9182, F1 Micro: 0.8358, F1 Macro: 0.8345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1558, Accuracy: 0.9235, F1 Micro: 0.8464, F1 Macro: 0.8451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.121, Accuracy: 0.9258, F1 Micro: 0.8501, F1 Macro: 0.85\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0833, Accuracy: 0.9306, F1 Micro: 0.8591, F1 Macro: 0.8585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0569, Accuracy: 0.928, F1 Micro: 0.8593, F1 Macro: 0.86\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0428, Accuracy: 0.9306, F1 Micro: 0.86, F1 Macro: 0.8588\n",
      "Epoch 8/10, Train Loss: 0.0301, Accuracy: 0.9298, F1 Micro: 0.8594, F1 Macro: 0.8595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0216, Accuracy: 0.9311, F1 Micro: 0.8611, F1 Macro: 0.8595\n",
      "Epoch 10/10, Train Loss: 0.0183, Accuracy: 0.9283, F1 Micro: 0.8576, F1 Macro: 0.8573\n",
      "Model 3 - Iteration 7660: Accuracy: 0.9311, F1 Micro: 0.8611, F1 Macro: 0.8595\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.84      0.87      0.85       504\n",
      "         radikalisme       0.84      0.93      0.88       481\n",
      "pencemaran_nama_baik       0.81      0.73      0.77       482\n",
      "\n",
      "           micro avg       0.86      0.86      0.86      1969\n",
      "           macro avg       0.86      0.86      0.86      1969\n",
      "        weighted avg       0.86      0.86      0.86      1969\n",
      "         samples avg       0.69      0.67      0.67      1969\n",
      "\n",
      "Training completed in 272.67596983909607 s\n",
      "Averaged - Iteration 7660: Accuracy: 0.9158, F1 Micro: 0.831, F1 Macro: 0.8306\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7840\n",
      "Acquired samples: 180\n",
      "Sampling duration: 3.9734084606170654 seconds\n",
      "New train size: 7840\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3475, Accuracy: 0.9006, F1 Micro: 0.7973, F1 Macro: 0.7975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2123, Accuracy: 0.9183, F1 Micro: 0.8356, F1 Macro: 0.8349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.9267, F1 Micro: 0.8537, F1 Macro: 0.8537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.117, Accuracy: 0.9265, F1 Micro: 0.8544, F1 Macro: 0.8543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.081, Accuracy: 0.9301, F1 Micro: 0.8567, F1 Macro: 0.8542\n",
      "Epoch 6/10, Train Loss: 0.0548, Accuracy: 0.9265, F1 Micro: 0.8559, F1 Macro: 0.8564\n",
      "Epoch 7/10, Train Loss: 0.0381, Accuracy: 0.925, F1 Micro: 0.8493, F1 Macro: 0.8473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0291, Accuracy: 0.9297, F1 Micro: 0.8589, F1 Macro: 0.8599\n",
      "Epoch 9/10, Train Loss: 0.023, Accuracy: 0.9299, F1 Micro: 0.8577, F1 Macro: 0.8577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0199, Accuracy: 0.9297, F1 Micro: 0.8594, F1 Macro: 0.8592\n",
      "Model 1 - Iteration 7840: Accuracy: 0.9297, F1 Micro: 0.8594, F1 Macro: 0.8592\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       502\n",
      "                sara       0.81      0.86      0.83       504\n",
      "         radikalisme       0.86      0.91      0.88       481\n",
      "pencemaran_nama_baik       0.79      0.78      0.79       482\n",
      "\n",
      "           micro avg       0.85      0.87      0.86      1969\n",
      "           macro avg       0.85      0.87      0.86      1969\n",
      "        weighted avg       0.85      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.67      1969\n",
      "\n",
      "Training completed in 276.033798456192 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3363, Accuracy: 0.9007, F1 Micro: 0.7918, F1 Macro: 0.7911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2088, Accuracy: 0.9175, F1 Micro: 0.8382, F1 Macro: 0.8381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.158, Accuracy: 0.9268, F1 Micro: 0.8541, F1 Macro: 0.8547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1152, Accuracy: 0.9287, F1 Micro: 0.8613, F1 Macro: 0.8628\n",
      "Epoch 5/10, Train Loss: 0.0789, Accuracy: 0.926, F1 Micro: 0.8501, F1 Macro: 0.8486\n",
      "Epoch 6/10, Train Loss: 0.0553, Accuracy: 0.9259, F1 Micro: 0.8526, F1 Macro: 0.8529\n",
      "Epoch 7/10, Train Loss: 0.0351, Accuracy: 0.9279, F1 Micro: 0.8527, F1 Macro: 0.8501\n",
      "Epoch 8/10, Train Loss: 0.0269, Accuracy: 0.9299, F1 Micro: 0.8574, F1 Macro: 0.8563\n",
      "Epoch 9/10, Train Loss: 0.023, Accuracy: 0.9275, F1 Micro: 0.8564, F1 Macro: 0.8567\n",
      "Epoch 10/10, Train Loss: 0.0192, Accuracy: 0.9286, F1 Micro: 0.8583, F1 Macro: 0.8587\n",
      "Model 2 - Iteration 7840: Accuracy: 0.9287, F1 Micro: 0.8613, F1 Macro: 0.8628\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       502\n",
      "                sara       0.76      0.91      0.83       504\n",
      "         radikalisme       0.87      0.93      0.90       481\n",
      "pencemaran_nama_baik       0.76      0.83      0.79       482\n",
      "\n",
      "           micro avg       0.83      0.89      0.86      1969\n",
      "           macro avg       0.84      0.89      0.86      1969\n",
      "        weighted avg       0.84      0.89      0.86      1969\n",
      "         samples avg       0.69      0.69      0.68      1969\n",
      "\n",
      "Training completed in 271.2167160511017 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3494, Accuracy: 0.8978, F1 Micro: 0.7834, F1 Macro: 0.7796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2129, Accuracy: 0.9195, F1 Micro: 0.8408, F1 Macro: 0.8414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.161, Accuracy: 0.9258, F1 Micro: 0.8526, F1 Macro: 0.8531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1152, Accuracy: 0.9282, F1 Micro: 0.8561, F1 Macro: 0.8557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0815, Accuracy: 0.9306, F1 Micro: 0.8593, F1 Macro: 0.8573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.054, Accuracy: 0.932, F1 Micro: 0.8638, F1 Macro: 0.8641\n",
      "Epoch 7/10, Train Loss: 0.0342, Accuracy: 0.9286, F1 Micro: 0.8546, F1 Macro: 0.8524\n",
      "Epoch 8/10, Train Loss: 0.0272, Accuracy: 0.9296, F1 Micro: 0.8601, F1 Macro: 0.8612\n",
      "Epoch 9/10, Train Loss: 0.023, Accuracy: 0.9308, F1 Micro: 0.8581, F1 Macro: 0.8561\n",
      "Epoch 10/10, Train Loss: 0.0189, Accuracy: 0.9308, F1 Micro: 0.858, F1 Macro: 0.8575\n",
      "Model 3 - Iteration 7840: Accuracy: 0.932, F1 Micro: 0.8638, F1 Macro: 0.8641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.91      0.93       502\n",
      "                sara       0.82      0.87      0.85       504\n",
      "         radikalisme       0.89      0.91      0.90       481\n",
      "pencemaran_nama_baik       0.76      0.78      0.77       482\n",
      "\n",
      "           micro avg       0.86      0.87      0.86      1969\n",
      "           macro avg       0.86      0.87      0.86      1969\n",
      "        weighted avg       0.86      0.87      0.86      1969\n",
      "         samples avg       0.69      0.68      0.68      1969\n",
      "\n",
      "Training completed in 275.0662488937378 s\n",
      "Averaged - Iteration 7840: Accuracy: 0.9164, F1 Micro: 0.8322, F1 Macro: 0.8319\n",
      "Total sampling time: 1276.75 seconds\n",
      "Total runtime: 15765.032037258148 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACCoAAAHqCAYAAADInz0eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD+AklEQVR4nOzdeZzVY//H8dfMNFvbkDbVtCiKpNIySpZ+IiJCpGhBEmUpW3vWhhvJkn2/lVIpe5aoZIvCLS20UKIUmqmpWc/5/XGYjELL1LeZeT0fj/OYc13n+n7P53I/7ly+vee6osLhcBhJkiRJkiRJkiRJkqS9IDroAiRJkiRJkiRJkiRJUslhUEGSJEmSJEmSJEmSJO01BhUkSZIkSZIkSZIkSdJeY1BBkiRJkiRJkiRJkiTtNQYVJEmSJEmSJEmSJEnSXmNQQZIkSZIkSZIkSZIk7TUGFSRJkiRJkiRJkiRJ0l5jUEGSJEmSJEmSJEmSJO01BhUkSZIkSZIkSZIkSdJeY1BBkiRJkiTt03r16kXt2rWDLkOSJEmSJBUSgwqStIsefPBBoqKiSElJCboUSZIkabc8/fTTREVFbfc1aNCg/HFvvfUWF198MYcffjgxMTE7HR744569e/fe7udDhw7NH7N+/frdmZIkSZJKENezklT0lAq6AEkqqsaNG0ft2rWZO3cuS5cupV69ekGXJEmSJO2Wm2++mTp16hToO/zww/Pfjx8/nokTJ3LkkUdSrVq1XfqOhIQEpkyZwoMPPkhcXFyBz55//nkSEhLIzMws0P/YY48RCoV26fskSZJUcuyr61lJ0rbcUUGSdsGKFSv48MMPGT16NJUqVWLcuHFBl7RdGRkZQZcgSZKkIuSUU07hggsuKPBq0qRJ/uejRo0iPT2dDz74gMaNG+/Sd5x88smkp6fzxhtvFOj/8MMPWbFiBaeeeuo218TGxhIfH79L3/dnoVDIh8aSJEnF2L66nt3TfA4sqSgyqCBJu2DcuHHsv//+nHrqqXTu3Hm7QYUNGzYwYMAAateuTXx8PDVq1KBHjx4FtvzKzMzkxhtv5JBDDiEhIYEDDzyQs846i2XLlgEwc+ZMoqKimDlzZoF7f/fdd0RFRfH000/n9/Xq1YuyZcuybNkyOnToQLly5Tj//PMBeP/99znnnHOoWbMm8fHxJCcnM2DAALZs2bJN3YsXL+bcc8+lUqVKJCYmUr9+fYYOHQrAe++9R1RUFFOnTt3muvHjxxMVFcVHH3200/88JUmSVDRUq1aN2NjY3bpH9erVOfbYYxk/fnyB/nHjxtGoUaMCv/H2h169em2zLW8oFOLee++lUaNGJCQkUKlSJU4++WQ+++yz/DFRUVH079+fcePG0bBhQ+Lj45k+fToAn3/+Oaeccgrly5enbNmynHDCCXz88ce7NTdJkiTt24JazxbW81mAG2+8kaioKBYuXEi3bt3Yf//9adOmDQC5ubnccsst1K1bl/j4eGrXrs2QIUPIysrarTlL0p7g0Q+StAvGjRvHWWedRVxcHF27duWhhx7i008/pUWLFgBs2rSJY445hkWLFnHRRRdx5JFHsn79el5++WV++OEHKlasSF5eHqeddhozZszgvPPO46qrrmLjxo28/fbbLFiwgLp16+50Xbm5ubRv3542bdpw1113Ubp0aQAmTZrE5s2bueyyyzjggAOYO3cu999/Pz/88AOTJk3Kv/5///sfxxxzDLGxsfTp04fatWuzbNkyXnnlFW677TaOP/54kpOTGTduHGeeeeY2/0zq1q1Lq1atduOfrCRJkoKUlpa2zVm6FStWLPTv6datG1dddRWbNm2ibNmy5ObmMmnSJAYOHLjDOx5cfPHFPP3005xyyin07t2b3Nxc3n//fT7++GOaN2+eP+7dd9/lhRdeoH///lSsWJHatWvz9ddfc8wxx1C+fHmuv/56YmNjeeSRRzj++OOZNWsWKSkphT5nSZIk7Xn76nq2sJ7P/tk555zDwQcfzKhRowiHwwD07t2bZ555hs6dO3PNNdfwySefkJqayqJFi7b7y2eSFCSDCpK0k+bNm8fixYu5//77AWjTpg01atRg3Lhx+UGFO++8kwULFvDiiy8W+Av9YcOG5S8an332WWbMmMHo0aMZMGBA/phBgwblj9lZWVlZnHPOOaSmphbov+OOO0hMTMxv9+nTh3r16jFkyBBWrlxJzZo1AbjiiisIh8PMnz8/vw/g9ttvByK/kXbBBRcwevRo0tLSSEpKAmDdunW89dZbBZK9kiRJKnratWu3Td+urk3/SefOnenfvz/Tpk3jggsu4K233mL9+vV07dqVp5566l+vf++993j66ae58soruffee/P7r7nmmm3qXbJkCV999RWHHXZYft+ZZ55JTk4Oc+bM4aCDDgKgR48e1K9fn+uvv55Zs2YV0kwlSZK0N+2r69nCej77Z40bNy6wq8OXX37JM888Q+/evXnssccAuPzyy6lcuTJ33XUX7733Hm3bti20fwaStLs8+kGSdtK4ceOoUqVK/qIuKiqKLl26MGHCBPLy8gCYMmUKjRs33mbXgT/G/zGmYsWKXHHFFX87Zldcdtll2/T9eRGckZHB+vXrad26NeFwmM8//xyIhA1mz57NRRddVGAR/Nd6evToQVZWFpMnT87vmzhxIrm5uVxwwQW7XLckSZKCN3bsWN5+++0Crz1h//335+STT+b5558HIseItW7dmlq1au3Q9VOmTCEqKoqRI0du89lf19LHHXdcgZBCXl4eb731Fp06dcoPKQAceOCBdOvWjTlz5pCenr4r05IkSVLA9tX1bGE+n/1D3759C7Rff/11AAYOHFig/5prrgHgtdde25kpStIe544KkrQT8vLymDBhAm3btmXFihX5/SkpKdx9993MmDGDk046iWXLlnH22Wf/472WLVtG/fr1KVWq8P4oLlWqFDVq1Nimf+XKlYwYMYKXX36Z3377rcBnaWlpACxfvhxgu2eo/VmDBg1o0aIF48aN4+KLLwYi4Y2jjjqKevXqFcY0JEmSFJCWLVsWODZhT+rWrRvdu3dn5cqVTJs2jf/85z87fO2yZcuoVq0aFSpU+NexderUKdBet24dmzdvpn79+tuMPfTQQwmFQqxatYqGDRvucD2SJEnaN+yr69nCfD77h7+uc7///nuio6O3eUZbtWpV9ttvP77//vsduq8k7S0GFSRpJ7z77rv89NNPTJgwgQkTJmzz+bhx4zjppJMK7fv+bmeFP3Zu+Kv4+Hiio6O3GXviiSfy66+/csMNN9CgQQPKlCnD6tWr6dWrF6FQaKfr6tGjB1dddRU//PADWVlZfPzxxzzwwAM7fR9JkiSVXKeffjrx8fH07NmTrKwszj333D3yPX/+7TVJkiSpsOzoenZPPJ+Fv1/n7s5uvZK0NxlUkKSdMG7cOCpXrszYsWO3+ezFF19k6tSpPPzww9StW5cFCxb8473q1q3LJ598Qk5ODrGxsdsds//++wOwYcOGAv07k3796quv+Oabb3jmmWfo0aNHfv9ftz37Y9vbf6sb4LzzzmPgwIE8//zzbNmyhdjYWLp06bLDNUmSJEmJiYl06tSJ5557jlNOOYWKFSvu8LV169blzTff5Ndff92hXRX+rFKlSpQuXZolS5Zs89nixYuJjo4mOTl5p+4pSZKkkmdH17N74vns9tSqVYtQKMS3337LoYcemt+/du1aNmzYsMPHrEnS3hL970MkSQBbtmzhxRdf5LTTTqNz587bvPr378/GjRt5+eWXOfvss/nyyy+ZOnXqNvcJh8MAnH322axfv367OxH8MaZWrVrExMQwe/bsAp8/+OCDO1x3TExMgXv+8f7ee+8tMK5SpUoce+yxPPnkk6xcuXK79fyhYsWKnHLKKTz33HOMGzeOk08+eaceLEuSJEkA1157LSNHjmT48OE7dd3ZZ59NOBzmpptu2uazv65d/yomJoaTTjqJl156ie+++y6/f+3atYwfP542bdpQvnz5napHkiRJJdOOrGf3xPPZ7enQoQMAY8aMKdA/evRoAE499dR/vYck7U3uqCBJO+jll19m48aNnH766dv9/KijjqJSpUqMGzeO8ePHM3nyZM455xwuuugimjVrxq+//srLL7/Mww8/TOPGjenRowfPPvssAwcOZO7cuRxzzDFkZGTwzjvvcPnll3PGGWeQlJTEOeecw/33309UVBR169bl1Vdf5eeff97huhs0aEDdunW59tprWb16NeXLl2fKlCnbnIUGcN9999GmTRuOPPJI+vTpQ506dfjuu+947bXX+OKLLwqM7dGjB507dwbglltu2fF/kJIkSSqy/ve///Hyyy8DsHTpUtLS0rj11lsBaNy4MR07dtyp+zVu3JjGjRvvdB1t27ale/fu3HfffXz77becfPLJhEIh3n//fdq2bUv//v3/8fpbb72Vt99+mzZt2nD55ZdTqlQpHnnkEbKysv7xbGFJkiQVbUGsZ/fU89nt1dKzZ08effRRNmzYwHHHHcfcuXN55pln6NSpE23btt2puUnSnmZQQZJ20Lhx40hISODEE0/c7ufR0dGceuqpjBs3jqysLN5//31GjhzJ1KlTeeaZZ6hcuTInnHACNWrUACJJ2tdff53bbruN8ePHM2XKFA444ADatGlDo0aN8u97//33k5OTw8MPP0x8fDznnnsud955J4cffvgO1R0bG8srr7zClVdeSWpqKgkJCZx55pn0799/m0V048aN+fjjjxk+fDgPPfQQmZmZ1KpVa7vnq3Xs2JH999+fUCj0t+ENSZIkFS/z58/f5rfF/mj37Nlzpx/s7o6nnnqKI444gieeeILrrruOpKQkmjdvTuvWrf/12oYNG/L+++8zePBgUlNTCYVCpKSk8Nxzz5GSkrIXqpckSVIQgljP7qnns9vz+OOPc9BBB/H0008zdepUqlatyuDBgxk5cmShz0uSdldUeEf2i5Ek6S9yc3OpVq0aHTt25Iknngi6HEmSJEmSJEmSJBUR0UEXIEkqmqZNm8a6devo0aNH0KVIkiRJkiRJkiSpCHFHBUnSTvnkk0/43//+xy233ELFihWZP39+0CVJkiRJkiRJkiSpCHFHBUnSTnnooYe47LLLqFy5Ms8++2zQ5UiSJEmSJEmSJKmIcUcFSZIkSZIkSZIkSZK017ijgiRJkiRJkiRJkiRJ2msMKkiSJEmSJEmSJEmSpL2mVNAFFJZQKMSPP/5IuXLliIqKCrocSZIk7UHhcJiNGzdSrVo1oqOLX/bWta0kSVLJ4dpWkiRJxcXOrG2LTVDhxx9/JDk5OegyJEmStBetWrWKGjVqBF1GoXNtK0mSVPK4tpUkSVJxsSNr22ITVChXrhwQmXT58uUDrkaSJEl7Unp6OsnJyflrwOLGta0kSVLJ4dpWkiRJxcXOrG2LTVDhj23Dypcv74JXkiSphCiuW8e6tpUkSSp5XNtKkiSpuNiRtW3xO/RMkiRJkiRJkiRJkiTtswwqSJIkSZIkSZIkSZKkvcaggiRJkiRJkiRJkiRJ2msMKkiSJEmSJEmSJEmSpL3GoIIkSZIkSZIkSZIkSdprDCpIkiRJkiRJkiRJkqS9xqCCJEmSJEmSJEmSJEnaawwqSJIkSZIkSZIkSZKkvcaggiRJkiRJkiRJkiRJ2msMKkiSJEmSJEmSJEmSpL3GoIIkSZIkSZIkSZIkSdprDCpIkiRJkiRJkiRJkqS9xqCCJEmSJEmSJEmSJEnaawwqSJIkSZIkSZIkSZKkvcaggiRJUjEQCsHHH8PixRAOB12NJEmStBvCYVj/Mfz2JYRyg65GkiRJ2mXhcJh5P87juw3fBV3KPqdU0AVIkiRp93z5JVx+OXz4YaRdtSr83/9tfdWpE2x9kiRJ0g7b9B3MvQTWvBNpxyRChWZwQMutrzK1ISoqyColSZKkf/Xdhu+45JVLeGd5ZG3bvFpzzjnsHDof1pmD9j8o4OqCFxUOF4/fuUtPTycpKYm0tDTKly8fdDmSJEl7XHo6jBwJ998PeXmQmBj55bPMzILjateGtm0joYW2baF69UDKLVTFfe1X3OcnSZK0jXAIlj4Cn18PuZsgJgGi4yAnfdux8RULBhcqtICEinu/5kJS3Nd+xX1+kiRJfxUKh3h03qNc9/Z1bMreRFxMHLmhXELhUP6YIw88Mj+0UK9CvQCrLVw7s/YzqCBJklTEhMMwcSIMHAg//RTpO+ccGD0aKlaMHAHx3nvw7ruR97l/2S23fv2toYXjj4dKlfb6FHZbcV/7Fff5SZIkFbBpOXx8Mfw8M9KudAykPAHl6kL6N/DL3K2vDV9AKGfbe5Q9qGB4Yf+mUKr03pzFLivua7/iPj9JkqQ/W/HbCnq/0pt3V7wLQJuabXjy9CcpH1+eqYunMnnhZN777r0CoYUmVZvQ+dDOnNPwHA454JCgSi8UBhVc8EqSpGJq8WLo1y8SQgCoVw/GjoWTTtr++E2b4IMPIuPffRfmz4dQqOCYI44oGFwoCkup4r72K+7zkyRJ+7hwGNa+B4SgcluIjtlD3xOCb8bCF4MgbzPElIYmt8Mh/SAqevvX5GXBb19uDS78OhfSl2w7LioGyh0CZetGAg9lf3+Vqxc5OiImbs/MaRcU97VfcZ+fJEnat4XCId5Z/g4xUTEcX/t4YvbQ2jYUDvHwZw9z/dvXk5GTQWKpRG5vdzv9W/Yn+i9r23UZ65i2eBqTFk7i3RXvkhfOy/+sUeVGnHPYOZzT8BwaVGywUzVszNrIqvRVrEpblf8zIyeDu066q1DmuCMMKrjglSRJAfj6a/j1V0hJgbhCfu6ZkQG33QZ33QU5OZCQAEOGwHXXRd7vqA0bYPbsrcGFr74q+HlCApx5JvTqBSecADF76Jn07irua7/iPj9JkrQP+3UezLsK1n0QaZeuAQddBAddCGVrF973bFwKH18E696PtCsfD0c9EdkZYWdlb4BfP9saXlj/CWSu+fvxUdFQOvlP4YW6Bd/H7t31V3Ff+xX3+UmSpH1TKBzixUUvctOsm1jw8wIAkssnc1HTi7io6UXUTKpZaN+1/LflXPzyxcz8biYAx9Y6lidOf2KHjnT4ZfMvTFs8jcmLJvPO8nfIDW3dHrdhpYYFjodYvXE1q9JWsTJtZYFAwh/tDZkbtrl/bHQsmcMytwlL7CkGFVzwSpKkvSg7GwYNgnvuibTLlInsTnDSSZHXIYdAVNSu3TschpdfhiuvhJUrI32nngr33QcH7cIz3L/6+WeYOTNyVMQ778DSpVs/q14devSAnj0jx0XsS4r72q+4z0+SJO2DtqyF/w2FZU8C4cjuBjEJkP3r7wOioOqJUK83VD8dYuJ37XtCefDNffDlUMjbAqXKQNM7od6lf7+Lws4Kh2HLakhbBJuWRUIRm5b9/n5ZZPeGfxJfCU5fDrFlC6eef1GYa7+xY8dy5513smbNGho3bsz9999Py5Yt/3b8mDFjeOihh1i5ciUVK1akc+fOpKamkvCnNPTq1au54YYbeOONN9i8eTP16tXjqaeeonnz5nt9fpIkSf8mHA4zbfE0bpx1I/9b+z8AkuKTiImO4dctkbVtFFG0r9eeS468hI6HdCQ2JnaXvisUDvHgpw9ywzs3sDlnM6VjS3NHuzu4vMXluxQM+HXLr7y0+CUmL5rM28veJmd7R579i6T4JGom1SQ5KZma5SM/r2l1DfGldnH9vpP2eFBhZxa8OTk5pKam8swzz7B69Wrq16/PHXfcwcknn5w/5qGHHuKhhx7iu+++A6Bhw4aMGDGCU045ZYdrcsErSZKCsHw5nHcefPpppF2hQmRXhT+rWXNraOGEEyJjdvTeV14Jr7229T733Qenn77rwYd/Eg7DvHnw9NMwfjz89tvWz1q1iuyycO65sN9+hf/dO8uHuZIkSYUkLwuW3AcLboHcjZG+2udHjmCIrwirpsGyx2HtjK3XxFeEOj2g7sWQdNiOf1f6ksguCus/jLSrnAApjxfuTg3/JhyGzLUFAwwbl20NMmSth7j9ofOv/36vQlJYa7+JEyfSo0cPHn74YVJSUhgzZgyTJk1iyZIlVK5ceZvx48eP56KLLuLJJ5+kdevWfPPNN/Tq1YvzzjuP0aNHA/Dbb7/RtGlT2rZty2WXXUalSpX49ttvqVu3LnXr1t2r85MkSfon4XCYV755hZEzR/LFmi8AKB9fnqtTrmZAqwEklEpg6qKpPP7547y74t386yqXqUyvxr3ofWRvDj7g4B3+vmW/LuOily9i9vezATi+9vE8cfoTHLR/Ifx2GfDblt945ZtXmLRwEm8te4vsvGziY+LzQwjJ5ZMj78snR0IJv78vF1+uUL5/V+3RoMLOLnhvuOEGnnvuOR577DEaNGjAm2++ycCBA/nwww9p2rQpAK+88goxMTEcfPDBhMNhnnnmGe68804+//xzGjZsWOiTliRJKgyTJ8PFF0N6Ouy/f+Qv+E87Df73P3jrrcjr/fcjOy78ISoKmjffGlw46qhtj4nIyoL//AdGjYLMTIiNhWuugWHDIrs17A1ZWfDKK5E5TZ8Oeb8fk7avHA3hw1xJkqTdFA7D6ldh/kDY9Pu2WhWaQ7N7oVLrbcdvWh7ZbWH5U7Dlx639FVtD3d5Q69zI7gjbE8qDxaPhqxGQlwmlysGRd0HdS/ZMAnd3ZKfBlp8gaefOA94dhbX2S0lJoUWLFjzwwAMAhEIhkpOTueKKKxg0aNA24/v378+iRYuYMWNrCOWaa67hk08+Yc6cOQAMGjSIDz74gPfff3+X63JtK0mS9qRwOMxr377GjTNvZN5P8wAoG1eWq1KuYmCrgVRI3Pa3xpb+upQn5j/BU188xdqMtfn9x9U6jkuOvISzDj2LxNjE7X5fKBzigbkPMOidQWzJ3UKZ2DL858T/0Ld53z12vMKm7E1sydlCxdIVidrX1s9/sUeDCju74K1WrRpDhw6lX79++X1nn302iYmJPPfcc3/7PRUqVODOO+/k4osv3qG6XPBKkqS9JTMTBg6Ehx6KtFu3huefj+x48FebN8Ps2VuDC19/XfDzsmULHhPx3XfQvz98+23k8//7Pxg7Fhrsveek2/jpJxg3LhJa+HP9QR4N4cNcSZKk3ZC2EOYNgDVvRdoJVSM7KNTp/u/HL4Ry4afpkV0WVr8K4d8TraXKQe1ukdBChWZbAwhpCyO7KPzySaR9YHto+SiUKbwzgYu6wlj7ZWdnU7p0aSZPnkynTp3y+3v27MmGDRt46aWXtrlm/PjxXH755bz11lu0bNmS5cuXc+qpp9K9e3eGDBkCwGGHHUb79u354YcfmDVrFtWrV+fyyy/nkksu+dtasrKyyMrKKjC/5ORk17aSJKlQhcNhpi+dzsiZI/n0x8h2t2Viy3BFyyu4pvU1VCxd8V/vkZOXw2vfvsbj8x/njaVvEAqHANg/YX8uOOICeh/ZmyOqHJE//ttfvuXily/m/ZWR535ta7flidOfoM7+dfbADIumnVnb7lSsIzs7m3nz5tGuXbutN4iOpl27dnz00UfbvSYrK6vANrgAiYmJ+Q9y/yovL48JEyaQkZFBq1atdqY8SZKkPW7JksguCH+EFAYNgpkztx9SAChdGk4+GUaPhgUL4Icf4KmnoGtXqFgRNm2K7FxwxRWRv+xv3z4SUqhaNXL8wjvvBBtSADjwQLj2Wvjqq8gRF/36RXaQWL0aUlMj9bVuHWkXJbuytm3dujXz5s1j7ty5ACxfvpzXX3+dDh065I95+eWXad68Oeeccw6VK1emadOmPPbYY/9YS1ZWFunp6QVekiRJhS7rV/jsSnj9iEhIIToODhsEHb+Bg3r+e0gBILoUVD8Njp0GnVZB41QoWzdybMTSR+DNFvBGU1jyAHydGnn/yycQWx5SnoDj3zCksAesX7+evLw8qlSpUqC/SpUqrFmzZrvXdOvWjZtvvpk2bdoQGxtL3bp1Of744/NDChBZ7z700EMcfPDBvPnmm1x22WVceeWVPPPMM39bS2pqKklJSfmv5OTkwpmkJEkSkYDCW8veovWTrekwvgOf/vgppWNLc33r61lx1QpS26XuUEgBIDYmlk4NOvFqt1f57qrvuOn4m6iZVJPfMn/j/rn30/jhxqQ8nsJj8x7jno/uofHDjXl/5fuUjSvLQ6c+xDs93jGksBtK7czgf1rwLl68eLvXtG/fntGjR3PsscdSt25dZsyYwYsvvkjeH/sH/+6rr76iVatWZGZmUrZsWaZOncphh/39GXfbS+ZKkiTtSc89B337QkYGVKoE//1vJFiwM6pXjxyb0KsXhELwxRfw9tuR3RbmzIHc3Eho4aabIClpD0xiN/xxbEXz5nD33fDqq5FdFt54A77/PhKuKEp2ZW3brVs31q9fT5s2bQiHw+Tm5tK3b9/tPswdOHAgQ4YM4dNPP+XKK68kLi6Onj17bve+qamp3HTTTYU3OUmSpD8L5cLSR+F/wyH710hfjU7Q9C4ot2NHU21X4oHQcBAcdj38PAuWPg6rpsCGL2HeFVvHVesALR+B0jV2axoqXDNnzmTUqFE8+OCDpKSksHTpUq666ipuueUWhg8fDkR2HGvevDmjRo0CoGnTpixYsICHH374b9e2gwcPZuDAgfntP3ZUkCRJ2h3hcJh3V7zLiJkj+HDVhwAklkrk8haXc/3R11O5zLbHuO6M5KRkRhw3gqHHDOWd5e/w+OePM23xNOaunsvc1XPzx51Q5wQeP/1xau9Xe7e+TzsZVNgV9957L5dccgkNGjQgKiqKunXrcuGFF/Lkk08WGFe/fn2++OIL0tLSmDx5Mj179mTWrFl/G1bwYa4kSdpbMjIi4YGnnoq027aNhBaqVdu9+0ZHw5FHRl433BD5nrw8KAq7ocbHw9lnR15r1sA330BMTNBV7Xk+zJUkSUXOmhkw72pIWxBpJx0OzcZA1RMK7zuioqFK28gr6374blzkaIgtP0XCEHW6bz0KQntExYoViYmJYe3atQX6165dS9W/SRQPHz6c7t2707t3bwAaNWpERkYGffr0YejQoURHR3PggQdu83z20EMPZcqUKX9bS3x8PPHx8bs5I0mSpEg44ZtfvmHW97MY99U4Zn8/G4CEUgn0bdaXG9rcQNWyhfvbUzHRMbSv15729drzc8bPPPvlszw+/3HWbFrDf078D5cceQlRrm0LxU4FFXZlwVupUiWmTZtGZmYmv/zyC9WqVWPQoEEcdNBBBcbFxcVRr149AJo1a8ann37KvffeyyOPPLLd+/owV5Ik7Q0LFsC558KiRZFgwciRMHTonvlL+TJlCv+ee0PVqkVvNwXwYa4kSSqmwmHYvBJ+nQ8rnoUfpkX64yrAEbdAvT6R4xv2lPgKUP+KyCscNqCwl8TFxdGsWTNmzJhBp06dgEiAdsaMGfTv33+712zevJno6ILHfcT8/h864XAYgKOPPpolS5YUGPPNN99Qq1atQp6BJEkShMIhFq5byKzvZjHr+1nM/n42azO2PruLj4mnT7M+DGoziGrldvO3yHZA5TKVubb1tVzb+lryQnnERJeA39Tai3bqv0p2ZcH7h4SEBKpXr05OTg5Tpkzh3HPP/cfxoVCowNEOf+XDXEmStCeFw/DEE5GdFDIz4cADYfx4OP74oCtTYfFhriRJKvLCYchYAb/OiwQTfp0Hv82HrF+2jomKgYMvh0Y3RkIEe5Mhhb1q4MCB9OzZk+bNm9OyZUvGjBlDRkYGF154IQA9evSgevXqpKamAtCxY0dGjx5N06ZN83cLGz58OB07dsxf4w4YMIDWrVszatQozj33XObOncujjz7Ko48+Gtg8JUlS8ZEXyuN/a//HrO8jwYT3v3+fX7b8UmBMfEw8R9U4iuNrH0/vI3tTo3wwx4kZUih8Ox2f3tkF7yeffMLq1atp0qQJq1ev5sYbbyQUCnH99dfn33Pw4MGccsop1KxZk40bNzJ+/HhmzpzJm2++WUjTlCRJ2nEbN8Kll8Lzz0faJ58MzzwDlXfvmDPtg3yYK0mSioxwCDYu2xpG+COckLNh27HRsZEjHg5oAYdcCfs13Ovlau/r0qUL69atY8SIEaxZs4YmTZowffp0qlSpAsDKlSsLhG6HDRtGVFQUw4YNY/Xq1VSqVImOHTty22235Y9p0aIFU6dOZfDgwdx8883UqVOHMWPGcP755+/1+UmSpKIrFA6xMWsjaVlp/LjxR97//n1mfT+LOSvnkJaVVmBs6djStE5uzXG1juO4WsfRonoLEkolBFS59qSdDirs7II3MzOTYcOGsXz5csqWLUuHDh3473//y3777Zc/5ueff6ZHjx789NNPJCUlccQRR/Dmm29y4okn7v4MJUmSdsLnn0eOeli6NHK8w6hRcO21kWMfVPz4MFeSJO1T8jIhY1Xk6IaMlZDxfeT9pmXw2xeQk77tNdFxsN8RUOFIqNAs8ko6HGLcibQk6t+//9/uDjZz5swC7VKlSjFy5EhGjhz5j/c87bTTOO200wqrREmSVISEw2Gy8rLYlL0p//VH4GBD5gbSMtO2eb9NOzON9Kx0woS3+x3l4srRpmYbjqt1HMfWOpZm1ZoRFxO3l2eqIESF/9ijtohLT08nKSmJtLQ0ypcvH3Q5kiSpCNi0CRYtggUL4OuvI69334XsbKhZM7KjQuvWQVep7Snua7/iPj9JkkqkcBiy1kUCCPlBhD/e/x5IyPz5n+8RkwD7NS4YSih/GPggt0gr7mu/4j4/SZKKkl82/8L4r8bz48YfycjJKBBA2JS9abt9oXCo0L4/LiaOCokVaFGtRWTHhNrH0aRqE0pF7/Tv1msftTNrP/9XlyRJxd7mzZFAwh9hhD9e3323/fGnnw5PPQUV9vIRvpIkSdqH5WVD7sbIrgY5f/z8/ZX75/ZGyE0vOC5rfSSIkJf5798TUxrK1IIyNSM/S//+c78jIOnQyLEOkiRJ0k74ceOPjP5oNA9/9jAZORm7dI/EUomUiStDubhyJCUkkRSfxH4J+xV8H5/0958lJHmEgwowqCBJkvYZ6enwzjvwxhuwZAmUKrXjr9jYgu2cnMg9vv4ali+P/ALb9lSpAg0bbn01aQItW0JU1F6duiRJkvZFuVvgm/tg0d2R3RAKQ+KBW8MHZWr+/v5PoYS4/V2MSpIkqVAs/205//ngPzz1xVNk52UD0KRqE46rdRxl48pSJrYMZePKbvMqE1ewv0xsGWKiYwKejYobgwqSJCkw4XAkSPD665Fwwpw5kJu7Z76rYsWtYYTDD9/6/oAD9sz3SZIkqQgL5cGKZ+GrEbD5h4KfxZSG2HIQWz7yKvWn93/0/7UvrkIkjJBYHWLig5mTJEmSSowFPy/g9jm3M2HBBPLCeQAcnXw0Q48Zysn1TibKYKz2AQYVJEnSXrVxI8yYEQkmvPEGrFpV8PODD4ZTToFWrSK/SJabG3nl5Gx9/0+vP8YB1Ku3NZRQufLen6skSZKKmHAYfnwDvrgB0hZE+krXhMa3QvXTIgEEz8+VJEnSPmru6rmkzkll2uJp+X0n1zuZIW2GcEytY4IrTNoO/8tKkiTtUeEwLFq0ddeE99+PhAn+kJAAbdtGwgmnnBIJF0iSJEl73S+fwufXw88zI+24/aHhUDikH8R4lq4kSZL2TeFwmJnfzWTUnFG8s/wdAKKI4qxDz2LIMUM48sAjA65Q2j6DCpIkqdCFQjB9OrzySiSgsHJlwc/r1oUOHSLBhOOPh8TEQMqUJEmSYOMy+HIIrHwh0o6Oh/pXQsPBkbCCJEmStA8Kh8O8+s2rjJozio9/+BiAmKgYLjjiAm44+gYOrXRowBVK/8yggiRJKjThMEybBiNGwIIFW/vj4yOBhD/CCQcfHFSFkiRJ0u8y18GCW2DpwxDKAaKgTg844mYoUzPo6iRJkqTtyg3lMunrSaTOSeWrn78CID4mnt5H9uba1tdSe7/awRYo7SCDCpIkabeFw5FjHYYPh/nzI31JSXD++XDqqZGQQunSgZYoSZIkReRmwOIxsPAOyN0Y6TvwZGhyB+x/RKClSZIkSX8nMzeTZ798lv988B+W/bYMgHJx5bi8xeVcfdTVVC1bNeAKpZ1jUEGSJO2ycBjefReGDYOPI7uLUbYsXH01DBwI+7tTriRJkvYVoVxY/hR8NRK2/BTpq9AMmvwHqv5fsLVJkiRJfyMtM41H5j3CPR/fw5pNawA4IPEArj7qavq16Mf+iT6EVdFkUEGSJO2SOXMiOyjMnBlpJyZC//5w/fVQsWKgpUmSJElbhcOw+hX4YhCkL4r0lakDjW+DWl0gKjrY+iRJkqTtWLNpDfd+fC8PfvYg6VnpANQoX4NrWl3DJUdeQpm4MgFXKO0egwqSJGmnfPppJKDw5puRdlwc9O0LgwdDVXcXkyRJ0r4kfQl82g/Wzoi04w+AhsPh4L4QEx9sbZIkSdJ2LPt1GXd9eBdPffEUWXlZABxa8VBuOPoGujbqSlxMXMAVSoXDoIIkSdohX34JI0bAyy9H2qVKwcUXw9ChkJwcbG2SJElSAbmbYcGtsPguCOVATALUHwCH3QBxSUFXJ0mSJG3j858+544P7mDSwkmEwiEAjqpxFIOOHkTH+h2JdicwFTMGFSRJ0j9atAhGjoRJkyLt6Gjo3j0SWjjooGBrkyRJkgoIh+GHl2DeVbB5ZaSv2qnQ/D4o6+JVkiRJ+5ZwOMzM72Zy+we389ayt/L7T6l3CoPaDOKYmscQFRUVYIXSnmNQQZIkbdfSpXDTTTB+PIRCEBUFXbrAjTdC/fpBVydJkiT9xabl8NmV8ONrkXaZWtDsXqh+emQxK0mSJO0jQuEQLy1+ids/uJ25q+cCEB0VTZeGXbjh6BtoXLVxwBVKe55BBUmSVEBubmQHhTvugLy8SN+ZZ0ZCC40aBVubJEmStI28TFj4H1iYGnkfHQuHXgcNh0Kp0kFXJ0mSJOXLzstm3P/GcccHd7DklyUAJJRK4KImF3FN62s4aH93AVPJYVBBkiTlW7UKunaFDz6ItDt0gJtvhmbNgq1LkiRJ2q4fp8NnV8CmpZF2lROg+QOQ1CDYuiRJkqQ/2ZyzmcfnP86dH97JD+k/AJAUn0S/Fv24MuVKqpStEnCF0t5nUEGSJAHwyivQqxf8+iuULw+PPQbnnht0VZIkSdJ2ZKyC+QNg1ZRIO7EaHDkaap7rMQ+SJEnaZ2zI3MDYuWMZ88kY1m9eD0DVslUZeNRALm1+KeXjywdcoRQcgwqSJJVw2dkweDCMHh1pN2sGEydC3brB1iVJkiRtI5QDi8fAgpsgNwOiYqD+VdDoRogtF3R1kiRJEgBrN63lno/v4cFPH2Rj9kYA6uxXh+uPvp5eTXqRUCoh4Aql4BlUkCSpBFuxArp0gU8/jbSvvhpuvx3i4wMtS5IkSdrW2lnw2eWQtjDSrtQGWjwI+zUKti5JkiTpd99v+J47P7yTJz5/gszcTAAaVmrI4DaD6XJ4F0pF+1ez0h/8f4MkSSXU5MnQuzekpcH++8PTT8PppwddlSRJkvQn4TCs+wC+uR9WvhDpi68ETe+EOj085kGSJEn7hEXrFnH7B7cz/qvx5IZyAWhZvSVD2gyhY/2OREdFB1yhtO8xqCBJUgmTmQnXXAMPPhhpt2oFEyZAzZrB1iVJkiTly/oFVjwLSx+D9EW/d0bBwX2h8W0Qt3+g5UmSJEkAn/34GalzUpm6aCphwgCcUOcEhhwzhLa12xJlsFb6WwYVJEkqQb75Bs49F778MtIeNAhuvhliY4OtS5IkSSIchp9nwtJHYdWLEMqO9JcqA7XOg0P6w/5NgqxQkiRJJUReKI9N2ZtIz0pnY/ZG0rPSt3m9/u3rvL387fxrOjXoxOA2g2lZvWWAlUtFh0EFSZJKiHHj4NJLISMDKlWCZ5+Fk08OuipJkiSVeFvWwopnIrsnbFq6tb9CM6h7CdTuCrHlg6tPkiRJxcL6zesZ/9V41m5a+7cBhD/6NmVv2qF7xkTF0LVRVwYdPYiGlRvu4RlIxYtBBUmSirmMDLjySnjyyUj7+OMjoYVq1QItS5IkSSVZOARr3onsnvDDSxCOnONLqXJQ+3yodwlUODLYGiVJklQsZGRncM/H93Dnh3eSnpW+U9fGRsdSPr58/qtcfLn89zXL16Rv877U2b/OHqpcKt4MKkiSVIx9/XXkqIeFCyEqCkaMgOHDISYm6MokSZJUIm3+EZY/CcuegIzvtvYfkAL1+kDNcyG2bGDlSZIkqfjIycvh8fmPc/Psm1mzaQ0ATao24diax24TOigfX55ycQXb5ePLE18qPuBZSMWXQQVJkoqhvDx4/HEYMAC2bIGqVSO7KPzf/wVdmSRJkoqUTcvh24che0Nk14NQzu8/cwv+zH+f8/efhXJgy2oI50XuHbsf1Oke2T1hv0ZBzlKSJEnFSDgcZtLCSQx9dyhLf40cLXbQ/gdxa9tb6XJ4F6KjogOuUBIYVJAkqVgJheDFFyM7JyxaFOk76ST473+hcuVga5MkSVIRkpcFi+6Er2+DvMzCvXelNpHdE5I7Q6nEwr23JEmSSrQZy2cwaMYgPvvxMwAql6nM8GOH06dZH+Ji4gKuTtKfGVSQJKkYCIfh1VcjAYUvvoj07bdf5JiHq6+GaEPCkiRJ2lFr3oFP+8HGbyLtKm2hygkQXQqifn9Fl4Lo2ILt/J+xf2n/6X18JShbO9DpSZIkqfj5/KfPGTRjEG8tewuAsnFlua71dQw4agDl4ssFXJ2k7TGoIElSERYOwzvvwLBhMHdupK9cuciRDwMGRMIKkiRJ0g7Z8hPMHwjfT4i0E6rCkaOh1nkQFRVsbZIkSdJ2LPt1GcPeG8aEBZE1bGx0LJc1v4yhxw6lchm3mJX2ZQYVJEkqombPjuyYMHt2pJ2YCFdeCdddBwccEGxtkiRJKkJCufDtg/DlMMjdCFHRcHB/OOJmiEsKujpJkiRpG2s3reXW2bfy8LyHyQ3lAtCtUTduaXsLB+1/UMDVSdoRBhUkSSpi5s6NBBTeiuxiRlwcXHYZDBoEVasGW5skSZKKmPWfwKeXwW+fR9oHtIQWD0GFI4OtS5IkSdqOjVkbuevDu7j7o7vJyMkA4OR6J5N6QipNqjYJtjhJO8WggiRJRcQXX8CIEfDKK5F2qVJw8cUwdCgkJwdamiRJkoqarF/hy8Gw9DEgDHH7Q5PboW7vyI4KkiRJ0j4kKzeLR+Y9wq2zb2Xd5nUAtKjWgjva3UHbOm0Drk7SrjCoIEnSHvDrr7BpUyRMUKoUxMQU/PnH+x056nfRIhg5EiZNirSjo6FHj8iuCge5i5kkSZJ2RjgMK56Bz6+DrPWRvoN6QZM7IMEzfCVJkrRvCYVDTFgwgWHvDmPFhhUAHHLAIdz2f7dx9qFnE7UjD1gl7ZMMKkiSVIjCYUhNjex8kJf37+Ojo7cNL/z15w8/QCgUCTWcd14ktFC//p6fiyRJkoqZDQsixzysmxNpJzWMHPNQ+Zhg65IkSZL+IhwO8+ayNxk8YzBfrPkCgAPLHsjI40ZyUdOLiI2JDbZASbvNoIIkSYUkHIbrr4e77oq04+IiYYV/CiyEQpFXTs4/3/vMM+Gmm6BRo8KrV5IkSSVEziZYcBMsvgfCeVCqDDS6EepfBdE+4JUkSdK+Izsvmxe+foHRH43m8zWfA1A+vjw3HH0DV6VcRZm4MgFXKKmwGFSQJKkQ5OVB377w+OOR9ujRMGBA5H04HAkj5OZGxuXmFnz/159/7TvgAI94kCRJ0i4Ih2HVizD/atj8Q6Qv+Sw4cgyUSQ6yMkmSJKmA37b8xqPzHuX+ufezeuNqABJLJXJZ88sYcswQDih9QMAVSips0UEXIElSUZedDd26RUIK0dHwxBNbQwoQObIhJgbi46F0aShfHipUgEqV4MADoUYNqF0b6taNHOnQsCEccQQceSS0aGFIQZIkSbtg4zKYeSrM6RwJKZQ9CI57DY6ZYkhBKsbGjh1L7dq1SUhIICUlhblz5/7j+DFjxlC/fn0SExNJTk5mwIABZGZmbnfs7bffTlRUFFdfffUeqFySVFIt/205V71xFcn3JDNoxiBWb1xN1bJVue3/bmPVgFXc3f5uQwpSMeWOCpIk7YbNm+Hss2H6dIiNhfHjoXPnoKuSJElSiZWXBQv/AwtHQV4mRMfBYTfAYYOhVGLQ1UnagyZOnMjAgQN5+OGHSUlJYcyYMbRv354lS5ZQuXLlbcaPHz+eQYMG8eSTT9K6dWu++eYbevXqRVRUFKNHjy4w9tNPP+WRRx7hiCOO2FvTkSQVcx+t+oi7P7qbqYunEgqHAGhUuREDWw2k6+FdiS8VH3CFkvY0gwqSJO2itDQ47TSYMwcSE2HqVGjfPuiqJEmSVCKF8mDlJPhqBGz8NtJXtR00HwvlDwm2Nkl7xejRo7nkkku48MILAXj44Yd57bXXePLJJxk0aNA24z/88EOOPvpounXrBkDt2rXp2rUrn3zySYFxmzZt4vzzz+exxx7j1ltv3fMTkSQVW3mhPKYunsrdH93Nxz98nN/fvm57rml1De0OakdUVFSAFUramzz6QZKkXbBuHbRtGwkpJCXB228bUpAkSVIAQrmw4jl4vSF82DUSUkg8EI6eAG3fMqQglRDZ2dnMmzePdu3a5fdFR0fTrl07Pvroo+1e07p1a+bNm5d/PMTy5ct5/fXX6dChQ4Fx/fr149RTTy1w73+SlZVFenp6gZckqWTbmLWRez++l4PvP5hzJp3Dxz98TFxMHBc1uYivLvuK6RdM58S6JxpSkEoYgwqSJO2kH36AY4+Fzz+HSpVg5kw4+uigq5K0qzzHV5JUJIVyYfkz8Nph8FF3SF8CcfvDEbfAaYuhVhfwQa9UYqxfv568vDyqVKlSoL9KlSqsWbNmu9d069aNm2++mTZt2hAbG0vdunU5/vjjGTJkSP6YCRMmMH/+fFJTU3e4ltTUVJKSkvJfycnJuzYpSVKR90P6D1z/9vUk35PM1W9ezYoNKzgg8QCGHzuc76/+nifOeILDKx8edJmSAuLRD5Ik7YRvv4UTT4Tvv4fk5MhOCvXrB12VpF3lOb6SpCInlAMr/gtf3wablkf64g+ABtfAIf0gtnyw9UkqMmbOnMmoUaN48MEHSUlJYenSpVx11VXccsstDB8+nFWrVnHVVVfx9ttvk5CQsMP3HTx4MAMHDsxvp6enG1aQpBJm/k/zGf3RaCZ+PZHcUC4AhxxwCAOOGkCPxj0oHVs64Aol7QsMKkiStIP+9z846SRYuxYOOSQSUqhZM+iqJO0Oz/GVJBUZedmw4hn4ehRkfBfpi68Eh14LB18OsWUDLU9SsCpWrEhMTAxr164t0L927VqqVq263WuGDx9O9+7d6d27NwCNGjUiIyODPn36MHToUObNm8fPP//MkUcemX9NXl4es2fP5oEHHiArK4uYmJht7hsfH098fHwhzk6SVBSEwiFe//Z17v7obmZ+NzO//7hax3FNq2s49ZBTiY5yo3dJW/kngiRJO+Cjj+C44yIhhcaNYfZsQwpSUec5vpKkIiEvC759CF6pB3P7REIKCZWh6V1wxgo47HpDCpKIi4ujWbNmzJgxI78vFAoxY8YMWrVqtd1rNm/eTHR0wcfDfwQPwuEwJ5xwAl999RVffPFF/qt58+acf/75fPHFF9sNKUiSSp4tOVt45LNHOGzsYXR8viMzv5tJTFQM3Rp149NLPmVmr5l0rN/RkIKkbbijgiRJ/+Kdd+CMM2DzZmjdGl57DfbbL+iqJO2ufzrHd/Hixdu9plu3bqxfv542bdoQDofJzc2lb9++2z3H99NPP93hWlJTU7npppt2bSKSpOIpLxOWPg4Lb4ctqyN9CVXhsBugXh8o5Xa5kgoaOHAgPXv2pHnz5rRs2ZIxY8aQkZGRv3tYjx49qF69OqmpqQB07NiR0aNH07Rp0/yjH4YPH07Hjh2JiYmhXLlyHH54wXPDy5QpwwEHHLBNvySpZJqycAp9X+vL+s3rASgfX54+R/bhypQrSU7y2B9J/8yggiRJ/2DqVDjvPMjOjhz78OKLUKZM0FVJCorn+EqS9rjcLbD0UVh0B2z5KdKXWB0OGwR1L4ZSicHWJ2mf1aVLF9atW8eIESNYs2YNTZo0Yfr06fnB3JUrVxbYQWHYsGFERUUxbNgwVq9eTaVKlejYsSO33XZbUFOQJBUhk76eRNcpXckL51ErqRZXH3U1FzW9iPLx5YMuTVIRERUOh8NBF1EY0tPTSUpKIi0tjfLl/UNQkrT7nn0WLroI8vLg7LNh3DjwmE1p31AYa7/s7GxKly7N5MmT6dSpU35/z5492bBhAy+99NI21xxzzDEcddRR3Hnnnfl9zz33HH369GHTpk28/PLLnHnmmQW2wc3LyyMqKoro6Oi/Pcd3T8xPklTE5GbAt4/Aov9A5u9nzJeuAYcNhroXQcyOB+AkFS3Ffe1X3OcnSSXRlIVT6DK5C3nhPHo27snjpz9OqWh/N1rSzq39/FNDkqTtuP9+uPLKyPteveCxx6CU/9aUipU/n+P7R1Dhj3N8+/fvv91rdvQc3z+78MILadCgATfccIPn+EqStpWzCb59CBbdCVnrIn2la0LDIXBQL4gxKStJkqR9x9RFUzlvynnkhfPofkR3njj9CWKifd4haef5Vy6SJP1JOAy33gojRkTaV18Nd98Nf/l7SUnFhOf4SpICk7MRvhkLi++GrMiZvpSpEwko1OkBMXHB1idJkiT9xUuLX+LcyeeSG8rl/Ebn89QZTxlSkLTLDCpIkvS7cBiuvRZGj460b7oJhg+HqKhg65K053iOryRpr8vNgMVjYPFoyP410le2Lhw+DGqfD9GxgZYnSZIkbc8rS17hnEnnkBvKpevhXXmm0zOGFCTtlqhwOBwOuojC4FlnkqTdkZcHl14KTzwRaY8ZA1ddFWhJkv5BcV/7Fff5SVKJlfUrvHcy/PpppF3ukEhAoVZX8ExfqcQq7mu/4j4/SSoJXv3mVc6aeBY5oRy6NOzCc2c9RynXr5K2Y2fWfv4pIkkq8bKz4YILYNKkyBEPTzwBvXoFXZUkSZKKlS1r4b0TYcNXEH8AHHkv1DoP/C00SZIk7cNe//Z1zn7hbHJCOZxz2DmGFCQVml06cXvs2LHUrl2bhIQEUlJSmDt37t+OzcnJ4eabb6Zu3bokJCTQuHFjpk+fXmBMamoqLVq0oFy5clSuXJlOnTqxZMmSXSlNkqSdkp4OZ5wRCSnExkZ+GlKQJElSocpYBe8cGwkpJFSFE2ZBnfMNKUiSJGmfNn3pdM6aeBbZedmcfejZjDtrnCEFSYVmp4MKEydOZODAgYwcOZL58+fTuHFj2rdvz88//7zd8cOGDeORRx7h/vvvZ+HChfTt25czzzyTzz//PH/MrFmz6NevHx9//DFvv/02OTk5nHTSSWRkZOz6zCRJ+hfvvw9HHAHTp0Pp0vDqq3DWWUFXJUmSpGJl4zJ45xjY+A2Urgknvg/7NQy6KkmSJOkfvbXsLTpN6ERWXhZnNjiT589+ntiY2KDLklSMRIXD4fDOXJCSkkKLFi144IEHAAiFQiQnJ3PFFVcwaNCgbcZXq1aNoUOH0q9fv/y+s88+m8TERJ577rntfse6deuoXLkys2bN4thjj92hujzrTJK0o7KyYORI+M9/IByG2rVhwgRISQm6Mkk7qriv/Yr7/CSpxEhbCO+2gy0/Qdl6cMIMKFMz6Kok7WOK+9qvuM9Pkoqjd5a/Q8fnO5KZm8kZ9c/ghXNeIC4mLuiyJBUBO7P226kdFbKzs5k3bx7t2rXbeoPoaNq1a8dHH3203WuysrJISEgo0JeYmMicOXP+9nvS0tIAqFChwt+OycrKIj09vcBLkqR/s2BBJJBwxx2RkMKFF8KXXxpSkCRJUiH79XN457hISCGpIZw425CCJEmS9nnvrng3P6TQ8ZCOhhQk7TE7FVRYv349eXl5VKlSpUB/lSpVWLNmzXavad++PaNHj+bbb78lFArx9ttv8+KLL/LTTz9td3woFOLqq6/m6KOP5vDDD//bWlJTU0lKSsp/JScn78xUJEklTCgE99wDzZtHggkVK8KLL8KTT4K/0CFJkqRCte4jmNEWstZDhWbQbhYkHhh0VZIkSdI/mvndTE4bfxqZuZmcevCpTDpnkiEFSXvMTgUVdsW9997LwQcfTIMGDYiLi6N///5ceOGFREdv/6v79evHggULmDBhwj/ed/DgwaSlpeW/Vq1atSfKlyQVA6tWwYknwsCBkWMfOnSAr76CM88MujJJkiQVO2vehfdOhJw0qNQG/m8GxB8QdFWSJEnSP5r13SxOHX8qW3K30OHgDkw5dwrxpeKDLktSMbZTQYWKFSsSExPD2rVrC/SvXbuWqlWrbveaSpUqMW3aNDIyMvj+++9ZvHgxZcuW5aCDDtpmbP/+/Xn11Vd57733qFGjxj/WEh8fT/ny5Qu8JEn6q+efhyOOgHffhdKl4aGH4NVX4W/+tSVJkiTtutWvwcwOkJsBVdtB2+kQlxR0VZIkSSoiMnMzCYfDe/173//+fTqM78DmnM2cXO9kQwqS9opSOzM4Li6OZs2aMWPGDDp16gREjmqYMWMG/fv3/8drExISqF69Ojk5OUyZMoVzzz03/7NwOMwVV1zB1KlTmTlzJnXq1Nn5mUiS9Ce//QaXXw5/bNDTsiX8979wyCHB1iVJkqRiauUk+KAbhHOh+unQZiLEJARdlSRJkoqAzNxMrnrjKp74/AniYuKomVSTWvvVomb5yM9aSbUi7aSa1Chfg1LRO/XXe/9ozso5nDLuFDbnbOakuicxtctUEkq5jpW05+30n2QDBw6kZ8+eNG/enJYtWzJmzBgyMjK48MILAejRowfVq1cnNTUVgE8++YTVq1fTpEkTVq9ezY033kgoFOL666/Pv2e/fv0YP348L730EuXKlWPNmjUAJCUlkZiYWBjzlCSVIDNmQM+esHo1xMTA8OEwZAjExgZdmSRJkoql5c/AJxdBOAS1zoNWz0K0i09JkiT9u+82fEfnFzoz76d5AGzJ3cKSX5aw5Jcl2x0fHRVN9XLV84MLtZIKBhlqJdWiTFyZHfruD1d9yCnjTiEjJ4N2B7VjWpdphhQk7TU7HVTo0qUL69atY8SIEaxZs4YmTZowffp0qlSpAsDKlSuJjt56okRmZibDhg1j+fLllC1blg4dOvDf//6X/fbbL3/MQw89BMDxxx9f4LueeuopevXqtfOzkiSVSFu2RAIJY8ZE2gcfHNlFISUl0LIkSZJUnH3zIHzWL/K+7sXQ4hGIjgm2JkmSJBUJb3z7Bue/eD6/Zf7GAYkH8N8z/8shBxzC92nf8/2G71mZtjLy/vf2qvRVZOdlsyp9FavSV/3tfQ9IPGC7QYZaSZG+iqUr8snqTzj5uZPZlL2J/6vzf7x03kskxvrLw5L2nqhwEIfd7AHp6ekkJSWRlpZG+fLlgy5HkrSXff45XHABLFwYaV92Gdx5J5TZsfCwpCKmuK/9ivv8JKnYWPgf+OKGyPtDroRm90BU9D9fI0l/UdzXfsV9fpK0K0LhEDfPupmbZ91MmDAtqrVg8rmTqZlU81+vW7tp7d8GGb5P+570rPR//f7SsaXJC+WRlZdF29ptebXbq5SOLV1Y05NUgu3M2q/wDrGRJCkAeXmRQMKIEZCTA1WqwJNPQocOQVcmSZKkYischq9GwoJbIu2GQ+CIWyEqKti6JEmStM/7ZfMvXDD1AqYvnQ7AZc0v45729xBfKv5fr42OiubAcgdyYLkDOarGUdsdk5aZ9rdBhpVpK/lp009sztkMwHG1juOVrq8YUpAUCIMKkqQia8UK6NED5syJtDt1gkcfhUqVAi1LkiRJxVk4DPOvgSX3RNqNR0HDwcHWJEmSpCLh09Wf0nlSZ1amrSSxVCKPnPYI3Rt3L9TvSEpI4oiEIziiyhHb/TwrN4tV6atIy0yjSdUmxHhsmaSAGFSQJBU54TA88wxceSVs3AjlysF990HPnv4SmyRJkvagUB58djksfTTSbnYf1L8i2JokSZK0zwuHwzw2/zGueOMKsvOyqVehHlPOnfK3YYI9Kb5UPPUq1Nvr3ytJf2VQQZJUpKxfD336wNSpkXabNvDss1CnTrB1SZIkqZgI5ULupsgrZ9PW97mb4Ltx8P0EiIqGlo9B3YuCrlaSJEn7uM05m7n8tct55stnADij/hk83elp9kvYL9jCJClgBhUkSUXG66/DRRfB2rUQGws33wzXXQcx7k4mSZJUcmWnQc6GbUMFf21vr297Y/Iy//n7okpB6+egVpe9Mj1JkiQVXct+XcbZL5zNl2u/JDoqmtQTUrmu9XVEuS2sJBlUkCTt+zZvhmuvhYceirQPOwyeew6aNg22LkmSJAUoLxvmD4BvHwLChX//qFIQWw5Kld36itsfGgyEau0L//skSZJUrLyy5BW6T+1OWlYalctUZsLZE2hbp23QZUnSPsOggiRpn7Z2LZx6KsybF2lffTWMGgWJiYGWJUmSpCBtWQNzOsO6DyLtmMStYYLYsgXDBdvr25ExMXHBzlGSJElFUl4ojxHvjWDUnFEAtKrRiknnTKJ6+eoBVyZJ+xaDCpKkfdaSJXDKKbBiBVSsCM8/D+3aBV2VJEmSArV+Lrx/FmxZDbFJ0HocVD816KokSZIk1mWso+uUrsxYMQOAK1teyZ0n3UmcIVhJ2oZBBUnSPumDD+D00+HXX6FuXZg+HerVC7oqSZIkBWrZk/DpZRDKhvKHwrHToPwhQVclSZIk8fEPH3POpHP4If0HysSW4fHTH+e8w88LuixJ2mcZVJAk7XOmTIHzz4esLEhJgVdegUqVgq5KkiRJgQnlwLwB8O3YSLtGJ2j1LMSWC7QsSZIkKRwO8+CnDzLgzQHkhHKof0B9ppw7hYaVGwZdmiTt0wwqSJL2KWPGwMCBEA7DGWfA+PFQunTQVUmSJCkwW9bCnHNg3fuRdqOb4fChEBUdbF2SJEkq8TKyM7j01UsZ99U4ADof1pknT3+ScvEGaiXp3xhUkCTtE0IhuOaaSFAB4PLL4b77ICYm0LIkSZIUpF8+hffPgs0/QGx5aPUc1OgYdFWSJEkS3/zyDWdNPIuv131NTFQMd554J1cfdTVRUVFBlyZJRYJBBUlS4DIzoXt3mDw50r7jDrjuOnBNL0mSVIItfxrm9oVQFpRvAMdOg/L1g65KkiRJ4sVFL9JrWi82Zm+katmqvND5BY6pdUzQZUlSkWJQQZIUqF9+iRzx8MEHEBcHTz8NXbsGXZUkSZICE8qB+dfAN/dH2tVPh9b/jeyoIEmSJAUoN5TL4HcGc9dHdwFwbK1jmdh5IlXLVg24MkkqegwqSJICs2IFnHIKLFkC++0H06bBcccFXZUkSZICk/kzzDkXfp4VaTe6EQ4fDlHRgZYlSZIkrdm0hi6TuzD7+9kAXNvqWkadMIrYmNiAK5OkosmggiQpEJ99BqeeCj//DMnJ8MYb0LBh0FVJkiQpML/Og9lnwuZVUKpcZBeFGmcEXZUkSZLEnJVzOHfSufy06SfKxZXjqTOe4uzDzg66LEkq0gwqSJL2utdfh3POgc2boUkTeO01qFYt6KokSZIUmOXPwtw+EMqCcofAsS9BUoOgq5IkSVIJFw6HGfPxGK57+zrywnk0rNSQKedOoX7F+kGXJklFnkEFSdJe9dhjcNllkJcHJ50EkydDuXJBVyVJkqRAhHLg8+tgyb2RdrXToPVzEJcUbF2SJEkq8TZmbaT3K7154esXAOjWqBuPnvYoZeLKBFyZJBUPBhUkSXtFOAzDh8Ntt0XaF14IjzwCsR7hJkmSVDJlroM558LPMyPtw4dDoxshKjrIqiRJkiQWrVvEWS+cxeL1i4mNjmV0+9H0a9GPqKiooEuTpGLDoIIkaY/LzobeveG//420b7wRRowA1/WSJEkl1K/zYfaZsHkllCoLrf4LyZ2CrkqSJEli4oKJXPzyxWTkZFC9XHUmnTOJVsmtgi5LkoodgwqSpD0qLQ3OPhtmzICYGHj0UbjooqCrkiRJUmBWjIO5vSEvE8odDMdOg6TDgq5KkiRJJdy6jHVc9/Z1PPPlMwD8X53/4/mzn6dymcoBVyZJxZP7KUqS9phvv4VjjomEFMqWhVdfNaQgSZJUYoVyYf418NEFkZBCtQ7Qfq4hBUkqRGPHjqV27dokJCSQkpLC3Llz/3H8mDFjqF+/PomJiSQnJzNgwAAyMzPzP09NTaVFixaUK1eOypUr06lTJ5YsWbKnpyFJe1UoHOKxeY9R/4H6+SGFwW0G89YFbxlSkKQ9yKCCJKnQZWXBLbdAo0bw1VdQtSrMng0nnxx0ZZIkSQpE5np472RYPDrSbjgUjn0Z4vYLtCxJKk4mTpzIwIEDGTlyJPPnz6dx48a0b9+en3/+ebvjx48fz6BBgxg5ciSLFi3iiSeeYOLEiQwZMiR/zKxZs+jXrx8ff/wxb7/9Njk5OZx00klkZGTsrWlJ0h715ZovafNkG/q82offMn+jcZXGfHjRh4w6YRQx0TFBlydJxZpHP0iSCtX770OfPrB4caTdvn3kuIeaNYOtS5IkSQH57QuY3QkyvodSZaDVs5B8VtBVSVKxM3r0aC655BIuvPBCAB5++GFee+01nnzySQYNGrTN+A8//JCjjz6abt26AVC7dm26du3KJ598kj9m+vTpBa55+umnqVy5MvPmzePYY4/dg7ORpD1rY9ZGRs4cyX2f3EdeOI+ycWW5+fibuSLlCkpF+1dnkrQ3uKOCJKlQ/Por9O4Nxx4bCSlUqQITJsAbbxhSkCRJKrG+ex7eah0JKZStCyd9bEhBkvaA7Oxs5s2bR7t27fL7oqOjadeuHR999NF2r2ndujXz5s3LPx5i+fLlvP7663To0OFvvyctLQ2AChUqFGL1krT3hMNhpiycwqFjD+Wej+8hL5xH58M6s6jfIga0GmBIQZL2Iv/ElSTtlnAYxo+HAQNg3bpI36WXQmoq7L9/sLVJkiQpIKFc+HIwLLor0j7wZDh6PMS5QJSkPWH9+vXk5eVRpUqVAv1VqlRh8R9bHv5Ft27dWL9+PW3atCEcDpObm0vfvn0LHP3wZ6FQiKuvvpqjjz6aww8//G9rycrKIisrK7+dnp6+CzOSpMK3/Lfl9H+9P28sfQOAg/Y/iAdOeYBTDj4l4MokqWRyRwVJ0i5buhROOgkuuCASUmjYEObMgYcfNqQgSZJUYmX9AjNP2RpSOGwwHPeqIQVJ2sfMnDmTUaNG8eCDDzJ//nxefPFFXnvtNW655Zbtju/Xrx8LFixgwoQJ/3jf1NRUkpKS8l/Jycl7onxJ2mFZuVncOvtWGj7YkDeWvkFsdCzDjhnGgssWGFKQpAC5o4IkaadlZ8Ndd8Ett0BmJiQkwPDhcO21EBcXdHWSJEkKzG9fwuwzIWMFxJSGVk9DzXOCrkqSir2KFSsSExPD2rVrC/SvXbuWqlWrbvea4cOH0717d3r37g1Ao0aNyMjIoE+fPgwdOpTo6K2/49a/f39effVVZs+eTY0aNf6xlsGDBzNw4MD8dnp6umEFSYF5d8W7XP7a5Sz5ZQkA/1fn/3iww4PUr1g/4MokSe6oIEnaKXPmQNOmMHRoJKRw4onw1VcwZIghBUlF09ixY6lduzYJCQmkpKTkn9H7d8aMGUP9+vVJTEwkOTmZAQMGkJmZmf95amoqLVq0oFy5clSuXJlOnTqxZMmSPT0NSQre9y/AW60jIYWyB0H7jw0pSNJeEhcXR7NmzZgxY0Z+XygUYsaMGbRq1Wq712zevLlAGAEgJiYGiJzh/sfP/v37M3XqVN59913q1Knzr7XEx8dTvnz5Ai9J2tvWblrLBS9ewAnPnsCSX5ZQpUwVxp01jne6v2NIQZL2EQYVJEk75Lff4NJL4ZhjYOFCqFQJnnsO3nwT6tULujpJ2jUTJ05k4MCBjBw5kvnz59O4cWPat2/Pzz//vN3x48ePZ9CgQYwcOZJFixbxxBNPMHHixALn+M6aNYt+/frx8ccf8/bbb5OTk8NJJ51ERkbG3pqWJO1doTz4/Ab4oAvkbYaqJ0H7T2G/RkFXJkklysCBA3nsscd45plnWLRoEZdddhkZGRlceOGFAPTo0YPBgwfnj+/YsSMPPfQQEyZMYMWKFbz99tsMHz6cjh075gcW+vXrx3PPPcf48eMpV64ca9asYc2aNWzZsiWQOUrSv8kL5fHgpw9S/4H6jPtqHFFE0a9FPxb3X0y3Rt2IiooKukRJ0u88+kGS9I/CYZg4Ea6+Gv7YQbJ3b7jjDqhQIdDSJGm3jR49mksuuST/4e3DDz/Ma6+9xpNPPsmgQYO2Gf/hhx9y9NFH061bNwBq165N165d+eSTT/LHTJ8+vcA1Tz/9NJUrV2bevHkce+yxe3A2khSAn2fDvKvgty8i7cNugCNug+iYQMuSpJKoS5curFu3jhEjRrBmzRqaNGnC9OnTqVKlCgArV64ssIPCsGHDiIqKYtiwYaxevZpKlSrRsWNHbrvttvwxDz30EADHH398ge966qmn6NWr1x6fkyTtjPk/zafvq3359MdPAWh2YDMeOvUhWlRvEXBlkqTtMaggSfpbK1bAZZdFdk0AaNAAHnkE/Hs2ScVBdnY28+bNK/BbZdHR0bRr146PPvpou9e0bt2a5557jrlz59KyZUuWL1/O66+/Tvfu3f/2e9LS0gCoYLpLUnGS8T18fj2sfCHSjk2Clo9ArS7B1iVJJVz//v3p37//dj+bOXNmgXapUqUYOXIkI0eO/Nv7/XEEhCTty9Iy0xj+3nDGfjqWUDhE+fjy3PZ/t3FZ88uIMUArSfssgwqSpG3k5MDo0XDTTbBlC8THw9ChcP31kfeSVBysX7+evLy8/N8w+0OVKlVYvHjxdq/p1q0b69evp02bNoTDYXJzc+nbt2+Box/+LBQKcfXVV3P00Udz+OGH/20tWVlZZGVl5bfT09N3YUaStBfkZsDC/8Ci/0BeJkRFQ90+cMTNkFAp6OokSZJUgoTDYV74+gUGvDmAnzb9BEDXw7ty90l3c2C5AwOuTpL0bwwqSJIK+OgjuPRS+OqrSPv//g8eeggOOSTYuiRpXzBz5kxGjRrFgw8+SEpKCkuXLuWqq67illtuYfjw4duM79evHwsWLGDOnDn/eN/U1FRuuummPVW2JO2+cBi+nwBfXA+bf4j0VT4emo2B/RsHWZkkSZJKoG9/+ZZ+r/fj7eVvA3BwhYN58NQHaXdQu4ArkyTtKIMKkiQANmyAIUPg4Ycjz6EPOCCyq0L37hAVFXR1klT4KlasSExMDGvXri3Qv3btWqpWrbrda4YPH0737t3p3bs3AI0aNSIjI4M+ffowdOjQAmf+9u/fn1dffZXZs2dTo0aNf6xl8ODBDBw4ML+dnp5OcnLyrk5NkgrXr/Ng3lWw7oNIu0wtaHo3JJ/lQlGSJEl7VWZuJrfPuZ3b59xOVl4W8THxDDlmCNcffT0JpRKCLk+StBMMKkhSCRcOw+TJcOWVsGZNpK9XL7jzTqhYMdDSJGmPiouLo1mzZsyYMYNOnToBkaMaZsyY8bfn+m7evLlAGAEgJiZy3uUf5/eGw2GuuOIKpk6dysyZM6lTp86/1hIfH0+8Z+tI2tdsWQNfDoXlTwFhiCkNDYdAg4FQKjHo6iRJklTCvLXsLfq93o+lvy4FoH3d9jzQ4QHqVagXcGWSpF1hUEGSSrDvvoN+/eD11yPtQw6BRx6B448PsipJ2nsGDhxIz549ad68OS1btmTMmDFkZGRw4YUXAtCjRw+qV69OamoqAB07dmT06NE0bdo0/+iH4cOH07Fjx/zAQr9+/Rg/fjwvvfQS5cqVY83vKbCkpCQSE/2LPUlFQF42fHMffHUz5G6M9NU+H5rcAaWrB1ubJEmSSpwfN/7IgDcH8MLXLwBQrVw1xrQfQ+fDOhPlDl+SVGQZVJCkEig3F8aMgZEjYfNmiIuDwYNh0CBIcIc0SSVIly5dWLduHSNGjGDNmjU0adKE6dOnU6VKFQBWrlxZYAeFYcOGERUVxbBhw1i9ejWVKlWiY8eO3HbbbfljHnroIQCO/0vq66mnnqJXr157fE6StMvCYfjxNZg/EDZ+G+mr0Bya3QuVWgdbmyRJkkqc3FAuD376IMPeHcbG7I1ER0VzRcsruLntzZSPLx90eZKk3RQV/mOP2iIuPT2dpKQk0tLSKF/ef0FJ0t9ZtAi6doUvv4y0jzsOHn4YGjQIti5J2hnFfe1X3OcnaR+UtgjmD4Cf3oy0E6pAk9uhTg+Iiv7nayVJu6W4r/2K+/wk7RlzV8+l76t9+XzN5wCkVE/hoVMfoumBTQOuTJL0T3Zm7eeOCpJUgnzxBZx4IqxfDxUqwF13Qa9e4A5pkiRJJVT2b/DVTfDNAxDOg+g4aDAAGg6BWP8ySZIkSXvXb1t+Y8iMITwy7xHChNkvYT9uP+F2Lml2CdEGaCWpWDGoIEklxKefQvv28Ntv0Lw5vPYaVK4cdFWSJEkKRCgPlj0G/xsGWb9E+mqcAU3vgnL1gq1NkiRJJU44HGbcV+O45q1r+DnjZwB6NO7BnSfeSeUyPsSUpOLIoIIklQAffginnALp6dCqFbzxBiQlBV2VJEmSArF2Jsy7Cjb8L9JOOgyOHAMHnhhkVZIkSSqhVvy2gotfvpj3vnsPgEMrHsqDpz7I8bWPD7YwSdIeZVBBkoq5WbPg1FMhIwOOOw5eeQXKlQu6KkmSJO11m1bA59fBqimRdux+cMTNcPBlEO3jAUmSJO19Ly56kYteuoi0rDQSSyUy/NjhXNP6GuJi4oIuTZK0h/kkQpKKsbffhjPOgC1b4MQTYdo0KF066KokSZK0V+VsgoW3w6K7IJQFUdFQry80ugkSKgZdnSRJkkqgzNxMrnvrOh749AEAWtVoxbizxlFn/zoBVyZJ2lsMKkhSMfXaa3D22ZCVBR06wJQpkJAQdFWSJEnaa8Jh+G4cfHEDbPkx0lelLTS7F/ZrFGxtkiRJKrGW/rqUcyedy+drPgfg+tbXc+v/3UpsTGzAlUmS9iaDCpJUDE2dCl26QE4OnHkmTJgAce6WJkmSVHL88inMuwrWfxRpl6kDR94NNTpBVFSgpUmSJKnkmrBgAn1e6cPG7I0ckHgAz575LB0O7hB0WZKkABhUkKRiZuJEOP98yMuLhBX++1+INYwsSZJUMmz5Cb4cAsufjrRLlYGGQ6HBAIhxey1JkiQFY0vOFq6efjWPzn8UgGNqHsP4s8dTo3yNgCuTJAXFoIIkFSPPPgsXXgihEPToAU8+CTExQVclSZKkPS4vC5aMgQW3Qu6mSF+dHtA4FUpXC7Q0SZIklWyL1y/m3Enn8tXPXxFFFEOPGcrI40dSKtq/opKkksx/C0hSMfHYY3DppZGjiHv3hkcegejooKuSJEnSHhUOw+qXYf41sGlZpO+AltDsXqh4VLC1SZIkqcT775f/5bLXLiMjJ4PKZSrz3JnPcWLdE4MuS5K0DzCoIEnFwAMPwBVXRN737w/33mtIQZIkqdjb8DXMvxrWvBNpJx4IjW+HOhdAlItBSZIkBScjO4Mr3riCp754CoC2tdsy7qxxHFjuwIArkyTtKwwqSFIRd/fdcO21kffXXAN33glRUcHWJEmSpD0oHIZFd8KXQyCcB9Fx0OAaaDgYYssFXZ0kSZJKuK9//ppzJ5/LwnULiY6KZuRxIxl6zFBioj2jVpK01S79isXYsWOpXbs2CQkJpKSkMHfu3L8dm5OTw80330zdunVJSEigcePGTJ8+vcCY2bNn07FjR6pVq0ZUVBTTpk3blbIkqcS57batIYWhQw0pSJIkFXu5GfDBefDFDZGQQo0z4LRF0GSUIQVJkiQFKhwO8+TnT9LisRYsXLeQqmWrMqPHDEYcN8KQgiRpGzsdVJg4cSIDBw5k5MiRzJ8/n8aNG9O+fXt+/vnn7Y4fNmwYjzzyCPfffz8LFy6kb9++nHnmmXz++ef5YzIyMmjcuDFjx47d9ZlIUgkSDsPw4TBsWKR9yy1w662GFCRJkoq1TcvhrVaw8gWIKgUtHoRjpkLZg4KuTJIkSSXcpuxNdJ/anYtfvpgtuVs4qe5JfNn3S46vfXzQpUmS9lFR4XA4vDMXpKSk0KJFCx544AEAQqEQycnJXHHFFQwaNGib8dWqVWPo0KH069cvv+/ss88mMTGR5557btuCoqKYOnUqnTp12qmJpKenk5SURFpaGuXLl9+payWpKAmH4YYbIrsnAPznP3DddcHWJEl7W3Ff+xX3+UnaBT+9FdlJIfs3SKgCbSZD5TZBVyVJKgTFfe1X3OcnCb5c8yXnTj6Xb375hpioGG5pews3tLmB6Khd2tRbklSE7czar9TO3Dg7O5t58+YxePDg/L7o6GjatWvHRx99tN1rsrKySEhIKNCXmJjInDlzduart3vfrKys/HZ6evpu3U+SioJwGK66Cu6/P9K+7z644opga5IkSdIeFA7Dov/Al0MgHIIDUuCYKVC6etCVSZIkqYQLh8M8Ou9Rrpp+FVl5WVQvV50JnSfQpqaBWknSv9upONv69evJy8ujSpUqBfqrVKnCmjVrtntN+/btGT16NN9++y2hUIi3336bF198kZ9++mnXqwZSU1NJSkrKfyUnJ+/W/SRpXxcKQd++kZBCVBQ88oghBUmSpGItZxN80AW+GBQJKdS9GNrNMqQgSZKkwKVnpXPelPPo+1pfsvKy6HBwB77o+4UhBUnSDtvj++7ce++9HHzwwTRo0IC4uDj69+/PhRdeSHT07n314MGDSUtLy3+tWrWqkCqWpH1PXh5cdBE8+mgkpPDkk9CnT9BVSZIkaY/ZuAzebg0rJ0FUKWjxILR8DGLig65MkiRJJdz8n+Zz5CNH8sLXL1AquhR3nngnr3R9hYqlKwZdmiSpCNmpox8qVqxITEwMa9euLdC/du1aqlatut1rKlWqxLRp08jMzOSXX36hWrVqDBo0iIMOOmjXqwbi4+OJj/cBjaTiLzcXevSA55+HmBh49lno1i3oqiRJkrTH/PgmfNgVsn+DhCrQZjJU9jfTJEmSFKxwOMwDcx/g2revJTsvm5pJNZnYeSJH1Tgq6NIkSUXQTm1rEBcXR7NmzZgxY0Z+XygUYsaMGbRq1eofr01ISKB69erk5uYyZcoUzjjjjF2rWJJKkOxsOO+8SEihVCmYONGQgiRJUrEVDsPCO2BWh0hI4YAUOHmeIQVJkiQFbkPmBjpP6syV068kOy+bM+qfweeXfm5IQZK0y3ZqRwWAgQMH0rNnT5o3b07Lli0ZM2YMGRkZXHjhhQD06NGD6tWrk5qaCsAnn3zC6tWradKkCatXr+bGG28kFApx/fXX599z06ZNLF26NL+9YsUKvvjiCypUqEDNmjV3d46SVCRlZsI558Crr0JcHEyeDB07Bl2VJEmS9oicTfDJRZGjHgDq9obmD3jUgyRJkgI3d/VcukzuwncbviM2OpY7T7yTK1OuJCoqKujSJElF2E4HFbp06cK6desYMWIEa9asoUmTJkyfPp0qVaoAsHLlSqKjt27UkJmZybBhw1i+fDlly5alQ4cO/Pe//2W//fbLH/PZZ5/Rtm3b/PbAgQMB6NmzJ08//fQuTk2Siq7Nm+HMM+GttyAhAaZNg/btg65KkiRJe8TGZTC7E6QtgOhYaHYf1LsUfPArSZKkAIXDYe75+B5ueOcGckO51NmvDhM7T6RF9RZBlyZJKgaiwuFwOOgiCkN6ejpJSUmkpaVRvnz5oMuRpF22aROcfjq89x6ULh3ZUeFPWS5JEsV/7Vfc5yfpT36cDh90hZwNkFAVjpkMlY4OuipJ0l5U3Nd+xX1+UnH1y+Zf6PVSL1795lUAOh/Wmcc7Pk5SQlLAlUmS9mU7s/bb6R0VJEl7Tno6dOgAH3wA5crB669DG48kliRJKn7CYVh4B3w5BAjDAUfBMVOgdLWgK5MkSVIJ9+GqDzlv8nmsSl9FXEwc97S/h8uaX+ZRD5KkQmVQQZL2Eb/9BiefDHPnwn77wZtvQsuWQVclSZKkQpezCT6+EFZNjrTr9obmD0BMfLB1SZIkqUQLhUPc+cGdDH13KHnhPOpVqMcLnV+g6YFNgy5NklQMGVSQpH3A+vVw0knw+edwwAHw1ltw5JFBVyVJkqRCt3EpzD4T0hZAdCw0ux8OvjToqiRJklTCrctYR49pPZi+dDoAXQ/vyiOnPUK5+HIBVyZJKq4MKkhSwNauhXbtYMECqFwZ3nkHGjUKuipJkiQVuh+nwwddIWcDJFSNHPVQqXXQVUmSJKmEm/39bLpO6cqPG38koVQC9518H72P7O1RD5KkPcqggiQFaPVqOOEEWLIEDjwQ3n0XGjQIuipJkiQVqnAYFt4OXw4FwnDAUZGQQulqQVcmSZKkEiwvlEfqnFRGzhxJKByiQcUGvND5BRpV8beoJEl7nkEFSQrIypXwf/8Hy5ZBcnIkpFCvXtBVSZIkqVDlbIKPL4RVkyPtupdA8/shJj7YuiRJklSirdm0hgtevIAZK2YA0KNxD8Z2GEvZuLIBVyZJKikMKkjSXvbTTzBrFgwaBN9/D3XqREIKtWsHXZkkSZIK1calMLsTpH0N0bHQ/AGo1yfoqiRJklTCzVg+g/NfPJ+1GWspHVuasR3G0qtJr6DLkiSVMNFBFyBJxd0PP8C4cdCnDxxyCFSrBl27RkIKBx8Ms2cbUpAkSSp2fnwDpreIhBQSqsIJMw0pSJL2uLFjx1K7dm0SEhJISUlh7ty5/zh+zJgx1K9fn8TERJKTkxkwYACZmZm7dU9J+65N2ZsY8d4ITvzviazNWEvDSg359JJPDSlIkgLhjgqSVMhWroSZMyO7JsyaFTna4c+ioqBJk8ixD9ddB1WqBFGlJEmS9ohwGBamwpfDgDBUbAVtJkPpakFXJkkq5iZOnMjAgQN5+OGHSUlJYcyYMbRv354lS5ZQuXLlbcaPHz+eQYMG8eSTT9K6dWu++eYbevXqRVRUFKNHj96le0ra92TlZjF96XSeX/A8Ly95mS25WwDo3bQ3955yL6VjSwdcoSSppIoKh8PhoIsoDOnp6SQlJZGWlkb58uWDLkdSCREOw3ffRQIJf4QTvvuu4JjoaDjySDjuuMirTRvYf/8AipWkYqS4r/2K+/ykYitnE3zcC1ZNibTr9YFm90FMfKBlSZL2bYW19ktJSaFFixY88MADAIRCIZKTk7niiisYNGjQNuP79+/PokWLmDFjRn7fNddcwyeffMKcOXN26Z57cn6SdlxeKI9Z38/i+a+eZ/KiyWzI3JD/Wb0K9bj5+Jvp2qhrcAVKkoqtnVn7uaOCJO2EcBiWL98aSpg5E1atKjgmJgaaNYPjj48EE44+GpKSAihWkiRJe0/6t/D+mZGjHqJjofkDHvUgSdprsrOzmTdvHoMHD87vi46Opl27dnz00UfbvaZ169Y899xzzJ07l5YtW7J8+XJef/11unfvvsv3lBSccDjMZz9+xvivxjPx64n8tOmn/M+qlavGeQ3Po2ujrjQ7sBlRUVEBVipJUoRBBUn6B+EwfPttwaMcVq8uOKZUKWjRYuuOCUcfDeXKBVKuJEmSgvDjG/BBV8hJg8QDoc0UqNQq6KokSSXI+vXrycvLo8pfzpesUqUKixcv3u413bp1Y/369bRp04ZwOExubi59+/ZlyJAhu3xPgKysLLKysvLb6enpuzotSTtg0bpFPL/geZ5f8DxLf12a379/wv50Pqwz3Rp145iaxxATHRNglZIkbcuggiT9STgMixdvDSXMnAlr1hQcExsLKSlbgwmtW0OZMoGUK0mSpCCFw7AwFb4cBoShYitoMxlKVwu6MkmS/tXMmTMZNWoUDz74ICkpKSxdupSrrrqKW265heHDh+/yfVNTU7npppsKsVJJf7UqbRUTFkxg/ILxfLHmi/z+xFKJnNHgDLod3o329doTFxMXXJGSJP0LgwqSSrRwGBYuLLhjws8/FxwTFwdHHbX1KIejjoLSpYOoVpIkSfuMnI3wcS9Y9WKkXe9SaHYf+DBYkhSAihUrEhMTw9q1awv0r127lqpVq273muHDh9O9e3d69+4NQKNGjcjIyKBPnz4MHTp0l+4JMHjwYAYOHJjfTk9PJzk5eVenJul36zevZ/LCyYz/ajzvr3w/v79UdCna121Pt0bdOL3+6ZSNKxtglZIk7TiDCpJKlFAIFizYulvC7Nmwfn3BMQkJ0KpVJJRw/PGR3RMSEoKoVpIkSfuk9G/h/U6QthCiY6H5WKh3SdBVSZJKsLi4OJo1a8aMGTPo1KkTAKFQiBkzZtC/f//tXrN582aio6ML9MXERLaGD4fDu3RPgPj4eOLj43d/UpLYlL2Jlxa/xPgF43lr2VvkhnLzPzu21rF0O7wbnQ/rzAGlDwiwSkmSdo1BBUklxquvQu/e8JdfBCAxMXJ8wx87JrRsCf73tCRJkrZr9evwYTfISYPEA6HNFKjUKuiqJEli4MCB9OzZk+bNm9OyZUvGjBlDRkYGF154IQA9evSgevXqpKamAtCxY0dGjx5N06ZN849+GD58OB07dswPLPzbPSUVvqzcLKYvnc7zC57n5SUvsyV3S/5nTas2pVujbnRp2IXkJHcqkSQVbQYVJJUIzz8PPXpAbi6UKQNHH711x4TmzSPHO0iSJEl/KxyGr0fB/4YDYajYGo6ZHAkrSJK0D+jSpQvr1q1jxIgRrFmzhiZNmjB9+nSqVKkCwMqVKwvsoDBs2DCioqIYNmwYq1evplKlSnTs2JHbbrtth+8pqXDkhfKY9f0snv/qeSYvmsyGzA35nx1c4WC6Ht6Vro260qBig+CKlCSpkEWFw+Fw0EUUhvT0dJKSkkhLS6N8+fJBlyNpH/Loo9C3b+TZ8gUXwBNPGEyQpKKuuK/9ivv8pCInZyN83AtWvRhp1+sLze6FGBeVkqTdV9zXfsV9ftKuCofDfPbjZ4z/ajwTv57IT5t+yv+sWrlqdGnYhW6NutHswGZERUUFWKkkSTtuZ9Z+0f/4qSQVcXfdBZdeGgkpXHYZPPOMIQVJUkFjx46ldu3aJCQkkJKSwty5c/9x/JgxY6hfvz6JiYkkJyczYMAAMjMzd+uekvZRmeth2ZPwZkokpBAdBy0fg5YPGVKQJEnSLlm0bhEj3hvBIQ8cQsvHWzLmkzH8tOkn9kvYj0uOvIR3e7zLyqtXMrr9aJpXa25IQZJUbHn0g6RiKRyGkSPhllsi7RtugNRUcF0vSfqziRMnMnDgQB5++GFSUlIYM2YM7du3Z8mSJVSuXHmb8ePHj2fQoEE8+eSTtG7dmm+++YZevXoRFRXF6NGjd+mekvYxGd/Dqmnww1RY9z6EQ5H+xAPhmBeh4lGBlidJkqSiZ1XaKiYsmMD4BeP5Ys0X+f2JpRI5o8EZdD28K+3rtie+VHxwRUqStJd59IOkYicUggED4L77Iu1Ro2Dw4GBrkiQVrsJa+6WkpNCiRQseeOABAEKhEMnJyVxxxRUMGjRom/H9+/dn0aJFzJgxI7/vmmuu4ZNPPmHOnDm7dM89OT9JOyAchrSvYdXUSDjht88Lfr5/E6hxJhzcFxIMG0mSCl9xX/sV9/lJf2f95vVMXjiZ8V+N5/2V7+f3l4ouRfu67enWqBun1z+dsnFlA6xSkqTCtTNrP3dUkFSs5OXBJZfAU09F2g88AP36BVuTJGnflJ2dzbx58xj8pzRbdHQ07dq146OPPtruNa1bt+a5555j7ty5tGzZkuXLl/P666/TvXv3Xb4nQFZWFllZWfnt9PT03Z2epH8SDsH6jyPBhFVTYdOyrZ9FRUOlNpFwQo1OULZ2UFVKkiSpiNmUvYmXFr/E+AXjeWvZW+SGcvM/O7bWsXQ7vBtnH3Y2FUtXDLBKSZL2DQYVJBUb2dlwwQUwaRJER0fCCj16BF2VJGlftX79evLy8qhSpUqB/ipVqrB48eLtXtOtWzfWr19PmzZtCIfD5Obm0rdvX4YMGbLL9wRITU3lpptu2s0ZSfpHedmw9l34YRr88BJkrtn6WXQ8VD0RkjtB9dMhoVJQVUqSJKkIem/Fezwy7xFeXvIyW3K35Pc3rdqUbo260aVhF5KTkgOsUJKkfY9BBUnFwubN0LkzvPEGxMbChAlw1llBVyVJKm5mzpzJqFGjePDBB0lJSWHp0qVcddVV3HLLLQwfPnyX7zt48GAGDhyY305PTyc52YdY0m7L2QQ/vRHZNeHH1yDnT7uVxJaHaqdC8plw4MkQWy64OiVJklQk5eTlMHjGYO7+6O78vnoV6tHt8G50bdSVBhUbBFidJEn7NoMKkoq89HQ47TR4/31ITIRp0+Ckk4KuSpK0r6tYsSIxMTGsXbu2QP/atWupWrXqdq8ZPnw43bt3p3fv3gA0atSIjIwM+vTpw9ChQ3fpngDx8fHEx8fv5owkAZC5Dla/HAknrHkHQluPVSGhKtQ4I3KsQ5W2EBMXXJ2SJEkq0n5I/4HzJp/HB6s+AKB3095c2vxSmh3YjKioqICrkyRp32dQQVKRtn49nHIKfPYZlC8Pr70GbdoEXZUkqSiIi4ujWbNmzJgxg06dOgEQCoWYMWMG/fv33+41mzdvJjo6ukBfTEwMAOFweJfuKakQbPru9yMdpsK6ORAObf2sbL3Irgk1OkHFoyAq+m9uIkmSJO2Yt5e9TbcXu7F+83rKx5fn6TOe5sxDzwy6LEmSihSDCpKKrB9/hBNPhIULoWJFePNNOPLIoKuSJBUlAwcOpGfPnjRv3pyWLVsyZswYMjIyuPDCCwHo0aMH1atXJzU1FYCOHTsyevRomjZtmn/0w/Dhw+nYsWN+YOHf7impEITDkLYgsmvCD1Phty8Kfr5/08iuCclnQlJD8DfaJEmSVAjyQnncMvsWbp51M2HCNK3alEnnTKJuhbpBlyZJUpFjUEFSkbRiBbRrB8uXQ/Xq8PbbcOihQVclSSpqunTpwrp16xgxYgRr1qyhSZMmTJ8+nSpVqgCwcuXKAjsoDBs2jKioKIYNG8bq1aupVKkSHTt25Lbbbtvhe0raReEQrP/o93DCNNi0bOtnUdFQ6ZhIOKHGGVC2dlBVSpIkqZj6OeNnzn/xfN5Z/g4Alza7lDEnjyGhVELAlUmSVDRFhcPhcNBFFIb09HSSkpJIS0ujfPnyQZcjaQ9atCgSUvjxRzjoIHjnHahTJ+iqJEl7U3Ff+xX3+Uk7LC8L1r77+7EOL0Hm2q2fRcfDgSdFjnSo3hESKgVVpSRJu6W4r/2K+/xUMsxZOYcuk7vw48YfKR1bmkdOe4QLjrgg6LIkSdrn7Mzazx0VJBUp8+dD+/awfj0cdlhkJ4Vq1YKuSpIkSYUmZyP8+EbkSIcfX4ec9K2fxSZBtVMjRzoceDLElg2uTkmSJBV74XCYuz68i8EzBpMXzuPQiocy+dzJHFbpsKBLkySpyDOoIKnImDMHTj0V0tOhWTOYPh0qVgy6KkmSJO22zJ/hh5cj4YQ170Aoe+tnCVUjuyYknwmVj4eYuKCqlCRJUgny25bf6PVSL15e8jIA5zc6n4dPe5iycYZlJUkqDAYVJBUJb70FnTrBli1w7LHwyivgboGSJElF2ObVsPIFWDUV1n8A4dDWz8rWiwQTapwJFVMgKjq4OiVJklTizPtxHp0ndea7Dd8RFxPHfSffR59mfYiKigq6NEmSig2DCpL2eS++CF27QnY2nHwyTJkCpUsHXZUkSZJ22cpJ8PGFkJuxtW//I7eGE5IOAx8CS5IkaS8Lh8M8/NnDXP3m1WTnZVNnvzpMPncyRx54ZNClSZJU7BhUkLRPe/ZZuPBCCIWgc2cYNw7i3O1XkiSpaArlwf+GwsI7Iu0KzaFOd6hxBpSpFWxtkiRJKtE2ZW+izyt9eH7B8wB0atCJp854iv0S9gu2MEmSiimDCpL2WWPHQv/+kfcXXgiPPgql/FNLkiSpaMr6FT7oCmveirQPvQ4aj4JoF3iSJEkK1oKfF3DOpHNYvH4xpaJLcUe7Oxhw1ACPepAkaQ/yiZCkfVJqKgwZEnl/5ZVwzz0Q/f/s3XlcVPX+x/H3zLAJBu4YKlJalobaVUHNzIq08rpVpplLaJqlmWGWVm55i7qZ0TWTFrQ9zXKpLDfSysTIXSt3SzNBTQVFBYTv74/5OTnJnnBYXs/HYx535sz5fs/7DDPDJ+7H7+HSxAAAAGXTsU3Stz2ktL2So5IUPlMK6W11KgAAAEDvbnpXQ78YqtNnT6vOJXU05645ui74OqtjAQBQ7tGoAKBUMUYaO1Z64f9XAx43Tpo0iUsUAwAAlFm/zpZ+GChlnZb8LpPaz5eqNrM6FQAAACq405mnNeKrEXprw1uSpI4NOur9Hu+rpl9Ni5MBAFAx0KgAoNTIzpaGDZNiY52PX3xReuwxazMBAACgiLLPSpvGSr9McT6u3VG67iPJu5q1uQAAAFDh7Tq6Sz3n9tTGpI2yyaaJHSbqqeufksPusDoaAAAVBo0KAEqFzEwpMlL64APn6gmxsdKQIVanAgAAQJGk/yl931tKWu583HiM1PQ/En/4BQAAgMU+/flTDfxsoFLTU1XTt6Y+vPNDRVweYXUsAAAqHBoVAFjuzBmpd29p4ULJ4ZDee0+65x6rUwEAAKBIjm2Uvu0hpf0qOXylNm9LwT0tDgUAAICKLiMrQ08se0IxP8RIktoFt9PsO2erjn8da4MBAFBB0agAwFJpaVL37tLy5ZK3tzR3rtSli9WpAAAAUCS/fij9cL+UdVqq3EBqP1+qEmp1KgAAAFRw+1P26+5P7taa39dIkka3Ha1nb3pWng5Pi5MBAFBx0agAwDLHj0u33y4lJEh+ftJnn0k33WR1KgAAABRa9llp4xPStqnOx5feKl33oeRV1dpcAAAAqPC+2vmV+s3vpz9P/6kqPlX0Tvd31LVRV6tjAQBQ4dGoAMAShw5JHTtKmzZJVapIX30ltW5tdSoAAAAU2pnD0ve9peSvnY+bPCmFPiPZHdbmAgAAQIWWlZ2lCSsn6NnvnpUktbi0heb2nKvLql5mcTIAACDRqADAAvv3S7fcIm3fLtWqJS1bJjVtanUqAAAAFNrR9dK3PaRT+yQPP6n1O1LwnVanAgAAQAWXdDJJfT7toxW/rpAkPdTyIU3tNFXeHt4WJwMAAOfQqACgRO3aJd18s7Rvn1SvnrR8uXTllVanAgAAQKHtfV9KHCxlnZEqN5TaL5CqNLE6FQAAACq4b379Rr0/7a2kk0ny8/TTm13e1D2h91gdCwAA/A2NCgBKzJYtzss9JCVJV1zhbFIIDrY6FQAAAAolO1Pa8Li0Pcb5OOh2qe0HklcVK1MBAACggss22frv9//VU18/pWyTrSY1m+iTuz/RVTWusjoaAADIAY0KAEpEYqJ0663SsWPOyzwsXSoFBlqdCgAAAIVy5pC0qpd0aKXz8TXjpNCJks1uZSoAAABUcEdPH1X/+f21aOciSVL/Zv312u2vyc/Lz+JkAAAgNzQqACh2K1dKXbpIJ09K4eHSV19JVatanQoAAACF8uda6bs7pFP7JY/KUpt3pXo9rE4FAACACi7xQKLunnu3fkv5Td4Ob716+6sadO0g2Ww2q6MBAIA80KgAoFgtWiTddZd05ox0003SggXSJZdYnQoAAACFsucdKfEBKTtduuRKqf0CKeBqq1MBAACgAjPGaPqP0xW1JEqZ2ZlqULWBPrn7EzWv3dzqaAAAoABoVABQbObMkfr2lc6eda6o8PHHko+P1akAAABQYNmZ0vpR0o5pzsdB/5bavi95BVibCwAAABVaanqqBn8+WB//9LEk6c6r71Rc1zgF+FCnAgBQVtCoAKBYvPWWNGSIZIx0zz3SO+9Inp5WpwIAAECBnU6Wvr9bOvSt8/E1E6TQ8ZLNbm0uAAAAVGibkzfrro/v0s6jO+Vh99CUW6ZoRPgILvUAAEAZU6S/ME2fPl0hISHy8fFReHi4EhMTc903MzNTzzzzjBo0aCAfHx81a9ZMixcv/kdzAiidzpyRPvzQeYmHwYOdTQpDhkjvvUeTAgAAQJny54/SkpbOJgWPS6T2C6WmE2lSAAAAgKVmbZil8LfCtfPoTtXzr6fvIr/TI60foUkBAIAyqNB/ZZozZ46ioqI0YcIErV+/Xs2aNVOnTp106NChHPd/+umn9frrr2vatGn6+eefNXToUPXo0UMbNmwo8pwASpetW6VHHpGCgqR775VWrJBsNmnMGCk2VnI4rE4IAACAAts9S1p2vXTqd8m/kdQpUarb1epUAAAAqMBOZZ7SwIUDNfCzgTpz9oxubXir1j+wXq3rtrY6GgAAKCKbMcYUZkB4eLhatWqlV199VZKUnZ2tevXq6eGHH9aYMWMu2D8oKEhPPfWUhg0b5tp25513qlKlSnr//feLNGdOUlNTFRAQoJSUFPn7+xfmlAAUwcmT0pw50ptvSj/88Nf2evWkQYOkyEgpONi6fACA8q28137l/fxQSmVlSOsflXa+5nxcp6vU9j3Jk/cgAADFqbzXfuX9/FD8dvy5Q3d9fJe2HNoiu82uZzo8o7HXj5Wd1b4AACh1ClP7eRRm4oyMDK1bt05jx451bbPb7YqIiFBCQkKOY9LT0+Xj4+O2rVKlSlq1alWR5wRgDWOkH3+U3npL+ugjZ7OCJHl4SF27Oi/3cMstrKAAAABQ5pxOklb1lA47/ztNoZOka57mUg8AAACw1Mc/faxBnw3SyYyTCvQL1Ed3fqQbL7vR6lgAAOAiKNRfnY4cOaKsrCwFBga6bQ8MDFRSUlKOYzp16qSpU6dq586dys7O1rJlyzRv3jwdPHiwyHNKzgaI1NRUtxuA4nHsmDRtmtS8uRQe7lxF4eRJ6YorpBdekH7/Xfr0U+nWW2lSAAAAKHOO/CAtbuFsUvD0l274XAodT5MCAABl0PTp0xUSEiIfHx+Fh4crMTEx1307dOggm812wa1z586ufU6ePKnhw4erbt26qlSpkho3bqzY2NiSOBVUcBlZGRrx1Qj1+qSXTmacVPv67bXhgQ00KQAAUI4UakWFonjllVc0ePBgXXXVVbLZbGrQoIEiIyM1c+bMfzRvdHS0Jk2adJFSAvg7Y6Rvv3U2JXzyiZSe7tzu4yPddZd0//1S+/aSzWZtTgAAAPwDu+OkHx+SsjMk/6ul9gsk/yutTgUAAIpgzpw5ioqKUmxsrMLDwxUTE6NOnTpp+/btqlWr1gX7z5s3TxkZGa7Hf/75p5o1a6aePXu6tkVFRenrr7/W+++/r5CQEC1dulQPPfSQgoKC1LVr1xI5L1Q8vx3/TXd/crcSDzgbbcZcN0aTb5osD3ux/98ZAACgBBXqn8jUqFFDDodDycnJbtuTk5NVu3btHMfUrFlTCxYsUFpamn777Tdt27ZNlStX1uWXX17kOSVp7NixSklJcd32799fmFMBkIvkZOm//5UaNZI6dJA++MDZpNC0qXNVhT/+kN57T7rhBpoUAAAAyqysDCnxQemH+51NCnV7SJ1+oEkBAIAybOrUqRo8eLAiIyNdKx/4+vrm+g/GqlWrptq1a7tuy5Ytk6+vr1ujwurVqzVgwAB16NBBISEhGjJkiJo1a5bnSg3AP7FoxyJd+/q1SjyQqKo+VfXFPV8oOiKaJgUAAMqhQjUqeHl5qUWLFoqPj3dty87OVnx8vNq0aZPnWB8fH9WpU0dnz57Vp59+qm7duv2jOb29veXv7+92A1A0WVnSV19Jd94p1a0rPfGEtHOnVLmyNHiwlJgobdwoDR8uVa1qdVoAAAD8I6cPSvE3SrtiJdmkpv+Rrv9E8rzE6mQAAKCIMjIytG7dOkVERLi22e12RUREKCEhoUBzxMXFqXfv3vLz83Nta9u2rT777DMdOHBAxhitWLFCO3bsUMeOHS/6OaBiy8jK0OPLHte/P/q3jp05prA6YdrwwAZ1vrJz/oMBAECZVOg2xKioKA0YMEAtW7ZUWFiYYmJilJaWpsjISElS//79VadOHUVHR0uSfvjhBx04cEDNmzfXgQMHNHHiRGVnZ+vxxx8v8JwAise+fdLMmc7b+YuStG7tvLRDr17OZgUAAACUE4cTpFV3OpsVPAOkth9KdW63OhUAAPiHjhw5oqysLAUGBrptDwwM1LZt2/Idn5iYqK1btyouLs5t+7Rp0zRkyBDVrVtXHh4estvtevPNN9W+fftc50pPT1f6uWuISkpNTS3k2aCi2fnnTt3z6T1ad3CdJOnhsIc1peMUeTm8LE4GAACKU6EbFXr16qXDhw9r/PjxSkpKUvPmzbV48WJXEbxv3z7Z7X8t1HDmzBk9/fTT2rNnjypXrqzbb79d7733nqpUqVLgOQFcPJmZ0uefS2++KS1ZIhnj3F61qtS/vzRokBQaam1GAAAAFINdb0hrh0vZmVJAY+n6BZL/FVanAgAApUBcXJxCQ0MVFhbmtn3atGlas2aNPvvsM9WvX1/ffvuthg0bpqCgILfVG84XHR2tSZMmlURslHHGGL2z6R0N/3K40jLTVK1SNcV1jVP3q7pbHQ0AAJQAmzHn/m/Ksi01NVUBAQFKSUnhMhBADnbulN56S3r7benQob+233ij8/IOPXpIPj6WxQMAoFDKe+1X3s8PJSwrXVr7sLT7TefjendKrWdxqQcAAEqJi1H7ZWRkyNfXV5988om6d+/u2j5gwAAdP35cCxcuzHVsWlqagoKC9Mwzz+iRRx5xbT99+rQCAgI0f/58de781/L7999/v37//XctXrw4x/lyWlGhXr161LZwk3ImRUMXDdXsrbMlSR1COui9Hu+prn9di5MBAIB/ojC1baFXVABQdpw+Lc2b51w94Ztv/toeGChFRjpXT2jY0Lp8AAAAKGan/pC+u1P6c40km9TsWanxGMlmszoZAAC4iLy8vNSiRQvFx8e7GhWys7MVHx+v4cOH5zl27ty5Sk9PV9++fd22Z2ZmKjMz0231XElyOBzKzs7OdT5vb295e3sX7URQISTsT1CfeX306/Ff5bA59MyNz+iJ656Qw+6wOhoAAChBNCoA5dDmzc7VE957Tzp+3LnNbpduu026/36pc2fJ09PSiAAAAChuh7+XvrtLOpMkeVaRrvtICrrV6lQAAKCYREVFacCAAWrZsqXCwsIUExOjtLQ0RUZGSpL69++vOnXqKDo62m1cXFycunfvrurVq7tt9/f31w033KDRo0erUqVKql+/vr755hu9++67mjp1aomdF8qPrOwsRa+K1sSVE5VlsnRZlcv04Z0fqnXd1lZHAwAAFqBRASgnTpyQZs92NigkJv61vX5958oJ990n1atnWTwAAACUFGOkXa9L60ZI2ZlSwDVS+wXSJQ2sTgYAAIpRr169dPjwYY0fP15JSUlq3ry5Fi9erMDAQEnSvn37LlgdYfv27Vq1apWWLl2a45yzZ8/W2LFjde+99+ro0aOqX7++nn32WQ0dOrTYzwfly/6U/eo3v5+++c257Guf0D567fbXFOATYHEyAABgFZsxxlgd4mLgOr6oiIxxNiW8+aazSSEtzbnd01Pq1k0aPFi6+WbJwappAIByprzXfuX9/FCMss5Ia4dLu+Ocj4PvlsLjJM/K1uYCAAC5Ku+1X3k/P+Rv/i/zNeizQTp25pgqe1XW9Nunq1/TfrJxOTIAAMqdwtR+rKgAlEFHj0rvv+9sUNi69a/tjRo5L+3Qv79Uq5Z1+QAAAGCBUwek7+6Q/kyUbHapWbR09WiJPwADAADAAqcyTylqSZReX/e6JKlVUCt9eOeHalitocXJAABAaUCjAlCGrFkjTZsmffqplJ7u3ObjI919t7NBoV07/g4NAABQIR36TlrVUzqTLHlVla6bLV3a0epUAAAAqKA2J2/WPZ/eo58P/yxJerzt45p802R5ObwsTgYAAEoLGhWAMiA9XXrqKemll/7a1ry589IOffpIVapYlQwAAACWMkba+Zq0bqRkzkpVmkrt50uVL7c6GQAAACogY4xeTXxVo5eNVnpWui6tfKne7fGuIi6PsDoaAAAoZexWBwCQt59+ksLD/2pS6NdPWrtWWr9eeughmhQAAPinpk+frpCQEPn4+Cg8PFyJiYm57tuhQwfZbLYLbp07d3btc/LkSQ0fPlx169ZVpUqV1LhxY8XGxpbEqaCiyUqXEgdLa4c7mxTq95Y6rqZJAQAAAJY4nHZYXT7qohGLRyg9K13/vvLf2jR0E00KAAAgR6yoAJRSxkivvio9/rh05oxUo4Y0c6bUpYvVyQAAKD/mzJmjqKgoxcbGKjw8XDExMerUqZO2b9+uWrVqXbD/vHnzlJGR4Xr8559/qlmzZurZs6drW1RUlL7++mu9//77CgkJ0dKlS/XQQw8pKChIXbt2LZHzQgVwOlladad0+HvJZpeavyBdNYrrgAEAAMASy3YvU/8F/ZV0MkneDm9N6ThFw1oNk436FAAA5IIVFYBSKClJ6txZGjHC2aRw663Sli00KQAAcLFNnTpVgwcPVmRkpGvlA19fX82cOTPH/atVq6batWu7bsuWLZOvr69bo8Lq1as1YMAAdejQQSEhIRoyZIiaNWuW50oNQKEc3SAtaeVsUvAMkG5YJF39GE0KAAAAKHEZWRl6fNnj6vh+RyWdTFLjmo2VODhRw8OG06QAAADyRKMCUMp8/rnUtKn01VeSt7c0bZr05ZdS7dpWJwMAoHzJyMjQunXrFBHx1zKkdrtdERERSkhIKNAccXFx6t27t/z8/Fzb2rZtq88++0wHDhyQMUYrVqzQjh071LFjx4t+DqiA9s2Vll0nndovXXKl1OkHKehWq1MBAACgAtr5505dN/M6vbj6RUnS0BZD9ePgH9U0sKnFyQAAQFnApR+AUuLUKWnUKOncJaybNpU+/FBq0sTaXAAAlFdHjhxRVlaWAgMD3bYHBgZq27Zt+Y5PTEzU1q1bFRcX57Z92rRpGjJkiOrWrSsPDw/Z7Xa9+eabat++fa5zpaenKz093fU4NTW1kGeDcs9kS1smSVufcT6+tJN03WzJq4qlsQAAAFDxGGP07qZ3NezLYUrLTFNVn6qK6xqnHlf3sDoaAAAoQ2hUAEqBdeuke++Vtm93Po6Kkp57zrmiAgAAKJ3i4uIUGhqqsLAwt+3Tpk3TmjVr9Nlnn6l+/fr69ttvNWzYMAUFBbmt3nC+6OhoTZo0qSRioyzKPCkl9Jd+n+98fNUoqfkLkt1hbS4AAABUOClnUvTgogf10daPJEk31L9B79/xvur617U4GQAAKGtoVAAslJUlTZkiPf20dPasFBQkvfOOlMv/hwEAAC6iGjVqyOFwKDk52W17cnKyaudzzaW0tDTNnj1bzzzzjNv206dP68knn9T8+fPVuXNnSVLTpk21ceNGTZkyJddGhbFjxyoqKsr1ODU1VfXq1SvKaaG8Ofmr9G1X6fgWye4lhb0hXT7A6lQAAACogNb8vkZ9Pu2jvcf3ymFzaFKHSRrTbowcNNACAIAisFsdAKio9u+Xbr5ZGjPG2aRwxx3S5s00KQAAUFK8vLzUokULxcfHu7ZlZ2crPj5ebdq0yXPs3LlzlZ6err59+7ptz8zMVGZmpux29zLb4XAoOzs71/m8vb3l7+/vdgOU/I20pJWzScEnULp5JU0KAAAAKHFZ2Vl69ttn1W5mO+09vlchVUL0XeR3eqr9UzQpAACAImNFBcACc+ZIQ4dKx49Lfn7S//4nRUZKNpvVyQAAqFiioqI0YMAAtWzZUmFhYYqJiVFaWpoiIyMlSf3791edOnUUHR3tNi4uLk7du3dX9erV3bb7+/vrhhtu0OjRo1WpUiXVr19f33zzjd59911NnTq1xM4L5cDO16W1wyVzVqrWQmq/QPJlOV0AAACUrN9Tf1e/+f208teVkqR7rrlHMzrPUIBPgLXBAABAmUejAlCCUlOl4cOl995zPg4Lkz74QGrY0NpcAABUVL169dLhw4c1fvx4JSUlqXnz5lq8eLECAwMlSfv27btgdYTt27dr1apVWrp0aY5zzp49W2PHjtW9996ro0ePqn79+nr22Wc1dOjQYj8flAPZmdK6kdLO15yP6/eWwuMkD19LYwEAAKDiWbBtgQZ9NkhHTx+Vn6efpt8+Xf2b9ZeNf20FAAAuApsxxlgd4mJITU1VQECAUlJSWCoXpdLq1VLfvtLevZLdLj31lDRunOTpaXUyAADKnvJe+5X380Mu0v+UVvWUkldIsknNnpUaj2HZLQAAyrnyXvuV9/Mrj05lntKoJaMUuy5WktTi0hb66M6PdEX1KyxOBgAASrvC1H6sqAAUs8xMafJk6dlnpexsKSREev996brrrE4GAACAUuP4VumbrlLaXsmjstT2A6luV6tTAQAAoILZkrxFvT/trZ8P/yxJGt12tP5z03/k5fCyOBkAAChvaFQAitGuXc5VFH74wfm4Xz9p2jQpgEu4AQAA4JzfP5NW3yudPSlVvlxq/5lUpYnVqQAAAFCBGGM0/cfpemzpY0rPSlftyrX1bvd3dUuDW6yOBgAAyikaFYBiYIz09tvSww9LaWnOxoTYWKl3b6uTAQAAoNQwRvo5Wtr0tCQjBd4otZsreVe3OhkAAAAqkMNphzXws4H6YscXkqTOV3TWrG6zVNOvpsXJAABAeUajAnCR/fmn9MAD0qefOh/fcIP07rtScLC1uQAAAFCKnD0lrRko7ZvjfHzFMKnFy5Ld09pcAAAAqFCW71mu/vP76+DJg/J2eOvFW17U8LDhstlsVkcDAADlHI0KwEUUHy/17y/98Yfk4SFNniyNHi05HFYnAwAAQKmRtl/6trt0bL1k85Bavipd8YDVqQAAAFCBZGRlaNzX4/Ti6hdlZHR1jas1+67ZahrY1OpoAACggqBRAbgI0tOlp56SXnrJ+bhRI+mDD6QWLazNBQAAgFLmcIL0XQ/pTLLkXUO6/lOpVnurUwEAAKAC2XV0l+759B6t/WOtJGnIv4bo5Vtflq+nr8XJAABARUKjAvAP/fSTdO+90qZNzsdDh0pTpkh+ftbmAgAAQCmz520p8QEpO0Oq0lRqv1CqHGJ1KgAAAFQQxhi9t/k9DftymE5mnFRVn6p6q+tbuuPqO6yOBgAAKiAaFYAiMkaaPt15aYczZ6QaNaS4OKlrV6uTAQAAoFTJPitteFza/rLzcd0eUpt3Jc/K1uYCAABAhZGanqoHFz2oD7d8KElqX7+93u/xvuoF1LM4GQAAqKhoVACKIClJGjhQ+uor5+Nbb5VmzZJq17Y2FwAAAEqZjOPS972lg0ucj6+ZIIWOl2x2S2MBAACg4ljz+xr1+bSP9h7fK4fNoYkdJmpsu7Fy2B1WRwMAABUYjQpAIX3xhbNJ4fBhydtbevFFafhwyWazOhkAAABKldTt0jddpRM7JIev1OYdKfguq1MBAACggsjKztIL37+g8SvGK8tkKaRKiD644wO1rdfW6mgAAAA0KgAFdeqU9Nhj0owZzsdNm0offCBdc421uQAAAFAK/bHYuZJCZorkGyzdsFCq2tzqVAAAAKggDqQeUN/5fbXy15WSpN7X9FZs51gF+ARYGwwAAOD/0agAFMD69dK990rbtjkfR0VJzz4r+fhYmwsAAACljDHStqnSxsclky3VvE66fp7kU8vqZAAAAKggFm5bqIGfDdTR00fl5+mnV29/VQOaDZCNJWEBAEApQqMCkIesLOmll6Snn5YyM6VLL5XeeUe65RarkwEAAKDUyTojJQ6V9r7jfNxgkNRyuuTwtjYXAAAAKoTTmac1aukozVjrXBK2xaUt9OGdH+rK6ldanAwAAOBCNCoAudi/X+rfX1q50vn4jjukN96Qqle3NBYAAABKo9MHpW/vkP5cI9kc0r9elq4cLvGv1gAAAFACtiRv0T2f3qOfDv8kSXqszWN69uZn5eXwsjgZAABAzmhUAHIwZ440dKh0/Ljk5yf9739SZCR/ZwYAAEAO/lwrfdtdOn1A8qoqtftYqh1hdSoAAABUAMYYvfbjaxq1dJTSs9IV6Beod3u8q44NOlodDQAAIE80KgDnSU2VHn5Yevdd5+OwMOn996UrrrA2FwAAAEqpX2dLP0Q6L/vgf7XUfqHkT/EIAACA4nfk1BENXDhQn+/4XJJ0+xW3a1a3WarlV8viZAAAAPmjUQH4f6tXS337Snv3Sna79OST0vjxkqen1ckAAABQ6phsafM46afnnI+DOkttP5C8AqzNBQAAgAoh6WSSWr3ZSr+n/i4vh5f+G/FfjQgfIRtLwgIAgDKCRgVUeGfPSpMnS//5j5SdLYWESO+9J7VrZ3UyAAAAlEqZqdLqvtIB579cU+MnpKbPSnaHtbkAAABQYcSsidHvqb+rQdUG+uTuT9S8dnOrIwEAABQKjQqo0Hbvlu69V/rhB+fjvn2lV1+VAviHcAAAAMjJid3St92klJ8ku7cUHidddq/VqQAAAFCBnEg/odi1sZKkqZ2m0qQAAADKJBoVUCEZI73zjvTww9LJk87GhNhYqXdvq5MBAACg1Er6WlrVU8o4KlW6VGq/UKreyupUAAAAqGBmbpiplPQUXVn9Sv37yn9bHQcAAKBIaFRAhXP8uDR4sPTJJ87H7ds7L/UQHGxpLAAAAJRWxkg7X5PWPSKZLKlaK6n9Ask3yOpkAAAAqGDOZp9VzA8xkqRHWz8qu81ubSAAAIAiolEBFcqxY1JEhLR+veThIU2eLI0eLTm4nDAAAABykpUhrXtY2vWG83FIXynsDcmjkrW5AAAAUCHN/2W+fj3+q6pXqq7+zfpbHQcAAKDIaFRAhXF+k0LNmtKiRVIrVuoFAABAbs4cllbdJR36VpJNav6CdPVjks1mdTIAAABUQMYYvZTwkiRpWKth8vX0tTgRAABA0bEuFCqEY8ekW275q0nh669pUgAAAEAejm2WlrRyNil4+ks3fCE1Hk2TAgAAKNWmT5+ukJAQ+fj4KDw8XImJibnu26FDB9lstgtunTt3dtvvl19+UdeuXRUQECA/Pz+1atVK+/btK+5TQQ5W71+tHw78IG+Ht4aFDbM6DgAAwD9CowLKvXNNCuvW/dWkcM01VqcCAABAqbV/nrSsrZT2m1S5odRxjVTndqtTAQAA5GnOnDmKiorShAkTtH79ejVr1kydOnXSoUOHctx/3rx5OnjwoOu2detWORwO9ezZ07XP7t271a5dO1111VVauXKlNm/erHHjxsnHx6ekTgvnObeaQr+m/VTLr5bFaQAAAP4ZLv2Acu3YMaljR2eTQo0aNCkAAAAgDyZb2vofacsE5+Pat0jt5kheVa3NBQAAUABTp07V4MGDFRkZKUmKjY3VokWLNHPmTI0ZM+aC/atVq+b2ePbs2fL19XVrVHjqqad0++2367///a9rW4MGDYrpDJCXnX/u1IJtCyRJUW2irA0DAABwEbCiAsqt48edTQpr1zqbFFasoEkBAAAAuTibJq3q9VeTQqNHpA5f0qQAAADKhIyMDK1bt04RERGubXa7XREREUpISCjQHHFxcerdu7f8/PwkSdnZ2Vq0aJGuvPJKderUSbVq1VJ4eLgWLFhQHKeAfMSsiZGR0e1X3K6ra15tdRwAAIB/jEYFlEvHjzsv93CuSYGVFAAAAJCrtN+kpddJ+z+R7J5S+FtSixjJzgJ0AACgbDhy5IiysrIUGBjotj0wMFBJSUn5jk9MTNTWrVt1//33u7YdOnRIJ0+e1PPPP69bb71VS5cuVY8ePXTHHXfom2++yXWu9PR0paamut3wz/x56k/N2jhLkjSqzSiL0wAAAFwc/OUN5c7fmxTi46XQUKtTAQAAoFQ6tEr67g4p/bDkU0u6fp5U8zqrUwEAAJSouLg4hYaGKiwszLUtOztbktStWzc9+uijkqTmzZtr9erVio2N1Q033JDjXNHR0Zo0aVLxh65AYtfG6vTZ02peu7luDLnR6jgAAAAXBSsqoFw5/3IP1as7mxSaNrU6FQAAAEqlXW9JX9/kbFKo2lzq9CNNCgAAoEyqUaOGHA6HkpOT3bYnJyerdu3aeY5NS0vT7NmzNWjQoAvm9PDwUOPGjd22X3311dq3b1+u840dO1YpKSmu2/79+wt5Njhf+tl0vfrjq5Kkx9o8JpvNZnEiAACAi4NGBZQbx49LnTpJP/7obFL4+muaFAAAAJCD7LPS2kekxMFSdqYU3FO6ZZXkF2x1MgAAgCLx8vJSixYtFB8f79qWnZ2t+Ph4tWnTJs+xc+fOVXp6uvr27XvBnK1atdL27dvdtu/YsUP169fPdT5vb2/5+/u73VB0H275UEknk1TXv67ubnK31XEAAAAuGi79gHIhJcXZpJCYSJMCAAAA8pB+VPq+l5S03Pm46WSpyVMS/zINAACUcVFRURowYIBatmypsLAwxcTEKC0tTZGRkZKk/v37q06dOoqOjnYbFxcXp+7du6t69eoXzDl69Gj16tVL7du314033qjFixfr888/18qVK0vilCo8Y4xeSnhJkjQibIQ8HZ4WJwIAALh4aFRAmZeS4rzcw7kmBS73AAAAgByl/yktaS2d3CV5+Elt3pPq9bA6FQAAwEXRq1cvHT58WOPHj1dSUpKaN2+uxYsXKzAwUJK0b98+2e3uC+xu375dq1at0tKlS3Ocs0ePHoqNjVV0dLRGjBihRo0a6dNPP1W7du2K/XwgLdm9RD8d/kmVvSprcIvBVscBAAC4qIp06Yfp06crJCREPj4+Cg8PV2JiYp77x8TEqFGjRqpUqZLq1aunRx99VGfOnHE9f+LECY0cOVL169dXpUqV1LZtW/34449FiYYKJqcmhWbNrE4FAACAUumnaGeTgm+wdMtqmhQAAEC5M3z4cP32229KT0/XDz/8oPDwcNdzK1eu1Ntvv+22f6NGjWSM0S233JLrnAMHDtTOnTt1+vRpbdy4Ud26dSuu+Pibc6sp3H/t/ariU8XaMAAAABdZoRsV5syZo6ioKE2YMEHr169Xs2bN1KlTJx06dCjH/T/88EONGTNGEyZM0C+//KK4uDjNmTNHTz75pGuf+++/X8uWLdN7772nLVu2qGPHjoqIiNCBAweKfmYo986/3EO1ajQpAAAAIA+nD0o7pzvvh8VKVVmCCwAAAKXXpqRNWr5nuew2ux5p/YjVcQAAAC66QjcqTJ06VYMHD1ZkZKQaN26s2NhY+fr6aubMmTnuv3r1al133XXq06ePQkJC1LFjR91zzz2uVRhOnz6tTz/9VP/973/Vvn17NWzYUBMnTlTDhg01Y8aMf3Z2KLfONSn88IOzSeHrr2lSAAAAQB5+ek7KOiPVaCtdeqvVaQAAAIA8TV0zVZJ0V+O7FFIlxNowAAAAxaBQjQoZGRlat26dIiIi/prAbldERIQSEhJyHNO2bVutW7fO1ZiwZ88effnll7r99tslSWfPnlVWVpZ8fHzcxlWqVEmrVq0q1MmgYkhJkW699a8mBVZSAAAAQJ7S9km73nDebzpZstmszQMAAADk4Y8Tf+ijLR9Jkh5r85jFaQAAAIqHR2F2PnLkiLKyshQYGOi2PTAwUNu2bctxTJ8+fXTkyBG1a9dOxhidPXtWQ4cOdV364ZJLLlGbNm00efJkXX311QoMDNRHH32khIQENWzYMNcs6enpSk9Pdz1OTU0tzKmgjEpNdTYprFnjbFJYvlxq3tzqVAAAACjVtv5Hys6QAm+Uat9kdRoAAAAgT9N+mKbM7ExdH3y9WtVpZXUcAACAYlHoSz8U1sqVK/Xcc8/ptdde0/r16zVv3jwtWrRIkydPdu3z3nvvyRijOnXqyNvbW//73/90zz33yG7PPV50dLQCAgJct3r16hX3qcBiqanOyz2sWSNVrepsUrj2WqtTAQAAoFQ7sVvaM8t5v+nkvPcFAAAALHYy46Ri18VKkka1GWVxGgAAgOJTqEaFGjVqyOFwKDk52W17cnKyateuneOYcePGqV+/frr//vsVGhqqHj166LnnnlN0dLSys7MlSQ0aNNA333yjkydPav/+/UpMTFRmZqYuv/zyXLOMHTtWKSkprtv+/fsLcyooY/7epBAfT5MCAAAACmDrM5I5K116q1TzOqvTAAAAAHmauWGmjp85riuqXaEujbpYHQcAAKDYFKpRwcvLSy1atFB8fLxrW3Z2tuLj49WmTZscx5w6deqClREcDockyRjjtt3Pz0+XXnqpjh07piVLlqhbt265ZvH29pa/v7/bDeXT+Zd7oEkBAAAABZayTfr1fef9ps9YmwUAAADIR1Z2lmLWxEiSHm39qOy2Yl8QGQAAwDKFrnSioqL05ptv6p133tEvv/yiBx98UGlpaYqMjJQk9e/fX2PHjnXt36VLF82YMUOzZ8/W3r17tWzZMo0bN05dunRxNSwsWbJEixcvdj1/44036qqrrnLNiYorNVW67TYpIYHLPQAAgOIxffp0hYSEyMfHR+Hh4UpMTMx13w4dOshms11w69y5s9t+v/zyi7p27aqAgAD5+fmpVatW2rdvX3GfCv5uy0TJZEt1u0nVubYvAAAASrf52+Zr7/G9ql6pugY0H2B1HAAAgGLlUdgBvXr10uHDhzV+/HglJSWpefPmWrx4sQIDAyVJ+/btc1tB4emnn5bNZtPTTz+tAwcOqGbNmurSpYueffZZ1z4pKSkaO3asfv/9d1WrVk133nmnnn32WXl6el6EU0RZdeKEs0lh9eq/mhT+9S+rUwEAgPJkzpw5ioqKUmxsrMLDwxUTE6NOnTpp+/btqlWr1gX7z5s3TxkZGa7Hf/75p5o1a6aePXu6tu3evVvt2rXToEGDNGnSJPn7++unn36Sj49PiZwT/t+xzdK+Oc77oaymAAAAgNLvpYSXJEkPtnxQvp6+FqcBAAAoXjbz9+svlFGpqakKCAhQSkoKl4EoB06ccF7ugSYFAACQk4tV+4WHh6tVq1Z69dVXJTkva1avXj09/PDDGjNmTL7jY2JiNH78eB08eFB+fn6SpN69e8vT01PvvfdekXNR214E3/aQfl8gBd8ttZtjdRoAAIBclffar7yf38Wyev9qXTfzOnk5vLRv5D4FVg60OhIAAEChFab24yJXKHXOb1KoUoUmBQAAUDwyMjK0bt06RUREuLbZ7XZFREQoISGhQHPExcWpd+/eriaF7OxsLVq0SFdeeaU6deqkWrVqKTw8XAsWLMhznvT0dKWmprrd8A8cXedsUrDZpdCJVqcBAAAA8nVuNYV+TfvRpAAAACoEGhVQqpx/uQeaFAAAQHE6cuSIsrKyXJcwOycwMFBJSUn5jk9MTNTWrVt1//33u7YdOnRIJ0+e1PPPP69bb71VS5cuVY8ePXTHHXfom2++yXWu6OhoBQQEuG716tUr+olB2jTO+b/175UCrrY2CwAAAJCP3Ud3a/4v8yVJUW2iLE4DAABQMmhUQKlxrknh++//alJo0cLqVAAAADmLi4tTaGiowsLCXNuys7MlSd26ddOjjz6q5s2ba8yYMfr3v/+t2NjYXOcaO3asUlJSXLf9+/cXe/5y6/Bq6eBXks0hhY63Og0AAACQr5fXvCwjo9sa3qbGNRtbHQcAAKBE0KiAUuHECen22/9qUli2jCYFAABQvGrUqCGHw6Hk5GS37cnJyapdu3aeY9PS0jR79mwNGjTogjk9PDzUuLH7Hxevvvpq7du3L9f5vL295e/v73ZDEW3+/9UULo+ULmlobRYAAAAgH0dPH9WsjbMkSaPajLI4DQAAQMmhUQGWO9eksGqVFBDgbFJo2dLqVAAAoLzz8vJSixYtFB8f79qWnZ2t+Ph4tWnTJs+xc+fOVXp6uvr27XvBnK1atdL27dvdtu/YsUP169e/eOGRs+QVUvLXkt1TuuZpq9MAAAAA+YpdG6tTmafULLCZbrrsJqvjAAAAlBgPqwOgYjt50r1JYflymhQAAEDJiYqK0oABA9SyZUuFhYUpJiZGaWlpioyMlCT1799fderUUXR0tNu4uLg4de/eXdWrV79gztGjR6tXr15q3769brzxRi1evFiff/65Vq5cWRKnVHEZ89dqCg2GSH40hgAAAKB0Sz+brmmJ0yQ5V1Ow2WwWJwIAACg5NCrAMidPSrfdxkoKAADAOr169dLhw4c1fvx4JSUlqXnz5lq8eLECAwMlSfv27ZPd7r4I2fbt27Vq1SotXbo0xzl79Oih2NhYRUdHa8SIEWrUqJE+/fRTtWvXrtjPp0I7uFQ6/L3k8JGaPGl1GgAAACBfH239SEknk1TnkjrqdU0vq+MAAACUKBoVYIm/r6SwbJnUqpXVqQAAQEU0fPhwDR8+PMfncloFoVGjRjLG5DnnwIEDNXDgwIsRDwVhjLT5/y/1cMVDkm+QtXkAAACAfBhjNDVhqiRpRPgIeTm8LE4EAABQsuz57wJcXCdPSp07S99952xSWLqUJgUAAAD8Awc+l46ulTz8pMZPWJ0GAAAAyNeyPcu05dAWVfaqrCEthlgdBwAAoMTRqIASda5J4dtvJX9/Z5NCWJjVqQAAAFBmmWxp8zjn/StHSD61rM0DAAAAFMCU1VMkSYOuHaQqPlWsDQMAAGABGhVQYtLS3JsUli2jSQEAAAD/0P5PpeObJU9/6erHrE4DAAAA5Gtz8mYt27NMdptdj4Q/YnUcAAAAS9CogBKRlibdfjtNCgAAALiIsrOkzROc96+KkryrWZsHAAAAKICpCVMlSXdefacuq3qZxWkAAACsQaMCit3fV1Lgcg8AAAC4KH77SEr9RfKqKjUaaXUaAAAAIF9/nPhDH275UJI0qs0oi9MAAABYh0YFFKtzTQrffONsUliyRAoPtzoVAAAAyrzsTGnLROf9qx+XvAIsjQMAAAAUxKuJryozO1PtgtspvC5/KAUAABUXjQooNmlp0r//7WxSuOQSZ5NC69ZWpwIAAEC5sPdd6eRuybumdOVwq9MAAAAA+UrLSFPs2lhJrKYAAABAowKKxbkmhZUrnU0KS5fSpAAAAICLJCtd2vKM836TsZJnZWvzAAAAAAUwa+MsHTtzTA2rNVSXK7tYHQcAAMBSNCrgojt1SurShSYFAAAAFJPdcdKpfVKlIKnhUKvTAAAAAPnKys7Sy2teliQ92vpROewOixMBAABYi0YFXFSnTjlXUlixgss9AAAAoBicPS399B/n/SZPSR6VrM0DAAAAFMCCbQu059geVatUTfc1v8/qOAAAAJajUQEXTU5NCm3aWJ0KAAAA5cquWOn0Qck3WGowyOo0AAAAQIG8lPCSJOnBlg/K19PX4jQAAADWo1EBF8W5yz2ca1JYvJgmBQAAAFxkmSeln6Kd90PHSw5va/MAAAAABZCwP0EJvyfIy+Gl4WHDrY4DAABQKtCogH/sXJPC119LlSs7mxTatrU6FQAAAMqdHa9K6Yelyg2ky/pbnQYAAAAokHOrKfQN7avalWtbnAYAAKB0oFEB/8ipU1LXrn81KSxZQpMCAAAAikFGivTLf533QydKdk9L4wAAAAAFsefYHs3fNl+SFNUmyuI0AAAApQeNCiiyc00K8fE0KQAAAKCYbY+RMo5J/ldL9e+xOg0AAABQIDFrYpRtsnVrw1vVpFYTq+MAAACUGjQqoEhOnZK6dfurSYHLPQAAAKDYpB+Vtk113m86SbI7rM0DAAAAFMDR00cVtyFOkjSqzSiL0wAAAJQuNCqg0E6fdjYpLF/ubFL46ivpuuusTgUAAIBy65cpUmaqVKWpVO9Oq9MAAAAABfL62td1KvOUmgY21c2X3Wx1HAAAgFKFRgUUyunTzss9LF8u+fk5mxTatbM6FQAAAMqtM4ek7a847zedLNn4TxgAAACUfhlZGZqWOE2SczUFm81mcSIAAIDShb/yocD+3qSweDFNCgAAAChmP78gZZ2SqrWS6nSxOg0AAABQIB9t+UgHTx5U0CVB6n1Nb6vjAAAAlDo0KqBAzr/cA00KAAAAKBGn/pB2vua833SyxL9CAwAAyNP06dMVEhIiHx8fhYeHKzExMdd9O3ToIJvNdsGtc+fOOe4/dOhQ2Ww2xcTEFFP68sMYo5cSXpIkjQgbIS+Hl8WJAAAASh8aFZCv06el7t2lZcu43AMAAABK0E/PSVlnpJrXSZd2tDoNAABAqTZnzhxFRUVpwoQJWr9+vZo1a6ZOnTrp0KFDOe4/b948HTx40HXbunWrHA6HevbsecG+8+fP15o1axQUFFTcp1EuLN+zXFsObZGfp5+GtBhidRwAAIBSiUYF5Ck93dmksHTpX00K119vdSoAAACUe2m/SbvfcN5v+h9WUwAAAMjH1KlTNXjwYEVGRqpx48aKjY2Vr6+vZs6cmeP+1apVU+3atV23ZcuWydfX94JGhQMHDujhhx/WBx98IE9Pz5I4lTJvSsIUSdKgawepaqWqFqcBAAAonWhUQK6MkYYN+6tJ4csvaVIAAABACdn6Hyk7Uwq8SQrsYHUaAACAUi0jI0Pr1q1TRESEa5vdbldERIQSEhIKNEdcXJx69+4tPz8/17bs7Gz169dPo0ePVpMmTS567vJoS/IWLd29VHabXSNbj7Q6DgAAQKnlYXUAlF6vvSbFxUl2u/TJJ1L79lYnAgAAQIVwYpe0Z5bzftPJ1mYBAAAoA44cOaKsrCwFBga6bQ8MDNS2bdvyHZ+YmKitW7cqLi7ObfsLL7wgDw8PjRgxosBZ0tPTlZ6e7nqcmppa4LHlwdQ1UyVJd1x9hy6repnFaQAAAEovVlRAjr75Rho50nn/+eelW2+1NA4AAAAqki3PSCZLuvQ2qWZbq9MAAACUe3FxcQoNDVVYWJhr27p16/TKK6/o7bfflq0Ql+GKjo5WQECA61avXr3iiFwqHTxxUB9s/kCSNKrNKIvTAAAAlG40KuACv/0m3XWXdPasdM890mOPWZ0IAAAAFUbKL9Kv7zvvN2M1BQAAgIKoUaOGHA6HkpOT3bYnJyerdu3aeY5NS0vT7NmzNWjQILft3333nQ4dOqTg4GB5eHjIw8NDv/32m0aNGqWQkJBc5xs7dqxSUlJct/379xf5vMqaVxNfVWZ2pq6rd51a121tdRwAAIBSjUYFuDl1SurRQzpyRLr2Wumtt6RCNEwDAAAA/8yWiZKMVLe7VK2FxWEAAADKBi8vL7Vo0ULx8fGubdnZ2YqPj1ebNm3yHDt37lylp6erb9++btv79eunzZs3a+PGja5bUFCQRo8erSVLluQ6n7e3t/z9/d1uFUFaRppmrJ0hidUUAAAACsLD6gAoPYyR7r9f2rBBqllTWrBA8vW1OhUAAAAqjGObpH0fS7JJTZ+xOg0AAECZEhUVpQEDBqhly5YKCwtTTEyM0tLSFBkZKUnq37+/6tSpo+joaLdxcXFx6t69u6pXr+62vXr16hds8/T0VO3atdWoUaPiPZky6O2Nb+vYmWNqULWBujbqanUcAACAUo9GBbhMmSJ99JHk4SF98okUHGx1IgAAAFQom8c7/zf4bqlKqLVZAAAAyphevXrp8OHDGj9+vJKSktS8eXMtXrxYgYGBkqR9+/bJbndfYHf79u1atWqVli5dakXkciMrO0tT10yVJD3a+lE57A6LEwEAAJR+NCpAkrRkiTRmjPP+K69I7dtbmwcAAAAVzJ8/Sgc+k2x2KXSi1WkAAADKpOHDh2v48OE5Prdy5coLtjVq1EjGmALP/+uvvxYxWfm2cPtC7Tm2R1V9quq+5vdZHQcAAKBMsOe/C8q7Xbuk3r2l7GznpR8efNDqRAAAAKhwNo9z/m9IXyngKmuzAAAAAIXwUsJLkqQHWz4oPy8/i9MAAACUDTQqVHAnTkjduknHj0tt2kivvirZbFanAgAAQIVyaJV0cIlk85CuGW91GgAAAKDA1vy+Rqv3r5aXw0vDw3JezQIAAAAXolGhAsvOlvr1k37+WQoKkj79VPL2tjoVAAAAKpxzqylcHild0sDaLAAAAEAhnFtN4d7Qe3XpJZdanAYAAKDsoFGhAnvmGWnhQsnLS5o3T7qUOhoAAAAlLelr6dBKye4lXfO01WkAAACAAtt7bK/m/TJPkhTVJsriNAAAAGULjQoV1Pz50qRJzvuvvy6Fh1ubBwAAABWQMdLm/29OaDhE8gu2Ng8AAABQCDFrYpRtstWpQSddU+saq+MAAACUKTQqVEA//ST17++8P2KEdN99lsYBAABARXVwsXQkQXL4SE2etDoNAAAAUGDHTh9T3IY4SdKoNqMsTgMAAFD20KhQwRw9KnXrJp08Kd14ozRlitWJAAAAUCEZI20e57x/xTCpEtchAwAAQNnx+rrXlZaZpqaBTRVxeYTVcQAAAMocGhUqkLNnpXvukXbvlkJCpI8/ljw9rU4FAACACun3hdLRdZKHn9T4CavTAAAAAAWWkZWhaYnTJElRraNks9ksTgQAAFD20KhQgYwdKy1dKvn6SgsWSDVqWJ0IAAAAFZLJ/ms1hUaPSD41rc0DAAAAFMLsrbP1x4k/FHRJkO4JvcfqOAAAAGUSjQoVxAcf/HWZh1mzpGbNrM0DAACACmzfXCllq+QZIF39mNVpAAAAgAIzxuilhJckSQ+HPSwvh5fFiQAAAMomGhUqgHXrpPvvd94fO1a6+25r8wAAAJQm06dPV0hIiHx8fBQeHq7ExMRc9+3QoYNsNtsFt86dO+e4/9ChQ2Wz2RQTE1NM6cug7LPSlgnO+1dFSV5Vrc0DAAAAFEL83nhtTt4sP08/PdDiAavjAAAAlFk0KpRzhw5JPXpIZ85It98uTZ5sdSIAAIDSY86cOYqKitKECRO0fv16NWvWTJ06ddKhQ4dy3H/evHk6ePCg67Z161Y5HA717Nnzgn3nz5+vNWvWKCgoqLhPo2z59UMpdbvkVU26aqTVaQAAAIBCObeawsBrB6pqJZpuAQAAiopGhXIsI0O66y5p/37pyiulDz+UHA6rUwEAAJQeU6dO1eDBgxUZGanGjRsrNjZWvr6+mjlzZo77V6tWTbVr13bdli1bJl9f3wsaFQ4cOKCHH35YH3zwgTw9PUviVMqG7Exp6yTn/caPS57+1uYBAAAACmHroa1avGux7Da7RrYeaXUcAACAMo1GhXJs5Ejpu+8kf39p4UIpIMDqRAAAAKVHRkaG1q1bp4iICNc2u92uiIgIJSQkFGiOuLg49e7dW35+fq5t2dnZ6tevn0aPHq0mTZoUaJ709HSlpqa63cqlPW9LJ/dIPrWkK4dbnQYAAAAolKkJUyVJPa7qocurXm5xGgAAgLKtSI0KhbmOryTFxMSoUaNGqlSpkurVq6dHH31UZ86ccT2flZWlcePG6bLLLlOlSpXUoEEDTZ48WcaYosSDpDfflGbMkGw26YMPpKuusjoRAABA6XLkyBFlZWUpMDDQbXtgYKCSkpLyHZ+YmKitW7fq/vvvd9v+wgsvyMPDQyNGjChwlujoaAUEBLhu9erVK/DYMiMrXdr6/9cha/yk5OGX9/4AAABAKZJ0MkkfbPlAkjSqzSiL0wAAAJR9HoUdcO46vrGxsQoPD1dMTIw6deqk7du3q1atWhfs/+GHH2rMmDGaOXOm2rZtqx07dui+++6TzWbT1KnODtQXXnhBM2bM0DvvvKMmTZpo7dq1ioyMVEBAQKH+wAun1aulYcOc9ydPlv79b2vzAAAAlEdxcXEKDQ1VWFiYa9u6dev0yiuvaP369bLZbAWea+zYsYqKinI9Tk1NLX/NCrvelE7tlyrVka54wOo0AAAAQKG8mviqMrIy1LZeW7Wp18bqOAAAAGVeoVdUKOx1fFevXq3rrrtOffr0UUhIiDp27Kh77rnHbRWG1atXq1u3burcubNCQkJ01113qWPHjvmu1IALHTgg3XmnlJkp3XWX9OSTVicCAAAonWrUqCGHw6Hk5GS37cnJyapdu3aeY9PS0jR79mwNGjTIbft3332nQ4cOKTg4WB4eHvLw8NBvv/2mUaNGKSQkJNf5vL295e/v73YrV86ekn561nn/mqclh4+1eQAAAIBCSMtI04y1MySxmgIAAMDFUqhGhaJcx7dt27Zat26dq+lgz549+vLLL3X77be77RMfH68dO3ZIkjZt2qRVq1bptttuyzVLhbmObyGcOSP16CElJUmhodKsWc5LPwAAAOBCXl5eatGiheLj413bsrOzFR8frzZt8v4XUnPnzlV6err69u3rtr1fv37avHmzNm7c6LoFBQVp9OjRWrJkSbGcR5mwc4Z0JknyC5EuH2h1GgAAAKBQ3tn0jo6ePqoGVRuoW6NuVscBAAAoFwp16Ye8ruO7bdu2HMf06dNHR44cUbt27WSM0dmzZzV06FA9ed4/9R8zZoxSU1N11VVXyeFwKCsrS88++6zuvffeXLNER0dr0qRJhYlfrhkjDR0q/fijVK2atHChVLmy1akAAABKt6ioKA0YMEAtW7ZUWFiYYmJilJaWpsjISElS//79VadOHUVHR7uNi4uLU/fu3VW9enW37dWrV79gm6enp2rXrq1GjRoV78mUVpknpJ+fd96/Zrzk8LI2DwAAAFAIWdlZennNy5Kkka1HymF3WJwIAACgfCj0pR8Ka+XKlXruuef02muvaf369Zo3b54WLVqkyZMnu/b5+OOP9cEHH+jDDz/U+vXr9c4772jKlCl65513cp137NixSklJcd32799f3KdSqv3vf9I770gOh/Txx9Jll1mdCAAAoPTr1auXpkyZovHjx6t58+bauHGjFi9e7GrM3bdvnw4ePOg2Zvv27Vq1atUFl31ALnZMk9KPSJdcIV3Wz+o0AAAAQKF8tv0z7Tq6S1V9qiqyeaTVcQAAAMqNQq2oUJTr+I4bN079+vXT/fffL0kKDQ1VWlqahgwZoqeeekp2u12jR4/WmDFj1Lt3b9c+v/32m6KjozVgwIAc5/X29pa3t3dh4pdb8fHSqP+/NNqUKdLNN1ubBwAAoCwZPny4hg8fnuNzK1euvGBbo0aNZIwp8Py//vprEZOVAxnHpZ9fdN4PnSjZC/WfHwAAAIDlXkp4SZI0tOVQ+Xn5WZwGAACg/CjUigpFuY7vqVOnZLe7H8bhcC6Pde4PvLntk52dXZh4FdLevdLdd0tZWVK/ftIjj1idCAAAAPh/216WMo9LAY2l4F5WpwEAAAAK5Yfff9D3+7+Xp91Tw8Nybm4GAABA0RT6nzQV9jq+Xbp00dSpU3XttdcqPDxcu3bt0rhx49SlSxdXw0KXLl307LPPKjg4WE2aNNGGDRs0depUDRw48CKeavmTliZ17y4dPSq1bCm9/rpks1mdCgAAAJCU/qezUUGSQp+RuJYvAAAAyphzqync2/ReBV0SZHEaAACA8qXQjQq9evXS4cOHNX78eCUlJal58+YXXMf3/NURnn76adlsNj399NM6cOCAatas6WpMOGfatGkaN26cHnroIR06dEhBQUF64IEHNH78+ItwiuWTMVJkpLR5sxQYKM2fL1WqZHUqAAAA4P/98qJ09oRUtblUr4fVaQAAAIBC2Xtsrz795VNJUlTrKIvTAAAAlD82U5gL7JZiqampCggIUEpKivz9/a2OU+yee0566inJ01NasUK67jqrEwEAAJSc8l77lfnzO50sfXa5lHVKuuFzqc6/rU4EAABQapX52i8fZfX8Ri4eqVd+eEUdG3TUkr5LrI4DAABQJhSm9rPn+SxKpUWLpKefdt5/9VWaFAAAAFDK/Py8s0mhergU1NnqNAAAAEChHDt9TG+tf0uSNKrNKIvTAAAAlE80KpQx27ZJffo4L/0wdKg0ZIjViQAAAIDznPpd2jnDeb/pZMlmszYPAAAAUEhvrHtDaZlpCq0Vqlsuv8XqOAAAAOUSjQplSEqK1L27lJoqtWsnvfKK1YkAAACAv/npOSk7Xap5vVQ7wuo0AAAAQKFkZGXof4n/kyRFtYmSjcZbAACAYkGjQhmRlSXde6+0fbtUt670ySeSl5fVqQAAAIDznPxV2u1cIlfN/sNqCgAAAChz5mydoz9O/KHalWvrnmvusToOAABAuUWjQhkxfry0aJHk4yMtWCAFBlqdCAAAAPibrZOl7EznSgq12ludBgAAACgUY4xeSnhJkjQibIS8PbwtTgQAAFB+0ahQBsydKz33nPP+W29JLVpYmwcAAAC4QOpOae87zvtNJ1ubBQAAACiCr/d+rU3Jm+Tr6asHWj5gdRwAAIByjUaFUm7TJum++5z3R41yXv4BAAAAKHW2TpJMlhTUWarR2uo0AAAAQKGdW01hYPOBqlapmsVpAAAAyjcaFUqxI0ek7t2lU6ekW26Rnn/e6kQAAABADo7/JP36ofN+02eszQIAAAAUwU+HftJXu76STTaNbD3S6jgAAADlHo0KpdTZs1KvXtKvv0qXXy7Nni15eFidCgAAAMjBlomSjFTvDqnav6xOAwAAABTa1ISpkqQeV/dQg2oNLE4DAABQ/tGoUEqNHi19/bXk5yctXChVY6UxAAAAlEbHNkr7P5Fkk0InWZ0GAAAAKLSkk0l6f8v7kqRRbUZZnAYAAKBioFGhFHr3XSkmxnn/vfeka66xNA4AAACQu83jnf9bv7dUhcIVAAAAZc/0xOnKyMpQ67qt1bZeW6vjAAAAVAg0KpQyiYnSkCHO++PHSz16WJsHAAAAyNWRH6QDn0s2uxQ60eo0AAAAQKGdyjylGWtnSJIea/OYxWkAAAAqDhoVSpGkJOmOO6T0dKlrV2nCBKsTAQAAAHk4t5rCZf0l/yutzQIAAABNnz5dISEh8vHxUXh4uBITE3Pdt0OHDrLZbBfcOnfuLEnKzMzUE088odDQUPn5+SkoKEj9+/fXH3/8UVKnUyLe2fiO/jz9py6verm6X9Xd6jgAAAAVBo0KpUR6unTnndKBA9LVVzsv+WDnpwMAAIDS6tB3UtJSyeYhXTPe6jQAAAAV3pw5cxQVFaUJEyZo/fr1atasmTp16qRDhw7luP+8efN08OBB123r1q1yOBzq2bOnJOnUqVNav369xo0bp/Xr12vevHnavn27unbtWpKnVayyTbZeXvOyJGlk+Eg57A6LEwEAAFQcHlYHgGSM9PDD0urVUkCAtGCB5O9vdSoAAAAgF8ZIm5923m8wSKp8mbV5AAAAoKlTp2rw4MGKjIyUJMXGxmrRokWaOXOmxowZc8H+1apVc3s8e/Zs+fr6uhoVAgICtGzZMrd9Xn31VYWFhWnfvn0KDg4upjMpOZ9v/1w7j+5UFZ8qirw20uo4AAAAFQr/Zr8UiI2V3nxTstmk2bOlK1k1FwAAAKVZcrx06FvJ7i1d87TVaQAAACq8jIwMrVu3ThEREa5tdrtdERERSkhIKNAccXFx6t27t/z8/HLdJyUlRTabTVWqVPmnkUuFKQlTJElDWwxVZa/KFqcBAACoWFhRwWLffiuNGOG8Hx0t3XqrtXkAAACAPBkjbRrnvN/wAcm3rrV5AAAAoCNHjigrK0uBgYFu2wMDA7Vt27Z8xycmJmrr1q2Ki4vLdZ8zZ87oiSee0D333CP/PJaDTU9PV3p6uutxampqAc6g5CUeSNSqfavkaffUw+EPWx0HAACgwmFFBQvt2yfddZd09qzUu7f0+ONWJwIAAADy8ceX0p9rJEclqclYq9MAAADgIoiLi1NoaKjCwsJyfD4zM1N33323jDGaMWNGnnNFR0crICDAdatXr15xRP7HXkp4SZLUJ7SPgi4JsjgNAABAxUOjgkVOnZJ69JAOH5aaN5fi4pyXfgAAAABKLWOkzf+/msKVw6VKta3NAwAAAElSjRo15HA4lJyc7LY9OTlZtWvnXbOlpaVp9uzZGjRoUI7Pn2tS+O2337Rs2bI8V1OQpLFjxyolJcV1279/f+FOpgT8evxXffLzJ5KkqDZRFqcBAAComGhUsIAx0uDB0vr1Uo0a0oIFkq+v1akAAACAfPw+Xzq2QfKoLF3NcmAAAAClhZeXl1q0aKH4+HjXtuzsbMXHx6tNmzZ5jp07d67S09PVt2/fC54716Swc+dOLV++XNWrV883i7e3t/z9/d1upc0ra15RtsnWLZffoqaBTa2OAwAAUCF5WB2gInrpJenDDyWHQ5o7V6pf3+pEAAAAQD6ys6TN4533G42UfGpYGgcAAADuoqKiNGDAALVs2VJhYWGKiYlRWlqaIiMjJUn9+/dXnTp1FB0d7TYuLi5O3bt3v6AJITMzU3fddZfWr1+vL774QllZWUpKSpIkVatWTV5eXiVzYhfZ8TPH9daGtyRJo9qMsjgNAABAxUWjQglbskR64gnn/ZgYqUMHK9MAAAAABbTvYynlJ8mzinQ1f9AFAAAobXr16qXDhw9r/PjxSkpKUvPmzbV48WIFBgZKkvbt2ye73X2B3e3bt2vVqlVaunTpBfMdOHBAn332mSSpefPmbs+tWLFCHcroHzbfXPemTmac1DW1rlHHBh2tjgMAAFBh0ahQgnbtknr3lrKzpYEDpWHDrE4EAAAAFED2WWnLBOf9q0dJXlUsjQMAAICcDR8+XMOHD8/xuZUrV16wrVGjRjLG5Lh/SEhIrs+VVRlZGXrlh1ckSVGto2Sz2SxOBAAAUHHZ898FF8OJE1K3btLx41Lr1tJrr0nUwQAAACgTfn1fOrFT8q4uNXrE6jQAAABAkXz808c6cOKAaleurT6hfayOAwAAUKHRqFACsrOl/v2ln3+WLr1U+vRTydvb6lQAAABAAWRlSFsmOe9f/YTkeYm1eQAAAIAiMMbopYSXJEkPhz0sbw/+QAsAAGAlGhVKwOTJ0oIFkpeXNH++FBRkdSIAAACggPbMktJ+lXwCpSu5dhkAAADKphW/rtDGpI3y9fTV0JZDrY4DAABQ4dGoUMwWLpQmTnTej42VwsMtjQMAAAAUXNYZaetk5/0mT0oevtbmAQAAAIro3GoKkc0jVa1SNYvTAAAAgEaFYvTzz1Lfvs77Dz8sRUZamwcAAAAolF1vSKcPSL51pYZDrE4DAAAAFMnPh3/Wlzu/lE02jWw90uo4AAAAEI0KxebYMalbN+nkSenGG6WXXrI6EQAAAFAIZ09JPz3nvN/kacnhY20eAAAAoIimJkyVJHW/qrsaVmtocRoAAABINCoUi6wsqU8fadcuqX596eOPJU9Pq1MBAAAAhbBjunQmWfK7TLqcpcEAAABQNiWfTNZ7m9+TJI1qM8riNAAAADiHRoVi8OST0uLFUqVK0oIFUo0aVicCAAAACiHzhPTLC877oeMlh5e1eQAAAIAimv7jdGVkZSi8Trja1mtrdRwAAAD8PxoVLrKPPpL++1/n/VmzpObNLY0DAAAAFN72V6T0P6VLrpRC+lqdBgAAACiSU5mn9NqPr0mSHmv7mGw2m8WJAAAAcA6NChfRhg3SoEHO+088IfXqZW0eAAAAoNAyjkm/THHeD50o2T0sjQMAAAAU1bub3tWfp//UZVUuU4+relgdBwAAAOehUeEiOXRI6t5dOn1auu026dlnrU4EAAAAFMEvU6XMFCngGqk+nbcAAAAom7JNtl5e87IkaWTrkXLYHRYnAgAAwPloVLgIMjOlnj2lffukK66QPvxQclD3AgAAoKw5c0TaHuO833SSZOM/FwAAAFA2fbHjC+34c4eq+FTRwGsHWh0HAAAAf8NfHi+CRx+Vvv1WuuQSaeFCqUoVqxMBAAAARfDLf6WzJ6Wq10p1WRoXAAAAZdeU1c7LmT3Q4gFV9qpscRoAAAD8HY0K/1BcnDR9uvP+++9LV19tbR4AAAAUzvTp0xUSEiIfHx+Fh4crMTEx1307dOggm812wa1z586SpMzMTD3xxBMKDQ2Vn5+fgoKC1L9/f/3xxx8ldTpFdzpJ2vGq837TyZLNZm0eAAAAoIh+PPCjvtv3nTzsHno47GGr4wAAACAHNCr8AwkJ0oMPOu8/84zUtau1eQAAAFA4c+bMUVRUlCZMmKD169erWbNm6tSpkw4dOpTj/vPmzdPBgwddt61bt8rhcKhnz56SpFOnTmn9+vUaN26c1q9fr3nz5mn79u3qWhYKxZ+ipazTUvXWUtDtVqcBAAAAiuylhJckSfdcc4/q+NexOA0AAABy4mF1gLLqwAHpjjukzEzn/z71lNWJAAAAUFhTp07V4MGDFRkZKUmKjY3VokWLNHPmTI0ZM+aC/atVq+b2ePbs2fL19XU1KgQEBGjZsmVu+7z66qsKCwvTvn37FBwcXExn8g+l7Zd2xTrvN2M1BQAAAJRdvx3/TZ/8/IkkaVSbURanAQAAQG5YUaGIpk6VkpKka66R3nlHsvNKAgAAlCkZGRlat26dIiIiXNvsdrsiIiKUkJBQoDni4uLUu3dv+fn55bpPSkqKbDabqlSp8k8jF58d/5OyM6RaN0iBN1udBgAAACiyGWtnKMtkKeLyCDWr3czqOAAAAMgFKyoU0QsvSJUqSZGRUuXKVqcBAABAYR05ckRZWVkKDAx02x4YGKht27blOz4xMVFbt25VXFxcrvucOXNGTzzxhO655x75+/vnul96errS09Ndj1NTUwtwBhdR08mSbz2pWitWUwAAAECZNv6G8aofUF9NajWxOgoAAADyQKNCEXl4SP/5j9UpAAAAYJW4uDiFhoYqLCwsx+czMzN19913yxijGTNm5DlXdHS0Jk2aVBwxC8bhIzUaYd3xAQAAgIvE19NXD7Z60OoYAAAAyAcXLAAAAECFVKNGDTkcDiUnJ7ttT05OVu3atfMcm5aWptmzZ2vQoEE5Pn+uSeG3337TsmXL8lxNQZLGjh2rlJQU123//v2FOxkAAAAAAAAAKENoVAAAAECF5OXlpRYtWig+Pt61LTs7W/Hx8WrTpk2eY+fOnav09HT17dv3gufONSns3LlTy5cvV/Xq1fPN4u3tLX9/f7cbAAAAAAAAAJRXXPoBAAAAFVZUVJQGDBigli1bKiwsTDExMUpLS1NkZKQkqX///qpTp46io6PdxsXFxal79+4XNCFkZmbqrrvu0vr16/XFF18oKytLSUlJkqRq1arJy8urZE4MAAAAAAAAAEoxGhUAAABQYfXq1UuHDx/W+PHjlZSUpObNm2vx4sUKDAyUJO3bt092u/siZNu3b9eqVau0dOnSC+Y7cOCAPvvsM0lS8+bN3Z5bsWKFOnToUCznAQAAAAAAAABlic0YY6wOcTGkpqYqICBAKSkpLJULAABQzpX32q+8nx8AAAD+Ut5rv/J+fgAAAPhLYWo/e57PAgAAAAAAAAAAAAAAXEQ0KgAAAAAAAAAAAAAAgBJDowIAAAAAAAAAAAAAACgxNCoAAAAAAAAAAAAAAIASU6RGhenTpyskJEQ+Pj4KDw9XYmJinvvHxMSoUaNGqlSpkurVq6dHH31UZ86ccT0fEhIim812wW3YsGFFiQcAAAAAAAAAAAAAAEopj8IOmDNnjqKiohQbG6vw8HDFxMSoU6dO2r59u2rVqnXB/h9++KHGjBmjmTNnqm3bttqxY4fuu+8+2Ww2TZ06VZL0448/KisryzVm69atuuWWW9SzZ89/cGoAAAAAAAAAAAAAAKC0KfSKClOnTtXgwYMVGRmpxo0bKzY2Vr6+vpo5c2aO+69evVrXXXed+vTpo5CQEHXs2FH33HOP2yoMNWvWVO3atV23L774Qg0aNNANN9xQ9DMDAAAAAAAAAAAAAAClTqEaFTIyMrRu3TpFRET8NYHdroiICCUkJOQ4pm3btlq3bp2rMWHPnj368ssvdfvtt+d6jPfff18DBw6UzWbLNUt6erpSU1PdbgAAAAAAAAAAAAAAoHQr1KUfjhw5oqysLAUGBrptDwwM1LZt23Ic06dPHx05ckTt2rWTMUZnz57V0KFD9eSTT+a4/4IFC3T8+HHdd999eWaJjo7WpEmTChMfAAAAAAAAAAAAAABYrNCXfiislStX6rnnntNrr72m9evXa968eVq0aJEmT56c4/5xcXG67bbbFBQUlOe8Y8eOVUpKiuu2f//+4ogPAAAAAAAAAAAAAAAuokKtqFCjRg05HA4lJye7bU9OTlbt2rVzHDNu3Dj169dP999/vyQpNDRUaWlpGjJkiJ566inZ7X/1Svz2229avny55s2bl28Wb29veXt7ux4bYySJS0AAAABUAOdqvnM1YHlDbQsAAFBxUNsCAACgvChMbVuoRgUvLy+1aNFC8fHx6t69uyQpOztb8fHxGj58eI5jTp065daMIEkOhyPHgLNmzVKtWrXUuXPnwsSSJJ04cUKSVK9evUKPBQAAQNl04sQJBQQEWB3joqO2BQAAqHiobQEAAFBeFKS2LVSjgiRFRUVpwIABatmypcLCwhQTE6O0tDRFRkZKkvr37686deooOjpaktSlSxdNnTpV1157rcLDw7Vr1y6NGzdOXbp0cTUsSM6Gh1mzZmnAgAHy8Ch0LAUFBWn//v265JJLZLPZCj2+KFJTU1WvXj3t379f/v7+JXJMK5S38yzL51OWspfWrKUll5U5SvrYF+N4xZ25OOa/WHP+k3msGlsajl/U8VYdt6yPt+I7zRijEydO5Hu5sLKK2rb4lLfzLMvnU5ayl9aspSUXtW3Jz1HS81PbUttWpPHUthcftW3xKW/nWZbPpyxlL61ZS0suatuSn6Ok56e2pbatSONLe21b6I6AXr166fDhwxo/frySkpLUvHlzLV68WIGBgZKkffv2ua2g8PTTT8tms+npp5/WgQMHVLNmTXXp0kXPPvus27zLly/Xvn37NHDgwMJGkiTZ7XbVrVu3SGP/KX9//1L1C724lLfzLMvnU5ayl9aspSWXlTlK+tgX43jFnbk45r9Yc/6TeawaWxqOX9TxVh23rI8v6e+V8vivzc6hti1+5e08y/L5lKXspTVraclFbVvyc5T0/NS21LYVaTy17cVDbVv8ytt5luXzKUvZS2vW0pKL2rbk5yjp+altqW0r0vjSWtsWfukCScOHD8/1Ug8rV650P4CHhyZMmKAJEybkOWfHjh3L7XXYAAAAAAAAAAAAAACAkz3/XQAAAAAAAAAAAAAAAC4OGhX+AW9vb02YMEHe3t5WRylW5e08y/L5lKXspTVracllZY6SPvbFOF5xZy6O+S/WnP9kHqvGlobjF3W8Vcct6+NLy3cr/pmK8nMsb+dZls+nLGUvrVlLSy5q25Kfo6Tnp7altq1I40vLdyv+mYrycyxv51mWz6csZS+tWUtLLmrbkp+jpOentqW2rUjjS8t3a25shustAAAAAAAAAAAAAACAEsKKCgAAAAAAAAAAAAAAoMTQqAAAAAAAAAAAAAAAAEoMjQoAAAAAAAAAAAAAAKDE0KiQi4kTJ8pms7ndrrrqqjzHzJ07V1dddZV8fHwUGhqqL7/8soTSFty3336rLl26KCgoSDabTQsWLHA9l5mZqSeeeEKhoaHy8/NTUFCQ+vfvrz/++CPPOYvyWl1MeZ2TJCUnJ+u+++5TUFCQfH19deutt2rnzp15zjlv3jy1bNlSVapUkZ+fn5o3b6733nvvouaOjo5Wq1atdMkll6hWrVrq3r27tm/f7rZPhw4dLnhthw4dmue8EydO1FVXXSU/Pz9VrVpVERER+uGHH4qcc8aMGWratKn8/f3l7++vNm3a6KuvvnI9f+bMGQ0bNkzVq1dX5cqVdeeddyo5OTnPOU+ePKnhw4erbt26qlSpkho3bqzY2NiLmqsor93f9z93e/HFFwuV7fnnn5fNZtPIkSNd2wr7OhX185jTsc8xxui2227L8XNSlGP//Vi//vprrq/h3LlzXeNy+s7I6ebn51fg95QxRuPHj1flypXz/D564IEH1KBBA1WqVEk1a9ZUt27dtG3btjznnjBhwgVzXn755a7nC/Ney+/cx48fr379+ql27dry8/PTv/71L3366aeu8QcOHFDfvn1VvXp1VapUSaGhoXrjjTfcvgPvvvtuXXrppapUqZIiIiJc33c5jV27dq0k6X//+58CAgJkt9vlcDhUs2bNC37muY2fPn26QkJC5OPjo/DwcCUmJmro0KGy2WyKiYkpVPZp06apa9euCggIkJ+fn1q1aqV9+/blOn7OnDm68847FRISIpvNpuuvv97te6V69eo5vs7Dhg2TJL322msKCgqS3W6XzWZTrVq13L5Hzs3791urVq0UGBgoDw8P+fn5ycfHR8HBwRoxYoRSUlIKNP78n9GOHTsu+GzmNva2225Tx44dXecWFhYmPz8/+fv7q3379jp9+nS+x/fz85PNZtOsWbNyfL9lZWVp3Lhxuuyyy1SpUiU1aNBAkydPljHG9burcuXKrnP38fFxe6/lJaf3C0oGtS21LbWtE7UttS21LbVtXuOpbaltqW3LBmpbaltqWydqW2pbaltq27zGU9tS25aJ2tYgRxMmTDBNmjQxBw8edN0OHz6c6/7ff/+9cTgc5r///a/5+eefzdNPP208PT3Nli1bSjB1/r788kvz1FNPmXnz5hlJZv78+a7njh8/biIiIsycOXPMtm3bTEJCggkLCzMtWrTIc87CvlYXW17nlJ2dbVq3bm2uv/56k5iYaLZt22aGDBligoODzcmTJ3Odc8WKFWbevHnm559/Nrt27TIxMTHG4XCYxYsXX7TcnTp1MrNmzTJbt241GzduNLfffvsFuW644QYzePBgt9c2JSUlz3k/+OADs2zZMrN7926zdetWM2jQIOPv728OHTpUpJyfffaZWbRokdmxY4fZvn27efLJJ42np6fZunWrMcaYoUOHmnr16pn4+Hizdu1a07p1a9O2bds85xw8eLBp0KCBWbFihdm7d695/fXXjcPhMAsXLrxouYry2p2/78GDB83MmTONzWYzu3fvLnCuxMREExISYpo2bWoeeeQR1/bCvk5F+Tzmduxzpk6dam677bYLPidFOXZOxzp79uwFr+GkSZNM5cqVzYkTJ1xj//6dsWnTJrN161bX4w4dOhhJ5r333ivwe+r55583AQEBplevXqZBgwamY8eOpl69embv3r1u30evv/66+eabb8zevXvNunXrTJcuXUy9evXM2bNnc5375ptvNna73cyaNcvEx8ebjh07muDgYHP69GljTOHea+fOfdOmTa7b1q1bXe+1du3amVatWpkffvjB7N6920yePNnY7Xazfv16c/ToUVO/fn1z3333mR9++MHs2bPHLFmyxMTFxbl9B/r6+poFCxaYTZs2ma5du5rLLrvM/PHHHzmO3bVrl5k9e7bx9PQ0jRs3Ni+99JLp2bOnqVy5srn22mtdP/Pcjh0TE2O8vLzMzJkzzU8//WQGDx5sfH19TZMmTUxQUJB5+eWX8xz/9+yVK1c2o0ePNuvXrze7du0yCxcuNMnJybmOnzdvnnnsscfMRx99ZCpVqmRq1Kjh9r1it9vN22+/7fq5LFu2zEgyK1asMMYY07BhQ1O1alXz+uuvm08++cTUrFnTOBwO8/vvvxtjjDl06JDbz/XceD8/P/PKK6+Ym2++2YSFhZm6deuar776ylxxxRXmzjvvdP288xp//s+oWrVqplOnTm6fzdzGjh071kyaNMmMGTPGSDIPP/yw2bp1q9m2bZuZM2eOOXPmTK7Hf/bZZ40k88wzzxhJplmzZjm+34YNG2aqV69uvvjiC7N3714zd+5cU7lyZfPKK6+4fnc9+uijpnLlyqZFixamdu3apnPnzuayyy5zfS5yMnv27AveL1WqVDHJycm5jsHFQ21LbUtt60RtS21LbUttS21LbUttW/ZR21LbUts6UdtS21LbUttS21LblvXalkaFXEyYMME0a9aswPvffffdpnPnzm7bwsPDzQMPPHCRk108+f3SM8b5S02S+e2333Ldp7CvVXH6+zlt377dSHIVQcYYk5WVZWrWrGnefPPNQs197bXXmqeffvpiRb3AoUOHjCTzzTffuLbdcMMNORYuhZGSkmIkmeXLl//DhH+pWrWqeeutt8zx48eNp6enmTt3ruu5X375xUgyCQkJuY5v0qSJeeaZZ9y2/etf/zJPPfXURcllzMV57bp162ZuuummAu9/4sQJc8UVV5hly5a5Hb+or9Pf5fV5zO3Y52zYsMHUqVPHHDx4sECf/byOnd+xzte8eXMzcOBAt215fWccP37c2Gw2c80117i25fdaZWdnm9q1a5sXX3zRNffx48eNt7e3+eijj/I8r02bNhlJZteuXbnO7efnZy699FK3jOfPXZj3Wm7nfu695ufnZ959912356pVq2befPNN88QTT5h27drlOnd2draRZAYMGHBB1q5du+Y6NiwszAwbNsz1OCsrywQFBZmHHnrI9TPP7dh/H7tv3z5jt9vNyJEjTf369V0Fb37ZjXF+f99www05PleQ8Z6enua2225z2/b375VHHnnENGjQwGRnZ+f4udy6dauRZMaNG5fjMUaMGOH649Y5578fPv74Y+Pl5WUyMzMLPP7bb781ksyMGTPy/Gyen90Y52dLktmwYUOer0tuc0gyPj4+Ob7fQkNDL/jc3nHHHebee+81xrh/5s797lq0aFG+n7nc3mvR0dEFPgcUHbWtE7XtX6ht/0JtmzNq2wtR27qjts0dtS21LYoXta0Tte1fqG3/Qm2bM2rbC1HbuqO2zR21LbVtceLSD3nYuXOngoKCdPnll+vee+91LWOSk4SEBEVERLht69SpkxISEoo7ZrFKSUmRzWZTlSpV8tyvMK9VSUpPT5ck+fj4uLbZ7XZ5e3tr1apVBZrDGKP4+Hht375d7du3L5acklzLzlSrVs1t+wcffKAaNWrommuu0dixY3Xq1KkCz5mRkaE33nhDAQEBatas2T/OmJWVpdmzZystLU1t2rTRunXrlJmZ6fbev+qqqxQcHJzne79t27b67LPPdODAARljtGLFCu3YsUMdO3a8KLnO+SevXXJyshYtWqRBgwYVeMywYcPUuXPnC74Livo6/V1en8fcji1Jp06dUp8+fTR9+nTVrl27wMfL7dh5Het869at08aNG3N8DXP7zli+fLmMMRoxYoRr3/xeq7179yopKcmVZ+fOnbr66qtls9k0ceLEXL+P0tLSNGvWLF122WWqV69ernOnpaXp2LFjrrwPPfSQmjVr5panMO+1v5/7unXrXO+1tm3bas6cOTp69Kiys7M1e/ZsnTlzRh06dNBnn32mli1bqmfPnqpVq5auvfZavfnmm25ZJbl91gMCAhQeHq7vvvsux7EZGRlat26d28/SbrcrIiJCGzZscP3Mczr2jBkz3MZmZ2drwIABatGihfbs2eN2zvllz87OliQFBQWpU6dOqlWrlsLDw13LaeU3XpK8vb21devWXL9XMjIy9P7772vgwIGy2Ww5fi6Dg4MlKcel+jIyMvTee+8pKytLt9xyywWvcUJCglJSUuTv7y8PD48CjT916pQeeOABNW7cWL/88ssFY84fe372Q4cOaePGjZKkAQMGKDAwUDfccEOev9f+PockNWrUKMf32y233KL4+Hjt2LFDkrRp0yatWrVKt912myT3z9y5313BwcGu1yG34+f2XivrtVJZQm1LbStR256P2jZv1LbuqG1zRm1LbUtt60RtW/KobaltJWrb81Hb5o3a1h21bc6obaltqW2dSrS2LfZWiDLqyy+/NB9//LHZtGmTWbx4sWnTpo0JDg42qampOe7v6elpPvzwQ7dt06dPN7Vq1SqJuEWifLrzTp8+bf71r3+ZPn365DlPYV+r4vT3c8rIyDDBwcGmZ8+e5ujRoyY9Pd08//zzRpLp2LFjnnMdP37c+Pn5GQ8PD+Pt7W3i4uKKLXdWVpbp3Lmzue6669y2v/7662bx4sVm8+bN5v333zd16tQxPXr0yHe+zz//3Pj5+RmbzWaCgoJMYmLiP8q3efNm4+fnZxwOhwkICDCLFi0yxjiXK/Py8rpg/1atWpnHH3881/nOnDlj+vfvbyQZDw8P4+XlZd55552LlsuYor9257zwwgumatWqeS6Hc76PPvrIXHPNNW7LSp3r1izq63S+vD6PeR3bGGOGDBliBg0a5Hqc32c/r2Pnd6zzPfjgg+bqq6++YHte3xm9e/c2ki543fN6rb7//nsjyfzxxx9uc19//fWmevXqF3wfTZ8+3fj5+RlJplGjRrl25Z4/9+uvv+6W19fX1/V+Ksx7Ladzr1KliqlSpYo5ffq0OXbsmOnYsaPrs+Hv72+WLFlijDHG29vbeHt7m7Fjx5r169eb119/3fj4+Ji3337bLevfv6t69uxp7HZ7jmNffvllI8msXr3abcyjjz5qfH19Xe+33I59/tjnnnvO3HLLLeaxxx4zYWFhbp25+WU/1zHu7e1tpk6dajZs2GCio6ONzWYzK1euzHe8McYEBwebli1b5vq9MmfOHONwOMyBAweMMTl/Lh988EHj7e1toqKiLvjZnRt/7r3299e4W7duJjg42Dz55JM5/uxzGn/us9mzZ09z99135/rZ/Hv2hIQEI8lIMhMnTjTr1683I0eONF5eXmbHjh15Hv/cHJLM+++/n+P7LSsryzzxxBPGZrMZDw8PY7PZzHPPPeea69x77ffff3f73XXuPHJy4MCBHN9ro0ePNmFhYTmOwcVFbUttS237F2rb/FHbXoja9kLUttS21LbUtlahtqW2pbb9C7Vt/qhtL0RteyFqW2pbaltralsaFQro2LFjxt/f37U80d+Vt4I3IyPDdOnSxVx77bX5Xh/q7/J7rYpTTue0du1a06xZMyPJOBwO06lTJ3PbbbeZW2+9Nc+5srKyzM6dO82GDRvMlClTTEBAgOv6OBfb0KFDTf369c3+/fvz3C8+Pj7P5Y7OOXnypNm5c6dJSEgwAwcONCEhIf/oWjLp6elm586dZu3atWbMmDGmRo0a5qeffipyIffiiy+aK6+80nz22Wdm06ZNZtq0aaZy5cpm2bJlFyVXTgr62p3TqFEjM3z48ALtu2/fPlOrVi2zadMm17aLWfDm9XnM79gLFy40DRs2dLvWWGEK3vOP/dNPP+V5rPOdOnXKBAQEmClTpuR7jPO/My699FJjt9sv2KegBe/5evbsabp3737B99Hx48fNjh07zDfffGO6dOli/vWvf+X6HzY5zX3s2DHj4eFhWrZsmeOYwrzXjh07Zux2u2upuuHDh5uwsDCzfPlys3HjRjNx4kQTEBBgNm/ebDw9PU2bNm3cxj/88MOmdevWbllzK3hzGvuvf/3rgiIkIyPDNGjQwPj6+rrebzkde+DAga6xa9euNYGBgebAgQOuAub8gje/7OeKoeuvv95tny5dupjevXvnO94YY6pUqWJq1qyZ6/dKx44dzb///W/X/n//XEZHR5uqVauaJk2a5Phe69ixo7nuuutyfK91797dVKtWzdx6660mIyPjgrE5jT//s5lfwfv37Od+1vrbEmKhoaFmzJgxuR7//Dkkmdtvvz3H99sLL7xg6tataz766COzefNm8+6775pq1apd8B9X/fr1c/vdVZoLXlyI2rbgqG0Lj9qW2jYv1LbUttS21LbUtrjYqG0Ljtq28KhtqW3zQm1LbUttS21LbfvP0KhQCC1btsz1jVSvXj3XB/uc8ePHm6ZNm5ZAsqLJ7YOVkZFhunfvbpo2bWqOHDlSpLnzeq2KU16/yI8fP24OHTpkjHFeb+Whhx4q1NyDBg3Kt5u3KIYNG2bq1q1r9uzZk+++J0+eNJLM4sWLC3WMhg0bunVV/VM333yzGTJkiOsX+7Fjx9yeDw4ONlOnTs1x7KlTp4ynp6f54osv3LYPGjTIdOrU6aLkyklhXrtz1x7auHFjgY47f/58139QnbtJMjabzTgcDrN8+fJCv07n5Pd5zO/Yw4cPd90//3m73Z7rdaVyO3Z+xzp79qxr7Lvvvms8PT1dn7n8tGzZ0tx7772uX+KFea127959wS9+Y4xp3769GTFiRJ7fR+np6cbX1/eCP1jkN3flypVNixYtchxTlPfawIEDza5du4zkfm1GY5zv6wceeMAEBwe7dVgbY8xrr71mgoKC3LL+/XVq3769ueSSS3Id63A4XN+b537mVatWdfujQE7H/t///uf6zn355Zdd7wObzWZsNpvrfVa/fv18s6enpxtJF3SeP/7446Zt27b5jj916pSRZO6//363fc59r/z666/GbrebBQsWuJ47//vrxRdfNAEBAebHH3/M8b12bnxsbOwF74fU1FTj7+9v6tWrl+t/OOU0/pFHHnG9Zuc+Rzl9NnPKvmfPnhwL3rvvvjvH7v2c5jg3Pqf3m5+fn3n11Vfdtk+ePNk0atTIGPPXe61WrVpuv7vOfeZykp6e7vZeO6d///6ma9euOY5B8aO2LThq24KjtnWits0ZtW3+rxW1LbUtta0TtS0Ki9q24KhtC47a1onaNmfUtvm/VtS21LbUtk7UtrmzCwVy8uRJ7d69W5deemmOz7dp00bx8fFu25YtW+Z23aWyIDMzU3fffbd27typ5cuXq3r16oWeI7/XyioBAQGqWbOmdu7cqbVr16pbt26FGp+dne26dtrFYIzR8OHDNX/+fH399de67LLL8h1z7to2hX1tL3b2c/O1aNFCnp6ebu/97du3a9++fbm+9zMzM5WZmSm73f3rx+FwuK539E9z5aQwr11cXJxatGhR4OvD3XzzzdqyZYs2btzourVs2VL33nuv635hXyepYJ/H/I791FNPafPmzW7PS9LLL7+sWbNmFerY+R3L4XC4vYZdu3ZVzZo18339zn1n7Ny5U82bNy/0a3XZZZepdu3abmNSU1P1ww8/6Nprr83z+8g4G/Zyfd/kNPcff/yhkydP6pprrslxTGHea7GxsXI4HGrWrJnr+mi5fTauu+46bd++3e25HTt2qH79+q6skrR582bX8+deh9DQ0FzHtmjRQvHx8W4/c29vb91www2ufXM69p49e1S5cmXFx8erX79+2rx5s9avX6+aNWtqxIgRCgoK0ujRo7VkyZJ8s3t5eUmSDhw4kOM++Y3PzMzM87WbNWuWatWqpc6dO7ueO/f99fDDD2vy5MlavHixLrnkkhzfa+fGDxw40O39kJqaqptvvlknTpzQ5MmT3a6rmd/4MWPGaPPmzVq1apU8PT31wgsvSLrws5lT9pCQEAUGBl5wnPNfk5yOf/4c5+T0mp09ezbX19IYo5deekl2u12RkZGu992591pun1MvLy/Xe+2c7OxsxcfHl7laqbygti04atuCobaltqW2daK2pbaltqW2Rcmjti04atuCobaltqW2daK2pbaltqW2LXbF3gpRRo0aNcqsXLnS7N2713z//fcmIiLC1KhRw9Vl1q9fP7dOr++//954eHiYKVOmmF9++cVMmDDBeHp6mi1btlh1Cjk6ceKE2bBhg9mwYYOrg2vDhg3mt99+MxkZGaZr166mbt26ZuPGjebgwYOuW3p6umuOm266yUybNs31OL/XyspzMsaYjz/+2KxYscLs3r3bLFiwwNSvX9/ccccdbnP8/ef53HPPmaVLl5rdu3ebn3/+2UyZMsV4eHiYN99886LlfvDBB01AQIBZuXKl22t96tQpY4wxu3btMs8884xZu3at2bt3r1m4cKG5/PLLTfv27d3madSokZk3b54xxtkROHbsWJOQkGB+/fVXs3btWhMZGWm8vb0v6L4qqDFjxphvvvnG7N2712zevNmMGTPG2Gw2s3TpUmOMc/mz4OBg8/XXX5u1a9eaNm3aXLDUz/kZjXEuO9WkSROzYsUKs2fPHjNr1izj4+NjXnvttYuSqyiv3TkpKSnG19fXzJgxo7AvlZu/L61V2NepoJ/Hghz775RDB3tRj53TsXbu3GlsNpv56quvcjx+1apVzeTJk92+M6pXr24qVapkZsyYUaT31PPPP2+qVKliunfvbmbOnGluueUWc+mll5qbbrrJ9X20e/du89xzz5m1a9ea3377zXz//femS5cuplq1am5L7P197uuvv95UrlzZvPHGG+bdd981NWvWNHa73ezbt6/Q77Xzvy+XLl1q7Ha7qVy5sjl06JDJyMgwDRs2NNdff7354YcfzK5du8yUKVOMzWYzixYtMomJicbDw8NcfvnlZvz48eaDDz4wvr6+5q233nL7DqxUqZJ5+eWXzZIlS0y3bt3MZZddZr777jvj4eFhnn32WdO6dWszYMAA4+vra95//30ze/Zs4+XlZa699lpTu3Ztc+eddxp/f3+zefNm1888t2MPHz7ceHt7m7ffftv8/PPPZsiQIaZKlSomKSnJbQmxgmZ3OBxm3Lhx5ptvvjHTpk0zDofDfPfdd67xzZs3Nw888IBr/Ntvv+0a7+XlZapXr27efPNN8/XXX7u+V1599VUTHBxsnnjiiQu+68PDw43kvF7Y4sWLTcuWLU3Lli3dlty78sorTY0aNcwTTzzh9l776KOPTGhoqPH39zd169Y1e/fudX1Ozu9Sz238woULzebNm10/o9OnT1/w2czKyjIeHh5u1877888/zYYNG8zgwYONJDNy5EizcOFCM3LkSOPj4+O2dN1NN91kXnnlFdf5//33ZY0aNcy1115rFi5c6PZ+u/nmm02dOnXMF198Ydq0aWPuv/9+U6NGDfP444+7fncNGTLEBAQEmLffftt8/fXX5t///rfrPM4//vn1wuzZs3N9v6D4UdtS21LbOlHbFg21LbVtTnmpbaltqW2pba1CbUttS23rRG1bNNS21LY55aW2pbaltrWmtqVRIRe9evUyl156qfHy8jJ16tQxvXr1cnsT3XDDDWbAgAFuYz7++GNz5ZVXGi8vL9OkSROzaNGiEk6dvxUrVriWDTn/NmDAALN3794cn5Pkdo2v+vXrmwkTJrge5/daWXlOxhjzyiuvmLp16xpPT08THBxsnn766Rx/aZ//83zqqadMw4YNjY+Pj6latapp06aNmT179kXNndtrPWvWLGOM8xpW7du3N9WqVTPe3t6mYcOGZvTo0Rdc6+r8MadPnzY9evQwQUFBxsvLy1x66aWma9euJjExscg5Bw4caOrXr2+8vLxMzZo1zc033+wqds8d86GHHjJVq1Y1vr6+pkePHubgwYO5ZjTGmIMHD5r77rvPBAUFGR8fH9OoUSPz0ksvmezs7IuSqyiv3Tmvv/66qVSpkjl+/HiBs+Tk74VgYV+ngn4eC3Lsv8up4C3qsXM61tixY029evVMVlZWrsevUqWK23fGf/7zH9frXpT3VHZ2thk3bpzx9vZ2LccUGBjo9n104MABc9ttt5latWoZT09PU7duXdOnTx+zbdu2POfu1auXqVy5sus1qFWrluv6WYV9r53/fVmlShXjcDjcloDasWOHueOOO0ytWrWMr6+vadq0qXn33Xddz3/++efG09PTOBwOc9VVV5k33ngj1+9Au91ubr75ZrN9+3bX2GuuucZV6LzxxhuueSdOnJjvzzynYxtjzLRp00xwcLDx8vIyYWFhZs2aNcYY41bwFja7w+EwzZo1c1vy6vPPPzd+fn7Gbre7xuf2XrXb7a7vlcWLFxtJZvv27Rd81wcHB+c4/vzfcee2nXsdz73Xqlatmutrtnfv3nzHBwYGGm9vb7ef0d8/m0uWLDGSTHR0tGvbrFmzcjxm3bp1zXfffef2vqtfv77p27ev6/i5vd4+Pj5u77fU1FTzyCOPmODgYGOz2UzVqlXNU0895VrqLadb48aNXedx/vHPfy3zer+g+FHbUttS2zpR2xYNtS21bU5zUttS21LbUttahdqW2pba1onatmiobaltc5qT2pbaltrWmtrWZowxAgAAAAAAAAAAAAAAKAH2/HcBAAAAAAAAAAAAAAC4OGhUAAAAAAAAAAAAAAAAJYZGBQAAAAAAAAAAAAAAUGJoVAAAAAAAAAAAAAAAACWGRgUAAAAAAAAAAAAAAFBiaFQAAAAAAAAAAAAAAAAlhkYFAAAAAAAAAAAAAABQYmhUAAAAAAAAAAAAAAAAJYZGBQAo5yZOnKjAwEDZbDYtWLCgQGNWrlwpm82m48ePF2u20iQkJEQxMTFWxwAAAEAeqG0LhtoWAACg9KO2LRhqW6D8olEBQIm77777ZLPZZLPZ5OXlpYYNG+qZZ57R2bNnrY6Wr8IUjaXBL7/8okmTJun111/XwYMHddtttxXbsTp06KCRI0cW2/wAAAClEbVtyaG2BQAAKF7UtiWH2hYAJA+rAwComG699VbNmjVL6enp+vLLLzVs2DB5enpq7NixhZ4rKytLNptNdju9V3+3e/duSVK3bt1ks9ksTgMAAFA+UduWDGpbAACA4kdtWzKobQGAFRUAWMTb21u1a9dW/fr1/6+9e4+puv7jOP4S8HK4mOgURXE48aA0MnCMYSkqTDHHFG+lJmoilJJZkqiVUW02Myu6ma46dPGS5iUXmqGJUyw4MMFMBkQiZijztnUIUTnf3x/MM09cxH79sPw9H3/x/Xy+38/38/kedvY623ufr5544glFR0dr165dkqS6ujqlpKSod+/e8vDwUHh4uLKzsx3XZmRkqEuXLtq1a5eCgoLUsWNHVVZWqq6uTqmpqfLz81PHjh0VEBCgjz76yHHd8ePHNXbsWHl6esrHx0czZ87U+fPnHf0jRozQwoULtWTJEnXt2lU9e/ZUWlqao9/f31+SFBcXp3bt2jmOy8vLNX78ePn4+MjT01NhYWHat2+f03qrqqo0btw4mUwm9evXTxs3bmy0ZdXly5eVkJCg7t27q3Pnzho1apSKiopafI4//vijRo0aJZPJpG7duikxMVE2m01Sw9ZhsbGxkiQXF5cWA+/u3btlNptlMpk0cuRIVVRUOPVfuHBB06ZNU+/eveXu7q7g4GBt2rTJ0T979mwdPHhQ6enpjqrriooK1dfXa+7cuerXr59MJpMCAwOVnp7e4ppufL4327lzp9P8i4qKNHLkSHl5ealz584aMmSI8vPzHf2HDx/WsGHDZDKZ5Ofnp4ULF6qmpsbRX11drdjYWMfnsWHDhhbnBAAA0BKyLdm2OWRbAADwb0O2Jds2h2wL4O9GoQKAfwSTyaSrV69KkpKTk/X9999r8+bNOnbsmKZMmaKYmBiVlZU5zv/jjz+0atUqffjhh/rpp5/Uo0cPxcfHa9OmTXr77bdVXFysdevWydPTU1JDmBw1apRCQkKUn5+vb775RufOndPUqVOd5vHJJ5/Iw8NDubm5eu211/Tyyy8rKytLkmS1WiVJFotFVVVVjmObzaaHHnpI+/fv19GjRxUTE6PY2FhVVlY6xo2Pj9dvv/2m7Oxsbdu2TevXr1d1dbXTvadMmaLq6mrt2bNHBQUFCg0NVVRUlC5evNjkM6upqdGYMWPk7e0tq9WqrVu3at++fUpOTpYkpaSkyGKxSGoI3FVVVU2Oc/r0aU2cOFGxsbEqLCxUQkKCli5d6nTOlStXNGTIEGVmZur48eNKTEzUzJkzlZeXJ0lKT09XRESE5s2b57iXn5+f7Ha7+vTpo61bt+rEiRNasWKFli9fri1btjQ5l9aaMWOG+vTpI6vVqoKCAi1dulTt27eX1PADJCYmRpMmTdKxY8f0xRdf6PDhw47nIjUE9NOnT+vAgQP68ssv9f777zf6PAAAAP4qsi3Z9naQbQEAwD8Z2ZZsezvItgBuiwEAbWzWrFnG+PHjDcMwDLvdbmRlZRkdO3Y0UlJSjFOnThmurq7GmTNnnK6Jiooyli1bZhiGYVgsFkOSUVhY6OgvKSkxJBlZWVlN3vOVV14xRo8e7dR2+vRpQ5JRUlJiGIZhREZGGg8++KDTOWFhYUZqaqrjWJKxY8eOW67x3nvvNd555x3DMAyjuLjYkGRYrVZHf1lZmSHJePPNNw3DMIxDhw4ZnTt3Nq5cueI0Tv/+/Y1169Y1eY/169cb3t7ehs1mc7RlZmYaLi4uxtmzZw3DMIwdO3YYt/qqX7ZsmREUFOTUlpqaakgyLl261Ox148aNMxYvXuw4joyMNJ566qkW72UYhrFgwQJj0qRJzfZbLBbjnnvucWr78zq8vLyMjIyMJq+fO3eukZiY6NR26NAhw8XFxaitrXX8r+Tl5Tn6b3xGNz4PAACA1iLbkm3JtgAA4G5BtiXbkm0BtCW3/3klBAA04euvv5anp6euXbsmu92u6dOnKy0tTdnZ2aqvr5fZbHY6v66uTt26dXMcd+jQQffdd5/juLCwUK6uroqMjGzyfkVFRTpw4ICjUvdm5eXljvvdPKYk9erV65YVmzabTWlpacrMzFRVVZWuX7+u2tpaR2VuSUmJ3NzcFBoa6rgmICBA3t7eTvOz2WxOa5Sk2tpax/vK/qy4uFiDBw+Wh4eHo+2BBx6Q3W5XSUmJfHx8Wpz3zeOEh4c7tUVERDgd19fXa+XKldqyZYvOnDmjq1evqq6uTu7u7rcc/7333tPHH3+syspK1dbW6urVq7r//vtbNbfmPPPMM0pISNBnn32m6OhoTZkyRf3795fU8CyPHTvmtC2YYRiy2+06efKkSktL5ebmpiFDhjj6Bw4c2GjbMgAAgNYi25Jt/xtkWwAA8E9CtiXb/jfItgBuB4UKAO6IkSNHau3aterQoYN8fX3l5tbwdWSz2eTq6qqCggK5uro6XXNzWDWZTE7vvjKZTC3ez2azKTY2VqtWrWrU16tXL8ffN7ahuqFdu3ay2+0tjp2SkqKsrCy9/vrrCggIkMlk0uTJkx1borWGzWZTr169nN7pdsM/IYitXr1a6enpeuuttxQcHCwPDw8tWrTolmvcvHmzUlJStGbNGkVERMjLy0urV69Wbm5us9e4uLjIMAyntmvXrjkdp6Wlafr06crMzNSePXv04osvavPmzYqLi5PNZlNSUpIWLlzYaOy+ffuqtLT0NlYOAABwa2TbxvMj2zYg2wIAgH8bsm3j+ZFtG5BtAfzdKFQAcEd4eHgoICCgUXtISIjq6+tVXV2tYcOGtXq84OBg2e12HTx4UNHR0Y36Q0NDtW3bNvn7+zvC9V/Rvn171dfXO7Xl5ORo9uzZiouLk9QQXisqKhz9gYGBun79uo4ePeqoBv3555916dIlp/mdPXtWbm5u8vf3b9VcBg0apIyMDNXU1Diqc3NycuTi4qLAwMBWr2nQoEHatWuXU9sPP/zQaI3jx4/Xo48+Kkmy2+0qLS1VUFCQ45wOHTo0+WyGDh2q+fPnO9qaqzS+oXv37vr999+d1lVYWNjoPLPZLLPZrKefflrTpk2TxWJRXFycQkNDdeLEiSb/v6SGKtzr16+roKBAYWFhkhqqpy9fvtzivAAAAJpDtiXbNodsCwAA/m3ItmTb5pBtAfzdXO70BADgZmazWTNmzFB8fLy2b9+ukydPKi8vT6+++qoyMzObvc7f31+zZs3SY489pp07d+rkyZPKzs7Wli1bJEkLFizQxYsXNW3aNFmtVpWXl2vv3r2aM2dOo5DWEn9/f+3fv19nz551BNYBAwZo+/btKiwsVFFRkaZPn+5UzTtw4EBFR0crMTFReXl5Onr0qBITE52qi6OjoxUREaEJEybo22+/VUVFhY4cOaLnnntO+fn5Tc5lxowZ6tSpk2bNmqXjx4/rwIEDevLJJzVz5sxWbx8mSY8//rjKysr07LPPqqSkRBs3blRGRobTOQMGDFBWVpaOHDmi4uJiJSUl6dy5c42eTW5urioqKnT+/HnZ7XYNGDBA+fn52rt3r0pLS/XCCy/IarW2OJ/w8HC5u7tr+fLlKi8vbzSf2tpaJScnKzs7W6dOnVJOTo6sVqsGDRokSUpNTdWRI0eUnJyswsJClZWV6auvvlJycrKkhh8gMTExSkpKUm5urgoKCpSQkHDL6m4AAIDbRbYl25JtAQDA3YJsS7Yl2wL4u1GoAOAfx2KxKD4+XosXL1ZgYKAmTJggq9Wqvn37tnjd2rVrNXnyZM2fP18DBw7UvHnzVFNTI0ny9fVVTk6O6uvrNXr0aAUHB2vRokXq0qWLXFxa/1W4Zs0aZWVlyc/PTyEhIZKkN954Q97e3ho6dKhiY2M1ZswYp/eaSdKnn34qHx8fDR8+XHFxcZo3b568vLzUqVMnSQ1ble3evVvDhw/XnDlzZDab9cgjj+jUqVPNhld3d3ft3btXFy9eVFhYmCZPnqyoqCi9++67rV6P1LCt1rZt27Rz504NHjxYH3zwgVauXOl0zvPPP6/Q0FCNGTNGI0aMUM+ePTVhwgSnc1JSUuTq6qqgoCB1795dlZWVSkpK0sSJE/Xwww8rPDxcFy5ccKrSbUrXrl31+eefa/fu3QoODtamTZuUlpbm6Hd1ddWFCxcUHx8vs9msqVOnauzYsXrppZckNbyv7uDBgyotLdWwYcMUEhKiFStWyNfX1zGGxWKRr6+vIiMjNXHiRCUmJqpHjx639dwAAABag2xLtiXbAgCAuwXZlmxLtgXwd2pn/PmFMgCA/7lff/1Vfn5+2rdvn6Kiou70dAAAAIC/jGwLAACAuwXZFgDaDoUKANAGvvvuO9lsNgUHB6uqqkpLlizRmTNnVFpaqvbt29/p6QEAAACtRrYFAADA3YJsCwB3jtudngAA/D+4du2ali9frl9++UVeXl4aOnSoNmzYQNgFAADAvw7ZFgAAAHcLsi0A3DnsqAAAAAAAAAAAAAAAANqMy52eAAAAAAAAAAAAAAAA+P9BoQIAAAAAAAAAAAAAAGgzFCoAAAAAAAAAAAAAAIA2Q6ECAAAAAAAAAAAAAABoMxQqAAAAAAAAAAAAAACANkOhAgAAAAAAAAAAAAAAaDMUKgAAAAAAAAAAAAAAgDZDoQIAAAAAAAAAAAAAAGgzFCoAAAAAAAAAAAAAAIA28x/HwiDzs55hMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning([50, 67, 42], 0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6739372,
     "sourceId": 11761127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15809.903571,
   "end_time": "2025-05-11T10:49:23.668399",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T06:25:53.764828",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c5e2178e603451a945ac0d186ee9541": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ea18cb931584933a38d08a90578f540": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "108fd0e30d0a401dbddec21071a042ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15ebc0620d9548c4ab8545f5fd0de4b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_803390b8a36646d5b10df2fb3a8a3955",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f5616b0fd8cb44a09221e4418e7f75e8",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:â€‡100%"
      }
     },
     "17c1a90608a74f488457020b526f1a8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c675c2e9d5d42a39e7b9300f4b82cf4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28fd95938d04458abbb25698c8697ff6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2943cf438d814e63b4de303835af7a52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2c0945119f3406d93e6e79e9489041a",
       "max": 229167.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7e6b33089b174d81bceb5835f8950252",
       "tabbable": null,
       "tooltip": null,
       "value": 229167.0
      }
     },
     "3025daa7ed854210855b6c92f424033f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "368beb5aad7a406b94eacf8e0f36f850": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "392e1cd9b7dd40308b8e11cba2a8245b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3c7e18e0f6644e7abe5ab92ab8a649d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4045a04adc054d0bb9f537300ddebda2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_17c1a90608a74f488457020b526f1a8f",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3025daa7ed854210855b6c92f424033f",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡229k/229kâ€‡[00:00&lt;00:00,â€‡6.25MB/s]"
      }
     },
     "438bad97a51348d5903564d79e98e560": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "43bd03d7e8664d81a771fcde6d804a9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5be525bcb19846db857da97695bdfcfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28fd95938d04458abbb25698c8697ff6",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_43bd03d7e8664d81a771fcde6d804a9b",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:â€‡100%"
      }
     },
     "5ec80db6385a4ad1b1e563484199d09f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ec5ba02b08bd47d59753e121608a1a01",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_438bad97a51348d5903564d79e98e560",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "7164040be63547fbb1956575d57c2028": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0ac985a852543b6bd26796e95073e25",
       "max": 1534.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_742afea99bbd4aa19c1761db2c34b492",
       "tabbable": null,
       "tooltip": null,
       "value": 1534.0
      }
     },
     "73347f57528a49db80d0377112fc7c08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15ebc0620d9548c4ab8545f5fd0de4b7",
        "IPY_MODEL_7164040be63547fbb1956575d57c2028",
        "IPY_MODEL_f7ad5d93f205437eb24f46fdcf975854"
       ],
       "layout": "IPY_MODEL_f2555413f0584202bc1cc3a3a45545e7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "742afea99bbd4aa19c1761db2c34b492": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "74415101475d4fb69ed261208a4f6f1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "782e78343dbb44249ac765f496985068": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e6b33089b174d81bceb5835f8950252": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "803390b8a36646d5b10df2fb3a8a3955": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8353c18efc054508975b7e6ccd340dee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85cb8f60e7db4535aa55462d521d3507": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9503e5a0261c46e8b48c5728c6e5e190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99da3137dcb44f08a25eb4d4a3a956a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ca1ab39bc774385b99dad898a137843": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5be525bcb19846db857da97695bdfcfb",
        "IPY_MODEL_2943cf438d814e63b4de303835af7a52",
        "IPY_MODEL_4045a04adc054d0bb9f537300ddebda2"
       ],
       "layout": "IPY_MODEL_85cb8f60e7db4535aa55462d521d3507",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9eb8457f385c4239aa06cba08a25efb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1c675c2e9d5d42a39e7b9300f4b82cf4",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_392e1cd9b7dd40308b8e11cba2a8245b",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "aca6a49c3b034c159d496034463706af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f5427a0ea7cb4e248fcc5ec93c57932e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0ea18cb931584933a38d08a90578f540",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:â€‡100%"
      }
     },
     "c0a1f682e53a44508e5f06f05577c697": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8353c18efc054508975b7e6ccd340dee",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3c7e18e0f6644e7abe5ab92ab8a649d6",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2.00/2.00â€‡[00:00&lt;00:00,â€‡177B/s]"
      }
     },
     "d0ac985a852543b6bd26796e95073e25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0fa3f77a72d4b418bcf0e4a9fd4ed5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e8e33819bb854dbdb97403d192c61cf0",
        "IPY_MODEL_5ec80db6385a4ad1b1e563484199d09f",
        "IPY_MODEL_d2bf19f807af4321afbe577b00ffa565"
       ],
       "layout": "IPY_MODEL_99da3137dcb44f08a25eb4d4a3a956a5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d2bf19f807af4321afbe577b00ffa565": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_782e78343dbb44249ac765f496985068",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_108fd0e30d0a401dbddec21071a042ed",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡112/112â€‡[00:00&lt;00:00,â€‡12.7kB/s]"
      }
     },
     "d2c0945119f3406d93e6e79e9489041a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfa7ff9363384a7db3657b32ca4ae831": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aca6a49c3b034c159d496034463706af",
        "IPY_MODEL_9eb8457f385c4239aa06cba08a25efb7",
        "IPY_MODEL_c0a1f682e53a44508e5f06f05577c697"
       ],
       "layout": "IPY_MODEL_e8ed90ad5c4b421e921de67a6e6adf21",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e8e33819bb854dbdb97403d192c61cf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c5e2178e603451a945ac0d186ee9541",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_368beb5aad7a406b94eacf8e0f36f850",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:â€‡100%"
      }
     },
     "e8ed90ad5c4b421e921de67a6e6adf21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec5ba02b08bd47d59753e121608a1a01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2555413f0584202bc1cc3a3a45545e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5427a0ea7cb4e248fcc5ec93c57932e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5616b0fd8cb44a09221e4418e7f75e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f7ad5d93f205437eb24f46fdcf975854": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9503e5a0261c46e8b48c5728c6e5e190",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_74415101475d4fb69ed261208a4f6f1b",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1.53k/1.53kâ€‡[00:00&lt;00:00,â€‡165kB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
