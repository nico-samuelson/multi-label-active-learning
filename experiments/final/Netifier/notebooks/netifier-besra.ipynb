{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbfd8b4",
   "metadata": {
    "papermill": {
     "duration": 0.007182,
     "end_time": "2025-01-28T13:15:17.706254",
     "exception": false,
     "start_time": "2025-01-28T13:15:17.699072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d831c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:17.717742Z",
     "iopub.status.busy": "2025-01-28T13:15:17.717512Z",
     "iopub.status.idle": "2025-01-28T13:15:39.512435Z",
     "shell.execute_reply": "2025-01-28T13:15:39.511436Z"
    },
    "papermill": {
     "duration": 21.802267,
     "end_time": "2025-01-28T13:15:39.514024",
     "exception": false,
     "start_time": "2025-01-28T13:15:17.711757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6382351",
   "metadata": {
    "papermill": {
     "duration": 0.005186,
     "end_time": "2025-01-28T13:15:39.524967",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.519781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47d708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.536607Z",
     "iopub.status.busy": "2025-01-28T13:15:39.536109Z",
     "iopub.status.idle": "2025-01-28T13:15:39.539600Z",
     "shell.execute_reply": "2025-01-28T13:15:39.538822Z"
    },
    "papermill": {
     "duration": 0.010506,
     "end_time": "2025-01-28T13:15:39.540780",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.530274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5480acbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.552085Z",
     "iopub.status.busy": "2025-01-28T13:15:39.551845Z",
     "iopub.status.idle": "2025-01-28T13:15:39.555263Z",
     "shell.execute_reply": "2025-01-28T13:15:39.554668Z"
    },
    "papermill": {
     "duration": 0.010274,
     "end_time": "2025-01-28T13:15:39.556456",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.546182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3dde14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.567823Z",
     "iopub.status.busy": "2025-01-28T13:15:39.567603Z",
     "iopub.status.idle": "2025-01-28T13:15:39.575749Z",
     "shell.execute_reply": "2025-01-28T13:15:39.575215Z"
    },
    "papermill": {
     "duration": 0.015141,
     "end_time": "2025-01-28T13:15:39.577007",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.561866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf02abe",
   "metadata": {
    "papermill": {
     "duration": 0.005222,
     "end_time": "2025-01-28T13:15:39.587626",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.582404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0435f3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.598956Z",
     "iopub.status.busy": "2025-01-28T13:15:39.598723Z",
     "iopub.status.idle": "2025-01-28T13:15:39.650635Z",
     "shell.execute_reply": "2025-01-28T13:15:39.649139Z"
    },
    "papermill": {
     "duration": 0.059337,
     "end_time": "2025-01-28T13:15:39.652348",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.593011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-besra'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4427cd",
   "metadata": {
    "papermill": {
     "duration": 0.005273,
     "end_time": "2025-01-28T13:15:39.663455",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.658182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcad706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.675892Z",
     "iopub.status.busy": "2025-01-28T13:15:39.675619Z",
     "iopub.status.idle": "2025-01-28T13:15:39.830340Z",
     "shell.execute_reply": "2025-01-28T13:15:39.829451Z"
    },
    "papermill": {
     "duration": 0.162598,
     "end_time": "2025-01-28T13:15:39.831642",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.669044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-2/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-2/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68f82f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.843813Z",
     "iopub.status.busy": "2025-01-28T13:15:39.843556Z",
     "iopub.status.idle": "2025-01-28T13:15:39.867674Z",
     "shell.execute_reply": "2025-01-28T13:15:39.866880Z"
    },
    "papermill": {
     "duration": 0.031188,
     "end_time": "2025-01-28T13:15:39.868867",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.837679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa63ef62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.880805Z",
     "iopub.status.busy": "2025-01-28T13:15:39.880562Z",
     "iopub.status.idle": "2025-01-28T13:15:39.896003Z",
     "shell.execute_reply": "2025-01-28T13:15:39.895212Z"
    },
    "papermill": {
     "duration": 0.022832,
     "end_time": "2025-01-28T13:15:39.897334",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.874502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6218,) (6218, 4)\n",
      "(1555,) (1555, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "train_labels = train_data.columns[2:6]\n",
    "val_labels = val_data.columns[2:6]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['processed_text'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['processed_text'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3c439",
   "metadata": {
    "papermill": {
     "duration": 0.005385,
     "end_time": "2025-01-28T13:15:39.908710",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.903325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299a76d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:39.920557Z",
     "iopub.status.busy": "2025-01-28T13:15:39.920332Z",
     "iopub.status.idle": "2025-01-28T13:15:41.152358Z",
     "shell.execute_reply": "2025-01-28T13:15:41.151719Z"
    },
    "papermill": {
     "duration": 1.239428,
     "end_time": "2025-01-28T13:15:41.153696",
     "exception": false,
     "start_time": "2025-01-28T13:15:39.914268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c29bd3bba449fe80d0d3b5b5c90d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b754f31825d243698e599854e3f328c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa880c7ffba4b16ac6b77f7dd6d1951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64932cbea9a4a09ae29f5b5af905665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8853c2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.167040Z",
     "iopub.status.busy": "2025-01-28T13:15:41.166778Z",
     "iopub.status.idle": "2025-01-28T13:15:41.171110Z",
     "shell.execute_reply": "2025-01-28T13:15:41.170330Z"
    },
    "papermill": {
     "duration": 0.012156,
     "end_time": "2025-01-28T13:15:41.172406",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.160250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c58c8",
   "metadata": {
    "papermill": {
     "duration": 0.006053,
     "end_time": "2025-01-28T13:15:41.184674",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.178621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24c9da59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.197325Z",
     "iopub.status.busy": "2025-01-28T13:15:41.197120Z",
     "iopub.status.idle": "2025-01-28T13:15:41.200433Z",
     "shell.execute_reply": "2025-01-28T13:15:41.199856Z"
    },
    "papermill": {
     "duration": 0.010997,
     "end_time": "2025-01-28T13:15:41.201649",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.190652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e334be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.214387Z",
     "iopub.status.busy": "2025-01-28T13:15:41.214154Z",
     "iopub.status.idle": "2025-01-28T13:15:41.218909Z",
     "shell.execute_reply": "2025-01-28T13:15:41.218318Z"
    },
    "papermill": {
     "duration": 0.012459,
     "end_time": "2025-01-28T13:15:41.220168",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.207709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    )  \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4771d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.232864Z",
     "iopub.status.busy": "2025-01-28T13:15:41.232659Z",
     "iopub.status.idle": "2025-01-28T13:15:41.245049Z",
     "shell.execute_reply": "2025-01-28T13:15:41.244460Z"
    },
    "papermill": {
     "duration": 0.020027,
     "end_time": "2025-01-28T13:15:41.246255",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.226228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(train_labels),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229c54a",
   "metadata": {
    "papermill": {
     "duration": 0.005743,
     "end_time": "2025-01-28T13:15:41.258099",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.252356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207eb163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.271068Z",
     "iopub.status.busy": "2025-01-28T13:15:41.270825Z",
     "iopub.status.idle": "2025-01-28T13:15:41.275852Z",
     "shell.execute_reply": "2025-01-28T13:15:41.275269Z"
    },
    "papermill": {
     "duration": 0.012786,
     "end_time": "2025-01-28T13:15:41.277076",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.264290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fb5d8",
   "metadata": {
    "papermill": {
     "duration": 0.00581,
     "end_time": "2025-01-28T13:15:41.289269",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.283459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ffb605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.301970Z",
     "iopub.status.busy": "2025-01-28T13:15:41.301726Z",
     "iopub.status.idle": "2025-01-28T13:15:41.320063Z",
     "shell.execute_reply": "2025-01-28T13:15:41.319438Z"
    },
    "papermill": {
     "duration": 0.026152,
     "end_time": "2025-01-28T13:15:41.321333",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.295181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (∆Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = NetifierDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    collected_indices = set()  # Initialize set to store selected indices\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    if accelerator.is_local_main_process:\n",
    "        num_of_candidates = len(score_changes[:math.ceil(0.1 * len(score_changes))])\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of clusters\n",
    "        if num_of_candidates <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            n_clusters = n_clusters\n",
    "        elif num_of_candidates > n_clusters and num_of_candidates < nearest_cp - current_train_size:\n",
    "            n_clusters = num_of_candidates\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            n_clusters = nearest_cp - current_train_size\n",
    "\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "            \n",
    "        kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "        kmeans.fit(score_changes)\n",
    "\n",
    "        if current_train_size > checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'pornografi': [y_train[i][0] for i in temp],\n",
    "                'sara': [y_train[i][1] for i in temp],\n",
    "                'radikalisme': [y_train[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Samples above threshold:\", num_of_candidates)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]  # Indices of samples in the current cluster\n",
    "                \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances to the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "                closest_sample_index = cluster_indices[np.argmin(cluster_distances)]  # Closest sample index\n",
    "                collected_indices.add(closest_sample_index)\n",
    "\n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "            \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'pornografi': [y_train[i][0] for i in temp],\n",
    "                    'sara': [y_train[i][1] for i in temp],\n",
    "                    'radikalisme': [y_train[i][2] for i in temp],\n",
    "                    'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "                })\n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Samples above threshold:\", num_of_candidates)\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a9170",
   "metadata": {
    "papermill": {
     "duration": 0.005715,
     "end_time": "2025-01-28T13:15:41.333221",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.327506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f981835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.345989Z",
     "iopub.status.busy": "2025-01-28T13:15:41.345754Z",
     "iopub.status.idle": "2025-01-28T13:15:41.355399Z",
     "shell.execute_reply": "2025-01-28T13:15:41.354805Z"
    },
    "papermill": {
     "duration": 0.017425,
     "end_time": "2025-01-28T13:15:41.356541",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.339116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4902d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.369082Z",
     "iopub.status.busy": "2025-01-28T13:15:41.368844Z",
     "iopub.status.idle": "2025-01-28T13:15:41.371889Z",
     "shell.execute_reply": "2025-01-28T13:15:41.371317Z"
    },
    "papermill": {
     "duration": 0.01054,
     "end_time": "2025-01-28T13:15:41.373050",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.362510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 7, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31a78b",
   "metadata": {
    "papermill": {
     "duration": 0.005814,
     "end_time": "2025-01-28T13:15:41.385094",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.379280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4486, Accuracy: 0.7886, F1 Micro: 0.0174, F1 Macro: 0.0157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3911, Accuracy: 0.8078, F1 Micro: 0.18, F1 Macro: 0.1337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.381, Accuracy: 0.8263, F1 Micro: 0.3293, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3204, Accuracy: 0.8355, F1 Micro: 0.416, F1 Macro: 0.3338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.281, Accuracy: 0.8527, F1 Micro: 0.5464, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2621, Accuracy: 0.8564, F1 Micro: 0.5767, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2083, Accuracy: 0.8581, F1 Micro: 0.5979, F1 Macro: 0.5834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1662, Accuracy: 0.8673, F1 Micro: 0.6473, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.149, Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "Model 1 - Iteration 388: Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.84      0.86       370\n",
      "                sara       0.63      0.47      0.54       248\n",
      "         radikalisme       0.67      0.62      0.64       243\n",
      "pencemaran_nama_baik       0.66      0.48      0.56       504\n",
      "\n",
      "           micro avg       0.73      0.60      0.66      1365\n",
      "           macro avg       0.71      0.60      0.65      1365\n",
      "        weighted avg       0.72      0.60      0.65      1365\n",
      "         samples avg       0.34      0.33      0.33      1365\n",
      "\n",
      "Training completed in 58.296865463256836 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4335, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3767, Accuracy: 0.8175, F1 Micro: 0.2561, F1 Macro: 0.1748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3582, Accuracy: 0.8288, F1 Micro: 0.3613, F1 Macro: 0.2669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2972, Accuracy: 0.8455, F1 Micro: 0.5013, F1 Macro: 0.467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2541, Accuracy: 0.855, F1 Micro: 0.5739, F1 Macro: 0.5572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2294, Accuracy: 0.8559, F1 Micro: 0.582, F1 Macro: 0.5669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1796, Accuracy: 0.8684, F1 Micro: 0.6509, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1463, Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "Epoch 10/10, Train Loss: 0.132, Accuracy: 0.8741, F1 Micro: 0.6753, F1 Macro: 0.6652\n",
      "Model 2 - Iteration 388: Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.57      0.52      0.54       248\n",
      "         radikalisme       0.65      0.79      0.71       243\n",
      "pencemaran_nama_baik       0.66      0.55      0.60       504\n",
      "\n",
      "           micro avg       0.70      0.67      0.68      1365\n",
      "           macro avg       0.69      0.68      0.68      1365\n",
      "        weighted avg       0.70      0.67      0.68      1365\n",
      "         samples avg       0.36      0.37      0.36      1365\n",
      "\n",
      "Training completed in 57.415642738342285 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5779, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.456, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4117, Accuracy: 0.7941, F1 Micro: 0.0666, F1 Macro: 0.0564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3928, Accuracy: 0.8244, F1 Micro: 0.3237, F1 Macro: 0.2387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3221, Accuracy: 0.8383, F1 Micro: 0.4396, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2818, Accuracy: 0.8533, F1 Micro: 0.5802, F1 Macro: 0.5753\n",
      "Epoch 7/10, Train Loss: 0.2525, Accuracy: 0.8531, F1 Micro: 0.5528, F1 Macro: 0.53\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1925, Accuracy: 0.8631, F1 Micro: 0.6244, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1636, Accuracy: 0.8678, F1 Micro: 0.6672, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1389, Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "Model 3 - Iteration 388: Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.84      0.86       370\n",
      "                sara       0.64      0.52      0.57       248\n",
      "         radikalisme       0.69      0.65      0.67       243\n",
      "pencemaran_nama_baik       0.70      0.52      0.59       504\n",
      "\n",
      "           micro avg       0.74      0.63      0.68      1365\n",
      "           macro avg       0.73      0.63      0.67      1365\n",
      "        weighted avg       0.74      0.63      0.68      1365\n",
      "         samples avg       0.35      0.35      0.34      1365\n",
      "\n",
      "Training completed in 57.88157033920288 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8697, F1 Micro: 0.6738, F1 Macro: 0.6678\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 119.37155199050903 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5729, Accuracy: 0.7919, F1 Micro: 0.0486, F1 Macro: 0.0419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4673, Accuracy: 0.8453, F1 Micro: 0.4886, F1 Macro: 0.4393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3886, Accuracy: 0.8614, F1 Micro: 0.6162, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3268, Accuracy: 0.8702, F1 Micro: 0.6275, F1 Macro: 0.6124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2812, Accuracy: 0.8733, F1 Micro: 0.6342, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2197, Accuracy: 0.885, F1 Micro: 0.7208, F1 Macro: 0.7206\n",
      "Epoch 7/10, Train Loss: 0.1814, Accuracy: 0.8841, F1 Micro: 0.6989, F1 Macro: 0.6911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1398, Accuracy: 0.8852, F1 Micro: 0.7295, F1 Macro: 0.7263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1148, Accuracy: 0.8852, F1 Micro: 0.7372, F1 Macro: 0.7343\n",
      "Epoch 10/10, Train Loss: 0.0941, Accuracy: 0.8844, F1 Micro: 0.7372, F1 Macro: 0.7357\n",
      "Model 1 - Iteration 971: Accuracy: 0.8852, F1 Micro: 0.7372, F1 Macro: 0.7343\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.62      0.62      0.62       248\n",
      "         radikalisme       0.71      0.77      0.74       243\n",
      "pencemaran_nama_baik       0.65      0.73      0.69       504\n",
      "\n",
      "           micro avg       0.72      0.76      0.74      1365\n",
      "           macro avg       0.72      0.75      0.73      1365\n",
      "        weighted avg       0.73      0.76      0.74      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 74.56349086761475 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5568, Accuracy: 0.7927, F1 Micro: 0.0595, F1 Macro: 0.0498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4492, Accuracy: 0.8542, F1 Micro: 0.5662, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3721, Accuracy: 0.8647, F1 Micro: 0.6302, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3144, Accuracy: 0.8772, F1 Micro: 0.6534, F1 Macro: 0.6418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2655, Accuracy: 0.8856, F1 Micro: 0.6864, F1 Macro: 0.6804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2046, Accuracy: 0.8878, F1 Micro: 0.7198, F1 Macro: 0.7183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1678, Accuracy: 0.8855, F1 Micro: 0.7264, F1 Macro: 0.7239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1203, Accuracy: 0.8892, F1 Micro: 0.7293, F1 Macro: 0.7265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.8897, F1 Micro: 0.7356, F1 Macro: 0.7299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.8856, F1 Micro: 0.7399, F1 Macro: 0.7411\n",
      "Model 2 - Iteration 971: Accuracy: 0.8856, F1 Micro: 0.7399, F1 Macro: 0.7411\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       370\n",
      "                sara       0.64      0.67      0.66       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.64      0.74      0.69       504\n",
      "\n",
      "           micro avg       0.72      0.76      0.74      1365\n",
      "           macro avg       0.73      0.76      0.74      1365\n",
      "        weighted avg       0.73      0.76      0.74      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 77.82192134857178 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5792, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4891, Accuracy: 0.8447, F1 Micro: 0.5637, F1 Macro: 0.5587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3973, Accuracy: 0.8578, F1 Micro: 0.6147, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3243, Accuracy: 0.8697, F1 Micro: 0.6316, F1 Macro: 0.6248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2775, Accuracy: 0.8813, F1 Micro: 0.6782, F1 Macro: 0.674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2201, Accuracy: 0.885, F1 Micro: 0.7058, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1778, Accuracy: 0.8852, F1 Micro: 0.7234, F1 Macro: 0.7211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1319, Accuracy: 0.887, F1 Micro: 0.7262, F1 Macro: 0.726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1073, Accuracy: 0.8913, F1 Micro: 0.7397, F1 Macro: 0.7362\n",
      "Epoch 10/10, Train Loss: 0.0845, Accuracy: 0.8894, F1 Micro: 0.7249, F1 Macro: 0.7169\n",
      "Model 3 - Iteration 971: Accuracy: 0.8913, F1 Micro: 0.7397, F1 Macro: 0.7362\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.89       370\n",
      "                sara       0.69      0.59      0.64       248\n",
      "         radikalisme       0.72      0.76      0.74       243\n",
      "pencemaran_nama_baik       0.69      0.68      0.68       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.75      0.72      0.74      1365\n",
      "        weighted avg       0.76      0.72      0.74      1365\n",
      "         samples avg       0.41      0.40      0.40      1365\n",
      "\n",
      "Training completed in 75.76845812797546 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8785, F1 Micro: 0.7064, F1 Macro: 0.7025\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 108.95428252220154 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.8309, F1 Micro: 0.4742, F1 Macro: 0.3862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4338, Accuracy: 0.8647, F1 Micro: 0.6659, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3611, Accuracy: 0.8842, F1 Micro: 0.7095, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2931, Accuracy: 0.8873, F1 Micro: 0.7185, F1 Macro: 0.7008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2492, Accuracy: 0.8927, F1 Micro: 0.7275, F1 Macro: 0.7196\n",
      "Epoch 6/10, Train Loss: 0.1951, Accuracy: 0.8878, F1 Micro: 0.7257, F1 Macro: 0.7121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1425, Accuracy: 0.8933, F1 Micro: 0.741, F1 Macro: 0.7392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1134, Accuracy: 0.8955, F1 Micro: 0.7488, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.8958, F1 Micro: 0.7527, F1 Macro: 0.7516\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.8923, F1 Micro: 0.7495, F1 Macro: 0.7492\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8958, F1 Micro: 0.7527, F1 Macro: 0.7516\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.73      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.67      0.69       504\n",
      "\n",
      "           micro avg       0.76      0.74      0.75      1365\n",
      "           macro avg       0.76      0.75      0.75      1365\n",
      "        weighted avg       0.77      0.74      0.75      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 89.40440249443054 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.8458, F1 Micro: 0.5544, F1 Macro: 0.4903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4123, Accuracy: 0.8691, F1 Micro: 0.6762, F1 Macro: 0.6812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3433, Accuracy: 0.8853, F1 Micro: 0.6994, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2756, Accuracy: 0.8884, F1 Micro: 0.716, F1 Macro: 0.6941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2342, Accuracy: 0.8941, F1 Micro: 0.7307, F1 Macro: 0.7218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1796, Accuracy: 0.8878, F1 Micro: 0.7412, F1 Macro: 0.7331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1307, Accuracy: 0.895, F1 Micro: 0.7468, F1 Macro: 0.7446\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.8917, F1 Micro: 0.7417, F1 Macro: 0.7371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0859, Accuracy: 0.8947, F1 Micro: 0.7478, F1 Macro: 0.7432\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.8934, F1 Micro: 0.7428, F1 Macro: 0.736\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8947, F1 Micro: 0.7478, F1 Macro: 0.7432\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.67      0.59      0.63       248\n",
      "         radikalisme       0.75      0.75      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.69      0.69       504\n",
      "\n",
      "           micro avg       0.76      0.73      0.75      1365\n",
      "           macro avg       0.76      0.73      0.74      1365\n",
      "        weighted avg       0.77      0.73      0.75      1365\n",
      "         samples avg       0.42      0.41      0.41      1365\n",
      "\n",
      "Training completed in 89.48916482925415 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5657, Accuracy: 0.7966, F1 Micro: 0.3219, F1 Macro: 0.1951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4417, Accuracy: 0.8639, F1 Micro: 0.6443, F1 Macro: 0.6477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3562, Accuracy: 0.8823, F1 Micro: 0.7131, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2902, Accuracy: 0.8869, F1 Micro: 0.7161, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2538, Accuracy: 0.8894, F1 Micro: 0.7219, F1 Macro: 0.7115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.197, Accuracy: 0.8856, F1 Micro: 0.753, F1 Macro: 0.75\n",
      "Epoch 7/10, Train Loss: 0.158, Accuracy: 0.8947, F1 Micro: 0.741, F1 Macro: 0.7369\n",
      "Epoch 8/10, Train Loss: 0.1288, Accuracy: 0.8977, F1 Micro: 0.7448, F1 Macro: 0.7403\n",
      "Epoch 9/10, Train Loss: 0.1026, Accuracy: 0.8925, F1 Micro: 0.7378, F1 Macro: 0.7363\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.8927, F1 Micro: 0.7486, F1 Macro: 0.7473\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8856, F1 Micro: 0.753, F1 Macro: 0.75\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.60      0.69      0.64       248\n",
      "         radikalisme       0.71      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.63      0.84      0.72       504\n",
      "\n",
      "           micro avg       0.70      0.82      0.75      1365\n",
      "           macro avg       0.71      0.80      0.75      1365\n",
      "        weighted avg       0.71      0.82      0.76      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 85.62667274475098 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.883, F1 Micro: 0.7213, F1 Macro: 0.7177\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 97.0063271522522 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5574, Accuracy: 0.837, F1 Micro: 0.4834, F1 Macro: 0.4266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4238, Accuracy: 0.8745, F1 Micro: 0.6857, F1 Macro: 0.6837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3414, Accuracy: 0.8861, F1 Micro: 0.7083, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2828, Accuracy: 0.8938, F1 Micro: 0.7522, F1 Macro: 0.7458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2252, Accuracy: 0.8959, F1 Micro: 0.7589, F1 Macro: 0.7531\n",
      "Epoch 6/10, Train Loss: 0.1829, Accuracy: 0.8981, F1 Micro: 0.7461, F1 Macro: 0.7392\n",
      "Epoch 7/10, Train Loss: 0.1426, Accuracy: 0.8969, F1 Micro: 0.745, F1 Macro: 0.7389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1069, Accuracy: 0.8983, F1 Micro: 0.7604, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.088, Accuracy: 0.8967, F1 Micro: 0.759, F1 Macro: 0.7574\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.8975, F1 Micro: 0.7538, F1 Macro: 0.7531\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8983, F1 Micro: 0.7604, F1 Macro: 0.7575\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.73      0.78      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.69      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.42      0.42      0.42      1365\n",
      "\n",
      "Training completed in 98.35952663421631 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5427, Accuracy: 0.842, F1 Micro: 0.4953, F1 Macro: 0.4477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.411, Accuracy: 0.8748, F1 Micro: 0.6848, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3334, Accuracy: 0.8888, F1 Micro: 0.727, F1 Macro: 0.7097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2736, Accuracy: 0.893, F1 Micro: 0.7524, F1 Macro: 0.747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2177, Accuracy: 0.8945, F1 Micro: 0.7532, F1 Macro: 0.7457\n",
      "Epoch 6/10, Train Loss: 0.1778, Accuracy: 0.8925, F1 Micro: 0.7352, F1 Macro: 0.7218\n",
      "Epoch 7/10, Train Loss: 0.1346, Accuracy: 0.8917, F1 Micro: 0.7342, F1 Macro: 0.7226\n",
      "Epoch 8/10, Train Loss: 0.0987, Accuracy: 0.8945, F1 Micro: 0.746, F1 Macro: 0.739\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.8933, F1 Micro: 0.7404, F1 Macro: 0.7313\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.8922, F1 Micro: 0.735, F1 Macro: 0.7266\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8945, F1 Micro: 0.7532, F1 Macro: 0.7457\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.68      0.58      0.63       248\n",
      "         radikalisme       0.77      0.73      0.75       243\n",
      "pencemaran_nama_baik       0.67      0.76      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.76      0.74      0.75      1365\n",
      "        weighted avg       0.76      0.75      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 97.17468929290771 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5626, Accuracy: 0.837, F1 Micro: 0.5261, F1 Macro: 0.5226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4197, Accuracy: 0.8808, F1 Micro: 0.7082, F1 Macro: 0.701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3381, Accuracy: 0.8878, F1 Micro: 0.7155, F1 Macro: 0.6965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2773, Accuracy: 0.8955, F1 Micro: 0.7521, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2264, Accuracy: 0.8927, F1 Micro: 0.7523, F1 Macro: 0.7492\n",
      "Epoch 6/10, Train Loss: 0.1915, Accuracy: 0.8948, F1 Micro: 0.7276, F1 Macro: 0.7128\n",
      "Epoch 7/10, Train Loss: 0.1445, Accuracy: 0.8958, F1 Micro: 0.7486, F1 Macro: 0.7439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.113, Accuracy: 0.8944, F1 Micro: 0.7524, F1 Macro: 0.7467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0892, Accuracy: 0.8952, F1 Micro: 0.7573, F1 Macro: 0.7555\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.8959, F1 Micro: 0.7394, F1 Macro: 0.7343\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8952, F1 Micro: 0.7573, F1 Macro: 0.7555\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.89       370\n",
      "                sara       0.67      0.66      0.66       248\n",
      "         radikalisme       0.74      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1365\n",
      "           macro avg       0.75      0.76      0.76      1365\n",
      "        weighted avg       0.75      0.77      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 100.37282657623291 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8863, F1 Micro: 0.7302, F1 Macro: 0.7265\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 87.47312951087952 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5292, Accuracy: 0.8541, F1 Micro: 0.6029, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3887, Accuracy: 0.8781, F1 Micro: 0.737, F1 Macro: 0.7428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3357, Accuracy: 0.893, F1 Micro: 0.7408, F1 Macro: 0.7344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2655, Accuracy: 0.8975, F1 Micro: 0.75, F1 Macro: 0.7456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2342, Accuracy: 0.8994, F1 Micro: 0.7702, F1 Macro: 0.7699\n",
      "Epoch 6/10, Train Loss: 0.1764, Accuracy: 0.902, F1 Micro: 0.7578, F1 Macro: 0.7536\n",
      "Epoch 7/10, Train Loss: 0.1388, Accuracy: 0.8998, F1 Micro: 0.752, F1 Macro: 0.7442\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.8992, F1 Micro: 0.7583, F1 Macro: 0.7525\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9008, F1 Micro: 0.7639, F1 Macro: 0.7646\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9, F1 Micro: 0.7546, F1 Macro: 0.7503\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8994, F1 Micro: 0.7702, F1 Macro: 0.7699\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 110.57368588447571 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.8586, F1 Micro: 0.6215, F1 Macro: 0.6079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3781, Accuracy: 0.8803, F1 Micro: 0.7135, F1 Macro: 0.7125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.32, Accuracy: 0.8923, F1 Micro: 0.7415, F1 Macro: 0.729\n",
      "Epoch 4/10, Train Loss: 0.2546, Accuracy: 0.8948, F1 Micro: 0.7366, F1 Macro: 0.7304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2182, Accuracy: 0.8989, F1 Micro: 0.7589, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1643, Accuracy: 0.9, F1 Micro: 0.7656, F1 Macro: 0.763\n",
      "Epoch 7/10, Train Loss: 0.1214, Accuracy: 0.8998, F1 Micro: 0.7649, F1 Macro: 0.7614\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9002, F1 Micro: 0.7518, F1 Macro: 0.7398\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.9, F1 Micro: 0.7512, F1 Macro: 0.7456\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.8991, F1 Micro: 0.7459, F1 Macro: 0.7412\n",
      "Model 2 - Iteration 2394: Accuracy: 0.9, F1 Micro: 0.7656, F1 Macro: 0.763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.76      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 110.38826656341553 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5441, Accuracy: 0.8477, F1 Micro: 0.6029, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3957, Accuracy: 0.8755, F1 Micro: 0.7115, F1 Macro: 0.717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.337, Accuracy: 0.8941, F1 Micro: 0.7466, F1 Macro: 0.7398\n",
      "Epoch 4/10, Train Loss: 0.2657, Accuracy: 0.8938, F1 Micro: 0.7273, F1 Macro: 0.7197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2264, Accuracy: 0.8984, F1 Micro: 0.7536, F1 Macro: 0.7498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1698, Accuracy: 0.8991, F1 Micro: 0.76, F1 Macro: 0.7564\n",
      "Epoch 7/10, Train Loss: 0.1275, Accuracy: 0.8961, F1 Micro: 0.757, F1 Macro: 0.7523\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.8978, F1 Micro: 0.75, F1 Macro: 0.741\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.8963, F1 Micro: 0.7438, F1 Macro: 0.7419\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.8958, F1 Micro: 0.7552, F1 Macro: 0.7527\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8991, F1 Micro: 0.76, F1 Macro: 0.7564\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.74      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1365\n",
      "           macro avg       0.77      0.75      0.76      1365\n",
      "        weighted avg       0.77      0.75      0.76      1365\n",
      "         samples avg       0.41      0.41      0.41      1365\n",
      "\n",
      "Training completed in 110.8065094947815 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8889, F1 Micro: 0.7372, F1 Macro: 0.7338\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 79.58904099464417 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5119, Accuracy: 0.86, F1 Micro: 0.6257, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3659, Accuracy: 0.8894, F1 Micro: 0.7256, F1 Macro: 0.7157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3078, Accuracy: 0.8991, F1 Micro: 0.7473, F1 Macro: 0.7446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2534, Accuracy: 0.9005, F1 Micro: 0.7581, F1 Macro: 0.7481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1994, Accuracy: 0.8983, F1 Micro: 0.7777, F1 Macro: 0.78\n",
      "Epoch 6/10, Train Loss: 0.1582, Accuracy: 0.8983, F1 Micro: 0.7511, F1 Macro: 0.7426\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9002, F1 Micro: 0.7702, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.9017, F1 Micro: 0.7682, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.0677, Accuracy: 0.9, F1 Micro: 0.7502, F1 Macro: 0.7439\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.8992, F1 Micro: 0.7645, F1 Macro: 0.7626\n",
      "Model 1 - Iteration 2777: Accuracy: 0.8983, F1 Micro: 0.7777, F1 Macro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       370\n",
      "                sara       0.65      0.79      0.71       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 121.90616345405579 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4958, Accuracy: 0.8591, F1 Micro: 0.6404, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3577, Accuracy: 0.8919, F1 Micro: 0.7324, F1 Macro: 0.7253\n",
      "Epoch 3/10, Train Loss: 0.2984, Accuracy: 0.8945, F1 Micro: 0.7318, F1 Macro: 0.7256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2457, Accuracy: 0.8992, F1 Micro: 0.7589, F1 Macro: 0.7512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1882, Accuracy: 0.9011, F1 Micro: 0.7712, F1 Macro: 0.7677\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.898, F1 Micro: 0.7581, F1 Macro: 0.7512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9006, F1 Micro: 0.7724, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9022, F1 Micro: 0.7614, F1 Macro: 0.7565\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.8977, F1 Micro: 0.7651, F1 Macro: 0.7627\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.8997, F1 Micro: 0.754, F1 Macro: 0.7491\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9006, F1 Micro: 0.7724, F1 Macro: 0.7712\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.68      0.70      0.69       248\n",
      "         radikalisme       0.73      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 122.16761469841003 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5242, Accuracy: 0.8622, F1 Micro: 0.6497, F1 Macro: 0.622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3728, Accuracy: 0.8917, F1 Micro: 0.74, F1 Macro: 0.7388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3056, Accuracy: 0.8969, F1 Micro: 0.7492, F1 Macro: 0.7478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2471, Accuracy: 0.8991, F1 Micro: 0.7604, F1 Macro: 0.7566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1964, Accuracy: 0.8916, F1 Micro: 0.7685, F1 Macro: 0.7698\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.8983, F1 Micro: 0.7577, F1 Macro: 0.7486\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.8984, F1 Micro: 0.7682, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9008, F1 Micro: 0.7703, F1 Macro: 0.7715\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.8978, F1 Micro: 0.7443, F1 Macro: 0.7391\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.8983, F1 Micro: 0.7702, F1 Macro: 0.7704\n",
      "Model 3 - Iteration 2777: Accuracy: 0.9008, F1 Micro: 0.7703, F1 Macro: 0.7715\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 123.43472719192505 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8907, F1 Micro: 0.7433, F1 Macro: 0.7406\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 71.59112739562988 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4993, Accuracy: 0.8683, F1 Micro: 0.6774, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3572, Accuracy: 0.883, F1 Micro: 0.7078, F1 Macro: 0.7083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2935, Accuracy: 0.8952, F1 Micro: 0.7608, F1 Macro: 0.7533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2376, Accuracy: 0.9005, F1 Micro: 0.7647, F1 Macro: 0.7579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1859, Accuracy: 0.8998, F1 Micro: 0.7667, F1 Macro: 0.7563\n",
      "Epoch 6/10, Train Loss: 0.1461, Accuracy: 0.903, F1 Micro: 0.7552, F1 Macro: 0.7475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1197, Accuracy: 0.9022, F1 Micro: 0.7693, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9025, F1 Micro: 0.7744, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9044, F1 Micro: 0.7666, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0588, Accuracy: 0.9006, F1 Micro: 0.7779, F1 Macro: 0.7781\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9006, F1 Micro: 0.7779, F1 Macro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 136.6479218006134 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4873, Accuracy: 0.8705, F1 Micro: 0.6637, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3518, Accuracy: 0.8839, F1 Micro: 0.706, F1 Macro: 0.7079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2921, Accuracy: 0.8984, F1 Micro: 0.7655, F1 Macro: 0.7599\n",
      "Epoch 4/10, Train Loss: 0.2358, Accuracy: 0.902, F1 Micro: 0.7569, F1 Macro: 0.7461\n",
      "Epoch 5/10, Train Loss: 0.1871, Accuracy: 0.9023, F1 Micro: 0.7646, F1 Macro: 0.7552\n",
      "Epoch 6/10, Train Loss: 0.1367, Accuracy: 0.8991, F1 Micro: 0.7408, F1 Macro: 0.7319\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.8991, F1 Micro: 0.7531, F1 Macro: 0.7433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.897, F1 Micro: 0.7661, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.8998, F1 Micro: 0.7573, F1 Macro: 0.7486\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9, F1 Micro: 0.763, F1 Macro: 0.7589\n",
      "Model 2 - Iteration 3122: Accuracy: 0.897, F1 Micro: 0.7661, F1 Macro: 0.7636\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.90       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 129.5251829624176 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5065, Accuracy: 0.8677, F1 Micro: 0.655, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3621, Accuracy: 0.8817, F1 Micro: 0.6992, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2946, Accuracy: 0.8958, F1 Micro: 0.7665, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2331, Accuracy: 0.9034, F1 Micro: 0.7704, F1 Macro: 0.7652\n",
      "Epoch 5/10, Train Loss: 0.1876, Accuracy: 0.8978, F1 Micro: 0.7488, F1 Macro: 0.7423\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9, F1 Micro: 0.744, F1 Macro: 0.7369\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.8998, F1 Micro: 0.7675, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.083, Accuracy: 0.9011, F1 Micro: 0.7577, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0677, Accuracy: 0.903, F1 Micro: 0.7734, F1 Macro: 0.7717\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9006, F1 Micro: 0.7657, F1 Macro: 0.7669\n",
      "Model 3 - Iteration 3122: Accuracy: 0.903, F1 Micro: 0.7734, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.69      0.67      0.68       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 131.43096232414246 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8921, F1 Micro: 0.7474, F1 Macro: 0.7449\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 64.8534619808197 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4849, Accuracy: 0.8617, F1 Micro: 0.6723, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.347, Accuracy: 0.8941, F1 Micro: 0.748, F1 Macro: 0.7486\n",
      "Epoch 3/10, Train Loss: 0.2869, Accuracy: 0.8939, F1 Micro: 0.7423, F1 Macro: 0.7278\n",
      "Epoch 4/10, Train Loss: 0.2416, Accuracy: 0.8998, F1 Micro: 0.7443, F1 Macro: 0.7352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1931, Accuracy: 0.9045, F1 Micro: 0.766, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1493, Accuracy: 0.8998, F1 Micro: 0.7718, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.9039, F1 Micro: 0.7764, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.902, F1 Micro: 0.7769, F1 Macro: 0.777\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.8994, F1 Micro: 0.7572, F1 Macro: 0.7518\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9025, F1 Micro: 0.7672, F1 Macro: 0.7628\n",
      "Model 1 - Iteration 3432: Accuracy: 0.902, F1 Micro: 0.7769, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 143.82875227928162 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4728, Accuracy: 0.8653, F1 Micro: 0.6854, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3369, Accuracy: 0.8955, F1 Micro: 0.7525, F1 Macro: 0.7493\n",
      "Epoch 3/10, Train Loss: 0.2779, Accuracy: 0.8984, F1 Micro: 0.7513, F1 Macro: 0.7381\n",
      "Epoch 4/10, Train Loss: 0.2331, Accuracy: 0.8995, F1 Micro: 0.7447, F1 Macro: 0.7384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1792, Accuracy: 0.9027, F1 Micro: 0.7542, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9017, F1 Micro: 0.7723, F1 Macro: 0.7703\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.8984, F1 Micro: 0.7638, F1 Macro: 0.7586\n",
      "Epoch 8/10, Train Loss: 0.0775, Accuracy: 0.902, F1 Micro: 0.7679, F1 Macro: 0.7633\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9008, F1 Micro: 0.7575, F1 Macro: 0.7537\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9027, F1 Micro: 0.7702, F1 Macro: 0.7665\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9017, F1 Micro: 0.7723, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 139.25780963897705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4929, Accuracy: 0.8586, F1 Micro: 0.676, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3496, Accuracy: 0.8964, F1 Micro: 0.7531, F1 Macro: 0.7497\n",
      "Epoch 3/10, Train Loss: 0.2863, Accuracy: 0.8963, F1 Micro: 0.75, F1 Macro: 0.7354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2451, Accuracy: 0.8994, F1 Micro: 0.7575, F1 Macro: 0.7534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1904, Accuracy: 0.902, F1 Micro: 0.7584, F1 Macro: 0.7543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1464, Accuracy: 0.9023, F1 Micro: 0.7691, F1 Macro: 0.7651\n",
      "Epoch 7/10, Train Loss: 0.1096, Accuracy: 0.8998, F1 Micro: 0.7658, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9022, F1 Micro: 0.7797, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9006, F1 Micro: 0.7712, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9019, F1 Micro: 0.7701, F1 Macro: 0.7687\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9022, F1 Micro: 0.7797, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.67      0.71      0.68       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 143.42480564117432 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8933, F1 Micro: 0.7511, F1 Macro: 0.7486\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 58.689838886260986 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4719, Accuracy: 0.8672, F1 Micro: 0.6432, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3373, Accuracy: 0.8952, F1 Micro: 0.739, F1 Macro: 0.7193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2746, Accuracy: 0.8978, F1 Micro: 0.7534, F1 Macro: 0.7452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2241, Accuracy: 0.9045, F1 Micro: 0.779, F1 Macro: 0.7741\n",
      "Epoch 5/10, Train Loss: 0.1809, Accuracy: 0.9025, F1 Micro: 0.7602, F1 Macro: 0.7474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1442, Accuracy: 0.9055, F1 Micro: 0.782, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9023, F1 Micro: 0.7753, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0759, Accuracy: 0.8978, F1 Micro: 0.7735, F1 Macro: 0.7721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9067, F1 Micro: 0.7831, F1 Macro: 0.7816\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9, F1 Micro: 0.7758, F1 Macro: 0.774\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9067, F1 Micro: 0.7831, F1 Macro: 0.7816\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.71      0.69       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 150.82576656341553 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4578, Accuracy: 0.872, F1 Micro: 0.6614, F1 Macro: 0.6572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3283, Accuracy: 0.8977, F1 Micro: 0.7555, F1 Macro: 0.7406\n",
      "Epoch 3/10, Train Loss: 0.2688, Accuracy: 0.9009, F1 Micro: 0.7525, F1 Macro: 0.746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2154, Accuracy: 0.9053, F1 Micro: 0.7764, F1 Macro: 0.7696\n",
      "Epoch 5/10, Train Loss: 0.171, Accuracy: 0.9066, F1 Micro: 0.775, F1 Macro: 0.7651\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.903, F1 Micro: 0.7675, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7763\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.8991, F1 Micro: 0.7757, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9033, F1 Micro: 0.7684, F1 Macro: 0.7635\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9039, F1 Micro: 0.7704, F1 Macro: 0.7656\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.75      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 147.89119386672974 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4754, Accuracy: 0.8687, F1 Micro: 0.6571, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3369, Accuracy: 0.8972, F1 Micro: 0.7563, F1 Macro: 0.7438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2737, Accuracy: 0.9017, F1 Micro: 0.7671, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2209, Accuracy: 0.9064, F1 Micro: 0.781, F1 Macro: 0.776\n",
      "Epoch 5/10, Train Loss: 0.1797, Accuracy: 0.9059, F1 Micro: 0.7767, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.1424, Accuracy: 0.9031, F1 Micro: 0.7742, F1 Macro: 0.7712\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9027, F1 Micro: 0.7727, F1 Macro: 0.7664\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.8989, F1 Micro: 0.7679, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.9013, F1 Micro: 0.7666, F1 Macro: 0.7628\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9031, F1 Micro: 0.7674, F1 Macro: 0.7639\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9064, F1 Micro: 0.781, F1 Macro: 0.776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.71      0.66      0.68       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 147.62099289894104 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8946, F1 Micro: 0.7544, F1 Macro: 0.7519\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 53.06378960609436 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4794, Accuracy: 0.8717, F1 Micro: 0.6481, F1 Macro: 0.6168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3326, Accuracy: 0.8966, F1 Micro: 0.7548, F1 Macro: 0.7493\n",
      "Epoch 3/10, Train Loss: 0.2808, Accuracy: 0.8989, F1 Micro: 0.7384, F1 Macro: 0.727\n",
      "Epoch 4/10, Train Loss: 0.229, Accuracy: 0.8994, F1 Micro: 0.7536, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1751, Accuracy: 0.8995, F1 Micro: 0.7724, F1 Macro: 0.7698\n",
      "Epoch 6/10, Train Loss: 0.1294, Accuracy: 0.9038, F1 Micro: 0.7713, F1 Macro: 0.7665\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9014, F1 Micro: 0.7657, F1 Macro: 0.7608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9016, F1 Micro: 0.78, F1 Macro: 0.7797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9009, F1 Micro: 0.7815, F1 Macro: 0.7822\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9056, F1 Micro: 0.7766, F1 Macro: 0.7732\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9009, F1 Micro: 0.7815, F1 Macro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 154.84796142578125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4687, Accuracy: 0.8806, F1 Micro: 0.7002, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3277, Accuracy: 0.8991, F1 Micro: 0.7628, F1 Macro: 0.7564\n",
      "Epoch 3/10, Train Loss: 0.2736, Accuracy: 0.8955, F1 Micro: 0.7266, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2186, Accuracy: 0.9044, F1 Micro: 0.7735, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.9, F1 Micro: 0.7753, F1 Macro: 0.7694\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.9047, F1 Micro: 0.7742, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0972, Accuracy: 0.8991, F1 Micro: 0.7777, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0761, Accuracy: 0.9045, F1 Micro: 0.7819, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9005, F1 Micro: 0.7761, F1 Macro: 0.7752\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9025, F1 Micro: 0.7709, F1 Macro: 0.7696\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9045, F1 Micro: 0.7819, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.72      0.69       248\n",
      "         radikalisme       0.76      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 155.8654429912567 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4914, Accuracy: 0.8712, F1 Micro: 0.6581, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3341, Accuracy: 0.8963, F1 Micro: 0.7526, F1 Macro: 0.7487\n",
      "Epoch 3/10, Train Loss: 0.2791, Accuracy: 0.9003, F1 Micro: 0.7421, F1 Macro: 0.7324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2295, Accuracy: 0.9028, F1 Micro: 0.7763, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1745, Accuracy: 0.9002, F1 Micro: 0.7808, F1 Macro: 0.7801\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.9033, F1 Micro: 0.7776, F1 Macro: 0.7741\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.8972, F1 Micro: 0.7747, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9017, F1 Micro: 0.7775, F1 Macro: 0.7769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.9025, F1 Micro: 0.7817, F1 Macro: 0.7827\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9047, F1 Micro: 0.7774, F1 Macro: 0.7776\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9025, F1 Micro: 0.7817, F1 Macro: 0.7827\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.68      0.72      0.70       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 154.63381910324097 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8954, F1 Micro: 0.7571, F1 Macro: 0.7549\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 50.655704975128174 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4637, Accuracy: 0.8659, F1 Micro: 0.6146, F1 Macro: 0.5916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3172, Accuracy: 0.8964, F1 Micro: 0.7631, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2618, Accuracy: 0.9022, F1 Micro: 0.7709, F1 Macro: 0.767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2181, Accuracy: 0.9077, F1 Micro: 0.7831, F1 Macro: 0.7812\n",
      "Epoch 5/10, Train Loss: 0.1746, Accuracy: 0.9017, F1 Micro: 0.77, F1 Macro: 0.7686\n",
      "Epoch 6/10, Train Loss: 0.1312, Accuracy: 0.9017, F1 Micro: 0.7521, F1 Macro: 0.7409\n",
      "Epoch 7/10, Train Loss: 0.0968, Accuracy: 0.9014, F1 Micro: 0.763, F1 Macro: 0.7616\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.9033, F1 Micro: 0.7801, F1 Macro: 0.7774\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.774\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9047, F1 Micro: 0.7767, F1 Macro: 0.77\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9077, F1 Micro: 0.7831, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.70      0.66      0.68       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 160.62310886383057 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4537, Accuracy: 0.8677, F1 Micro: 0.6173, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3133, Accuracy: 0.8959, F1 Micro: 0.7665, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2621, Accuracy: 0.8992, F1 Micro: 0.7741, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2134, Accuracy: 0.9061, F1 Micro: 0.7831, F1 Macro: 0.7822\n",
      "Epoch 5/10, Train Loss: 0.1738, Accuracy: 0.9031, F1 Micro: 0.7778, F1 Macro: 0.7758\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9048, F1 Micro: 0.7755, F1 Macro: 0.7703\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9052, F1 Micro: 0.7689, F1 Macro: 0.7634\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9058, F1 Micro: 0.7777, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9028, F1 Micro: 0.7691, F1 Macro: 0.7666\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9058, F1 Micro: 0.7742, F1 Macro: 0.7691\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9061, F1 Micro: 0.7831, F1 Macro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.78      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 160.59852242469788 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4707, Accuracy: 0.8592, F1 Micro: 0.588, F1 Macro: 0.5587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3182, Accuracy: 0.8928, F1 Micro: 0.7598, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2604, Accuracy: 0.8997, F1 Micro: 0.7664, F1 Macro: 0.7657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2154, Accuracy: 0.9067, F1 Micro: 0.7819, F1 Macro: 0.78\n",
      "Epoch 5/10, Train Loss: 0.1746, Accuracy: 0.9006, F1 Micro: 0.7677, F1 Macro: 0.766\n",
      "Epoch 6/10, Train Loss: 0.1293, Accuracy: 0.9019, F1 Micro: 0.7426, F1 Macro: 0.7297\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9017, F1 Micro: 0.7693, F1 Macro: 0.7692\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9031, F1 Micro: 0.7739, F1 Macro: 0.7701\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9025, F1 Micro: 0.7751, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9025, F1 Micro: 0.7773, F1 Macro: 0.7778\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9067, F1 Micro: 0.7819, F1 Macro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 160.43780326843262 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8965, F1 Micro: 0.7595, F1 Macro: 0.7573\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 44.72142958641052 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4535, Accuracy: 0.8839, F1 Micro: 0.7046, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3146, Accuracy: 0.8955, F1 Micro: 0.7591, F1 Macro: 0.7511\n",
      "Epoch 3/10, Train Loss: 0.2625, Accuracy: 0.9017, F1 Micro: 0.7521, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2117, Accuracy: 0.9005, F1 Micro: 0.7763, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1711, Accuracy: 0.9059, F1 Micro: 0.7805, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9044, F1 Micro: 0.7819, F1 Macro: 0.7804\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9033, F1 Micro: 0.772, F1 Macro: 0.7638\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9038, F1 Micro: 0.7735, F1 Macro: 0.7689\n",
      "Epoch 9/10, Train Loss: 0.0583, Accuracy: 0.905, F1 Micro: 0.77, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9062, F1 Micro: 0.7791, F1 Macro: 0.7741\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9044, F1 Micro: 0.7819, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.77      0.85      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 171.4956066608429 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4458, Accuracy: 0.878, F1 Micro: 0.6808, F1 Macro: 0.6771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3077, Accuracy: 0.8984, F1 Micro: 0.7578, F1 Macro: 0.7502\n",
      "Epoch 3/10, Train Loss: 0.2557, Accuracy: 0.8994, F1 Micro: 0.7463, F1 Macro: 0.737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2098, Accuracy: 0.9036, F1 Micro: 0.7778, F1 Macro: 0.7744\n",
      "Epoch 5/10, Train Loss: 0.1656, Accuracy: 0.903, F1 Micro: 0.7772, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.9028, F1 Micro: 0.7819, F1 Macro: 0.7801\n",
      "Epoch 7/10, Train Loss: 0.0952, Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9036, F1 Micro: 0.7742, F1 Macro: 0.7687\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.903, F1 Micro: 0.7716, F1 Macro: 0.7673\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.9031, F1 Micro: 0.766, F1 Macro: 0.7582\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9028, F1 Micro: 0.7819, F1 Macro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.68      0.70      0.69       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 166.9332458972931 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4628, Accuracy: 0.8791, F1 Micro: 0.6854, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3142, Accuracy: 0.8983, F1 Micro: 0.7584, F1 Macro: 0.7537\n",
      "Epoch 3/10, Train Loss: 0.2571, Accuracy: 0.8989, F1 Micro: 0.746, F1 Macro: 0.7333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2112, Accuracy: 0.9009, F1 Micro: 0.7805, F1 Macro: 0.7791\n",
      "Epoch 5/10, Train Loss: 0.1684, Accuracy: 0.9038, F1 Micro: 0.7727, F1 Macro: 0.7681\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.902, F1 Micro: 0.779, F1 Macro: 0.7779\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9002, F1 Micro: 0.7748, F1 Macro: 0.7725\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.9042, F1 Micro: 0.7698, F1 Macro: 0.7608\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9048, F1 Micro: 0.777, F1 Macro: 0.7756\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9061, F1 Micro: 0.775, F1 Macro: 0.7718\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9009, F1 Micro: 0.7805, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.72      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 165.98178339004517 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.897, F1 Micro: 0.7613, F1 Macro: 0.7592\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 41.8946635723114 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4468, Accuracy: 0.8844, F1 Micro: 0.7191, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3109, Accuracy: 0.8995, F1 Micro: 0.763, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2523, Accuracy: 0.9072, F1 Micro: 0.7863, F1 Macro: 0.7806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2044, Accuracy: 0.9052, F1 Micro: 0.7878, F1 Macro: 0.7846\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9052, F1 Micro: 0.7816, F1 Macro: 0.7781\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9052, F1 Micro: 0.785, F1 Macro: 0.7827\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9045, F1 Micro: 0.7776, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9017, F1 Micro: 0.7784, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9023, F1 Micro: 0.778, F1 Macro: 0.7777\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.905, F1 Micro: 0.7741, F1 Macro: 0.7726\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9052, F1 Micro: 0.7878, F1 Macro: 0.7846\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 172.16860580444336 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.43, Accuracy: 0.8813, F1 Micro: 0.7034, F1 Macro: 0.6971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3048, Accuracy: 0.9, F1 Micro: 0.7557, F1 Macro: 0.7456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2487, Accuracy: 0.9053, F1 Micro: 0.782, F1 Macro: 0.7765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1981, Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7806\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9045, F1 Micro: 0.7784, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9072, F1 Micro: 0.7837, F1 Macro: 0.7802\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.905, F1 Micro: 0.7699, F1 Macro: 0.7593\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9011, F1 Micro: 0.7766, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9053, F1 Micro: 0.7815, F1 Macro: 0.7786\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9006, F1 Micro: 0.758, F1 Macro: 0.7485\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7806\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.25496888160706 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4519, Accuracy: 0.88, F1 Micro: 0.7219, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3129, Accuracy: 0.9003, F1 Micro: 0.7673, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.254, Accuracy: 0.9045, F1 Micro: 0.7808, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2061, Accuracy: 0.9023, F1 Micro: 0.7834, F1 Macro: 0.7825\n",
      "Epoch 5/10, Train Loss: 0.1656, Accuracy: 0.903, F1 Micro: 0.7796, F1 Macro: 0.7785\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9033, F1 Micro: 0.7784, F1 Macro: 0.7765\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9025, F1 Micro: 0.7723, F1 Macro: 0.7648\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.8997, F1 Micro: 0.7699, F1 Macro: 0.7606\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9022, F1 Micro: 0.7816, F1 Macro: 0.7803\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9008, F1 Micro: 0.7566, F1 Macro: 0.751\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9023, F1 Micro: 0.7834, F1 Macro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 172.31780362129211 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8976, F1 Micro: 0.7631, F1 Macro: 0.761\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 35.65749144554138 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4407, Accuracy: 0.8628, F1 Micro: 0.5943, F1 Macro: 0.5583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3035, Accuracy: 0.9019, F1 Micro: 0.7691, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2513, Accuracy: 0.9019, F1 Micro: 0.7768, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2106, Accuracy: 0.9062, F1 Micro: 0.7834, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9073, F1 Micro: 0.7865, F1 Macro: 0.7822\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9017, F1 Micro: 0.7617, F1 Macro: 0.7547\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9022, F1 Micro: 0.7817, F1 Macro: 0.7794\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9059, F1 Micro: 0.7787, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9014, F1 Micro: 0.7757, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9027, F1 Micro: 0.7753, F1 Macro: 0.773\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9073, F1 Micro: 0.7865, F1 Macro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.70      0.65      0.67       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 177.6354205608368 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4271, Accuracy: 0.8666, F1 Micro: 0.6065, F1 Macro: 0.5708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.9036, F1 Micro: 0.7681, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2447, Accuracy: 0.9036, F1 Micro: 0.7752, F1 Macro: 0.7712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2045, Accuracy: 0.9053, F1 Micro: 0.7815, F1 Macro: 0.7754\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9056, F1 Micro: 0.7745, F1 Macro: 0.7695\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9048, F1 Micro: 0.7682, F1 Macro: 0.7607\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.8984, F1 Micro: 0.7697, F1 Macro: 0.7698\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.905, F1 Micro: 0.7686, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9011, F1 Micro: 0.7735, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9039, F1 Micro: 0.7704, F1 Macro: 0.7637\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9053, F1 Micro: 0.7815, F1 Macro: 0.7754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.69      0.63      0.66       248\n",
      "         radikalisme       0.79      0.79      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 175.80301427841187 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4479, Accuracy: 0.8673, F1 Micro: 0.6314, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3042, Accuracy: 0.8991, F1 Micro: 0.7635, F1 Macro: 0.7607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2503, Accuracy: 0.9011, F1 Micro: 0.7762, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9038, F1 Micro: 0.7817, F1 Macro: 0.7758\n",
      "Epoch 5/10, Train Loss: 0.1655, Accuracy: 0.9041, F1 Micro: 0.7812, F1 Macro: 0.779\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9003, F1 Micro: 0.7692, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9022, F1 Micro: 0.7783, F1 Macro: 0.7767\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9023, F1 Micro: 0.7746, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9042, F1 Micro: 0.7821, F1 Macro: 0.7787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9016, F1 Micro: 0.7826, F1 Macro: 0.7832\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9016, F1 Micro: 0.7826, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.62      0.78      0.69       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 179.55836486816406 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8981, F1 Micro: 0.7646, F1 Macro: 0.7624\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 33.807854890823364 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4273, Accuracy: 0.8711, F1 Micro: 0.7314, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3012, Accuracy: 0.9033, F1 Micro: 0.7637, F1 Macro: 0.7564\n",
      "Epoch 3/10, Train Loss: 0.2415, Accuracy: 0.9009, F1 Micro: 0.7575, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2025, Accuracy: 0.9073, F1 Micro: 0.7788, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9023, F1 Micro: 0.7838, F1 Macro: 0.7802\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.9056, F1 Micro: 0.7714, F1 Macro: 0.7599\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9052, F1 Micro: 0.7827, F1 Macro: 0.7782\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9041, F1 Micro: 0.7761, F1 Macro: 0.7718\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9055, F1 Micro: 0.7801, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9038, F1 Micro: 0.7742, F1 Macro: 0.7692\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9023, F1 Micro: 0.7838, F1 Macro: 0.7802\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 181.67414379119873 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4176, Accuracy: 0.8866, F1 Micro: 0.7454, F1 Macro: 0.745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2925, Accuracy: 0.9048, F1 Micro: 0.762, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2365, Accuracy: 0.9058, F1 Micro: 0.7746, F1 Macro: 0.7726\n",
      "Epoch 4/10, Train Loss: 0.1994, Accuracy: 0.9047, F1 Micro: 0.7715, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.9034, F1 Micro: 0.7844, F1 Macro: 0.779\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.905, F1 Micro: 0.7718, F1 Macro: 0.7584\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9034, F1 Micro: 0.7759, F1 Macro: 0.7746\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.905, F1 Micro: 0.7758, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9062, F1 Micro: 0.7783, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0381, Accuracy: 0.9069, F1 Micro: 0.782, F1 Macro: 0.7795\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9034, F1 Micro: 0.7844, F1 Macro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.8233528137207 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4371, Accuracy: 0.8756, F1 Micro: 0.7305, F1 Macro: 0.7355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3015, Accuracy: 0.9041, F1 Micro: 0.7688, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2041, Accuracy: 0.9028, F1 Micro: 0.7758, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9045, F1 Micro: 0.779, F1 Macro: 0.774\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9045, F1 Micro: 0.7678, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9023, F1 Micro: 0.7805, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9052, F1 Micro: 0.7661, F1 Macro: 0.7589\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9031, F1 Micro: 0.7765, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.8995, F1 Micro: 0.7754, F1 Macro: 0.7729\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9023, F1 Micro: 0.7805, F1 Macro: 0.7802\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.69      0.68      0.69       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 185.2356059551239 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8984, F1 Micro: 0.7658, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 29.664615392684937 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4139, Accuracy: 0.8847, F1 Micro: 0.7146, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.9017, F1 Micro: 0.7783, F1 Macro: 0.7718\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.8975, F1 Micro: 0.7521, F1 Macro: 0.7415\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.897, F1 Micro: 0.7733, F1 Macro: 0.7713\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.9062, F1 Micro: 0.7743, F1 Macro: 0.7695\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.9045, F1 Micro: 0.7635, F1 Macro: 0.7521\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9045, F1 Micro: 0.7745, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9055, F1 Micro: 0.7747, F1 Macro: 0.7678\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.8988, F1 Micro: 0.7775, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9011, F1 Micro: 0.7794, F1 Macro: 0.7751\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9011, F1 Micro: 0.7794, F1 Macro: 0.7751\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 187.45493841171265 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.8886, F1 Micro: 0.7294, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2808, Accuracy: 0.9022, F1 Micro: 0.7808, F1 Macro: 0.7775\n",
      "Epoch 3/10, Train Loss: 0.2331, Accuracy: 0.9019, F1 Micro: 0.759, F1 Macro: 0.7472\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9023, F1 Micro: 0.7802, F1 Macro: 0.7803\n",
      "Epoch 5/10, Train Loss: 0.1425, Accuracy: 0.9072, F1 Micro: 0.7757, F1 Macro: 0.7714\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9027, F1 Micro: 0.7544, F1 Macro: 0.7426\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9036, F1 Micro: 0.7685, F1 Macro: 0.7608\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9028, F1 Micro: 0.7722, F1 Macro: 0.7641\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9023, F1 Micro: 0.7748, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0379, Accuracy: 0.9003, F1 Micro: 0.7769, F1 Macro: 0.7745\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9022, F1 Micro: 0.7808, F1 Macro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 186.1471712589264 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4223, Accuracy: 0.8838, F1 Micro: 0.7169, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2871, Accuracy: 0.8977, F1 Micro: 0.7728, F1 Macro: 0.7688\n",
      "Epoch 3/10, Train Loss: 0.2384, Accuracy: 0.9005, F1 Micro: 0.7557, F1 Macro: 0.7439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.8984, F1 Micro: 0.7771, F1 Macro: 0.7792\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9048, F1 Micro: 0.7675, F1 Macro: 0.7547\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.8997, F1 Micro: 0.7422, F1 Macro: 0.7297\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9011, F1 Micro: 0.7635, F1 Macro: 0.7557\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9028, F1 Micro: 0.7741, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.0458, Accuracy: 0.9005, F1 Micro: 0.774, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0378, Accuracy: 0.8997, F1 Micro: 0.7758, F1 Macro: 0.7726\n",
      "Model 3 - Iteration 5063: Accuracy: 0.8984, F1 Micro: 0.7771, F1 Macro: 0.7792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.63      0.75      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 188.1368978023529 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8985, F1 Micro: 0.7666, F1 Macro: 0.7644\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.53781223297119 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4124, Accuracy: 0.8844, F1 Micro: 0.7169, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.8958, F1 Micro: 0.7735, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.9061, F1 Micro: 0.7758, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1886, Accuracy: 0.9016, F1 Micro: 0.7788, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.903, F1 Micro: 0.7842, F1 Macro: 0.7813\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9016, F1 Micro: 0.7732, F1 Macro: 0.7696\n",
      "Epoch 7/10, Train Loss: 0.0941, Accuracy: 0.9009, F1 Micro: 0.776, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9002, F1 Micro: 0.7788, F1 Macro: 0.7791\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9017, F1 Micro: 0.7656, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.898, F1 Micro: 0.7637, F1 Macro: 0.7613\n",
      "Model 1 - Iteration 5263: Accuracy: 0.903, F1 Micro: 0.7842, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 197.03693270683289 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3973, Accuracy: 0.888, F1 Micro: 0.7328, F1 Macro: 0.7298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.8973, F1 Micro: 0.7732, F1 Macro: 0.7657\n",
      "Epoch 3/10, Train Loss: 0.2225, Accuracy: 0.9053, F1 Micro: 0.7706, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9044, F1 Micro: 0.7794, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.1395, Accuracy: 0.9052, F1 Micro: 0.7746, F1 Macro: 0.7626\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9031, F1 Micro: 0.7595, F1 Macro: 0.7537\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9061, F1 Micro: 0.775, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9039, F1 Micro: 0.7821, F1 Macro: 0.7785\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9008, F1 Micro: 0.7722, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9059, F1 Micro: 0.7862, F1 Macro: 0.7823\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9059, F1 Micro: 0.7862, F1 Macro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 195.9982225894928 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8853, F1 Micro: 0.7249, F1 Macro: 0.7206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.273, Accuracy: 0.8988, F1 Micro: 0.7717, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9055, F1 Micro: 0.7762, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7756\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9045, F1 Micro: 0.7746, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9022, F1 Micro: 0.757, F1 Macro: 0.7439\n",
      "Epoch 7/10, Train Loss: 0.087, Accuracy: 0.9028, F1 Micro: 0.772, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.9038, F1 Micro: 0.7822, F1 Macro: 0.779\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.9009, F1 Micro: 0.7736, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9062, F1 Micro: 0.782, F1 Macro: 0.7797\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9038, F1 Micro: 0.7822, F1 Macro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 197.15810537338257 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8989, F1 Micro: 0.7677, F1 Macro: 0.7653\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 20.43740224838257 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3904, Accuracy: 0.8875, F1 Micro: 0.726, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2705, Accuracy: 0.9, F1 Micro: 0.764, F1 Macro: 0.7603\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.902, F1 Micro: 0.7493, F1 Macro: 0.7381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9062, F1 Micro: 0.7719, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.8998, F1 Micro: 0.7822, F1 Macro: 0.7826\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9009, F1 Micro: 0.7744, F1 Macro: 0.7711\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.902, F1 Micro: 0.7757, F1 Macro: 0.7732\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.8947, F1 Micro: 0.7771, F1 Macro: 0.7776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.9055, F1 Micro: 0.7843, F1 Macro: 0.782\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.8989, F1 Micro: 0.7745, F1 Macro: 0.7728\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9055, F1 Micro: 0.7843, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 203.2738709449768 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.8884, F1 Micro: 0.7334, F1 Macro: 0.7294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.9005, F1 Micro: 0.7683, F1 Macro: 0.7663\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.9039, F1 Micro: 0.7533, F1 Macro: 0.7406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9047, F1 Micro: 0.7698, F1 Macro: 0.7585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1372, Accuracy: 0.9025, F1 Micro: 0.7818, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9045, F1 Micro: 0.7764, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9023, F1 Micro: 0.7846, F1 Macro: 0.7818\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9047, F1 Micro: 0.7807, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.0426, Accuracy: 0.9042, F1 Micro: 0.7772, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.9003, F1 Micro: 0.7746, F1 Macro: 0.7708\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9023, F1 Micro: 0.7846, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 202.1910755634308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.8867, F1 Micro: 0.7234, F1 Macro: 0.7206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.8997, F1 Micro: 0.7612, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9013, F1 Micro: 0.7456, F1 Macro: 0.7317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9019, F1 Micro: 0.7657, F1 Macro: 0.7494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.8983, F1 Micro: 0.7803, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.9023, F1 Micro: 0.7716, F1 Macro: 0.7668\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9, F1 Micro: 0.775, F1 Macro: 0.7693\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9013, F1 Micro: 0.7754, F1 Macro: 0.7701\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9045, F1 Micro: 0.7751, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.907, F1 Micro: 0.7817, F1 Macro: 0.7741\n",
      "Model 3 - Iteration 5441: Accuracy: 0.907, F1 Micro: 0.7817, F1 Macro: 0.7741\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.70      0.58      0.64       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 202.75743103027344 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.8992, F1 Micro: 0.7686, F1 Macro: 0.7661\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.4503276348114 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8855, F1 Micro: 0.689, F1 Macro: 0.6784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.898, F1 Micro: 0.7542, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.9013, F1 Micro: 0.7844, F1 Macro: 0.7829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9072, F1 Micro: 0.7916, F1 Macro: 0.7885\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.9073, F1 Micro: 0.784, F1 Macro: 0.7773\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9014, F1 Micro: 0.7801, F1 Macro: 0.7778\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9014, F1 Micro: 0.7638, F1 Macro: 0.7524\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9034, F1 Micro: 0.7838, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.9038, F1 Micro: 0.7786, F1 Macro: 0.7732\n",
      "Epoch 10/10, Train Loss: 0.0395, Accuracy: 0.9053, F1 Micro: 0.7869, F1 Macro: 0.7858\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9072, F1 Micro: 0.7916, F1 Macro: 0.7885\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.77      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.82      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 206.55558729171753 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8809, F1 Micro: 0.6809, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.9016, F1 Micro: 0.7677, F1 Macro: 0.7551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.9061, F1 Micro: 0.789, F1 Macro: 0.7864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9069, F1 Micro: 0.7907, F1 Macro: 0.7884\n",
      "Epoch 5/10, Train Loss: 0.1298, Accuracy: 0.9114, F1 Micro: 0.7893, F1 Macro: 0.7818\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9048, F1 Micro: 0.7841, F1 Macro: 0.7806\n",
      "Epoch 7/10, Train Loss: 0.0682, Accuracy: 0.9045, F1 Micro: 0.7768, F1 Macro: 0.7657\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9059, F1 Micro: 0.7784, F1 Macro: 0.7726\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9059, F1 Micro: 0.7853, F1 Macro: 0.7794\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.903, F1 Micro: 0.7772, F1 Macro: 0.7734\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9069, F1 Micro: 0.7907, F1 Macro: 0.7884\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.76      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 206.94284343719482 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3972, Accuracy: 0.883, F1 Micro: 0.6949, F1 Macro: 0.6845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2596, Accuracy: 0.8988, F1 Micro: 0.7598, F1 Macro: 0.7474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.213, Accuracy: 0.9047, F1 Micro: 0.7837, F1 Macro: 0.7803\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9023, F1 Micro: 0.7794, F1 Macro: 0.7723\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.908, F1 Micro: 0.7778, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9066, F1 Micro: 0.785, F1 Macro: 0.7804\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9006, F1 Micro: 0.7738, F1 Macro: 0.7666\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9034, F1 Micro: 0.7725, F1 Macro: 0.7668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.9052, F1 Micro: 0.7854, F1 Macro: 0.7841\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.9039, F1 Micro: 0.784, F1 Macro: 0.7838\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9052, F1 Micro: 0.7854, F1 Macro: 0.7841\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.78      0.79      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 208.487384557724 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.8996, F1 Micro: 0.7696, F1 Macro: 0.7672\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.864742040634155 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8923, F1 Micro: 0.7297, F1 Macro: 0.7206\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.8941, F1 Micro: 0.723, F1 Macro: 0.701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9061, F1 Micro: 0.7639, F1 Macro: 0.7569\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9028, F1 Micro: 0.7524, F1 Macro: 0.7396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9019, F1 Micro: 0.7653, F1 Macro: 0.7607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9053, F1 Micro: 0.7747, F1 Macro: 0.7669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.9062, F1 Micro: 0.7865, F1 Macro: 0.7841\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.905, F1 Micro: 0.7845, F1 Macro: 0.7827\n",
      "Epoch 9/10, Train Loss: 0.0445, Accuracy: 0.9027, F1 Micro: 0.7847, F1 Macro: 0.7847\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9008, F1 Micro: 0.767, F1 Macro: 0.7623\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9062, F1 Micro: 0.7865, F1 Macro: 0.7841\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.70      0.69      0.69       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 214.601478099823 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.367, Accuracy: 0.8922, F1 Micro: 0.7433, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.8983, F1 Micro: 0.7474, F1 Macro: 0.7319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2053, Accuracy: 0.9086, F1 Micro: 0.7792, F1 Macro: 0.7738\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9078, F1 Micro: 0.7774, F1 Macro: 0.7681\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9048, F1 Micro: 0.7757, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9077, F1 Micro: 0.7812, F1 Macro: 0.7743\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9023, F1 Micro: 0.7786, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0481, Accuracy: 0.9066, F1 Micro: 0.7875, F1 Macro: 0.784\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.902, F1 Micro: 0.7808, F1 Macro: 0.7775\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9052, F1 Micro: 0.7841, F1 Macro: 0.7785\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9066, F1 Micro: 0.7875, F1 Macro: 0.784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 214.09482192993164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.387, Accuracy: 0.8903, F1 Micro: 0.7256, F1 Macro: 0.7151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.8952, F1 Micro: 0.7359, F1 Macro: 0.7201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9061, F1 Micro: 0.7703, F1 Macro: 0.7645\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.8998, F1 Micro: 0.7453, F1 Macro: 0.7346\n",
      "Epoch 5/10, Train Loss: 0.1261, Accuracy: 0.9005, F1 Micro: 0.7645, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9059, F1 Micro: 0.7723, F1 Macro: 0.7662\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9011, F1 Micro: 0.7709, F1 Macro: 0.7666\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.9019, F1 Micro: 0.77, F1 Macro: 0.7657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.9023, F1 Micro: 0.7808, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9006, F1 Micro: 0.7765, F1 Macro: 0.7738\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9023, F1 Micro: 0.7808, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.71      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 212.89605903625488 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.8998, F1 Micro: 0.7704, F1 Macro: 0.7679\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.032163143157959 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3807, Accuracy: 0.8867, F1 Micro: 0.7213, F1 Macro: 0.7029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.246, Accuracy: 0.8995, F1 Micro: 0.7534, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2007, Accuracy: 0.9034, F1 Micro: 0.7718, F1 Macro: 0.7654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1592, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.7649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.903, F1 Micro: 0.7765, F1 Macro: 0.7746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0895, Accuracy: 0.907, F1 Micro: 0.7857, F1 Macro: 0.7826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9056, F1 Micro: 0.786, F1 Macro: 0.7827\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.9052, F1 Micro: 0.7776, F1 Macro: 0.7759\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9058, F1 Micro: 0.7792, F1 Macro: 0.7779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9052, F1 Micro: 0.7874, F1 Macro: 0.7847\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9052, F1 Micro: 0.7874, F1 Macro: 0.7847\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 225.4584457874298 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3648, Accuracy: 0.8856, F1 Micro: 0.7354, F1 Macro: 0.7246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2372, Accuracy: 0.9022, F1 Micro: 0.7647, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1946, Accuracy: 0.9091, F1 Micro: 0.7849, F1 Macro: 0.7799\n",
      "Epoch 4/10, Train Loss: 0.1592, Accuracy: 0.9038, F1 Micro: 0.7753, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.905, F1 Micro: 0.7879, F1 Macro: 0.7852\n",
      "Epoch 6/10, Train Loss: 0.0858, Accuracy: 0.9072, F1 Micro: 0.7832, F1 Macro: 0.7771\n",
      "Epoch 7/10, Train Loss: 0.0645, Accuracy: 0.9059, F1 Micro: 0.7825, F1 Macro: 0.7771\n",
      "Epoch 8/10, Train Loss: 0.0467, Accuracy: 0.9064, F1 Micro: 0.7846, F1 Macro: 0.7796\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.9059, F1 Micro: 0.7811, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9059, F1 Micro: 0.7806, F1 Macro: 0.7746\n",
      "Model 2 - Iteration 6041: Accuracy: 0.905, F1 Micro: 0.7879, F1 Macro: 0.7852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 218.63170766830444 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3875, Accuracy: 0.8817, F1 Micro: 0.7256, F1 Macro: 0.7123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2426, Accuracy: 0.9017, F1 Micro: 0.7615, F1 Macro: 0.7503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1989, Accuracy: 0.907, F1 Micro: 0.7824, F1 Macro: 0.7771\n",
      "Epoch 4/10, Train Loss: 0.1627, Accuracy: 0.9036, F1 Micro: 0.7773, F1 Macro: 0.7694\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9019, F1 Micro: 0.7813, F1 Macro: 0.7793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9044, F1 Micro: 0.7836, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.8977, F1 Micro: 0.7782, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9034, F1 Micro: 0.785, F1 Macro: 0.7842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9058, F1 Micro: 0.7864, F1 Macro: 0.7864\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.9034, F1 Micro: 0.7793, F1 Macro: 0.7766\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9058, F1 Micro: 0.7864, F1 Macro: 0.7864\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.66      0.73      0.70       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.79      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 221.25440406799316 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9001, F1 Micro: 0.7712, F1 Macro: 0.7688\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.193735122680664 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3636, Accuracy: 0.8845, F1 Micro: 0.6786, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2368, Accuracy: 0.9002, F1 Micro: 0.7613, F1 Macro: 0.7503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1991, Accuracy: 0.9087, F1 Micro: 0.7917, F1 Macro: 0.7872\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.903, F1 Micro: 0.7736, F1 Macro: 0.7695\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9022, F1 Micro: 0.7678, F1 Macro: 0.7582\n",
      "Epoch 6/10, Train Loss: 0.0852, Accuracy: 0.9052, F1 Micro: 0.7724, F1 Macro: 0.7648\n",
      "Epoch 7/10, Train Loss: 0.064, Accuracy: 0.903, F1 Micro: 0.78, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9062, F1 Micro: 0.7832, F1 Macro: 0.7784\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9038, F1 Micro: 0.7715, F1 Macro: 0.7634\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9048, F1 Micro: 0.783, F1 Macro: 0.7804\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9087, F1 Micro: 0.7917, F1 Macro: 0.7872\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 222.2566819190979 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3479, Accuracy: 0.8877, F1 Micro: 0.7023, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.9019, F1 Micro: 0.7701, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1966, Accuracy: 0.9066, F1 Micro: 0.7854, F1 Macro: 0.7811\n",
      "Epoch 4/10, Train Loss: 0.1487, Accuracy: 0.9042, F1 Micro: 0.7765, F1 Macro: 0.772\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9036, F1 Micro: 0.7746, F1 Macro: 0.7662\n",
      "Epoch 6/10, Train Loss: 0.0847, Accuracy: 0.9059, F1 Micro: 0.773, F1 Macro: 0.7664\n",
      "Epoch 7/10, Train Loss: 0.0598, Accuracy: 0.905, F1 Micro: 0.7811, F1 Macro: 0.778\n",
      "Epoch 8/10, Train Loss: 0.0485, Accuracy: 0.9034, F1 Micro: 0.7796, F1 Macro: 0.7729\n",
      "Epoch 9/10, Train Loss: 0.0358, Accuracy: 0.9025, F1 Micro: 0.7752, F1 Macro: 0.7708\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.902, F1 Micro: 0.7737, F1 Macro: 0.7711\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9066, F1 Micro: 0.7854, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 221.58082032203674 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3686, Accuracy: 0.8881, F1 Micro: 0.7058, F1 Macro: 0.6876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2333, Accuracy: 0.8995, F1 Micro: 0.7644, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.9045, F1 Micro: 0.7816, F1 Macro: 0.7789\n",
      "Epoch 4/10, Train Loss: 0.1498, Accuracy: 0.9039, F1 Micro: 0.7793, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9048, F1 Micro: 0.7754, F1 Macro: 0.7598\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9034, F1 Micro: 0.7767, F1 Macro: 0.7693\n",
      "Epoch 7/10, Train Loss: 0.0638, Accuracy: 0.9003, F1 Micro: 0.7717, F1 Macro: 0.7696\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9014, F1 Micro: 0.771, F1 Macro: 0.7634\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9003, F1 Micro: 0.7758, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9014, F1 Micro: 0.7811, F1 Macro: 0.7804\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9045, F1 Micro: 0.7816, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.85      0.90       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 221.96221470832825 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9004, F1 Micro: 0.7719, F1 Macro: 0.7694\n",
      "Total sampling time: 1064.51 seconds\n",
      "Total runtime: 11843.780545473099 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d282b",
   "metadata": {
    "papermill": {
     "duration": 0.005729,
     "end_time": "2025-01-28T13:15:41.413350",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.407621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a18623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T13:15:41.426132Z",
     "iopub.status.busy": "2025-01-28T13:15:41.425880Z",
     "iopub.status.idle": "2025-01-28T16:33:07.008947Z",
     "shell.execute_reply": "2025-01-28T16:33:07.008003Z"
    },
    "papermill": {
     "duration": 11845.591301,
     "end_time": "2025-01-28T16:33:07.010559",
     "exception": false,
     "start_time": "2025-01-28T13:15:41.419258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6045, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3839, Accuracy: 0.8116, F1 Micro: 0.2087, F1 Macro: 0.1506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3762, Accuracy: 0.8338, F1 Micro: 0.4095, F1 Macro: 0.342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3143, Accuracy: 0.8447, F1 Micro: 0.4918, F1 Macro: 0.4438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2621, Accuracy: 0.8547, F1 Micro: 0.565, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2351, Accuracy: 0.8594, F1 Micro: 0.6053, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1834, Accuracy: 0.8698, F1 Micro: 0.6445, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1462, Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "Epoch 10/10, Train Loss: 0.1326, Accuracy: 0.877, F1 Micro: 0.6883, F1 Macro: 0.6815\n",
      "Model 1 - Iteration 388: Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.81      0.85       370\n",
      "                sara       0.58      0.56      0.57       248\n",
      "         radikalisme       0.66      0.74      0.70       243\n",
      "pencemaran_nama_baik       0.66      0.63      0.64       504\n",
      "\n",
      "           micro avg       0.71      0.69      0.70      1365\n",
      "           macro avg       0.70      0.69      0.69      1365\n",
      "        weighted avg       0.71      0.69      0.70      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 56.86565327644348 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5934, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.439, Accuracy: 0.7909, F1 Micro: 0.0388, F1 Macro: 0.034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3734, Accuracy: 0.8158, F1 Micro: 0.2398, F1 Macro: 0.1687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3544, Accuracy: 0.8344, F1 Micro: 0.4137, F1 Macro: 0.3332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2904, Accuracy: 0.8481, F1 Micro: 0.5272, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2499, Accuracy: 0.8531, F1 Micro: 0.5595, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2281, Accuracy: 0.8608, F1 Micro: 0.607, F1 Macro: 0.5951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1774, Accuracy: 0.8648, F1 Micro: 0.6211, F1 Macro: 0.6062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1432, Accuracy: 0.8706, F1 Micro: 0.662, F1 Macro: 0.6503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1304, Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "Model 2 - Iteration 388: Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.85      0.87       370\n",
      "                sara       0.57      0.51      0.54       248\n",
      "         radikalisme       0.66      0.68      0.67       243\n",
      "pencemaran_nama_baik       0.66      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.67      1365\n",
      "           macro avg       0.70      0.64      0.67      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.37      0.36      0.35      1365\n",
      "\n",
      "Training completed in 58.64758563041687 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6052, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4376, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3737, Accuracy: 0.8203, F1 Micro: 0.274, F1 Macro: 0.1872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3629, Accuracy: 0.8316, F1 Micro: 0.3896, F1 Macro: 0.2908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3015, Accuracy: 0.842, F1 Micro: 0.4648, F1 Macro: 0.4003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2601, Accuracy: 0.8594, F1 Micro: 0.5917, F1 Macro: 0.565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2378, Accuracy: 0.8631, F1 Micro: 0.6158, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1836, Accuracy: 0.8686, F1 Micro: 0.6453, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1511, Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "Epoch 10/10, Train Loss: 0.13, Accuracy: 0.8755, F1 Micro: 0.6876, F1 Macro: 0.6794\n",
      "Model 3 - Iteration 388: Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.86      0.86       370\n",
      "                sara       0.60      0.54      0.57       248\n",
      "         radikalisme       0.67      0.71      0.69       243\n",
      "pencemaran_nama_baik       0.67      0.60      0.63       504\n",
      "\n",
      "           micro avg       0.71      0.68      0.70      1365\n",
      "           macro avg       0.70      0.68      0.69      1365\n",
      "        weighted avg       0.71      0.68      0.69      1365\n",
      "         samples avg       0.37      0.37      0.36      1365\n",
      "\n",
      "Training completed in 57.483014822006226 s\n",
      "Averaged - Iteration 388: Accuracy: 0.871, F1 Micro: 0.6888, F1 Macro: 0.6828\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 121.80804634094238 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5695, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4711, Accuracy: 0.8388, F1 Micro: 0.4522, F1 Macro: 0.3757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3769, Accuracy: 0.86, F1 Micro: 0.5733, F1 Macro: 0.5361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3032, Accuracy: 0.8739, F1 Micro: 0.6647, F1 Macro: 0.6584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2593, Accuracy: 0.8873, F1 Micro: 0.7211, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1989, Accuracy: 0.8906, F1 Micro: 0.7222, F1 Macro: 0.716\n",
      "Epoch 7/10, Train Loss: 0.1619, Accuracy: 0.8888, F1 Micro: 0.6975, F1 Macro: 0.6793\n",
      "Epoch 8/10, Train Loss: 0.1331, Accuracy: 0.8913, F1 Micro: 0.7196, F1 Macro: 0.701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1113, Accuracy: 0.8917, F1 Micro: 0.7405, F1 Macro: 0.732\n",
      "Epoch 10/10, Train Loss: 0.0829, Accuracy: 0.8919, F1 Micro: 0.7381, F1 Macro: 0.7328\n",
      "Model 1 - Iteration 971: Accuracy: 0.8917, F1 Micro: 0.7405, F1 Macro: 0.732\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.87      0.88       370\n",
      "                sara       0.68      0.57      0.62       248\n",
      "         radikalisme       0.75      0.71      0.73       243\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.75      0.71      0.73      1365\n",
      "        weighted avg       0.76      0.72      0.74      1365\n",
      "         samples avg       0.42      0.41      0.40      1365\n",
      "\n",
      "Training completed in 71.97944664955139 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5561, Accuracy: 0.7872, F1 Micro: 0.0044, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4578, Accuracy: 0.8417, F1 Micro: 0.4738, F1 Macro: 0.4045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3668, Accuracy: 0.8616, F1 Micro: 0.5921, F1 Macro: 0.5801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3096, Accuracy: 0.8763, F1 Micro: 0.6675, F1 Macro: 0.6577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.262, Accuracy: 0.8839, F1 Micro: 0.7139, F1 Macro: 0.7134\n",
      "Epoch 6/10, Train Loss: 0.2003, Accuracy: 0.8844, F1 Micro: 0.7077, F1 Macro: 0.7078\n",
      "Epoch 7/10, Train Loss: 0.1725, Accuracy: 0.8819, F1 Micro: 0.6772, F1 Macro: 0.6632\n",
      "Epoch 8/10, Train Loss: 0.1465, Accuracy: 0.8883, F1 Micro: 0.7095, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1156, Accuracy: 0.8867, F1 Micro: 0.7324, F1 Macro: 0.7226\n",
      "Epoch 10/10, Train Loss: 0.087, Accuracy: 0.8878, F1 Micro: 0.7133, F1 Macro: 0.7007\n",
      "Model 2 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7324, F1 Macro: 0.7226\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.89      0.87       370\n",
      "                sara       0.67      0.55      0.60       248\n",
      "         radikalisme       0.74      0.72      0.73       243\n",
      "pencemaran_nama_baik       0.68      0.70      0.69       504\n",
      "\n",
      "           micro avg       0.74      0.73      0.73      1365\n",
      "           macro avg       0.74      0.71      0.72      1365\n",
      "        weighted avg       0.73      0.73      0.73      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 69.72760248184204 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5591, Accuracy: 0.7955, F1 Micro: 0.0788, F1 Macro: 0.0657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4608, Accuracy: 0.8377, F1 Micro: 0.45, F1 Macro: 0.3568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3732, Accuracy: 0.8644, F1 Micro: 0.6104, F1 Macro: 0.5798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3122, Accuracy: 0.8795, F1 Micro: 0.6826, F1 Macro: 0.6626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2644, Accuracy: 0.8884, F1 Micro: 0.7328, F1 Macro: 0.7263\n",
      "Epoch 6/10, Train Loss: 0.2067, Accuracy: 0.8897, F1 Micro: 0.7071, F1 Macro: 0.6982\n",
      "Epoch 7/10, Train Loss: 0.1627, Accuracy: 0.8889, F1 Micro: 0.7058, F1 Macro: 0.6949\n",
      "Epoch 8/10, Train Loss: 0.1314, Accuracy: 0.89, F1 Micro: 0.7115, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1089, Accuracy: 0.8867, F1 Micro: 0.7484, F1 Macro: 0.7439\n",
      "Epoch 10/10, Train Loss: 0.0874, Accuracy: 0.8891, F1 Micro: 0.7167, F1 Macro: 0.7056\n",
      "Model 3 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7484, F1 Macro: 0.7439\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.90      0.87       370\n",
      "                sara       0.64      0.63      0.64       248\n",
      "         radikalisme       0.77      0.74      0.76       243\n",
      "pencemaran_nama_baik       0.63      0.81      0.71       504\n",
      "\n",
      "           micro avg       0.71      0.79      0.75      1365\n",
      "           macro avg       0.72      0.77      0.74      1365\n",
      "        weighted avg       0.72      0.79      0.75      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 69.78806257247925 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8797, F1 Micro: 0.7146, F1 Macro: 0.7078\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 108.66034460067749 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5502, Accuracy: 0.8347, F1 Micro: 0.4602, F1 Macro: 0.4093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4221, Accuracy: 0.8666, F1 Micro: 0.6293, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3474, Accuracy: 0.8792, F1 Micro: 0.6644, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3024, Accuracy: 0.8847, F1 Micro: 0.676, F1 Macro: 0.6535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.245, Accuracy: 0.8969, F1 Micro: 0.7655, F1 Macro: 0.7641\n",
      "Epoch 6/10, Train Loss: 0.1816, Accuracy: 0.8994, F1 Micro: 0.7636, F1 Macro: 0.7594\n",
      "Epoch 7/10, Train Loss: 0.139, Accuracy: 0.8927, F1 Micro: 0.7407, F1 Macro: 0.7332\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.898, F1 Micro: 0.7516, F1 Macro: 0.7439\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.8948, F1 Micro: 0.7456, F1 Macro: 0.7421\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.8936, F1 Micro: 0.7299, F1 Macro: 0.7181\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8969, F1 Micro: 0.7655, F1 Macro: 0.7641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.84      0.89       370\n",
      "                sara       0.65      0.70      0.68       248\n",
      "         radikalisme       0.72      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 84.20271801948547 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5436, Accuracy: 0.8386, F1 Micro: 0.4812, F1 Macro: 0.4239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4111, Accuracy: 0.8681, F1 Micro: 0.6321, F1 Macro: 0.6058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.338, Accuracy: 0.8816, F1 Micro: 0.6899, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2957, Accuracy: 0.8884, F1 Micro: 0.7114, F1 Macro: 0.6919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2413, Accuracy: 0.8953, F1 Micro: 0.7533, F1 Macro: 0.7466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1772, Accuracy: 0.8972, F1 Micro: 0.7568, F1 Macro: 0.7483\n",
      "Epoch 7/10, Train Loss: 0.1382, Accuracy: 0.8953, F1 Micro: 0.746, F1 Macro: 0.7374\n",
      "Epoch 8/10, Train Loss: 0.115, Accuracy: 0.8944, F1 Micro: 0.7317, F1 Macro: 0.7233\n",
      "Epoch 9/10, Train Loss: 0.0879, Accuracy: 0.8948, F1 Micro: 0.7514, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.8948, F1 Micro: 0.7578, F1 Macro: 0.7519\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8948, F1 Micro: 0.7578, F1 Macro: 0.7519\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.67      0.62      0.64       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.68      0.76      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.77      0.76      1365\n",
      "           macro avg       0.75      0.76      0.75      1365\n",
      "        weighted avg       0.75      0.77      0.76      1365\n",
      "         samples avg       0.43      0.44      0.42      1365\n",
      "\n",
      "Training completed in 86.78225803375244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.853, F1 Micro: 0.5939, F1 Macro: 0.4997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4128, Accuracy: 0.8686, F1 Micro: 0.621, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3374, Accuracy: 0.8794, F1 Micro: 0.6655, F1 Macro: 0.6384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2933, Accuracy: 0.8872, F1 Micro: 0.6956, F1 Macro: 0.6699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2393, Accuracy: 0.8961, F1 Micro: 0.7712, F1 Macro: 0.7698\n",
      "Epoch 6/10, Train Loss: 0.1767, Accuracy: 0.8975, F1 Micro: 0.7565, F1 Macro: 0.75\n",
      "Epoch 7/10, Train Loss: 0.1356, Accuracy: 0.895, F1 Micro: 0.7464, F1 Macro: 0.7392\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.8967, F1 Micro: 0.7482, F1 Macro: 0.745\n",
      "Epoch 9/10, Train Loss: 0.0833, Accuracy: 0.8966, F1 Micro: 0.7543, F1 Macro: 0.7464\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.8945, F1 Micro: 0.7501, F1 Macro: 0.7414\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8961, F1 Micro: 0.7712, F1 Macro: 0.7698\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.70      0.84      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1365\n",
      "           macro avg       0.73      0.82      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.77      1365\n",
      "         samples avg       0.43      0.45      0.43      1365\n",
      "\n",
      "Training completed in 83.7984344959259 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8851, F1 Micro: 0.7314, F1 Macro: 0.7259\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 97.94596910476685 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.85, F1 Micro: 0.5707, F1 Macro: 0.5408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4075, Accuracy: 0.8705, F1 Micro: 0.6709, F1 Macro: 0.6619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3155, Accuracy: 0.8892, F1 Micro: 0.7462, F1 Macro: 0.7407\n",
      "Epoch 4/10, Train Loss: 0.2619, Accuracy: 0.893, F1 Micro: 0.7422, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2049, Accuracy: 0.8973, F1 Micro: 0.7533, F1 Macro: 0.7466\n",
      "Epoch 6/10, Train Loss: 0.1599, Accuracy: 0.8958, F1 Micro: 0.7398, F1 Macro: 0.7363\n",
      "Epoch 7/10, Train Loss: 0.1262, Accuracy: 0.8945, F1 Micro: 0.7461, F1 Macro: 0.7398\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.8988, F1 Micro: 0.7488, F1 Macro: 0.7364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.8981, F1 Micro: 0.7591, F1 Macro: 0.7507\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.8981, F1 Micro: 0.7419, F1 Macro: 0.7345\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8981, F1 Micro: 0.7591, F1 Macro: 0.7507\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.58      0.61       248\n",
      "         radikalisme       0.75      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1365\n",
      "           macro avg       0.76      0.74      0.75      1365\n",
      "        weighted avg       0.77      0.75      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 96.05924654006958 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5386, Accuracy: 0.848, F1 Micro: 0.5875, F1 Macro: 0.5822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3955, Accuracy: 0.8759, F1 Micro: 0.6934, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.313, Accuracy: 0.8861, F1 Micro: 0.7455, F1 Macro: 0.7429\n",
      "Epoch 4/10, Train Loss: 0.2634, Accuracy: 0.8909, F1 Micro: 0.7413, F1 Macro: 0.7332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2088, Accuracy: 0.8956, F1 Micro: 0.7645, F1 Macro: 0.7614\n",
      "Epoch 6/10, Train Loss: 0.1682, Accuracy: 0.8942, F1 Micro: 0.7377, F1 Macro: 0.7295\n",
      "Epoch 7/10, Train Loss: 0.1255, Accuracy: 0.895, F1 Micro: 0.7383, F1 Macro: 0.7293\n",
      "Epoch 8/10, Train Loss: 0.0998, Accuracy: 0.8923, F1 Micro: 0.7223, F1 Macro: 0.7038\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.8955, F1 Micro: 0.7473, F1 Macro: 0.7399\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.8944, F1 Micro: 0.7406, F1 Macro: 0.732\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8956, F1 Micro: 0.7645, F1 Macro: 0.7614\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.89       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.77      1365\n",
      "         samples avg       0.43      0.45      0.43      1365\n",
      "\n",
      "Training completed in 94.85547494888306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.8469, F1 Micro: 0.5765, F1 Macro: 0.5555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3974, Accuracy: 0.8828, F1 Micro: 0.7176, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3113, Accuracy: 0.8897, F1 Micro: 0.7549, F1 Macro: 0.7566\n",
      "Epoch 4/10, Train Loss: 0.2613, Accuracy: 0.8969, F1 Micro: 0.7496, F1 Macro: 0.7451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.209, Accuracy: 0.9023, F1 Micro: 0.7637, F1 Macro: 0.7576\n",
      "Epoch 6/10, Train Loss: 0.1653, Accuracy: 0.8981, F1 Micro: 0.7622, F1 Macro: 0.7596\n",
      "Epoch 7/10, Train Loss: 0.1294, Accuracy: 0.8989, F1 Micro: 0.7514, F1 Macro: 0.7465\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.8977, F1 Micro: 0.7546, F1 Macro: 0.7456\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.8953, F1 Micro: 0.7476, F1 Macro: 0.7423\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.8981, F1 Micro: 0.7506, F1 Macro: 0.7442\n",
      "Model 3 - Iteration 1969: Accuracy: 0.9023, F1 Micro: 0.7637, F1 Macro: 0.7576\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.58      0.63       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.75      0.67      0.71       504\n",
      "\n",
      "           micro avg       0.79      0.74      0.76      1365\n",
      "           macro avg       0.78      0.74      0.76      1365\n",
      "        weighted avg       0.79      0.74      0.76      1365\n",
      "         samples avg       0.42      0.41      0.41      1365\n",
      "\n",
      "Training completed in 94.6291856765747 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8885, F1 Micro: 0.7391, F1 Macro: 0.7335\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 89.20641326904297 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5288, Accuracy: 0.8562, F1 Micro: 0.5564, F1 Macro: 0.494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3823, Accuracy: 0.8863, F1 Micro: 0.7093, F1 Macro: 0.702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3051, Accuracy: 0.8931, F1 Micro: 0.7237, F1 Macro: 0.7118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2377, Accuracy: 0.9006, F1 Micro: 0.7578, F1 Macro: 0.7454\n",
      "Epoch 5/10, Train Loss: 0.192, Accuracy: 0.8956, F1 Micro: 0.7356, F1 Macro: 0.7281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1482, Accuracy: 0.903, F1 Micro: 0.7751, F1 Macro: 0.7708\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.898, F1 Micro: 0.7605, F1 Macro: 0.7619\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9005, F1 Micro: 0.7691, F1 Macro: 0.7645\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.8988, F1 Micro: 0.7713, F1 Macro: 0.7673\n",
      "Epoch 10/10, Train Loss: 0.0615, Accuracy: 0.9019, F1 Micro: 0.7625, F1 Macro: 0.7602\n",
      "Model 1 - Iteration 2394: Accuracy: 0.903, F1 Micro: 0.7751, F1 Macro: 0.7708\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.69      0.67      0.68       248\n",
      "         radikalisme       0.77      0.76      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 110.42803764343262 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5218, Accuracy: 0.8525, F1 Micro: 0.5551, F1 Macro: 0.5157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3777, Accuracy: 0.883, F1 Micro: 0.6966, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3067, Accuracy: 0.8927, F1 Micro: 0.7315, F1 Macro: 0.7268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2401, Accuracy: 0.8931, F1 Micro: 0.7377, F1 Macro: 0.7208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1964, Accuracy: 0.8963, F1 Micro: 0.7428, F1 Macro: 0.7347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1478, Accuracy: 0.9052, F1 Micro: 0.7693, F1 Macro: 0.7616\n",
      "Epoch 7/10, Train Loss: 0.1215, Accuracy: 0.9013, F1 Micro: 0.7678, F1 Macro: 0.767\n",
      "Epoch 8/10, Train Loss: 0.0916, Accuracy: 0.8936, F1 Micro: 0.7644, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.8948, F1 Micro: 0.7611, F1 Macro: 0.7595\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9006, F1 Micro: 0.7625, F1 Macro: 0.7585\n",
      "Model 2 - Iteration 2394: Accuracy: 0.9052, F1 Micro: 0.7693, F1 Macro: 0.7616\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.71      0.59      0.65       248\n",
      "         radikalisme       0.81      0.74      0.77       243\n",
      "pencemaran_nama_baik       0.74      0.71      0.73       504\n",
      "\n",
      "           micro avg       0.80      0.74      0.77      1365\n",
      "           macro avg       0.80      0.73      0.76      1365\n",
      "        weighted avg       0.80      0.74      0.77      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 112.41519832611084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.8527, F1 Micro: 0.5357, F1 Macro: 0.4575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3817, Accuracy: 0.8844, F1 Micro: 0.6914, F1 Macro: 0.6836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3069, Accuracy: 0.8944, F1 Micro: 0.7315, F1 Macro: 0.7252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2437, Accuracy: 0.8975, F1 Micro: 0.7423, F1 Macro: 0.7298\n",
      "Epoch 5/10, Train Loss: 0.2001, Accuracy: 0.8984, F1 Micro: 0.739, F1 Macro: 0.7288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1555, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.766\n",
      "Epoch 7/10, Train Loss: 0.1226, Accuracy: 0.8992, F1 Micro: 0.7522, F1 Macro: 0.7485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.8975, F1 Micro: 0.7721, F1 Macro: 0.7698\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.8986, F1 Micro: 0.7646, F1 Macro: 0.7619\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.8992, F1 Micro: 0.7659, F1 Macro: 0.7634\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8975, F1 Micro: 0.7721, F1 Macro: 0.7698\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.90      0.89       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.81      0.77      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 111.39415740966797 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8912, F1 Micro: 0.7457, F1 Macro: 0.7403\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 79.83580994606018 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5061, Accuracy: 0.858, F1 Micro: 0.6239, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.366, Accuracy: 0.8927, F1 Micro: 0.7313, F1 Macro: 0.7302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.8948, F1 Micro: 0.7424, F1 Macro: 0.7279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2435, Accuracy: 0.9009, F1 Micro: 0.76, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1888, Accuracy: 0.9033, F1 Micro: 0.7769, F1 Macro: 0.7701\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.9003, F1 Micro: 0.7692, F1 Macro: 0.7674\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9052, F1 Micro: 0.77, F1 Macro: 0.7628\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.902, F1 Micro: 0.7679, F1 Macro: 0.7612\n",
      "Epoch 9/10, Train Loss: 0.0635, Accuracy: 0.8995, F1 Micro: 0.7479, F1 Macro: 0.742\n",
      "Epoch 10/10, Train Loss: 0.0583, Accuracy: 0.8991, F1 Micro: 0.7647, F1 Macro: 0.7611\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9033, F1 Micro: 0.7769, F1 Macro: 0.7701\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 121.59786057472229 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4941, Accuracy: 0.8559, F1 Micro: 0.6196, F1 Macro: 0.582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3654, Accuracy: 0.8888, F1 Micro: 0.719, F1 Macro: 0.718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2989, Accuracy: 0.8955, F1 Micro: 0.7408, F1 Macro: 0.7249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2502, Accuracy: 0.8984, F1 Micro: 0.7534, F1 Macro: 0.7422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1921, Accuracy: 0.9022, F1 Micro: 0.771, F1 Macro: 0.7671\n",
      "Epoch 6/10, Train Loss: 0.1493, Accuracy: 0.8995, F1 Micro: 0.7686, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.902, F1 Micro: 0.7719, F1 Macro: 0.7676\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.8981, F1 Micro: 0.7655, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.8991, F1 Micro: 0.7496, F1 Macro: 0.7376\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.9, F1 Micro: 0.7654, F1 Macro: 0.7622\n",
      "Model 2 - Iteration 2777: Accuracy: 0.902, F1 Micro: 0.7719, F1 Macro: 0.7676\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 123.33912920951843 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4966, Accuracy: 0.8573, F1 Micro: 0.6204, F1 Macro: 0.5381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3699, Accuracy: 0.8903, F1 Micro: 0.7339, F1 Macro: 0.7324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2978, Accuracy: 0.8977, F1 Micro: 0.7478, F1 Macro: 0.7337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2484, Accuracy: 0.9008, F1 Micro: 0.7617, F1 Macro: 0.7513\n",
      "Epoch 5/10, Train Loss: 0.1879, Accuracy: 0.9013, F1 Micro: 0.7597, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1431, Accuracy: 0.9036, F1 Micro: 0.7704, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9027, F1 Micro: 0.772, F1 Macro: 0.7662\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.8988, F1 Micro: 0.7705, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9013, F1 Micro: 0.7608, F1 Macro: 0.756\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.8997, F1 Micro: 0.7624, F1 Macro: 0.7589\n",
      "Model 3 - Iteration 2777: Accuracy: 0.9027, F1 Micro: 0.772, F1 Macro: 0.7662\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.69      0.61      0.65       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 123.28115725517273 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8931, F1 Micro: 0.7504, F1 Macro: 0.7449\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 72.62457776069641 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5023, Accuracy: 0.8702, F1 Micro: 0.676, F1 Macro: 0.6556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3492, Accuracy: 0.8911, F1 Micro: 0.7522, F1 Macro: 0.7485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2844, Accuracy: 0.8973, F1 Micro: 0.7577, F1 Macro: 0.7497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2318, Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7753\n",
      "Epoch 5/10, Train Loss: 0.1829, Accuracy: 0.8984, F1 Micro: 0.7504, F1 Macro: 0.7502\n",
      "Epoch 6/10, Train Loss: 0.1533, Accuracy: 0.8981, F1 Micro: 0.7683, F1 Macro: 0.7664\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.8995, F1 Micro: 0.7584, F1 Macro: 0.7539\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9, F1 Micro: 0.7599, F1 Macro: 0.7564\n",
      "Epoch 9/10, Train Loss: 0.065, Accuracy: 0.9019, F1 Micro: 0.7583, F1 Macro: 0.7528\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9016, F1 Micro: 0.7729, F1 Macro: 0.7713\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7753\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 129.9604742527008 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4959, Accuracy: 0.8739, F1 Micro: 0.7073, F1 Macro: 0.6936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3485, Accuracy: 0.8919, F1 Micro: 0.7504, F1 Macro: 0.7453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.283, Accuracy: 0.8955, F1 Micro: 0.7617, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2325, Accuracy: 0.9025, F1 Micro: 0.7763, F1 Macro: 0.7745\n",
      "Epoch 5/10, Train Loss: 0.1852, Accuracy: 0.9002, F1 Micro: 0.7522, F1 Macro: 0.751\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9008, F1 Micro: 0.7597, F1 Macro: 0.7579\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.8981, F1 Micro: 0.7556, F1 Macro: 0.7528\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9005, F1 Micro: 0.7633, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.8972, F1 Micro: 0.7487, F1 Macro: 0.7396\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.8977, F1 Micro: 0.7657, F1 Macro: 0.7621\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9025, F1 Micro: 0.7763, F1 Macro: 0.7745\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 129.91029286384583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.497, Accuracy: 0.873, F1 Micro: 0.7008, F1 Macro: 0.6718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3549, Accuracy: 0.8909, F1 Micro: 0.7554, F1 Macro: 0.7524\n",
      "Epoch 3/10, Train Loss: 0.2882, Accuracy: 0.8936, F1 Micro: 0.7501, F1 Macro: 0.7417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2391, Accuracy: 0.9011, F1 Micro: 0.7804, F1 Macro: 0.7811\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.9011, F1 Micro: 0.7612, F1 Macro: 0.761\n",
      "Epoch 6/10, Train Loss: 0.1555, Accuracy: 0.9009, F1 Micro: 0.7747, F1 Macro: 0.7735\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9002, F1 Micro: 0.7571, F1 Macro: 0.7522\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9023, F1 Micro: 0.7672, F1 Macro: 0.7621\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9002, F1 Micro: 0.7582, F1 Macro: 0.7475\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9013, F1 Micro: 0.7692, F1 Macro: 0.7648\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9011, F1 Micro: 0.7804, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.65      0.75      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 128.74339389801025 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8944, F1 Micro: 0.7543, F1 Macro: 0.7495\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 66.98982858657837 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4843, Accuracy: 0.8759, F1 Micro: 0.6879, F1 Macro: 0.6682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3345, Accuracy: 0.8964, F1 Micro: 0.7523, F1 Macro: 0.743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2705, Accuracy: 0.9002, F1 Micro: 0.7591, F1 Macro: 0.7453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.214, Accuracy: 0.9017, F1 Micro: 0.7717, F1 Macro: 0.7648\n",
      "Epoch 5/10, Train Loss: 0.1822, Accuracy: 0.9048, F1 Micro: 0.7594, F1 Macro: 0.7503\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9055, F1 Micro: 0.7707, F1 Macro: 0.7675\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9041, F1 Micro: 0.7702, F1 Macro: 0.7644\n",
      "Epoch 8/10, Train Loss: 0.0859, Accuracy: 0.9038, F1 Micro: 0.7696, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.907, F1 Micro: 0.7723, F1 Macro: 0.763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.9069, F1 Micro: 0.7788, F1 Macro: 0.775\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9069, F1 Micro: 0.7788, F1 Macro: 0.775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.77      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.76      0.70      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 143.12424874305725 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4771, Accuracy: 0.8731, F1 Micro: 0.6694, F1 Macro: 0.6567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3354, Accuracy: 0.8959, F1 Micro: 0.756, F1 Macro: 0.7484\n",
      "Epoch 3/10, Train Loss: 0.2718, Accuracy: 0.8991, F1 Micro: 0.7557, F1 Macro: 0.7455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.219, Accuracy: 0.9055, F1 Micro: 0.7733, F1 Macro: 0.764\n",
      "Epoch 5/10, Train Loss: 0.1925, Accuracy: 0.9019, F1 Micro: 0.7539, F1 Macro: 0.7497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9047, F1 Micro: 0.7747, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9009, F1 Micro: 0.7655, F1 Macro: 0.7592\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9014, F1 Micro: 0.7684, F1 Macro: 0.763\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.903, F1 Micro: 0.7592, F1 Macro: 0.7461\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.903, F1 Micro: 0.7709, F1 Macro: 0.7662\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9047, F1 Micro: 0.7747, F1 Macro: 0.7721\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.44      0.43      0.42      1365\n",
      "\n",
      "Training completed in 139.12373781204224 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4829, Accuracy: 0.8803, F1 Micro: 0.6911, F1 Macro: 0.6759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3376, Accuracy: 0.8969, F1 Micro: 0.7574, F1 Macro: 0.75\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2715, Accuracy: 0.9034, F1 Micro: 0.772, F1 Macro: 0.758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.22, Accuracy: 0.9044, F1 Micro: 0.7743, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.1909, Accuracy: 0.9052, F1 Micro: 0.7621, F1 Macro: 0.749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1357, Accuracy: 0.9062, F1 Micro: 0.7758, F1 Macro: 0.7705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9073, F1 Micro: 0.7877, F1 Macro: 0.7867\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9064, F1 Micro: 0.7674, F1 Macro: 0.7602\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9059, F1 Micro: 0.7747, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9061, F1 Micro: 0.7814, F1 Macro: 0.7773\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9073, F1 Micro: 0.7877, F1 Macro: 0.7867\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.68      0.71      0.70       248\n",
      "         radikalisme       0.80      0.79      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 142.91272139549255 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8959, F1 Micro: 0.7576, F1 Macro: 0.7531\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 59.676127910614014 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4741, Accuracy: 0.8737, F1 Micro: 0.6499, F1 Macro: 0.6456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3269, Accuracy: 0.8981, F1 Micro: 0.7587, F1 Macro: 0.7556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2723, Accuracy: 0.8916, F1 Micro: 0.7676, F1 Macro: 0.769\n",
      "Epoch 4/10, Train Loss: 0.2359, Accuracy: 0.9011, F1 Micro: 0.7505, F1 Macro: 0.7427\n",
      "Epoch 5/10, Train Loss: 0.175, Accuracy: 0.9047, F1 Micro: 0.7647, F1 Macro: 0.753\n",
      "Epoch 6/10, Train Loss: 0.132, Accuracy: 0.9048, F1 Micro: 0.765, F1 Macro: 0.7566\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.9045, F1 Micro: 0.7629, F1 Macro: 0.7552\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.902, F1 Micro: 0.7613, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9041, F1 Micro: 0.7709, F1 Macro: 0.7668\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9027, F1 Micro: 0.7685, F1 Macro: 0.7653\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9041, F1 Micro: 0.7709, F1 Macro: 0.7668\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.63      0.65       248\n",
      "         radikalisme       0.79      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.76      0.77      1365\n",
      "           macro avg       0.78      0.75      0.77      1365\n",
      "        weighted avg       0.79      0.76      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 148.02276396751404 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4669, Accuracy: 0.8783, F1 Micro: 0.6801, F1 Macro: 0.67\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3276, Accuracy: 0.8978, F1 Micro: 0.7651, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2715, Accuracy: 0.8931, F1 Micro: 0.7714, F1 Macro: 0.7718\n",
      "Epoch 4/10, Train Loss: 0.2336, Accuracy: 0.9002, F1 Micro: 0.7461, F1 Macro: 0.7388\n",
      "Epoch 5/10, Train Loss: 0.1818, Accuracy: 0.902, F1 Micro: 0.7461, F1 Macro: 0.7327\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.9028, F1 Micro: 0.7637, F1 Macro: 0.7582\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.905, F1 Micro: 0.7695, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9027, F1 Micro: 0.7639, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.8998, F1 Micro: 0.7758, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9023, F1 Micro: 0.7761, F1 Macro: 0.778\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9023, F1 Micro: 0.7761, F1 Macro: 0.778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.76      0.85      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 149.43340229988098 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4665, Accuracy: 0.8727, F1 Micro: 0.6574, F1 Macro: 0.6416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3274, Accuracy: 0.8972, F1 Micro: 0.7588, F1 Macro: 0.7552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2747, Accuracy: 0.8961, F1 Micro: 0.7759, F1 Macro: 0.7767\n",
      "Epoch 4/10, Train Loss: 0.2356, Accuracy: 0.905, F1 Micro: 0.7645, F1 Macro: 0.7559\n",
      "Epoch 5/10, Train Loss: 0.1815, Accuracy: 0.9039, F1 Micro: 0.7593, F1 Macro: 0.7432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1346, Accuracy: 0.9052, F1 Micro: 0.7789, F1 Macro: 0.7734\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9019, F1 Micro: 0.7639, F1 Macro: 0.7555\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.9034, F1 Micro: 0.7709, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.902, F1 Micro: 0.7731, F1 Macro: 0.7672\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9059, F1 Micro: 0.7774, F1 Macro: 0.7752\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9052, F1 Micro: 0.7789, F1 Macro: 0.7734\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.67      0.66      0.67       248\n",
      "         radikalisme       0.80      0.75      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 148.3517677783966 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8968, F1 Micro: 0.7596, F1 Macro: 0.7552\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 53.87213325500488 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4706, Accuracy: 0.8737, F1 Micro: 0.6669, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3353, Accuracy: 0.8975, F1 Micro: 0.7604, F1 Macro: 0.7583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2744, Accuracy: 0.9013, F1 Micro: 0.7764, F1 Macro: 0.7735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2204, Accuracy: 0.9023, F1 Micro: 0.7794, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.1716, Accuracy: 0.9014, F1 Micro: 0.7676, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.1327, Accuracy: 0.8989, F1 Micro: 0.7762, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.905, F1 Micro: 0.7813, F1 Macro: 0.7782\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9031, F1 Micro: 0.7763, F1 Macro: 0.7721\n",
      "Epoch 9/10, Train Loss: 0.0581, Accuracy: 0.9022, F1 Micro: 0.7759, F1 Macro: 0.7712\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9013, F1 Micro: 0.7687, F1 Macro: 0.7649\n",
      "Model 1 - Iteration 3886: Accuracy: 0.905, F1 Micro: 0.7813, F1 Macro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.79      0.78      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 155.0758171081543 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.464, Accuracy: 0.8731, F1 Micro: 0.6775, F1 Macro: 0.6792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3334, Accuracy: 0.8964, F1 Micro: 0.7614, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2724, Accuracy: 0.902, F1 Micro: 0.7784, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.22, Accuracy: 0.9019, F1 Micro: 0.7812, F1 Macro: 0.7776\n",
      "Epoch 5/10, Train Loss: 0.1691, Accuracy: 0.9009, F1 Micro: 0.7673, F1 Macro: 0.764\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9003, F1 Micro: 0.7715, F1 Macro: 0.7617\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.8983, F1 Micro: 0.7644, F1 Macro: 0.7563\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9016, F1 Micro: 0.7701, F1 Macro: 0.7683\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.8986, F1 Micro: 0.7708, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9003, F1 Micro: 0.7717, F1 Macro: 0.7666\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9019, F1 Micro: 0.7812, F1 Macro: 0.7776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 153.0603265762329 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4619, Accuracy: 0.8778, F1 Micro: 0.6964, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3315, Accuracy: 0.8981, F1 Micro: 0.769, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.272, Accuracy: 0.8995, F1 Micro: 0.7797, F1 Macro: 0.7775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2183, Accuracy: 0.9006, F1 Micro: 0.7826, F1 Macro: 0.7808\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9045, F1 Micro: 0.7781, F1 Macro: 0.7743\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.8991, F1 Micro: 0.7794, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.1026, Accuracy: 0.9031, F1 Micro: 0.7716, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.9052, F1 Micro: 0.7712, F1 Macro: 0.7651\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9038, F1 Micro: 0.7795, F1 Macro: 0.7778\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.903, F1 Micro: 0.7656, F1 Macro: 0.7602\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9006, F1 Micro: 0.7826, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.93      0.89       370\n",
      "                sara       0.63      0.76      0.69       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 154.02394127845764 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8974, F1 Micro: 0.7618, F1 Macro: 0.7576\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 50.48705244064331 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4554, Accuracy: 0.8802, F1 Micro: 0.712, F1 Macro: 0.7158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3125, Accuracy: 0.8983, F1 Micro: 0.7474, F1 Macro: 0.7408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2535, Accuracy: 0.9033, F1 Micro: 0.7753, F1 Macro: 0.7706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.903, F1 Micro: 0.7777, F1 Macro: 0.7726\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9033, F1 Micro: 0.7579, F1 Macro: 0.7433\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9055, F1 Micro: 0.7768, F1 Macro: 0.7709\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9038, F1 Micro: 0.7701, F1 Macro: 0.767\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9036, F1 Micro: 0.7752, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9028, F1 Micro: 0.7801, F1 Macro: 0.7799\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9008, F1 Micro: 0.7763, F1 Macro: 0.7755\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9028, F1 Micro: 0.7801, F1 Macro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 162.49070262908936 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4497, Accuracy: 0.8766, F1 Micro: 0.7059, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3136, Accuracy: 0.898, F1 Micro: 0.7497, F1 Macro: 0.746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2583, Accuracy: 0.903, F1 Micro: 0.7831, F1 Macro: 0.7798\n",
      "Epoch 4/10, Train Loss: 0.2138, Accuracy: 0.9019, F1 Micro: 0.7786, F1 Macro: 0.7705\n",
      "Epoch 5/10, Train Loss: 0.1707, Accuracy: 0.9034, F1 Micro: 0.7753, F1 Macro: 0.767\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7698\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.902, F1 Micro: 0.7766, F1 Macro: 0.7723\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9022, F1 Micro: 0.775, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9044, F1 Micro: 0.7773, F1 Macro: 0.7742\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.8973, F1 Micro: 0.7626, F1 Macro: 0.7587\n",
      "Model 2 - Iteration 4120: Accuracy: 0.903, F1 Micro: 0.7831, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.71      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.68149781227112 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4564, Accuracy: 0.8798, F1 Micro: 0.7048, F1 Macro: 0.7048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3142, Accuracy: 0.8994, F1 Micro: 0.7498, F1 Macro: 0.7438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.258, Accuracy: 0.903, F1 Micro: 0.7826, F1 Macro: 0.7781\n",
      "Epoch 4/10, Train Loss: 0.2101, Accuracy: 0.903, F1 Micro: 0.7819, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.9027, F1 Micro: 0.7784, F1 Macro: 0.7748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1309, Accuracy: 0.9058, F1 Micro: 0.7827, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.8984, F1 Micro: 0.7708, F1 Macro: 0.7657\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9039, F1 Micro: 0.7779, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9044, F1 Micro: 0.7748, F1 Macro: 0.7736\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.8977, F1 Micro: 0.7676, F1 Macro: 0.7662\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9058, F1 Micro: 0.7827, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.70      0.68      0.69       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 161.0487961769104 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.898, F1 Micro: 0.7636, F1 Macro: 0.7596\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 46.14842653274536 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4524, Accuracy: 0.8878, F1 Micro: 0.7325, F1 Macro: 0.7308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3067, Accuracy: 0.8984, F1 Micro: 0.7496, F1 Macro: 0.7382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2509, Accuracy: 0.9013, F1 Micro: 0.7819, F1 Macro: 0.7795\n",
      "Epoch 4/10, Train Loss: 0.2049, Accuracy: 0.9059, F1 Micro: 0.7685, F1 Macro: 0.7605\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9023, F1 Micro: 0.774, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9055, F1 Micro: 0.7821, F1 Macro: 0.7806\n",
      "Epoch 7/10, Train Loss: 0.0923, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7651\n",
      "Epoch 8/10, Train Loss: 0.0719, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9061, F1 Micro: 0.7834, F1 Macro: 0.7834\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9052, F1 Micro: 0.7698, F1 Macro: 0.7648\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9061, F1 Micro: 0.7834, F1 Macro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 168.61465501785278 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4403, Accuracy: 0.8842, F1 Micro: 0.7169, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3056, Accuracy: 0.8988, F1 Micro: 0.7566, F1 Macro: 0.7457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2554, Accuracy: 0.8998, F1 Micro: 0.7818, F1 Macro: 0.7784\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.9048, F1 Micro: 0.7715, F1 Macro: 0.7638\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.9008, F1 Micro: 0.7663, F1 Macro: 0.7632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1226, Accuracy: 0.9039, F1 Micro: 0.7838, F1 Macro: 0.786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9019, F1 Micro: 0.7857, F1 Macro: 0.7848\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9058, F1 Micro: 0.7844, F1 Macro: 0.7829\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7777\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.903, F1 Micro: 0.7729, F1 Macro: 0.7716\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9019, F1 Micro: 0.7857, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.62      0.78      0.69       248\n",
      "         radikalisme       0.72      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 170.29504537582397 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4447, Accuracy: 0.8883, F1 Micro: 0.7262, F1 Macro: 0.7165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3053, Accuracy: 0.9005, F1 Micro: 0.7628, F1 Macro: 0.7553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2546, Accuracy: 0.8992, F1 Micro: 0.778, F1 Macro: 0.7755\n",
      "Epoch 4/10, Train Loss: 0.2043, Accuracy: 0.9053, F1 Micro: 0.7703, F1 Macro: 0.7601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9044, F1 Micro: 0.7784, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.9038, F1 Micro: 0.7809, F1 Macro: 0.7772\n",
      "Epoch 7/10, Train Loss: 0.0928, Accuracy: 0.907, F1 Micro: 0.7757, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9077, F1 Micro: 0.7855, F1 Macro: 0.7825\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9056, F1 Micro: 0.7823, F1 Macro: 0.7804\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9009, F1 Micro: 0.7772, F1 Macro: 0.777\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9077, F1 Micro: 0.7855, F1 Macro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 172.274085521698 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8986, F1 Micro: 0.7654, F1 Macro: 0.7616\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 41.99223780632019 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4375, Accuracy: 0.8766, F1 Micro: 0.6815, F1 Macro: 0.6727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3044, Accuracy: 0.8953, F1 Micro: 0.7765, F1 Macro: 0.7737\n",
      "Epoch 3/10, Train Loss: 0.253, Accuracy: 0.9023, F1 Micro: 0.7569, F1 Macro: 0.7449\n",
      "Epoch 4/10, Train Loss: 0.2057, Accuracy: 0.9006, F1 Micro: 0.7525, F1 Macro: 0.7427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1559, Accuracy: 0.9022, F1 Micro: 0.7852, F1 Macro: 0.7842\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9047, F1 Micro: 0.7754, F1 Macro: 0.7701\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9031, F1 Micro: 0.7602, F1 Macro: 0.7493\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9045, F1 Micro: 0.7795, F1 Macro: 0.7759\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.903, F1 Micro: 0.7749, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9034, F1 Micro: 0.7754, F1 Macro: 0.7756\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9022, F1 Micro: 0.7852, F1 Macro: 0.7842\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 171.5823996067047 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4313, Accuracy: 0.8819, F1 Micro: 0.707, F1 Macro: 0.6968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3035, Accuracy: 0.8906, F1 Micro: 0.7703, F1 Macro: 0.7685\n",
      "Epoch 3/10, Train Loss: 0.2521, Accuracy: 0.9027, F1 Micro: 0.7632, F1 Macro: 0.7526\n",
      "Epoch 4/10, Train Loss: 0.2047, Accuracy: 0.9008, F1 Micro: 0.7652, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1579, Accuracy: 0.9023, F1 Micro: 0.78, F1 Macro: 0.7784\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9005, F1 Micro: 0.7643, F1 Macro: 0.7619\n",
      "Epoch 7/10, Train Loss: 0.0962, Accuracy: 0.9031, F1 Micro: 0.7567, F1 Macro: 0.7472\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9025, F1 Micro: 0.7741, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9038, F1 Micro: 0.7845, F1 Macro: 0.7843\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.9034, F1 Micro: 0.7723, F1 Macro: 0.7685\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9038, F1 Micro: 0.7845, F1 Macro: 0.7843\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.65      0.73      0.68       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 173.3452706336975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4335, Accuracy: 0.8825, F1 Micro: 0.7211, F1 Macro: 0.7093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3017, Accuracy: 0.8948, F1 Micro: 0.7778, F1 Macro: 0.7766\n",
      "Epoch 3/10, Train Loss: 0.2486, Accuracy: 0.9048, F1 Micro: 0.7696, F1 Macro: 0.7585\n",
      "Epoch 4/10, Train Loss: 0.2056, Accuracy: 0.9033, F1 Micro: 0.7602, F1 Macro: 0.751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9069, F1 Micro: 0.7904, F1 Macro: 0.7889\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.905, F1 Micro: 0.7833, F1 Macro: 0.7831\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.905, F1 Micro: 0.7797, F1 Macro: 0.7744\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9014, F1 Micro: 0.7684, F1 Macro: 0.7599\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9038, F1 Micro: 0.7817, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9044, F1 Micro: 0.7786, F1 Macro: 0.7756\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9069, F1 Micro: 0.7904, F1 Macro: 0.7889\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.76      0.71       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.42117524147034 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.899, F1 Micro: 0.767, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 37.020371198654175 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4309, Accuracy: 0.8797, F1 Micro: 0.6973, F1 Macro: 0.6906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2937, Accuracy: 0.9008, F1 Micro: 0.7614, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2502, Accuracy: 0.9002, F1 Micro: 0.7741, F1 Macro: 0.7743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.9048, F1 Micro: 0.7847, F1 Macro: 0.7838\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.9016, F1 Micro: 0.7783, F1 Macro: 0.7738\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.8998, F1 Micro: 0.7756, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9034, F1 Micro: 0.7779, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9067, F1 Micro: 0.7798, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9025, F1 Micro: 0.7658, F1 Macro: 0.7557\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9055, F1 Micro: 0.7793, F1 Macro: 0.7745\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9048, F1 Micro: 0.7847, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 177.14078855514526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4286, Accuracy: 0.8791, F1 Micro: 0.7028, F1 Macro: 0.6993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.8989, F1 Micro: 0.7537, F1 Macro: 0.7472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.8953, F1 Micro: 0.771, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2039, Accuracy: 0.9067, F1 Micro: 0.7858, F1 Macro: 0.7828\n",
      "Epoch 5/10, Train Loss: 0.1578, Accuracy: 0.8997, F1 Micro: 0.7819, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.9047, F1 Micro: 0.7898, F1 Macro: 0.7885\n",
      "Epoch 7/10, Train Loss: 0.0899, Accuracy: 0.9075, F1 Micro: 0.7871, F1 Macro: 0.7853\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9016, F1 Micro: 0.7726, F1 Macro: 0.7662\n",
      "Epoch 9/10, Train Loss: 0.0571, Accuracy: 0.9048, F1 Micro: 0.772, F1 Macro: 0.7627\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9023, F1 Micro: 0.7784, F1 Macro: 0.7776\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9047, F1 Micro: 0.7898, F1 Macro: 0.7885\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.64      0.77      0.70       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 179.97193264961243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4291, Accuracy: 0.8795, F1 Micro: 0.6973, F1 Macro: 0.6897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2956, Accuracy: 0.8983, F1 Micro: 0.7512, F1 Macro: 0.7442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2502, Accuracy: 0.9, F1 Micro: 0.7792, F1 Macro: 0.7798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2042, Accuracy: 0.907, F1 Micro: 0.7854, F1 Macro: 0.7844\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.8967, F1 Micro: 0.7812, F1 Macro: 0.7832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9061, F1 Micro: 0.7898, F1 Macro: 0.7892\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9039, F1 Micro: 0.7694, F1 Macro: 0.7651\n",
      "Epoch 8/10, Train Loss: 0.0685, Accuracy: 0.9016, F1 Micro: 0.7655, F1 Macro: 0.7597\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9053, F1 Micro: 0.7752, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.9052, F1 Micro: 0.7756, F1 Macro: 0.7703\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9061, F1 Micro: 0.7898, F1 Macro: 0.7892\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.75      0.71       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 179.0163378715515 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8995, F1 Micro: 0.7685, F1 Macro: 0.7652\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 33.127058267593384 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4252, Accuracy: 0.8864, F1 Micro: 0.7048, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2857, Accuracy: 0.902, F1 Micro: 0.7691, F1 Macro: 0.7631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.9033, F1 Micro: 0.7788, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.9056, F1 Micro: 0.7898, F1 Macro: 0.7883\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9052, F1 Micro: 0.7698, F1 Macro: 0.769\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9075, F1 Micro: 0.7804, F1 Macro: 0.7777\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9, F1 Micro: 0.7676, F1 Macro: 0.7649\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9091, F1 Micro: 0.7815, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9036, F1 Micro: 0.7797, F1 Macro: 0.7794\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.9027, F1 Micro: 0.7707, F1 Macro: 0.7666\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9056, F1 Micro: 0.7898, F1 Macro: 0.7883\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.72      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 183.22253823280334 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4205, Accuracy: 0.8803, F1 Micro: 0.6779, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2869, Accuracy: 0.9017, F1 Micro: 0.7692, F1 Macro: 0.7655\n",
      "Epoch 3/10, Train Loss: 0.2346, Accuracy: 0.8991, F1 Micro: 0.7639, F1 Macro: 0.7534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9036, F1 Micro: 0.7869, F1 Macro: 0.7855\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9036, F1 Micro: 0.7587, F1 Macro: 0.7495\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9027, F1 Micro: 0.7687, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9038, F1 Micro: 0.7675, F1 Macro: 0.7637\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9038, F1 Micro: 0.7775, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9064, F1 Micro: 0.7776, F1 Macro: 0.775\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.9036, F1 Micro: 0.7705, F1 Macro: 0.7667\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9036, F1 Micro: 0.7869, F1 Macro: 0.7855\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.76      0.70       248\n",
      "         radikalisme       0.71      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 181.74398970603943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4238, Accuracy: 0.8844, F1 Micro: 0.6994, F1 Macro: 0.6808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2853, Accuracy: 0.9013, F1 Micro: 0.7697, F1 Macro: 0.7617\n",
      "Epoch 3/10, Train Loss: 0.2343, Accuracy: 0.9013, F1 Micro: 0.7656, F1 Macro: 0.7557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1858, Accuracy: 0.9069, F1 Micro: 0.7891, F1 Macro: 0.788\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9033, F1 Micro: 0.7605, F1 Macro: 0.7555\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9027, F1 Micro: 0.7787, F1 Macro: 0.7769\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9044, F1 Micro: 0.7776, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9042, F1 Micro: 0.7802, F1 Macro: 0.7785\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9042, F1 Micro: 0.7827, F1 Macro: 0.7841\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9042, F1 Micro: 0.7752, F1 Macro: 0.773\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9069, F1 Micro: 0.7891, F1 Macro: 0.788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.46877932548523 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8998, F1 Micro: 0.7699, F1 Macro: 0.7666\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.007425546646118 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.422, Accuracy: 0.8878, F1 Micro: 0.7498, F1 Macro: 0.7477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2731, Accuracy: 0.9014, F1 Micro: 0.7723, F1 Macro: 0.7704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.9045, F1 Micro: 0.7759, F1 Macro: 0.7738\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9002, F1 Micro: 0.738, F1 Macro: 0.7209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1387, Accuracy: 0.9031, F1 Micro: 0.7817, F1 Macro: 0.779\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9039, F1 Micro: 0.7696, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9036, F1 Micro: 0.7854, F1 Macro: 0.784\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9017, F1 Micro: 0.7741, F1 Macro: 0.7702\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.9036, F1 Micro: 0.7714, F1 Macro: 0.7687\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.9056, F1 Micro: 0.7746, F1 Macro: 0.7695\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9036, F1 Micro: 0.7854, F1 Macro: 0.784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.68      0.70      0.69       248\n",
      "         radikalisme       0.76      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 193.43807411193848 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.417, Accuracy: 0.8842, F1 Micro: 0.7468, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.8969, F1 Micro: 0.7609, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2323, Accuracy: 0.9022, F1 Micro: 0.773, F1 Macro: 0.7698\n",
      "Epoch 4/10, Train Loss: 0.1871, Accuracy: 0.8978, F1 Micro: 0.7311, F1 Macro: 0.7131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9017, F1 Micro: 0.7769, F1 Macro: 0.7708\n",
      "Epoch 6/10, Train Loss: 0.1102, Accuracy: 0.9006, F1 Micro: 0.7609, F1 Macro: 0.7556\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.9013, F1 Micro: 0.7756, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.8994, F1 Micro: 0.7713, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9016, F1 Micro: 0.7834, F1 Macro: 0.7847\n",
      "Epoch 10/10, Train Loss: 0.0376, Accuracy: 0.902, F1 Micro: 0.7812, F1 Macro: 0.7785\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9016, F1 Micro: 0.7834, F1 Macro: 0.7847\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.61      0.77      0.68       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.48      0.47      0.47      1365\n",
      "\n",
      "Training completed in 192.77483797073364 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4193, Accuracy: 0.8861, F1 Micro: 0.7506, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2738, Accuracy: 0.898, F1 Micro: 0.7698, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9067, F1 Micro: 0.7806, F1 Macro: 0.7761\n",
      "Epoch 4/10, Train Loss: 0.1871, Accuracy: 0.8969, F1 Micro: 0.7229, F1 Macro: 0.7\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.9002, F1 Micro: 0.7793, F1 Macro: 0.7776\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9033, F1 Micro: 0.7707, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9028, F1 Micro: 0.7791, F1 Macro: 0.7768\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9041, F1 Micro: 0.7767, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9052, F1 Micro: 0.7875, F1 Macro: 0.7901\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9066, F1 Micro: 0.7857, F1 Macro: 0.7846\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9052, F1 Micro: 0.7875, F1 Macro: 0.7901\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.64      0.80      0.71       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 191.61065459251404 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9001, F1 Micro: 0.7708, F1 Macro: 0.7679\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.833613634109497 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8902, F1 Micro: 0.7176, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2735, Accuracy: 0.9009, F1 Micro: 0.7728, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.9078, F1 Micro: 0.7828, F1 Macro: 0.7786\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9042, F1 Micro: 0.7755, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9067, F1 Micro: 0.7767, F1 Macro: 0.7695\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9006, F1 Micro: 0.7753, F1 Macro: 0.7718\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9052, F1 Micro: 0.7731, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9027, F1 Micro: 0.7872, F1 Macro: 0.7857\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.9039, F1 Micro: 0.7829, F1 Macro: 0.7794\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9047, F1 Micro: 0.772, F1 Macro: 0.7672\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9027, F1 Micro: 0.7872, F1 Macro: 0.7857\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.94      0.92       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.48      0.48      0.47      1365\n",
      "\n",
      "Training completed in 197.2690234184265 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4012, Accuracy: 0.8859, F1 Micro: 0.7052, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2734, Accuracy: 0.9017, F1 Micro: 0.7685, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.223, Accuracy: 0.905, F1 Micro: 0.7709, F1 Macro: 0.7627\n",
      "Epoch 4/10, Train Loss: 0.1901, Accuracy: 0.9062, F1 Micro: 0.7708, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.9028, F1 Micro: 0.7743, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9052, F1 Micro: 0.7785, F1 Macro: 0.7703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9028, F1 Micro: 0.7814, F1 Macro: 0.7808\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9006, F1 Micro: 0.7776, F1 Macro: 0.7733\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9042, F1 Micro: 0.7762, F1 Macro: 0.7708\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.903, F1 Micro: 0.7767, F1 Macro: 0.7715\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9028, F1 Micro: 0.7814, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.63      0.77      0.70       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.7570095062256 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.8884, F1 Micro: 0.7119, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2733, Accuracy: 0.898, F1 Micro: 0.7682, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2259, Accuracy: 0.907, F1 Micro: 0.7807, F1 Macro: 0.777\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7724\n",
      "Epoch 5/10, Train Loss: 0.1435, Accuracy: 0.9048, F1 Micro: 0.7777, F1 Macro: 0.7729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9059, F1 Micro: 0.7828, F1 Macro: 0.7792\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9052, F1 Micro: 0.7748, F1 Macro: 0.7705\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9055, F1 Micro: 0.7767, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0401, Accuracy: 0.9052, F1 Micro: 0.7736, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.9052, F1 Micro: 0.7794, F1 Macro: 0.7756\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9059, F1 Micro: 0.7828, F1 Macro: 0.7792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.79      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 196.60124588012695 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9003, F1 Micro: 0.7716, F1 Macro: 0.7687\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 20.283398151397705 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3978, Accuracy: 0.8922, F1 Micro: 0.739, F1 Macro: 0.7221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.903, F1 Micro: 0.7627, F1 Macro: 0.7537\n",
      "Epoch 3/10, Train Loss: 0.2197, Accuracy: 0.8998, F1 Micro: 0.7389, F1 Macro: 0.7311\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9033, F1 Micro: 0.7609, F1 Macro: 0.7538\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9017, F1 Micro: 0.7574, F1 Macro: 0.7467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9023, F1 Micro: 0.7705, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9017, F1 Micro: 0.7772, F1 Macro: 0.7722\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.8952, F1 Micro: 0.7754, F1 Macro: 0.7764\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9053, F1 Micro: 0.7746, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9052, F1 Micro: 0.7734, F1 Macro: 0.7681\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9017, F1 Micro: 0.7772, F1 Macro: 0.7722\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 202.9854416847229 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.391, Accuracy: 0.8859, F1 Micro: 0.7207, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2741, Accuracy: 0.9017, F1 Micro: 0.7624, F1 Macro: 0.7522\n",
      "Epoch 3/10, Train Loss: 0.224, Accuracy: 0.9025, F1 Micro: 0.7536, F1 Macro: 0.7492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1744, Accuracy: 0.9031, F1 Micro: 0.7667, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9, F1 Micro: 0.7668, F1 Macro: 0.7621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9022, F1 Micro: 0.7719, F1 Macro: 0.7666\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9025, F1 Micro: 0.7629, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9013, F1 Micro: 0.779, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9016, F1 Micro: 0.7663, F1 Macro: 0.7617\n",
      "Epoch 10/10, Train Loss: 0.0378, Accuracy: 0.9002, F1 Micro: 0.7725, F1 Macro: 0.7681\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9013, F1 Micro: 0.779, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.71      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 206.61795210838318 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3951, Accuracy: 0.8911, F1 Micro: 0.7371, F1 Macro: 0.7217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.9, F1 Micro: 0.7626, F1 Macro: 0.75\n",
      "Epoch 3/10, Train Loss: 0.2199, Accuracy: 0.9044, F1 Micro: 0.7626, F1 Macro: 0.7564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9059, F1 Micro: 0.776, F1 Macro: 0.7726\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.9033, F1 Micro: 0.772, F1 Macro: 0.7639\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.9036, F1 Micro: 0.7648, F1 Macro: 0.7565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9041, F1 Micro: 0.7807, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.8986, F1 Micro: 0.7814, F1 Macro: 0.781\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.9059, F1 Micro: 0.7811, F1 Macro: 0.7754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.907, F1 Micro: 0.7823, F1 Macro: 0.7748\n",
      "Model 3 - Iteration 5441: Accuracy: 0.907, F1 Micro: 0.7823, F1 Macro: 0.7748\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.69      0.60      0.64       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 206.27705836296082 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9005, F1 Micro: 0.772, F1 Macro: 0.769\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.624451160430908 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8923, F1 Micro: 0.7407, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2558, Accuracy: 0.8992, F1 Micro: 0.7778, F1 Macro: 0.7743\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9038, F1 Micro: 0.7563, F1 Macro: 0.7452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9084, F1 Micro: 0.778, F1 Macro: 0.774\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9017, F1 Micro: 0.7735, F1 Macro: 0.7711\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9069, F1 Micro: 0.7723, F1 Macro: 0.7663\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9048, F1 Micro: 0.7768, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0483, Accuracy: 0.9055, F1 Micro: 0.7706, F1 Macro: 0.7633\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9002, F1 Micro: 0.7727, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9023, F1 Micro: 0.785, F1 Macro: 0.7852\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9023, F1 Micro: 0.785, F1 Macro: 0.7852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.66      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 208.3093044757843 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3809, Accuracy: 0.8898, F1 Micro: 0.7343, F1 Macro: 0.7221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2555, Accuracy: 0.8994, F1 Micro: 0.7788, F1 Macro: 0.7747\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9038, F1 Micro: 0.755, F1 Macro: 0.7437\n",
      "Epoch 4/10, Train Loss: 0.1738, Accuracy: 0.9069, F1 Micro: 0.769, F1 Macro: 0.7621\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.8988, F1 Micro: 0.7722, F1 Macro: 0.7698\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9014, F1 Micro: 0.7423, F1 Macro: 0.7292\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9038, F1 Micro: 0.7758, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9038, F1 Micro: 0.7698, F1 Macro: 0.7653\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9023, F1 Micro: 0.7751, F1 Macro: 0.7728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9066, F1 Micro: 0.7801, F1 Macro: 0.7747\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9066, F1 Micro: 0.7801, F1 Macro: 0.7747\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.79      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 207.1700677871704 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3817, Accuracy: 0.8936, F1 Micro: 0.7479, F1 Macro: 0.7395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2557, Accuracy: 0.8983, F1 Micro: 0.7773, F1 Macro: 0.7735\n",
      "Epoch 3/10, Train Loss: 0.211, Accuracy: 0.9075, F1 Micro: 0.7696, F1 Macro: 0.7592\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9073, F1 Micro: 0.7753, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.126, Accuracy: 0.9038, F1 Micro: 0.7867, F1 Macro: 0.7868\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9058, F1 Micro: 0.7667, F1 Macro: 0.7607\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9038, F1 Micro: 0.775, F1 Macro: 0.7727\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9062, F1 Micro: 0.7748, F1 Macro: 0.7727\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.905, F1 Micro: 0.7844, F1 Macro: 0.7822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9062, F1 Micro: 0.7878, F1 Macro: 0.7862\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9062, F1 Micro: 0.7878, F1 Macro: 0.7862\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 207.9742124080658 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9007, F1 Micro: 0.7727, F1 Macro: 0.7697\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.335155725479126 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8925, F1 Micro: 0.7329, F1 Macro: 0.7232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2508, Accuracy: 0.9016, F1 Micro: 0.7682, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9, F1 Micro: 0.7849, F1 Macro: 0.7858\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9031, F1 Micro: 0.7565, F1 Macro: 0.7493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.9013, F1 Micro: 0.7865, F1 Macro: 0.7878\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9034, F1 Micro: 0.7701, F1 Macro: 0.7658\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.903, F1 Micro: 0.7799, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9013, F1 Micro: 0.7772, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9044, F1 Micro: 0.7776, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9059, F1 Micro: 0.7767, F1 Macro: 0.7727\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9013, F1 Micro: 0.7865, F1 Macro: 0.7878\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.66      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 214.2103099822998 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3725, Accuracy: 0.8917, F1 Micro: 0.7279, F1 Macro: 0.7195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.251, Accuracy: 0.8989, F1 Micro: 0.7643, F1 Macro: 0.7614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.8986, F1 Micro: 0.7823, F1 Macro: 0.7808\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9022, F1 Micro: 0.757, F1 Macro: 0.7518\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.893, F1 Micro: 0.7708, F1 Macro: 0.7704\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9006, F1 Micro: 0.7643, F1 Macro: 0.7582\n",
      "Epoch 7/10, Train Loss: 0.0647, Accuracy: 0.9034, F1 Micro: 0.7682, F1 Macro: 0.7577\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9019, F1 Micro: 0.7683, F1 Macro: 0.7633\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9006, F1 Micro: 0.7754, F1 Macro: 0.7722\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9053, F1 Micro: 0.777, F1 Macro: 0.7741\n",
      "Model 2 - Iteration 5841: Accuracy: 0.8986, F1 Micro: 0.7823, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.58      0.82      0.68       248\n",
      "         radikalisme       0.69      0.90      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.72      0.86      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 212.4899640083313 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3776, Accuracy: 0.8914, F1 Micro: 0.7278, F1 Macro: 0.7142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.9, F1 Micro: 0.7708, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7847\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9031, F1 Micro: 0.7547, F1 Macro: 0.7499\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.8981, F1 Micro: 0.7784, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9042, F1 Micro: 0.7749, F1 Macro: 0.7708\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9047, F1 Micro: 0.7782, F1 Macro: 0.7745\n",
      "Epoch 8/10, Train Loss: 0.0518, Accuracy: 0.9038, F1 Micro: 0.7819, F1 Macro: 0.7808\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.905, F1 Micro: 0.7779, F1 Macro: 0.7737\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9048, F1 Micro: 0.7777, F1 Macro: 0.7735\n",
      "Model 3 - Iteration 5841: Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7847\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.60      0.81      0.69       248\n",
      "         radikalisme       0.71      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 213.1550064086914 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9007, F1 Micro: 0.7733, F1 Macro: 0.7704\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.154932022094727 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3694, Accuracy: 0.8911, F1 Micro: 0.7587, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9039, F1 Micro: 0.7796, F1 Macro: 0.7752\n",
      "Epoch 3/10, Train Loss: 0.1958, Accuracy: 0.9023, F1 Micro: 0.7523, F1 Macro: 0.7352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1606, Accuracy: 0.9045, F1 Micro: 0.7845, F1 Macro: 0.7801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9039, F1 Micro: 0.7859, F1 Macro: 0.7856\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.905, F1 Micro: 0.7686, F1 Macro: 0.7592\n",
      "Epoch 7/10, Train Loss: 0.0656, Accuracy: 0.9094, F1 Micro: 0.7757, F1 Macro: 0.7706\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9027, F1 Micro: 0.7766, F1 Macro: 0.7732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9066, F1 Micro: 0.7869, F1 Macro: 0.7811\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.8991, F1 Micro: 0.7748, F1 Macro: 0.769\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9066, F1 Micro: 0.7869, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 221.99705958366394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3649, Accuracy: 0.8895, F1 Micro: 0.7553, F1 Macro: 0.7507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.902, F1 Micro: 0.7801, F1 Macro: 0.7772\n",
      "Epoch 3/10, Train Loss: 0.1977, Accuracy: 0.9047, F1 Micro: 0.7663, F1 Macro: 0.755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1608, Accuracy: 0.9055, F1 Micro: 0.7848, F1 Macro: 0.7804\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9025, F1 Micro: 0.7752, F1 Macro: 0.7738\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.8995, F1 Micro: 0.7676, F1 Macro: 0.7613\n",
      "Epoch 7/10, Train Loss: 0.0652, Accuracy: 0.9083, F1 Micro: 0.7774, F1 Macro: 0.7697\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.9053, F1 Micro: 0.7764, F1 Macro: 0.7754\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.898, F1 Micro: 0.7718, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9013, F1 Micro: 0.7813, F1 Macro: 0.7795\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9055, F1 Micro: 0.7848, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.73      0.89      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 218.59052228927612 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3664, Accuracy: 0.8878, F1 Micro: 0.7563, F1 Macro: 0.7539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2415, Accuracy: 0.9031, F1 Micro: 0.7806, F1 Macro: 0.777\n",
      "Epoch 3/10, Train Loss: 0.1977, Accuracy: 0.9047, F1 Micro: 0.7742, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9056, F1 Micro: 0.7927, F1 Macro: 0.789\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9045, F1 Micro: 0.7839, F1 Macro: 0.7815\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9008, F1 Micro: 0.7754, F1 Macro: 0.7691\n",
      "Epoch 7/10, Train Loss: 0.0672, Accuracy: 0.9019, F1 Micro: 0.7723, F1 Macro: 0.7624\n",
      "Epoch 8/10, Train Loss: 0.0447, Accuracy: 0.9052, F1 Micro: 0.7814, F1 Macro: 0.7801\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9034, F1 Micro: 0.7736, F1 Macro: 0.7664\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9014, F1 Micro: 0.7813, F1 Macro: 0.7778\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9056, F1 Micro: 0.7927, F1 Macro: 0.789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.85      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.80      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 219.1962878704071 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9009, F1 Micro: 0.774, F1 Macro: 0.7711\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.677582025527954 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.365, Accuracy: 0.8811, F1 Micro: 0.6749, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2275, Accuracy: 0.9014, F1 Micro: 0.7684, F1 Macro: 0.7575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1905, Accuracy: 0.9059, F1 Micro: 0.7795, F1 Macro: 0.7731\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9038, F1 Micro: 0.7753, F1 Macro: 0.7725\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9042, F1 Micro: 0.7724, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0819, Accuracy: 0.9056, F1 Micro: 0.7805, F1 Macro: 0.7755\n",
      "Epoch 7/10, Train Loss: 0.0618, Accuracy: 0.903, F1 Micro: 0.7729, F1 Macro: 0.7663\n",
      "Epoch 8/10, Train Loss: 0.0457, Accuracy: 0.9045, F1 Micro: 0.774, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9055, F1 Micro: 0.7772, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9038, F1 Micro: 0.7665, F1 Macro: 0.762\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9056, F1 Micro: 0.7805, F1 Macro: 0.7755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.79      0.74      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 226.25973653793335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3591, Accuracy: 0.8792, F1 Micro: 0.6644, F1 Macro: 0.6227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2277, Accuracy: 0.8994, F1 Micro: 0.7636, F1 Macro: 0.7523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.19, Accuracy: 0.9047, F1 Micro: 0.7703, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.9027, F1 Micro: 0.7855, F1 Macro: 0.7858\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9089, F1 Micro: 0.7825, F1 Macro: 0.7749\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9039, F1 Micro: 0.7751, F1 Macro: 0.7691\n",
      "Epoch 7/10, Train Loss: 0.0629, Accuracy: 0.9025, F1 Micro: 0.7636, F1 Macro: 0.7558\n",
      "Epoch 8/10, Train Loss: 0.0445, Accuracy: 0.9016, F1 Micro: 0.763, F1 Macro: 0.76\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9025, F1 Micro: 0.7786, F1 Macro: 0.7753\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.9067, F1 Micro: 0.7773, F1 Macro: 0.7769\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9027, F1 Micro: 0.7855, F1 Macro: 0.7858\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.67      0.85      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 226.01013374328613 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3611, Accuracy: 0.882, F1 Micro: 0.6775, F1 Macro: 0.6346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2278, Accuracy: 0.8998, F1 Micro: 0.77, F1 Macro: 0.7598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1902, Accuracy: 0.9053, F1 Micro: 0.7705, F1 Macro: 0.7598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1535, Accuracy: 0.9047, F1 Micro: 0.7875, F1 Macro: 0.7876\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9028, F1 Micro: 0.7777, F1 Macro: 0.7774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9075, F1 Micro: 0.7878, F1 Macro: 0.7869\n",
      "Epoch 7/10, Train Loss: 0.0641, Accuracy: 0.9022, F1 Micro: 0.7657, F1 Macro: 0.759\n",
      "Epoch 8/10, Train Loss: 0.0433, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7673\n",
      "Epoch 9/10, Train Loss: 0.0347, Accuracy: 0.9041, F1 Micro: 0.7759, F1 Macro: 0.7752\n",
      "Epoch 10/10, Train Loss: 0.0292, Accuracy: 0.9017, F1 Micro: 0.7735, F1 Macro: 0.7695\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9075, F1 Micro: 0.7878, F1 Macro: 0.7869\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.80      0.81      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 227.34609651565552 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9011, F1 Micro: 0.7745, F1 Macro: 0.7716\n",
      "Total sampling time: 1076.31 seconds\n",
      "Total runtime: 11844.795112371445 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5b4a3",
   "metadata": {
    "papermill": {
     "duration": 0.135358,
     "end_time": "2025-01-28T16:33:07.278783",
     "exception": false,
     "start_time": "2025-01-28T16:33:07.143425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78927fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T16:33:07.539457Z",
     "iopub.status.busy": "2025-01-28T16:33:07.539108Z",
     "iopub.status.idle": "2025-01-28T19:51:37.844281Z",
     "shell.execute_reply": "2025-01-28T19:51:37.843537Z"
    },
    "papermill": {
     "duration": 11910.436962,
     "end_time": "2025-01-28T19:51:37.845763",
     "exception": false,
     "start_time": "2025-01-28T16:33:07.408801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 7, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5968, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5341, Accuracy: 0.7977, F1 Micro: 0.0988, F1 Macro: 0.0805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4743, Accuracy: 0.8219, F1 Micro: 0.2893, F1 Macro: 0.1949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8345, F1 Micro: 0.4638, F1 Macro: 0.3804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3545, Accuracy: 0.8441, F1 Micro: 0.5035, F1 Macro: 0.4353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3005, Accuracy: 0.8581, F1 Micro: 0.6093, F1 Macro: 0.5602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2678, Accuracy: 0.8648, F1 Micro: 0.628, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2021, Accuracy: 0.8675, F1 Micro: 0.657, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1931, Accuracy: 0.8689, F1 Micro: 0.6917, F1 Macro: 0.6904\n",
      "Epoch 10/10, Train Loss: 0.1319, Accuracy: 0.8711, F1 Micro: 0.6833, F1 Macro: 0.6768\n",
      "Model 1 - Iteration 388: Accuracy: 0.8689, F1 Micro: 0.6917, F1 Macro: 0.6904\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.81      0.85       370\n",
      "                sara       0.56      0.54      0.55       248\n",
      "         radikalisme       0.69      0.78      0.73       243\n",
      "pencemaran_nama_baik       0.62      0.63      0.63       504\n",
      "\n",
      "           micro avg       0.69      0.69      0.69      1365\n",
      "           macro avg       0.69      0.69      0.69      1365\n",
      "        weighted avg       0.70      0.69      0.69      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 58.249072551727295 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5772, Accuracy: 0.7869, F1 Micro: 0.0015, F1 Macro: 0.001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5189, Accuracy: 0.795, F1 Micro: 0.0787, F1 Macro: 0.0656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4483, Accuracy: 0.832, F1 Micro: 0.4173, F1 Macro: 0.3637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3743, Accuracy: 0.8405, F1 Micro: 0.4734, F1 Macro: 0.426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3321, Accuracy: 0.8566, F1 Micro: 0.5977, F1 Macro: 0.5621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2774, Accuracy: 0.8625, F1 Micro: 0.6474, F1 Macro: 0.6397\n",
      "Epoch 7/10, Train Loss: 0.2571, Accuracy: 0.863, F1 Micro: 0.6375, F1 Macro: 0.6287\n",
      "Epoch 8/10, Train Loss: 0.1862, Accuracy: 0.8658, F1 Micro: 0.6419, F1 Macro: 0.6313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1764, Accuracy: 0.8667, F1 Micro: 0.6537, F1 Macro: 0.6414\n",
      "Epoch 10/10, Train Loss: 0.1241, Accuracy: 0.8678, F1 Micro: 0.6484, F1 Macro: 0.6367\n",
      "Model 2 - Iteration 388: Accuracy: 0.8667, F1 Micro: 0.6537, F1 Macro: 0.6414\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.81      0.86       370\n",
      "                sara       0.64      0.37      0.47       248\n",
      "         radikalisme       0.69      0.67      0.68       243\n",
      "pencemaran_nama_baik       0.64      0.50      0.57       504\n",
      "\n",
      "           micro avg       0.73      0.59      0.65      1365\n",
      "           macro avg       0.72      0.59      0.64      1365\n",
      "        weighted avg       0.72      0.59      0.65      1365\n",
      "         samples avg       0.35      0.34      0.34      1365\n",
      "\n",
      "Training completed in 54.317023515701294 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5975, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5331, Accuracy: 0.7891, F1 Micro: 0.0217, F1 Macro: 0.0195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4706, Accuracy: 0.8233, F1 Micro: 0.2988, F1 Macro: 0.2114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4013, Accuracy: 0.8339, F1 Micro: 0.4337, F1 Macro: 0.3452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3537, Accuracy: 0.8436, F1 Micro: 0.5086, F1 Macro: 0.441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2988, Accuracy: 0.8544, F1 Micro: 0.6077, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2637, Accuracy: 0.8564, F1 Micro: 0.6114, F1 Macro: 0.5816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2036, Accuracy: 0.8642, F1 Micro: 0.6393, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1917, Accuracy: 0.8637, F1 Micro: 0.6602, F1 Macro: 0.6554\n",
      "Epoch 10/10, Train Loss: 0.1299, Accuracy: 0.8645, F1 Micro: 0.65, F1 Macro: 0.6377\n",
      "Model 3 - Iteration 388: Accuracy: 0.8637, F1 Micro: 0.6602, F1 Macro: 0.6554\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.78      0.84       370\n",
      "                sara       0.59      0.47      0.53       248\n",
      "         radikalisme       0.64      0.70      0.67       243\n",
      "pencemaran_nama_baik       0.64      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.71      0.62      0.66      1365\n",
      "           macro avg       0.70      0.62      0.66      1365\n",
      "        weighted avg       0.71      0.62      0.66      1365\n",
      "         samples avg       0.36      0.35      0.34      1365\n",
      "\n",
      "Training completed in 58.54371237754822 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8665, F1 Micro: 0.6685, F1 Macro: 0.6624\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 121.05910181999207 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5599, Accuracy: 0.793, F1 Micro: 0.098, F1 Macro: 0.0755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4595, Accuracy: 0.8477, F1 Micro: 0.5796, F1 Macro: 0.5079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3881, Accuracy: 0.867, F1 Micro: 0.6748, F1 Macro: 0.6779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3269, Accuracy: 0.8778, F1 Micro: 0.675, F1 Macro: 0.6534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2563, Accuracy: 0.882, F1 Micro: 0.6861, F1 Macro: 0.6664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2151, Accuracy: 0.892, F1 Micro: 0.7401, F1 Macro: 0.7352\n",
      "Epoch 7/10, Train Loss: 0.1758, Accuracy: 0.8878, F1 Micro: 0.7289, F1 Macro: 0.7233\n",
      "Epoch 8/10, Train Loss: 0.1392, Accuracy: 0.8863, F1 Micro: 0.7217, F1 Macro: 0.7175\n",
      "Epoch 9/10, Train Loss: 0.1132, Accuracy: 0.8859, F1 Micro: 0.7314, F1 Macro: 0.7255\n",
      "Epoch 10/10, Train Loss: 0.0867, Accuracy: 0.8861, F1 Micro: 0.7212, F1 Macro: 0.7112\n",
      "Model 1 - Iteration 971: Accuracy: 0.892, F1 Micro: 0.7401, F1 Macro: 0.7352\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.70      0.57      0.63       248\n",
      "         radikalisme       0.70      0.80      0.75       243\n",
      "pencemaran_nama_baik       0.71      0.65      0.68       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.75      0.72      0.74      1365\n",
      "        weighted avg       0.76      0.72      0.74      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 70.92883062362671 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5552, Accuracy: 0.79, F1 Micro: 0.0857, F1 Macro: 0.0713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4479, Accuracy: 0.8583, F1 Micro: 0.6135, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3845, Accuracy: 0.8737, F1 Micro: 0.6909, F1 Macro: 0.6858\n",
      "Epoch 4/10, Train Loss: 0.3251, Accuracy: 0.8769, F1 Micro: 0.6595, F1 Macro: 0.6338\n",
      "Epoch 5/10, Train Loss: 0.2631, Accuracy: 0.8823, F1 Micro: 0.6829, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2218, Accuracy: 0.8831, F1 Micro: 0.7182, F1 Macro: 0.711\n",
      "Epoch 7/10, Train Loss: 0.1738, Accuracy: 0.8877, F1 Micro: 0.7157, F1 Macro: 0.7016\n",
      "Epoch 8/10, Train Loss: 0.1327, Accuracy: 0.8855, F1 Micro: 0.7158, F1 Macro: 0.7078\n",
      "Epoch 9/10, Train Loss: 0.1117, Accuracy: 0.8823, F1 Micro: 0.6866, F1 Macro: 0.6716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0863, Accuracy: 0.8867, F1 Micro: 0.7191, F1 Macro: 0.7086\n",
      "Model 2 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7191, F1 Macro: 0.7086\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.84      0.87       370\n",
      "                sara       0.67      0.51      0.58       248\n",
      "         radikalisme       0.71      0.70      0.71       243\n",
      "pencemaran_nama_baik       0.72      0.63      0.67       504\n",
      "\n",
      "           micro avg       0.76      0.68      0.72      1365\n",
      "           macro avg       0.75      0.67      0.71      1365\n",
      "        weighted avg       0.76      0.68      0.72      1365\n",
      "         samples avg       0.40      0.39      0.38      1365\n",
      "\n",
      "Training completed in 68.96427917480469 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.562, Accuracy: 0.7939, F1 Micro: 0.1213, F1 Macro: 0.0737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4662, Accuracy: 0.8545, F1 Micro: 0.6006, F1 Macro: 0.5582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.398, Accuracy: 0.8667, F1 Micro: 0.6723, F1 Macro: 0.6659\n",
      "Epoch 4/10, Train Loss: 0.3343, Accuracy: 0.8744, F1 Micro: 0.6579, F1 Macro: 0.6328\n",
      "Epoch 5/10, Train Loss: 0.2648, Accuracy: 0.8773, F1 Micro: 0.6722, F1 Macro: 0.6524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2212, Accuracy: 0.8845, F1 Micro: 0.7219, F1 Macro: 0.7137\n",
      "Epoch 7/10, Train Loss: 0.1807, Accuracy: 0.8831, F1 Micro: 0.6979, F1 Macro: 0.6842\n",
      "Epoch 8/10, Train Loss: 0.1456, Accuracy: 0.8794, F1 Micro: 0.698, F1 Macro: 0.6859\n",
      "Epoch 9/10, Train Loss: 0.1173, Accuracy: 0.8805, F1 Micro: 0.7029, F1 Macro: 0.6964\n",
      "Epoch 10/10, Train Loss: 0.0863, Accuracy: 0.8828, F1 Micro: 0.7201, F1 Macro: 0.7132\n",
      "Model 3 - Iteration 971: Accuracy: 0.8845, F1 Micro: 0.7219, F1 Macro: 0.7137\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.86      0.87       370\n",
      "                sara       0.68      0.52      0.59       248\n",
      "         radikalisme       0.69      0.78      0.73       243\n",
      "pencemaran_nama_baik       0.69      0.64      0.67       504\n",
      "\n",
      "           micro avg       0.74      0.70      0.72      1365\n",
      "           macro avg       0.73      0.70      0.71      1365\n",
      "        weighted avg       0.74      0.70      0.72      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 67.49398422241211 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8771, F1 Micro: 0.6978, F1 Macro: 0.6908\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 108.73253011703491 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5622, Accuracy: 0.8413, F1 Micro: 0.492, F1 Macro: 0.4398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.417, Accuracy: 0.8672, F1 Micro: 0.6545, F1 Macro: 0.6303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3396, Accuracy: 0.8869, F1 Micro: 0.7438, F1 Macro: 0.7434\n",
      "Epoch 4/10, Train Loss: 0.2738, Accuracy: 0.8889, F1 Micro: 0.7239, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2292, Accuracy: 0.8956, F1 Micro: 0.7546, F1 Macro: 0.7519\n",
      "Epoch 6/10, Train Loss: 0.1793, Accuracy: 0.8952, F1 Micro: 0.7428, F1 Macro: 0.7354\n",
      "Epoch 7/10, Train Loss: 0.144, Accuracy: 0.8988, F1 Micro: 0.749, F1 Macro: 0.7421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1107, Accuracy: 0.8973, F1 Micro: 0.7578, F1 Macro: 0.7553\n",
      "Epoch 9/10, Train Loss: 0.0847, Accuracy: 0.8958, F1 Micro: 0.7577, F1 Macro: 0.7557\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.897, F1 Micro: 0.7427, F1 Macro: 0.7288\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8973, F1 Micro: 0.7578, F1 Macro: 0.7553\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.68      0.65      0.66       248\n",
      "         radikalisme       0.77      0.75      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.75      0.76      1365\n",
      "           macro avg       0.76      0.75      0.76      1365\n",
      "        weighted avg       0.76      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 83.96939396858215 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5534, Accuracy: 0.8406, F1 Micro: 0.588, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.409, Accuracy: 0.8725, F1 Micro: 0.6625, F1 Macro: 0.6338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3343, Accuracy: 0.8844, F1 Micro: 0.7293, F1 Macro: 0.7279\n",
      "Epoch 4/10, Train Loss: 0.2724, Accuracy: 0.8877, F1 Micro: 0.7263, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2345, Accuracy: 0.8922, F1 Micro: 0.7414, F1 Macro: 0.7398\n",
      "Epoch 6/10, Train Loss: 0.1785, Accuracy: 0.8945, F1 Micro: 0.7381, F1 Macro: 0.7283\n",
      "Epoch 7/10, Train Loss: 0.1407, Accuracy: 0.8955, F1 Micro: 0.7355, F1 Macro: 0.727\n",
      "Epoch 8/10, Train Loss: 0.113, Accuracy: 0.8961, F1 Micro: 0.7381, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.8984, F1 Micro: 0.749, F1 Macro: 0.7461\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.8939, F1 Micro: 0.7465, F1 Macro: 0.741\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8984, F1 Micro: 0.749, F1 Macro: 0.7461\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.84      0.89       370\n",
      "                sara       0.74      0.57      0.65       248\n",
      "         radikalisme       0.75      0.78      0.76       243\n",
      "pencemaran_nama_baik       0.73      0.65      0.69       504\n",
      "\n",
      "           micro avg       0.79      0.71      0.75      1365\n",
      "           macro avg       0.79      0.71      0.75      1365\n",
      "        weighted avg       0.79      0.71      0.75      1365\n",
      "         samples avg       0.41      0.40      0.40      1365\n",
      "\n",
      "Training completed in 84.44423723220825 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5594, Accuracy: 0.8458, F1 Micro: 0.522, F1 Macro: 0.4859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4191, Accuracy: 0.8652, F1 Micro: 0.6298, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3363, Accuracy: 0.8813, F1 Micro: 0.7292, F1 Macro: 0.7258\n",
      "Epoch 4/10, Train Loss: 0.2699, Accuracy: 0.888, F1 Micro: 0.7233, F1 Macro: 0.7122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2278, Accuracy: 0.8958, F1 Micro: 0.7486, F1 Macro: 0.7455\n",
      "Epoch 6/10, Train Loss: 0.1771, Accuracy: 0.8942, F1 Micro: 0.7365, F1 Macro: 0.728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1489, Accuracy: 0.8953, F1 Micro: 0.75, F1 Macro: 0.744\n",
      "Epoch 8/10, Train Loss: 0.1179, Accuracy: 0.8931, F1 Micro: 0.7436, F1 Macro: 0.7397\n",
      "Epoch 9/10, Train Loss: 0.0841, Accuracy: 0.8933, F1 Micro: 0.7321, F1 Macro: 0.7268\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.8938, F1 Micro: 0.7391, F1 Macro: 0.7324\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8953, F1 Micro: 0.75, F1 Macro: 0.744\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.84      0.88       370\n",
      "                sara       0.71      0.57      0.63       248\n",
      "         radikalisme       0.69      0.85      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.69      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.74      0.75      1365\n",
      "           macro avg       0.76      0.74      0.74      1365\n",
      "        weighted avg       0.77      0.74      0.75      1365\n",
      "         samples avg       0.41      0.41      0.41      1365\n",
      "\n",
      "Training completed in 84.50801253318787 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8838, F1 Micro: 0.7159, F1 Macro: 0.71\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 98.4310393333435 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.857, F1 Micro: 0.6393, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.391, Accuracy: 0.8789, F1 Micro: 0.678, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3062, Accuracy: 0.8939, F1 Micro: 0.7307, F1 Macro: 0.7301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2608, Accuracy: 0.8959, F1 Micro: 0.7394, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2113, Accuracy: 0.8975, F1 Micro: 0.7459, F1 Macro: 0.7346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1788, Accuracy: 0.8973, F1 Micro: 0.7615, F1 Macro: 0.7581\n",
      "Epoch 7/10, Train Loss: 0.1215, Accuracy: 0.8966, F1 Micro: 0.7373, F1 Macro: 0.7329\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.897, F1 Micro: 0.7542, F1 Macro: 0.7504\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.8984, F1 Micro: 0.7603, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.8994, F1 Micro: 0.7588, F1 Macro: 0.7528\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8973, F1 Micro: 0.7615, F1 Macro: 0.7581\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.72      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1365\n",
      "           macro avg       0.75      0.77      0.76      1365\n",
      "        weighted avg       0.76      0.77      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 100.10036158561707 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5449, Accuracy: 0.8558, F1 Micro: 0.6435, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3894, Accuracy: 0.8805, F1 Micro: 0.6771, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3103, Accuracy: 0.8908, F1 Micro: 0.7256, F1 Macro: 0.7213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2654, Accuracy: 0.8934, F1 Micro: 0.7369, F1 Macro: 0.7291\n",
      "Epoch 5/10, Train Loss: 0.2105, Accuracy: 0.8942, F1 Micro: 0.7247, F1 Macro: 0.7108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1745, Accuracy: 0.8956, F1 Micro: 0.7479, F1 Macro: 0.7417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1248, Accuracy: 0.8969, F1 Micro: 0.7519, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.8981, F1 Micro: 0.7528, F1 Macro: 0.7493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.8961, F1 Micro: 0.7567, F1 Macro: 0.7535\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.8947, F1 Micro: 0.7295, F1 Macro: 0.718\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8961, F1 Micro: 0.7567, F1 Macro: 0.7535\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.67      0.66      0.67       248\n",
      "         radikalisme       0.75      0.74      0.75       243\n",
      "pencemaran_nama_baik       0.69      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.76      0.75      0.75      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 102.32295322418213 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5477, Accuracy: 0.853, F1 Micro: 0.6311, F1 Macro: 0.6097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3916, Accuracy: 0.877, F1 Micro: 0.6714, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3121, Accuracy: 0.8859, F1 Micro: 0.7049, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2636, Accuracy: 0.8922, F1 Micro: 0.7262, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.895, F1 Micro: 0.7279, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1676, Accuracy: 0.8941, F1 Micro: 0.7443, F1 Macro: 0.7396\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.8945, F1 Micro: 0.7377, F1 Macro: 0.7341\n",
      "Epoch 8/10, Train Loss: 0.0981, Accuracy: 0.895, F1 Micro: 0.7387, F1 Macro: 0.7289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.8981, F1 Micro: 0.7576, F1 Macro: 0.7527\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.8963, F1 Micro: 0.7434, F1 Macro: 0.7319\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8981, F1 Micro: 0.7576, F1 Macro: 0.7527\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.74      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1365\n",
      "           macro avg       0.76      0.74      0.75      1365\n",
      "        weighted avg       0.77      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.41      1365\n",
      "\n",
      "Training completed in 101.31913352012634 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8871, F1 Micro: 0.7266, F1 Macro: 0.7212\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 88.46867442131042 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5286, Accuracy: 0.8612, F1 Micro: 0.6334, F1 Macro: 0.621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3738, Accuracy: 0.8839, F1 Micro: 0.7057, F1 Macro: 0.688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3028, Accuracy: 0.895, F1 Micro: 0.7415, F1 Macro: 0.7254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2442, Accuracy: 0.9008, F1 Micro: 0.7671, F1 Macro: 0.7654\n",
      "Epoch 5/10, Train Loss: 0.2015, Accuracy: 0.8972, F1 Micro: 0.7477, F1 Macro: 0.7372\n",
      "Epoch 6/10, Train Loss: 0.1548, Accuracy: 0.8986, F1 Micro: 0.7658, F1 Macro: 0.7654\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.8984, F1 Micro: 0.7609, F1 Macro: 0.7557\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.8948, F1 Micro: 0.7554, F1 Macro: 0.7496\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.8992, F1 Micro: 0.7524, F1 Macro: 0.7457\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.8966, F1 Micro: 0.7519, F1 Macro: 0.7468\n",
      "Model 1 - Iteration 2394: Accuracy: 0.9008, F1 Micro: 0.7671, F1 Macro: 0.7654\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.84      0.89       370\n",
      "                sara       0.71      0.65      0.68       248\n",
      "         radikalisme       0.71      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 109.4487783908844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5192, Accuracy: 0.873, F1 Micro: 0.662, F1 Macro: 0.6639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3723, Accuracy: 0.8859, F1 Micro: 0.7177, F1 Macro: 0.7045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3045, Accuracy: 0.8941, F1 Micro: 0.7307, F1 Macro: 0.7163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.245, Accuracy: 0.8919, F1 Micro: 0.763, F1 Macro: 0.7644\n",
      "Epoch 5/10, Train Loss: 0.2096, Accuracy: 0.8975, F1 Micro: 0.7407, F1 Macro: 0.73\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1547, Accuracy: 0.8977, F1 Micro: 0.7691, F1 Macro: 0.7703\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.8933, F1 Micro: 0.7608, F1 Macro: 0.7612\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.8986, F1 Micro: 0.7487, F1 Macro: 0.744\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.8944, F1 Micro: 0.7522, F1 Macro: 0.7523\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.8986, F1 Micro: 0.759, F1 Macro: 0.7555\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8977, F1 Micro: 0.7691, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.82      0.88       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.77      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 112.42042541503906 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.8612, F1 Micro: 0.6442, F1 Macro: 0.642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3813, Accuracy: 0.8838, F1 Micro: 0.7029, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3065, Accuracy: 0.8941, F1 Micro: 0.7443, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2467, Accuracy: 0.8977, F1 Micro: 0.7603, F1 Macro: 0.7575\n",
      "Epoch 5/10, Train Loss: 0.199, Accuracy: 0.8969, F1 Micro: 0.7465, F1 Macro: 0.7361\n",
      "Epoch 6/10, Train Loss: 0.1596, Accuracy: 0.898, F1 Micro: 0.7559, F1 Macro: 0.7544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1186, Accuracy: 0.8941, F1 Micro: 0.7631, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.8977, F1 Micro: 0.7651, F1 Macro: 0.7647\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.8955, F1 Micro: 0.7547, F1 Macro: 0.7518\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.8959, F1 Micro: 0.7513, F1 Macro: 0.7467\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8977, F1 Micro: 0.7651, F1 Macro: 0.7647\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 112.92651748657227 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8894, F1 Micro: 0.7347, F1 Macro: 0.7303\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 80.32193207740784 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4965, Accuracy: 0.8658, F1 Micro: 0.7097, F1 Macro: 0.714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3573, Accuracy: 0.8914, F1 Micro: 0.7412, F1 Macro: 0.7387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3048, Accuracy: 0.8986, F1 Micro: 0.7509, F1 Macro: 0.7403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2453, Accuracy: 0.9011, F1 Micro: 0.7756, F1 Macro: 0.7677\n",
      "Epoch 5/10, Train Loss: 0.2008, Accuracy: 0.8967, F1 Micro: 0.7709, F1 Macro: 0.771\n",
      "Epoch 6/10, Train Loss: 0.1521, Accuracy: 0.9011, F1 Micro: 0.7592, F1 Macro: 0.7485\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.8991, F1 Micro: 0.76, F1 Macro: 0.7517\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9016, F1 Micro: 0.751, F1 Macro: 0.7402\n",
      "Epoch 9/10, Train Loss: 0.0628, Accuracy: 0.9003, F1 Micro: 0.7589, F1 Macro: 0.7489\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9022, F1 Micro: 0.7676, F1 Macro: 0.7594\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9011, F1 Micro: 0.7756, F1 Macro: 0.7677\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.71      0.59      0.65       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 121.4929826259613 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4924, Accuracy: 0.868, F1 Micro: 0.6888, F1 Macro: 0.6934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3538, Accuracy: 0.8948, F1 Micro: 0.7438, F1 Macro: 0.74\n",
      "Epoch 3/10, Train Loss: 0.306, Accuracy: 0.897, F1 Micro: 0.7361, F1 Macro: 0.7193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.243, Accuracy: 0.8973, F1 Micro: 0.7683, F1 Macro: 0.7654\n",
      "Epoch 5/10, Train Loss: 0.1994, Accuracy: 0.8964, F1 Micro: 0.7678, F1 Macro: 0.7673\n",
      "Epoch 6/10, Train Loss: 0.1532, Accuracy: 0.9022, F1 Micro: 0.762, F1 Macro: 0.7498\n",
      "Epoch 7/10, Train Loss: 0.1171, Accuracy: 0.8995, F1 Micro: 0.7598, F1 Macro: 0.7555\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9, F1 Micro: 0.7605, F1 Macro: 0.7551\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.8986, F1 Micro: 0.7497, F1 Macro: 0.7369\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9003, F1 Micro: 0.7535, F1 Macro: 0.7464\n",
      "Model 2 - Iteration 2777: Accuracy: 0.8973, F1 Micro: 0.7683, F1 Macro: 0.7654\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.85      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.65      0.84      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 120.41668152809143 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5066, Accuracy: 0.862, F1 Micro: 0.6881, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3614, Accuracy: 0.8886, F1 Micro: 0.7312, F1 Macro: 0.73\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3058, Accuracy: 0.8948, F1 Micro: 0.7386, F1 Macro: 0.7272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2413, Accuracy: 0.8992, F1 Micro: 0.7671, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2026, Accuracy: 0.8988, F1 Micro: 0.7694, F1 Macro: 0.769\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.902, F1 Micro: 0.7531, F1 Macro: 0.7421\n",
      "Epoch 7/10, Train Loss: 0.1194, Accuracy: 0.9, F1 Micro: 0.7581, F1 Macro: 0.7513\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9013, F1 Micro: 0.755, F1 Macro: 0.7459\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.8994, F1 Micro: 0.7586, F1 Macro: 0.753\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.8969, F1 Micro: 0.7454, F1 Macro: 0.7301\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8988, F1 Micro: 0.7694, F1 Macro: 0.769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.83      0.89       370\n",
      "                sara       0.65      0.70      0.68       248\n",
      "         radikalisme       0.70      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 123.7789044380188 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.891, F1 Micro: 0.7408, F1 Macro: 0.7365\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 72.3986463546753 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4848, Accuracy: 0.8716, F1 Micro: 0.6836, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3455, Accuracy: 0.8952, F1 Micro: 0.7438, F1 Macro: 0.7336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2804, Accuracy: 0.8952, F1 Micro: 0.7726, F1 Macro: 0.7724\n",
      "Epoch 4/10, Train Loss: 0.2386, Accuracy: 0.9011, F1 Micro: 0.7628, F1 Macro: 0.7569\n",
      "Epoch 5/10, Train Loss: 0.1984, Accuracy: 0.9031, F1 Micro: 0.7726, F1 Macro: 0.7716\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9013, F1 Micro: 0.7687, F1 Macro: 0.7594\n",
      "Epoch 7/10, Train Loss: 0.1144, Accuracy: 0.9009, F1 Micro: 0.7652, F1 Macro: 0.7628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9055, F1 Micro: 0.773, F1 Macro: 0.7656\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9028, F1 Micro: 0.7677, F1 Macro: 0.7688\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9027, F1 Micro: 0.7664, F1 Macro: 0.7579\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9055, F1 Micro: 0.773, F1 Macro: 0.7656\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.73      0.58      0.64       248\n",
      "         radikalisme       0.80      0.78      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.75      0.77      1365\n",
      "           macro avg       0.79      0.74      0.77      1365\n",
      "        weighted avg       0.79      0.75      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 131.7040274143219 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4821, Accuracy: 0.8806, F1 Micro: 0.7032, F1 Macro: 0.6908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3492, Accuracy: 0.8931, F1 Micro: 0.7431, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.8959, F1 Micro: 0.7713, F1 Macro: 0.7724\n",
      "Epoch 4/10, Train Loss: 0.2467, Accuracy: 0.903, F1 Micro: 0.7585, F1 Macro: 0.7518\n",
      "Epoch 5/10, Train Loss: 0.1952, Accuracy: 0.9028, F1 Micro: 0.766, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.1521, Accuracy: 0.8967, F1 Micro: 0.7538, F1 Macro: 0.7354\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.9002, F1 Micro: 0.7655, F1 Macro: 0.7631\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.8998, F1 Micro: 0.7641, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.9039, F1 Micro: 0.773, F1 Macro: 0.7716\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.8997, F1 Micro: 0.7617, F1 Macro: 0.7585\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9039, F1 Micro: 0.773, F1 Macro: 0.7716\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.70      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 131.03420877456665 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4916, Accuracy: 0.8706, F1 Micro: 0.6868, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3517, Accuracy: 0.8919, F1 Micro: 0.7347, F1 Macro: 0.7232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2829, Accuracy: 0.8964, F1 Micro: 0.7676, F1 Macro: 0.7637\n",
      "Epoch 4/10, Train Loss: 0.24, Accuracy: 0.9017, F1 Micro: 0.7649, F1 Macro: 0.7592\n",
      "Epoch 5/10, Train Loss: 0.1953, Accuracy: 0.9025, F1 Micro: 0.7596, F1 Macro: 0.7539\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9005, F1 Micro: 0.7635, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.1107, Accuracy: 0.9006, F1 Micro: 0.7622, F1 Macro: 0.7558\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9013, F1 Micro: 0.7671, F1 Macro: 0.761\n",
      "Epoch 9/10, Train Loss: 0.0695, Accuracy: 0.9033, F1 Micro: 0.7654, F1 Macro: 0.7613\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.902, F1 Micro: 0.7668, F1 Macro: 0.7643\n",
      "Model 3 - Iteration 3122: Accuracy: 0.8964, F1 Micro: 0.7676, F1 Macro: 0.7637\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.63      0.67      0.65       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.80      0.76      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 129.4156048297882 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8926, F1 Micro: 0.7451, F1 Macro: 0.7408\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 65.34611797332764 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4901, Accuracy: 0.868, F1 Micro: 0.6321, F1 Macro: 0.6224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3371, Accuracy: 0.8931, F1 Micro: 0.7527, F1 Macro: 0.7443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2767, Accuracy: 0.9056, F1 Micro: 0.7686, F1 Macro: 0.7554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2297, Accuracy: 0.9019, F1 Micro: 0.7721, F1 Macro: 0.7695\n",
      "Epoch 5/10, Train Loss: 0.1823, Accuracy: 0.9008, F1 Micro: 0.7515, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9059, F1 Micro: 0.7759, F1 Macro: 0.7756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9069, F1 Micro: 0.7791, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.9053, F1 Micro: 0.7785, F1 Macro: 0.7748\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.9033, F1 Micro: 0.771, F1 Macro: 0.7671\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.903, F1 Micro: 0.7759, F1 Macro: 0.7732\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9069, F1 Micro: 0.7791, F1 Macro: 0.7734\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.71      0.62      0.66       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 144.1812343597412 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4906, Accuracy: 0.8656, F1 Micro: 0.6331, F1 Macro: 0.6207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3402, Accuracy: 0.8919, F1 Micro: 0.7397, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2752, Accuracy: 0.9023, F1 Micro: 0.7628, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2255, Accuracy: 0.8998, F1 Micro: 0.7648, F1 Macro: 0.7563\n",
      "Epoch 5/10, Train Loss: 0.1819, Accuracy: 0.902, F1 Micro: 0.7538, F1 Macro: 0.7423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9017, F1 Micro: 0.7663, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9036, F1 Micro: 0.7739, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9017, F1 Micro: 0.7784, F1 Macro: 0.7771\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9009, F1 Micro: 0.7739, F1 Macro: 0.7713\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9025, F1 Micro: 0.7696, F1 Macro: 0.7649\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9017, F1 Micro: 0.7784, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 145.81556820869446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4944, Accuracy: 0.863, F1 Micro: 0.631, F1 Macro: 0.6195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.341, Accuracy: 0.8864, F1 Micro: 0.735, F1 Macro: 0.7187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2787, Accuracy: 0.8988, F1 Micro: 0.7429, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2267, Accuracy: 0.8989, F1 Micro: 0.7606, F1 Macro: 0.75\n",
      "Epoch 5/10, Train Loss: 0.1868, Accuracy: 0.8998, F1 Micro: 0.7497, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9027, F1 Micro: 0.7705, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9045, F1 Micro: 0.774, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9019, F1 Micro: 0.7736, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.8997, F1 Micro: 0.7765, F1 Macro: 0.7763\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9019, F1 Micro: 0.7705, F1 Macro: 0.7668\n",
      "Model 3 - Iteration 3432: Accuracy: 0.8997, F1 Micro: 0.7765, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.89       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 145.86927890777588 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8939, F1 Micro: 0.7492, F1 Macro: 0.7452\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 59.723657846450806 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4769, Accuracy: 0.8737, F1 Micro: 0.7301, F1 Macro: 0.7311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3325, Accuracy: 0.8947, F1 Micro: 0.7517, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.269, Accuracy: 0.8986, F1 Micro: 0.7552, F1 Macro: 0.7537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2168, Accuracy: 0.9038, F1 Micro: 0.771, F1 Macro: 0.7658\n",
      "Epoch 5/10, Train Loss: 0.1829, Accuracy: 0.9033, F1 Micro: 0.7681, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.1413, Accuracy: 0.8989, F1 Micro: 0.7522, F1 Macro: 0.7427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9067, F1 Micro: 0.7715, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.9048, F1 Micro: 0.7799, F1 Macro: 0.7792\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9047, F1 Micro: 0.7759, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9052, F1 Micro: 0.7856, F1 Macro: 0.7825\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9052, F1 Micro: 0.7856, F1 Macro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 153.11349749565125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4709, Accuracy: 0.8809, F1 Micro: 0.7288, F1 Macro: 0.7289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3331, Accuracy: 0.895, F1 Micro: 0.7537, F1 Macro: 0.7546\n",
      "Epoch 3/10, Train Loss: 0.2713, Accuracy: 0.8964, F1 Micro: 0.7403, F1 Macro: 0.7395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2214, Accuracy: 0.9058, F1 Micro: 0.7838, F1 Macro: 0.7833\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.9034, F1 Micro: 0.7709, F1 Macro: 0.7638\n",
      "Epoch 6/10, Train Loss: 0.1432, Accuracy: 0.9025, F1 Micro: 0.7617, F1 Macro: 0.755\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9069, F1 Micro: 0.7718, F1 Macro: 0.7685\n",
      "Epoch 8/10, Train Loss: 0.0802, Accuracy: 0.9067, F1 Micro: 0.7801, F1 Macro: 0.7803\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9056, F1 Micro: 0.7827, F1 Macro: 0.7807\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.902, F1 Micro: 0.7765, F1 Macro: 0.7775\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9058, F1 Micro: 0.7838, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.77      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 146.78744387626648 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4811, Accuracy: 0.8753, F1 Micro: 0.72, F1 Macro: 0.7229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3328, Accuracy: 0.8925, F1 Micro: 0.7507, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.271, Accuracy: 0.8994, F1 Micro: 0.7542, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2207, Accuracy: 0.9041, F1 Micro: 0.7744, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9058, F1 Micro: 0.7732, F1 Macro: 0.7668\n",
      "Epoch 6/10, Train Loss: 0.1332, Accuracy: 0.9025, F1 Micro: 0.7742, F1 Macro: 0.7677\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.9017, F1 Micro: 0.7536, F1 Macro: 0.746\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9009, F1 Micro: 0.7723, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.905, F1 Micro: 0.7711, F1 Macro: 0.7673\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9011, F1 Micro: 0.7737, F1 Macro: 0.773\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9041, F1 Micro: 0.7744, F1 Macro: 0.7687\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.69      0.64      0.66       248\n",
      "         radikalisme       0.77      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 148.99478340148926 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8951, F1 Micro: 0.7528, F1 Macro: 0.7489\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 53.55621957778931 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4647, Accuracy: 0.8702, F1 Micro: 0.6672, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3249, Accuracy: 0.8953, F1 Micro: 0.7354, F1 Macro: 0.7252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2678, Accuracy: 0.9022, F1 Micro: 0.7669, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2208, Accuracy: 0.9038, F1 Micro: 0.773, F1 Macro: 0.7731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1811, Accuracy: 0.903, F1 Micro: 0.7769, F1 Macro: 0.7763\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9053, F1 Micro: 0.7749, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9013, F1 Micro: 0.7617, F1 Macro: 0.758\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9042, F1 Micro: 0.7715, F1 Macro: 0.7676\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9013, F1 Micro: 0.7722, F1 Macro: 0.77\n",
      "Model 1 - Iteration 3886: Accuracy: 0.903, F1 Micro: 0.7769, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.68      0.67      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 155.46422791481018 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4599, Accuracy: 0.8773, F1 Micro: 0.6913, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3257, Accuracy: 0.8923, F1 Micro: 0.7221, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2677, Accuracy: 0.9047, F1 Micro: 0.7774, F1 Macro: 0.7729\n",
      "Epoch 4/10, Train Loss: 0.2198, Accuracy: 0.9011, F1 Micro: 0.7524, F1 Macro: 0.7505\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9027, F1 Micro: 0.7722, F1 Macro: 0.7694\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.9038, F1 Micro: 0.7695, F1 Macro: 0.7662\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.902, F1 Micro: 0.7691, F1 Macro: 0.7679\n",
      "Epoch 8/10, Train Loss: 0.0716, Accuracy: 0.9038, F1 Micro: 0.7645, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9038, F1 Micro: 0.7823, F1 Macro: 0.7828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9064, F1 Micro: 0.7848, F1 Macro: 0.7838\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9064, F1 Micro: 0.7848, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.69      0.71      0.70       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 155.6064338684082 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.467, Accuracy: 0.8778, F1 Micro: 0.6929, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.323, Accuracy: 0.8928, F1 Micro: 0.7252, F1 Macro: 0.7174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2661, Accuracy: 0.9033, F1 Micro: 0.7732, F1 Macro: 0.7675\n",
      "Epoch 4/10, Train Loss: 0.2159, Accuracy: 0.9041, F1 Micro: 0.7655, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1797, Accuracy: 0.9066, F1 Micro: 0.777, F1 Macro: 0.7729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1259, Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.0989, Accuracy: 0.9042, F1 Micro: 0.7736, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9009, F1 Micro: 0.7742, F1 Macro: 0.7736\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.8988, F1 Micro: 0.7742, F1 Macro: 0.7761\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.8995, F1 Micro: 0.7721, F1 Macro: 0.7719\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.68      0.74      0.71       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 156.08532905578613 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.896, F1 Micro: 0.7555, F1 Macro: 0.752\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 50.66366982460022 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4619, Accuracy: 0.8778, F1 Micro: 0.6739, F1 Macro: 0.6641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3231, Accuracy: 0.8991, F1 Micro: 0.7517, F1 Macro: 0.7431\n",
      "Epoch 3/10, Train Loss: 0.2615, Accuracy: 0.9005, F1 Micro: 0.7405, F1 Macro: 0.7252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2206, Accuracy: 0.9053, F1 Micro: 0.7739, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9064, F1 Micro: 0.7744, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9062, F1 Micro: 0.7751, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.093, Accuracy: 0.9094, F1 Micro: 0.7815, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9067, F1 Micro: 0.7842, F1 Macro: 0.782\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7834\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9056, F1 Micro: 0.7721, F1 Macro: 0.7665\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9067, F1 Micro: 0.7842, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 166.8202555179596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4517, Accuracy: 0.8686, F1 Micro: 0.6274, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3214, Accuracy: 0.8997, F1 Micro: 0.748, F1 Macro: 0.7407\n",
      "Epoch 3/10, Train Loss: 0.2696, Accuracy: 0.8952, F1 Micro: 0.716, F1 Macro: 0.7009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2252, Accuracy: 0.8994, F1 Micro: 0.7508, F1 Macro: 0.7476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.168, Accuracy: 0.9031, F1 Micro: 0.7589, F1 Macro: 0.7434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9042, F1 Micro: 0.7732, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9061, F1 Micro: 0.7825, F1 Macro: 0.7807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9069, F1 Micro: 0.7833, F1 Macro: 0.7835\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.9045, F1 Micro: 0.7658, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.907, F1 Micro: 0.7845, F1 Macro: 0.7848\n",
      "Model 2 - Iteration 4120: Accuracy: 0.907, F1 Micro: 0.7845, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.72      0.70      0.71       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 169.22085237503052 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4638, Accuracy: 0.8675, F1 Micro: 0.6221, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3225, Accuracy: 0.8988, F1 Micro: 0.7527, F1 Macro: 0.7468\n",
      "Epoch 3/10, Train Loss: 0.2657, Accuracy: 0.8986, F1 Micro: 0.7297, F1 Macro: 0.7161\n",
      "Epoch 4/10, Train Loss: 0.2226, Accuracy: 0.8983, F1 Micro: 0.7482, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1655, Accuracy: 0.9055, F1 Micro: 0.7745, F1 Macro: 0.7688\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.9045, F1 Micro: 0.7706, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0994, Accuracy: 0.9062, F1 Micro: 0.7778, F1 Macro: 0.7736\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9048, F1 Micro: 0.7735, F1 Macro: 0.7703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9066, F1 Micro: 0.7779, F1 Macro: 0.7739\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.9044, F1 Micro: 0.7743, F1 Macro: 0.7719\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9066, F1 Micro: 0.7779, F1 Macro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.71      0.62      0.67       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.79      0.76      0.77      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.45      0.43      0.43      1365\n",
      "\n",
      "Training completed in 163.39235949516296 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.897, F1 Micro: 0.758, F1 Macro: 0.7546\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 45.889400482177734 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4489, Accuracy: 0.8852, F1 Micro: 0.7183, F1 Macro: 0.7057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3161, Accuracy: 0.902, F1 Micro: 0.7626, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2539, Accuracy: 0.9023, F1 Micro: 0.7633, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2022, Accuracy: 0.907, F1 Micro: 0.7837, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9039, F1 Micro: 0.7862, F1 Macro: 0.7831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1293, Accuracy: 0.9056, F1 Micro: 0.7916, F1 Macro: 0.7917\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9045, F1 Micro: 0.7728, F1 Macro: 0.7656\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9053, F1 Micro: 0.7859, F1 Macro: 0.7826\n",
      "Epoch 9/10, Train Loss: 0.0581, Accuracy: 0.9047, F1 Micro: 0.7875, F1 Macro: 0.7884\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9044, F1 Micro: 0.7742, F1 Macro: 0.774\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9056, F1 Micro: 0.7916, F1 Macro: 0.7917\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.64      0.79      0.71       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 170.90817165374756 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4395, Accuracy: 0.8873, F1 Micro: 0.7303, F1 Macro: 0.7219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3178, Accuracy: 0.9013, F1 Micro: 0.7636, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.255, Accuracy: 0.9033, F1 Micro: 0.7683, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9041, F1 Micro: 0.7721, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9041, F1 Micro: 0.7835, F1 Macro: 0.7838\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.903, F1 Micro: 0.7657, F1 Macro: 0.7627\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9056, F1 Micro: 0.7778, F1 Macro: 0.7748\n",
      "Epoch 8/10, Train Loss: 0.0716, Accuracy: 0.9027, F1 Micro: 0.7709, F1 Macro: 0.7648\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9013, F1 Micro: 0.7757, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.905, F1 Micro: 0.777, F1 Macro: 0.777\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9041, F1 Micro: 0.7835, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.64      0.77      0.70       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 169.0994839668274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4513, Accuracy: 0.8841, F1 Micro: 0.7174, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3193, Accuracy: 0.8981, F1 Micro: 0.7645, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2572, Accuracy: 0.9041, F1 Micro: 0.7693, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.9077, F1 Micro: 0.7872, F1 Macro: 0.7818\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9027, F1 Micro: 0.781, F1 Macro: 0.7824\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9061, F1 Micro: 0.7758, F1 Macro: 0.7725\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9028, F1 Micro: 0.7797, F1 Macro: 0.7773\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9041, F1 Micro: 0.7791, F1 Macro: 0.7771\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.8995, F1 Micro: 0.7753, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9062, F1 Micro: 0.7722, F1 Macro: 0.7705\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9077, F1 Micro: 0.7872, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.69      0.65      0.67       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 167.57757210731506 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8978, F1 Micro: 0.7604, F1 Macro: 0.7572\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 41.79199814796448 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.431, Accuracy: 0.8895, F1 Micro: 0.7392, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2967, Accuracy: 0.8978, F1 Micro: 0.7668, F1 Macro: 0.7632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2419, Accuracy: 0.9023, F1 Micro: 0.7805, F1 Macro: 0.777\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9075, F1 Micro: 0.7761, F1 Macro: 0.7707\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9023, F1 Micro: 0.7672, F1 Macro: 0.7596\n",
      "Epoch 6/10, Train Loss: 0.1238, Accuracy: 0.9052, F1 Micro: 0.7657, F1 Macro: 0.7612\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9028, F1 Micro: 0.7746, F1 Macro: 0.7709\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9034, F1 Micro: 0.7745, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.8997, F1 Micro: 0.7738, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7766\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9023, F1 Micro: 0.7805, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 171.9127197265625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4277, Accuracy: 0.8877, F1 Micro: 0.7328, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2995, Accuracy: 0.8978, F1 Micro: 0.7691, F1 Macro: 0.7649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.9036, F1 Micro: 0.7814, F1 Macro: 0.779\n",
      "Epoch 4/10, Train Loss: 0.1989, Accuracy: 0.9056, F1 Micro: 0.7746, F1 Macro: 0.7699\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9031, F1 Micro: 0.7808, F1 Macro: 0.7783\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9042, F1 Micro: 0.7681, F1 Macro: 0.7659\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9045, F1 Micro: 0.779, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9013, F1 Micro: 0.7801, F1 Macro: 0.7789\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9009, F1 Micro: 0.7732, F1 Macro: 0.7724\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9034, F1 Micro: 0.768, F1 Macro: 0.7604\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9036, F1 Micro: 0.7814, F1 Macro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.66      0.71      0.69       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 171.6095175743103 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4355, Accuracy: 0.8831, F1 Micro: 0.7276, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2974, Accuracy: 0.897, F1 Micro: 0.7685, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2431, Accuracy: 0.9055, F1 Micro: 0.7873, F1 Macro: 0.7842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2025, Accuracy: 0.9078, F1 Micro: 0.7902, F1 Macro: 0.788\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9019, F1 Micro: 0.783, F1 Macro: 0.7791\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9091, F1 Micro: 0.7799, F1 Macro: 0.7777\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.907, F1 Micro: 0.7791, F1 Macro: 0.7786\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.902, F1 Micro: 0.7685, F1 Macro: 0.7653\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9052, F1 Micro: 0.7785, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.905, F1 Micro: 0.7743, F1 Macro: 0.7674\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9078, F1 Micro: 0.7902, F1 Macro: 0.788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 174.72839832305908 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8983, F1 Micro: 0.7622, F1 Macro: 0.759\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 37.11891722679138 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4298, Accuracy: 0.8856, F1 Micro: 0.7423, F1 Macro: 0.7373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2932, Accuracy: 0.8978, F1 Micro: 0.7599, F1 Macro: 0.7543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9027, F1 Micro: 0.7687, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.9059, F1 Micro: 0.7839, F1 Macro: 0.7771\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.9067, F1 Micro: 0.7803, F1 Macro: 0.7702\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9011, F1 Micro: 0.7653, F1 Macro: 0.7598\n",
      "Epoch 7/10, Train Loss: 0.0941, Accuracy: 0.9014, F1 Micro: 0.7784, F1 Macro: 0.7773\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.9028, F1 Micro: 0.7821, F1 Macro: 0.7807\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9055, F1 Micro: 0.7723, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9059, F1 Micro: 0.7811, F1 Macro: 0.7768\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9059, F1 Micro: 0.7839, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.79      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 177.19316720962524 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4265, Accuracy: 0.8827, F1 Micro: 0.7417, F1 Macro: 0.7364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2988, Accuracy: 0.8977, F1 Micro: 0.7616, F1 Macro: 0.7562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2527, Accuracy: 0.9034, F1 Micro: 0.7691, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2061, Accuracy: 0.9019, F1 Micro: 0.7816, F1 Macro: 0.7773\n",
      "Epoch 5/10, Train Loss: 0.163, Accuracy: 0.9036, F1 Micro: 0.7662, F1 Macro: 0.7542\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.9005, F1 Micro: 0.7695, F1 Macro: 0.7696\n",
      "Epoch 7/10, Train Loss: 0.0934, Accuracy: 0.9059, F1 Micro: 0.7775, F1 Macro: 0.7767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9053, F1 Micro: 0.7897, F1 Macro: 0.7881\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9041, F1 Micro: 0.7731, F1 Macro: 0.7654\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9052, F1 Micro: 0.7686, F1 Macro: 0.7629\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9053, F1 Micro: 0.7897, F1 Macro: 0.7881\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.65      0.77      0.71       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 179.23216128349304 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4389, Accuracy: 0.8814, F1 Micro: 0.7304, F1 Macro: 0.7251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2943, Accuracy: 0.9003, F1 Micro: 0.7632, F1 Macro: 0.7569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2491, Accuracy: 0.9073, F1 Micro: 0.7838, F1 Macro: 0.7765\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.9058, F1 Micro: 0.7832, F1 Macro: 0.776\n",
      "Epoch 5/10, Train Loss: 0.1553, Accuracy: 0.9062, F1 Micro: 0.777, F1 Macro: 0.7673\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9014, F1 Micro: 0.7739, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9083, F1 Micro: 0.7865, F1 Macro: 0.7853\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.905, F1 Micro: 0.7833, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9038, F1 Micro: 0.7758, F1 Macro: 0.7736\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9041, F1 Micro: 0.766, F1 Macro: 0.7598\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9083, F1 Micro: 0.7865, F1 Macro: 0.7853\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.77      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.79      0.79      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 177.5673758983612 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8989, F1 Micro: 0.764, F1 Macro: 0.7608\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 33.40440034866333 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4177, Accuracy: 0.8802, F1 Micro: 0.7053, F1 Macro: 0.7066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2875, Accuracy: 0.9002, F1 Micro: 0.7491, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2315, Accuracy: 0.9016, F1 Micro: 0.7721, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9047, F1 Micro: 0.7864, F1 Macro: 0.7844\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7716\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.9039, F1 Micro: 0.7628, F1 Macro: 0.7547\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9061, F1 Micro: 0.7747, F1 Macro: 0.7686\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9003, F1 Micro: 0.7749, F1 Macro: 0.7753\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.9022, F1 Micro: 0.782, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9034, F1 Micro: 0.7696, F1 Macro: 0.7638\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9047, F1 Micro: 0.7864, F1 Macro: 0.7844\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 183.3359730243683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4138, Accuracy: 0.8817, F1 Micro: 0.7138, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8969, F1 Micro: 0.7347, F1 Macro: 0.7343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2338, Accuracy: 0.9017, F1 Micro: 0.774, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1883, Accuracy: 0.9047, F1 Micro: 0.7832, F1 Macro: 0.7789\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9036, F1 Micro: 0.7739, F1 Macro: 0.772\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9062, F1 Micro: 0.768, F1 Macro: 0.7614\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9011, F1 Micro: 0.766, F1 Macro: 0.7561\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9019, F1 Micro: 0.7747, F1 Macro: 0.7723\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.8997, F1 Micro: 0.7769, F1 Macro: 0.7763\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9056, F1 Micro: 0.7799, F1 Macro: 0.7757\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9047, F1 Micro: 0.7832, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 183.52818274497986 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4215, Accuracy: 0.8841, F1 Micro: 0.7198, F1 Macro: 0.7148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2881, Accuracy: 0.9002, F1 Micro: 0.7513, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2304, Accuracy: 0.9039, F1 Micro: 0.772, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9045, F1 Micro: 0.7917, F1 Macro: 0.7909\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9061, F1 Micro: 0.7836, F1 Macro: 0.7788\n",
      "Epoch 6/10, Train Loss: 0.1158, Accuracy: 0.9095, F1 Micro: 0.7821, F1 Macro: 0.7769\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9067, F1 Micro: 0.7729, F1 Macro: 0.7675\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9025, F1 Micro: 0.7783, F1 Macro: 0.775\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.903, F1 Micro: 0.7873, F1 Macro: 0.7871\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9052, F1 Micro: 0.7741, F1 Macro: 0.764\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9045, F1 Micro: 0.7917, F1 Macro: 0.7909\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.65      0.77      0.70       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.85      0.76       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 183.88163995742798 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8993, F1 Micro: 0.7655, F1 Macro: 0.7624\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 28.86173915863037 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4109, Accuracy: 0.882, F1 Micro: 0.7031, F1 Macro: 0.7017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2731, Accuracy: 0.9014, F1 Micro: 0.7703, F1 Macro: 0.7669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9056, F1 Micro: 0.7838, F1 Macro: 0.7807\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9016, F1 Micro: 0.7545, F1 Macro: 0.7497\n",
      "Epoch 5/10, Train Loss: 0.1492, Accuracy: 0.9073, F1 Micro: 0.7738, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.9041, F1 Micro: 0.7669, F1 Macro: 0.7612\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.898, F1 Micro: 0.7735, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.898, F1 Micro: 0.7722, F1 Macro: 0.7724\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9028, F1 Micro: 0.7833, F1 Macro: 0.7789\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9009, F1 Micro: 0.7789, F1 Macro: 0.7744\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9056, F1 Micro: 0.7838, F1 Macro: 0.7807\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 189.36614751815796 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4072, Accuracy: 0.887, F1 Micro: 0.7109, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2741, Accuracy: 0.8986, F1 Micro: 0.7544, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2294, Accuracy: 0.9028, F1 Micro: 0.7708, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.905, F1 Micro: 0.7735, F1 Macro: 0.7712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9084, F1 Micro: 0.7757, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.9023, F1 Micro: 0.7775, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9033, F1 Micro: 0.7788, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.902, F1 Micro: 0.7831, F1 Macro: 0.7852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9042, F1 Micro: 0.7865, F1 Macro: 0.7851\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.9036, F1 Micro: 0.7839, F1 Macro: 0.7821\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9042, F1 Micro: 0.7865, F1 Macro: 0.7851\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.66      0.73      0.70       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 199.82157254219055 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4158, Accuracy: 0.8761, F1 Micro: 0.6847, F1 Macro: 0.6802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9023, F1 Micro: 0.7665, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2266, Accuracy: 0.9039, F1 Micro: 0.7809, F1 Macro: 0.7778\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9044, F1 Micro: 0.768, F1 Macro: 0.7625\n",
      "Epoch 5/10, Train Loss: 0.1468, Accuracy: 0.9078, F1 Micro: 0.7769, F1 Macro: 0.7748\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9067, F1 Micro: 0.7753, F1 Macro: 0.7704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.9058, F1 Micro: 0.7811, F1 Macro: 0.7768\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9064, F1 Micro: 0.7807, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9053, F1 Micro: 0.78, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9056, F1 Micro: 0.78, F1 Macro: 0.7738\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9058, F1 Micro: 0.7811, F1 Macro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.67      0.68      0.67       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 191.1700460910797 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8996, F1 Micro: 0.7667, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.908037424087524 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4068, Accuracy: 0.8895, F1 Micro: 0.7317, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.9027, F1 Micro: 0.772, F1 Macro: 0.7673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9036, F1 Micro: 0.7732, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9058, F1 Micro: 0.7855, F1 Macro: 0.7853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.9061, F1 Micro: 0.7885, F1 Macro: 0.7856\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.907, F1 Micro: 0.7774, F1 Macro: 0.7671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9045, F1 Micro: 0.7889, F1 Macro: 0.7874\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9045, F1 Micro: 0.7812, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.9028, F1 Micro: 0.7794, F1 Macro: 0.7769\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9042, F1 Micro: 0.7833, F1 Macro: 0.7811\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9045, F1 Micro: 0.7889, F1 Macro: 0.7874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.94      0.92       370\n",
      "                sara       0.66      0.74      0.69       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 200.71132564544678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4001, Accuracy: 0.8875, F1 Micro: 0.7094, F1 Macro: 0.7015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2756, Accuracy: 0.9008, F1 Micro: 0.7583, F1 Macro: 0.754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2234, Accuracy: 0.9034, F1 Micro: 0.7736, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9052, F1 Micro: 0.7851, F1 Macro: 0.7836\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9031, F1 Micro: 0.782, F1 Macro: 0.7809\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9094, F1 Micro: 0.7743, F1 Macro: 0.7634\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9047, F1 Micro: 0.781, F1 Macro: 0.7767\n",
      "Epoch 8/10, Train Loss: 0.0566, Accuracy: 0.9028, F1 Micro: 0.7741, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0426, Accuracy: 0.9003, F1 Micro: 0.7758, F1 Macro: 0.7712\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9038, F1 Micro: 0.7778, F1 Macro: 0.7756\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9052, F1 Micro: 0.7851, F1 Macro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.72      0.70       248\n",
      "         radikalisme       0.73      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 196.97145986557007 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4082, Accuracy: 0.887, F1 Micro: 0.7285, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9028, F1 Micro: 0.7732, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2221, Accuracy: 0.9033, F1 Micro: 0.774, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9052, F1 Micro: 0.7825, F1 Macro: 0.7814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9075, F1 Micro: 0.7898, F1 Macro: 0.7853\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9073, F1 Micro: 0.7808, F1 Macro: 0.7705\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9052, F1 Micro: 0.7854, F1 Macro: 0.7841\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9055, F1 Micro: 0.7783, F1 Macro: 0.7738\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.902, F1 Micro: 0.7763, F1 Macro: 0.7733\n",
      "Epoch 10/10, Train Loss: 0.0371, Accuracy: 0.9014, F1 Micro: 0.7787, F1 Macro: 0.7745\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9075, F1 Micro: 0.7898, F1 Macro: 0.7853\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.69      0.67      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.76       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.5380618572235 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9, F1 Micro: 0.7679, F1 Macro: 0.7648\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 20.17268133163452 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3908, Accuracy: 0.8888, F1 Micro: 0.7443, F1 Macro: 0.734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.275, Accuracy: 0.8989, F1 Micro: 0.748, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2161, Accuracy: 0.9036, F1 Micro: 0.7651, F1 Macro: 0.7556\n",
      "Epoch 4/10, Train Loss: 0.1731, Accuracy: 0.8969, F1 Micro: 0.7238, F1 Macro: 0.7031\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9033, F1 Micro: 0.7574, F1 Macro: 0.7446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.9033, F1 Micro: 0.7806, F1 Macro: 0.7742\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9048, F1 Micro: 0.777, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.9055, F1 Micro: 0.7817, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9041, F1 Micro: 0.7865, F1 Macro: 0.7848\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.9045, F1 Micro: 0.7729, F1 Macro: 0.769\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9041, F1 Micro: 0.7865, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 206.1813588142395 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8881, F1 Micro: 0.7432, F1 Macro: 0.7353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.898, F1 Micro: 0.7452, F1 Macro: 0.729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2168, Accuracy: 0.9019, F1 Micro: 0.757, F1 Macro: 0.7426\n",
      "Epoch 4/10, Train Loss: 0.1812, Accuracy: 0.8978, F1 Micro: 0.7357, F1 Macro: 0.7202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.905, F1 Micro: 0.7606, F1 Macro: 0.7527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9011, F1 Micro: 0.7738, F1 Macro: 0.7689\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.898, F1 Micro: 0.7452, F1 Macro: 0.736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9069, F1 Micro: 0.7833, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.903, F1 Micro: 0.7769, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9011, F1 Micro: 0.7807, F1 Macro: 0.7788\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9069, F1 Micro: 0.7833, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.70      0.67      0.68       248\n",
      "         radikalisme       0.77      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 205.86140990257263 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3935, Accuracy: 0.8884, F1 Micro: 0.7385, F1 Macro: 0.727\n",
      "Epoch 2/10, Train Loss: 0.2738, Accuracy: 0.895, F1 Micro: 0.7329, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2149, Accuracy: 0.9044, F1 Micro: 0.7664, F1 Macro: 0.7546\n",
      "Epoch 4/10, Train Loss: 0.1763, Accuracy: 0.8986, F1 Micro: 0.7365, F1 Macro: 0.72\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9031, F1 Micro: 0.7502, F1 Macro: 0.7413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.903, F1 Micro: 0.7816, F1 Macro: 0.7796\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.9047, F1 Micro: 0.7744, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9045, F1 Micro: 0.7848, F1 Macro: 0.7857\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.7768\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9045, F1 Micro: 0.7848, F1 Macro: 0.7857\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.75      0.69       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.78      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 204.8433301448822 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9003, F1 Micro: 0.7689, F1 Macro: 0.7659\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.831533670425415 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3843, Accuracy: 0.8927, F1 Micro: 0.7395, F1 Macro: 0.7305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9009, F1 Micro: 0.7761, F1 Macro: 0.7714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9075, F1 Micro: 0.7807, F1 Macro: 0.7779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1754, Accuracy: 0.9061, F1 Micro: 0.7863, F1 Macro: 0.7832\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9038, F1 Micro: 0.7636, F1 Macro: 0.7521\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.908, F1 Micro: 0.7798, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9006, F1 Micro: 0.7776, F1 Macro: 0.7751\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.9072, F1 Micro: 0.7795, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.766\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.9, F1 Micro: 0.7768, F1 Macro: 0.7757\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9061, F1 Micro: 0.7863, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 210.50818395614624 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8902, F1 Micro: 0.7261, F1 Macro: 0.7115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.8981, F1 Micro: 0.7736, F1 Macro: 0.7735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.213, Accuracy: 0.905, F1 Micro: 0.7738, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9058, F1 Micro: 0.7825, F1 Macro: 0.7779\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9053, F1 Micro: 0.7651, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9058, F1 Micro: 0.7767, F1 Macro: 0.7699\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9042, F1 Micro: 0.7811, F1 Macro: 0.7799\n",
      "Epoch 8/10, Train Loss: 0.0551, Accuracy: 0.8998, F1 Micro: 0.7801, F1 Macro: 0.7822\n",
      "Epoch 9/10, Train Loss: 0.0452, Accuracy: 0.9059, F1 Micro: 0.769, F1 Macro: 0.7609\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.905, F1 Micro: 0.7743, F1 Macro: 0.7702\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9058, F1 Micro: 0.7825, F1 Macro: 0.7779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 210.14345526695251 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3921, Accuracy: 0.8889, F1 Micro: 0.7173, F1 Macro: 0.702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2543, Accuracy: 0.8969, F1 Micro: 0.7634, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.212, Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1757, Accuracy: 0.9059, F1 Micro: 0.787, F1 Macro: 0.7841\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.9073, F1 Micro: 0.7772, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.9052, F1 Micro: 0.7664, F1 Macro: 0.7554\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.8998, F1 Micro: 0.779, F1 Macro: 0.7801\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.9055, F1 Micro: 0.7778, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9045, F1 Micro: 0.78, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.9041, F1 Micro: 0.7801, F1 Macro: 0.7778\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9059, F1 Micro: 0.787, F1 Macro: 0.7841\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 209.9280068874359 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9006, F1 Micro: 0.7697, F1 Macro: 0.7667\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.203778505325317 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3684, Accuracy: 0.8863, F1 Micro: 0.7011, F1 Macro: 0.6697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.8995, F1 Micro: 0.7589, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9031, F1 Micro: 0.7771, F1 Macro: 0.7726\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9028, F1 Micro: 0.7656, F1 Macro: 0.7561\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.9044, F1 Micro: 0.7771, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9042, F1 Micro: 0.7815, F1 Macro: 0.7801\n",
      "Epoch 7/10, Train Loss: 0.0667, Accuracy: 0.9027, F1 Micro: 0.7683, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.9059, F1 Micro: 0.7842, F1 Macro: 0.7848\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.9061, F1 Micro: 0.7783, F1 Macro: 0.7718\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9013, F1 Micro: 0.7767, F1 Macro: 0.7782\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9059, F1 Micro: 0.7842, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.76      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 217.1851987838745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3629, Accuracy: 0.8928, F1 Micro: 0.7236, F1 Macro: 0.7072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9008, F1 Micro: 0.761, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2038, Accuracy: 0.9039, F1 Micro: 0.775, F1 Macro: 0.7694\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9028, F1 Micro: 0.7656, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9058, F1 Micro: 0.7777, F1 Macro: 0.7712\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9023, F1 Micro: 0.755, F1 Macro: 0.7466\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.9067, F1 Micro: 0.7767, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.9064, F1 Micro: 0.7854, F1 Macro: 0.7825\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9039, F1 Micro: 0.7779, F1 Macro: 0.7719\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9025, F1 Micro: 0.7744, F1 Macro: 0.7692\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9064, F1 Micro: 0.7854, F1 Macro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 215.91094756126404 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3728, Accuracy: 0.8831, F1 Micro: 0.6868, F1 Macro: 0.6546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.9003, F1 Micro: 0.7623, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9041, F1 Micro: 0.7757, F1 Macro: 0.7698\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.9033, F1 Micro: 0.7686, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9092, F1 Micro: 0.7868, F1 Macro: 0.7818\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9034, F1 Micro: 0.7728, F1 Macro: 0.7717\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.9036, F1 Micro: 0.7751, F1 Macro: 0.7716\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9078, F1 Micro: 0.781, F1 Macro: 0.7753\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9048, F1 Micro: 0.7765, F1 Macro: 0.7728\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9009, F1 Micro: 0.7788, F1 Macro: 0.7774\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9092, F1 Micro: 0.7868, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.70      0.64      0.67       248\n",
      "         radikalisme       0.80      0.82      0.81       243\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 215.79772782325745 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9009, F1 Micro: 0.7705, F1 Macro: 0.7675\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.9320068359375 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.371, Accuracy: 0.8911, F1 Micro: 0.749, F1 Macro: 0.7441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.8958, F1 Micro: 0.7603, F1 Macro: 0.758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2033, Accuracy: 0.9028, F1 Micro: 0.7761, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1637, Accuracy: 0.9041, F1 Micro: 0.7779, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9038, F1 Micro: 0.7819, F1 Macro: 0.7785\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.9039, F1 Micro: 0.7664, F1 Macro: 0.7595\n",
      "Epoch 7/10, Train Loss: 0.0623, Accuracy: 0.9028, F1 Micro: 0.7722, F1 Macro: 0.764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.7771\n",
      "Epoch 9/10, Train Loss: 0.0333, Accuracy: 0.9034, F1 Micro: 0.7785, F1 Macro: 0.7769\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.9017, F1 Micro: 0.7688, F1 Macro: 0.7648\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 223.65770077705383 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3608, Accuracy: 0.8903, F1 Micro: 0.7469, F1 Macro: 0.7443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2387, Accuracy: 0.8969, F1 Micro: 0.7577, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9006, F1 Micro: 0.7712, F1 Macro: 0.7678\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9003, F1 Micro: 0.7668, F1 Macro: 0.7621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9022, F1 Micro: 0.7819, F1 Macro: 0.783\n",
      "Epoch 6/10, Train Loss: 0.0916, Accuracy: 0.9042, F1 Micro: 0.772, F1 Macro: 0.765\n",
      "Epoch 7/10, Train Loss: 0.0613, Accuracy: 0.9061, F1 Micro: 0.7796, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0455, Accuracy: 0.9031, F1 Micro: 0.7771, F1 Macro: 0.7739\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9056, F1 Micro: 0.7695, F1 Macro: 0.7633\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.9038, F1 Micro: 0.7763, F1 Macro: 0.7713\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9022, F1 Micro: 0.7819, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.74      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 220.03633379936218 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3731, Accuracy: 0.8838, F1 Micro: 0.7362, F1 Macro: 0.7291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2396, Accuracy: 0.8981, F1 Micro: 0.7648, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9017, F1 Micro: 0.7698, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1605, Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9019, F1 Micro: 0.7825, F1 Macro: 0.7835\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9062, F1 Micro: 0.7734, F1 Macro: 0.7698\n",
      "Epoch 7/10, Train Loss: 0.0635, Accuracy: 0.9045, F1 Micro: 0.7817, F1 Macro: 0.7775\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.9031, F1 Micro: 0.7653, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9025, F1 Micro: 0.7779, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.9038, F1 Micro: 0.7705, F1 Macro: 0.7684\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9019, F1 Micro: 0.7825, F1 Macro: 0.7835\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.64      0.78      0.71       248\n",
      "         radikalisme       0.71      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 222.64699029922485 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.901, F1 Micro: 0.7711, F1 Macro: 0.7682\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.288259983062744 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3635, Accuracy: 0.8931, F1 Micro: 0.7552, F1 Macro: 0.7502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2325, Accuracy: 0.8986, F1 Micro: 0.7744, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1961, Accuracy: 0.9019, F1 Micro: 0.7806, F1 Macro: 0.7725\n",
      "Epoch 4/10, Train Loss: 0.1531, Accuracy: 0.9066, F1 Micro: 0.7793, F1 Macro: 0.7724\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9039, F1 Micro: 0.7715, F1 Macro: 0.7635\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.9045, F1 Micro: 0.766, F1 Macro: 0.7564\n",
      "Epoch 7/10, Train Loss: 0.0641, Accuracy: 0.9034, F1 Micro: 0.7723, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0487, Accuracy: 0.8958, F1 Micro: 0.7737, F1 Macro: 0.7727\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9048, F1 Micro: 0.773, F1 Macro: 0.7677\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9061, F1 Micro: 0.7733, F1 Macro: 0.7691\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9019, F1 Micro: 0.7806, F1 Macro: 0.7725\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.94      0.89       370\n",
      "                sara       0.68      0.63      0.66       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 225.25726461410522 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3539, Accuracy: 0.8927, F1 Micro: 0.7558, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2318, Accuracy: 0.9013, F1 Micro: 0.7781, F1 Macro: 0.7736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.902, F1 Micro: 0.7874, F1 Macro: 0.7856\n",
      "Epoch 4/10, Train Loss: 0.1481, Accuracy: 0.905, F1 Micro: 0.7822, F1 Macro: 0.7792\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.9069, F1 Micro: 0.7713, F1 Macro: 0.7611\n",
      "Epoch 6/10, Train Loss: 0.0852, Accuracy: 0.9038, F1 Micro: 0.7636, F1 Macro: 0.7529\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.9058, F1 Micro: 0.7712, F1 Macro: 0.7649\n",
      "Epoch 8/10, Train Loss: 0.0464, Accuracy: 0.9039, F1 Micro: 0.7829, F1 Macro: 0.7817\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.9048, F1 Micro: 0.7718, F1 Macro: 0.7678\n",
      "Epoch 10/10, Train Loss: 0.0276, Accuracy: 0.9036, F1 Micro: 0.7789, F1 Macro: 0.7771\n",
      "Model 2 - Iteration 6218: Accuracy: 0.902, F1 Micro: 0.7874, F1 Macro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.89       370\n",
      "                sara       0.67      0.75      0.71       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.79      1365\n",
      "           macro avg       0.73      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 226.24813628196716 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3629, Accuracy: 0.8888, F1 Micro: 0.7495, F1 Macro: 0.7456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2308, Accuracy: 0.8998, F1 Micro: 0.7747, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.9058, F1 Micro: 0.7875, F1 Macro: 0.7823\n",
      "Epoch 4/10, Train Loss: 0.1519, Accuracy: 0.9059, F1 Micro: 0.7701, F1 Macro: 0.761\n",
      "Epoch 5/10, Train Loss: 0.1137, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.7686\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9092, F1 Micro: 0.786, F1 Macro: 0.7817\n",
      "Epoch 7/10, Train Loss: 0.0623, Accuracy: 0.9055, F1 Micro: 0.7845, F1 Macro: 0.7836\n",
      "Epoch 8/10, Train Loss: 0.0474, Accuracy: 0.9022, F1 Micro: 0.7742, F1 Macro: 0.7714\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9058, F1 Micro: 0.7758, F1 Macro: 0.7713\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9048, F1 Micro: 0.7696, F1 Macro: 0.7651\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9058, F1 Micro: 0.7875, F1 Macro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 224.68383646011353 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9011, F1 Micro: 0.7717, F1 Macro: 0.7687\n",
      "Total sampling time: 1071.1 seconds\n",
      "Total runtime: 11909.610866069794 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3e304",
   "metadata": {
    "papermill": {
     "duration": 0.25388,
     "end_time": "2025-01-28T19:51:38.360774",
     "exception": false,
     "start_time": "2025-01-28T19:51:38.106894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc73953c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:51:38.956120Z",
     "iopub.status.busy": "2025-01-28T19:51:38.955759Z",
     "iopub.status.idle": "2025-01-28T23:10:17.344378Z",
     "shell.execute_reply": "2025-01-28T23:10:17.343381Z"
    },
    "papermill": {
     "duration": 11918.64579,
     "end_time": "2025-01-28T23:10:17.345773",
     "exception": false,
     "start_time": "2025-01-28T19:51:38.699983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6311, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.461, Accuracy: 0.7878, F1 Micro: 0.0102, F1 Macro: 0.0093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4762, Accuracy: 0.8163, F1 Micro: 0.2452, F1 Macro: 0.1696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3711, Accuracy: 0.8416, F1 Micro: 0.5034, F1 Macro: 0.4652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3263, Accuracy: 0.8502, F1 Micro: 0.5686, F1 Macro: 0.5659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.296, Accuracy: 0.8605, F1 Micro: 0.6162, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2432, Accuracy: 0.8637, F1 Micro: 0.6529, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1889, Accuracy: 0.867, F1 Micro: 0.664, F1 Macro: 0.6618\n",
      "Epoch 9/10, Train Loss: 0.1726, Accuracy: 0.8648, F1 Micro: 0.6137, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1499, Accuracy: 0.8716, F1 Micro: 0.6889, F1 Macro: 0.6888\n",
      "Model 1 - Iteration 388: Accuracy: 0.8716, F1 Micro: 0.6889, F1 Macro: 0.6888\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.76      0.83       370\n",
      "                sara       0.58      0.65      0.61       248\n",
      "         radikalisme       0.64      0.71      0.67       243\n",
      "pencemaran_nama_baik       0.69      0.59      0.64       504\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1365\n",
      "           macro avg       0.71      0.68      0.69      1365\n",
      "        weighted avg       0.73      0.67      0.69      1365\n",
      "         samples avg       0.35      0.36      0.34      1365\n",
      "\n",
      "Training completed in 58.107372999191284 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6458, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4782, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4997, Accuracy: 0.8066, F1 Micro: 0.1702, F1 Macro: 0.128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4081, Accuracy: 0.8309, F1 Micro: 0.3702, F1 Macro: 0.2912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3573, Accuracy: 0.8436, F1 Micro: 0.4967, F1 Macro: 0.4694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3293, Accuracy: 0.8577, F1 Micro: 0.6068, F1 Macro: 0.5983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2642, Accuracy: 0.8611, F1 Micro: 0.6176, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2083, Accuracy: 0.865, F1 Micro: 0.6685, F1 Macro: 0.6627\n",
      "Epoch 9/10, Train Loss: 0.1909, Accuracy: 0.8631, F1 Micro: 0.6131, F1 Macro: 0.6064\n",
      "Epoch 10/10, Train Loss: 0.1612, Accuracy: 0.8672, F1 Micro: 0.6314, F1 Macro: 0.627\n",
      "Model 2 - Iteration 388: Accuracy: 0.865, F1 Micro: 0.6685, F1 Macro: 0.6627\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.79      0.83       370\n",
      "                sara       0.56      0.56      0.56       248\n",
      "         radikalisme       0.64      0.66      0.65       243\n",
      "pencemaran_nama_baik       0.69      0.56      0.61       504\n",
      "\n",
      "           micro avg       0.70      0.64      0.67      1365\n",
      "           macro avg       0.69      0.64      0.66      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.34      0.35      0.33      1365\n",
      "\n",
      "Training completed in 55.832085609436035 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5999, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4533, Accuracy: 0.7878, F1 Micro: 0.0102, F1 Macro: 0.0093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4549, Accuracy: 0.8241, F1 Micro: 0.3159, F1 Macro: 0.2124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.366, Accuracy: 0.8339, F1 Micro: 0.4661, F1 Macro: 0.4318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3339, Accuracy: 0.8428, F1 Micro: 0.4836, F1 Macro: 0.454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3115, Accuracy: 0.8558, F1 Micro: 0.6071, F1 Macro: 0.6058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2574, Accuracy: 0.8616, F1 Micro: 0.6336, F1 Macro: 0.6331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2035, Accuracy: 0.8645, F1 Micro: 0.6436, F1 Macro: 0.6384\n",
      "Epoch 9/10, Train Loss: 0.1904, Accuracy: 0.865, F1 Micro: 0.623, F1 Macro: 0.6111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1592, Accuracy: 0.8681, F1 Micro: 0.682, F1 Macro: 0.679\n",
      "Model 3 - Iteration 388: Accuracy: 0.8681, F1 Micro: 0.682, F1 Macro: 0.679\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.79      0.85       370\n",
      "                sara       0.58      0.62      0.60       248\n",
      "         radikalisme       0.63      0.64      0.64       243\n",
      "pencemaran_nama_baik       0.66      0.60      0.63       504\n",
      "\n",
      "           micro avg       0.70      0.66      0.68      1365\n",
      "           macro avg       0.70      0.66      0.68      1365\n",
      "        weighted avg       0.71      0.66      0.68      1365\n",
      "         samples avg       0.35      0.36      0.34      1365\n",
      "\n",
      "Training completed in 58.937403440475464 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8682, F1 Micro: 0.6798, F1 Macro: 0.6768\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 120.85472846031189 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5937, Accuracy: 0.7894, F1 Micro: 0.0246, F1 Macro: 0.0215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4733, Accuracy: 0.8487, F1 Micro: 0.5328, F1 Macro: 0.4934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3931, Accuracy: 0.8637, F1 Micro: 0.6455, F1 Macro: 0.6399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3312, Accuracy: 0.8752, F1 Micro: 0.7184, F1 Macro: 0.7167\n",
      "Epoch 5/10, Train Loss: 0.2728, Accuracy: 0.8842, F1 Micro: 0.6977, F1 Macro: 0.688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2286, Accuracy: 0.89, F1 Micro: 0.7265, F1 Macro: 0.7174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1687, Accuracy: 0.8941, F1 Micro: 0.7426, F1 Macro: 0.7379\n",
      "Epoch 8/10, Train Loss: 0.151, Accuracy: 0.8906, F1 Micro: 0.7131, F1 Macro: 0.7085\n",
      "Epoch 9/10, Train Loss: 0.111, Accuracy: 0.8881, F1 Micro: 0.7131, F1 Macro: 0.709\n",
      "Epoch 10/10, Train Loss: 0.0899, Accuracy: 0.8914, F1 Micro: 0.7313, F1 Macro: 0.7253\n",
      "Model 1 - Iteration 971: Accuracy: 0.8941, F1 Micro: 0.7426, F1 Macro: 0.7379\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.84      0.87       370\n",
      "                sara       0.69      0.57      0.63       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.73      0.66      0.69       504\n",
      "\n",
      "           micro avg       0.77      0.72      0.74      1365\n",
      "           macro avg       0.76      0.72      0.74      1365\n",
      "        weighted avg       0.77      0.72      0.74      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 70.45198702812195 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.61, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5002, Accuracy: 0.8322, F1 Micro: 0.4073, F1 Macro: 0.3445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.421, Accuracy: 0.8514, F1 Micro: 0.5675, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3632, Accuracy: 0.8741, F1 Micro: 0.7024, F1 Macro: 0.6992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2967, Accuracy: 0.8805, F1 Micro: 0.7125, F1 Macro: 0.7077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2486, Accuracy: 0.8847, F1 Micro: 0.7207, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1833, Accuracy: 0.8902, F1 Micro: 0.7244, F1 Macro: 0.7187\n",
      "Epoch 8/10, Train Loss: 0.1558, Accuracy: 0.8873, F1 Micro: 0.7022, F1 Macro: 0.6924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1196, Accuracy: 0.8888, F1 Micro: 0.7285, F1 Macro: 0.7279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0948, Accuracy: 0.8867, F1 Micro: 0.7312, F1 Macro: 0.7312\n",
      "Model 2 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7312, F1 Macro: 0.7312\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.84      0.87       370\n",
      "                sara       0.64      0.62      0.63       248\n",
      "         radikalisme       0.73      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.68      0.66      0.67       504\n",
      "\n",
      "           micro avg       0.74      0.72      0.73      1365\n",
      "           macro avg       0.74      0.72      0.73      1365\n",
      "        weighted avg       0.74      0.72      0.73      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 76.1288595199585 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5739, Accuracy: 0.7992, F1 Micro: 0.1217, F1 Macro: 0.0984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.464, Accuracy: 0.8431, F1 Micro: 0.5049, F1 Macro: 0.4727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3872, Accuracy: 0.8656, F1 Micro: 0.6614, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3329, Accuracy: 0.8753, F1 Micro: 0.7062, F1 Macro: 0.7019\n",
      "Epoch 5/10, Train Loss: 0.2677, Accuracy: 0.8805, F1 Micro: 0.6929, F1 Macro: 0.675\n",
      "Epoch 6/10, Train Loss: 0.2253, Accuracy: 0.8797, F1 Micro: 0.6883, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1763, Accuracy: 0.8889, F1 Micro: 0.72, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1378, Accuracy: 0.8884, F1 Micro: 0.7256, F1 Macro: 0.7218\n",
      "Epoch 9/10, Train Loss: 0.1064, Accuracy: 0.8886, F1 Micro: 0.7218, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0889, Accuracy: 0.8897, F1 Micro: 0.7261, F1 Macro: 0.7207\n",
      "Model 3 - Iteration 971: Accuracy: 0.8897, F1 Micro: 0.7261, F1 Macro: 0.7207\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       370\n",
      "                sara       0.68      0.54      0.60       248\n",
      "         radikalisme       0.76      0.73      0.75       243\n",
      "pencemaran_nama_baik       0.71      0.62      0.66       504\n",
      "\n",
      "           micro avg       0.77      0.69      0.73      1365\n",
      "           macro avg       0.76      0.68      0.72      1365\n",
      "        weighted avg       0.77      0.69      0.72      1365\n",
      "         samples avg       0.39      0.38      0.38      1365\n",
      "\n",
      "Training completed in 71.90754961967468 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8792, F1 Micro: 0.7065, F1 Macro: 0.7034\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 109.34347128868103 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5762, Accuracy: 0.8322, F1 Micro: 0.3932, F1 Macro: 0.3282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4189, Accuracy: 0.8586, F1 Micro: 0.5851, F1 Macro: 0.5648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3479, Accuracy: 0.8827, F1 Micro: 0.7081, F1 Macro: 0.6957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2987, Accuracy: 0.8883, F1 Micro: 0.7442, F1 Macro: 0.7442\n",
      "Epoch 5/10, Train Loss: 0.2403, Accuracy: 0.893, F1 Micro: 0.7263, F1 Macro: 0.716\n",
      "Epoch 6/10, Train Loss: 0.1992, Accuracy: 0.895, F1 Micro: 0.7407, F1 Macro: 0.7372\n",
      "Epoch 7/10, Train Loss: 0.1437, Accuracy: 0.893, F1 Micro: 0.7237, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1167, Accuracy: 0.8931, F1 Micro: 0.75, F1 Macro: 0.7464\n",
      "Epoch 9/10, Train Loss: 0.0885, Accuracy: 0.8952, F1 Micro: 0.7434, F1 Macro: 0.7376\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.8959, F1 Micro: 0.7498, F1 Macro: 0.7406\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8931, F1 Micro: 0.75, F1 Macro: 0.7464\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.64      0.64      0.64       248\n",
      "         radikalisme       0.71      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.69      0.70       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.74      0.75      0.75      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 83.63512921333313 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5912, Accuracy: 0.8133, F1 Micro: 0.2975, F1 Macro: 0.2116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4467, Accuracy: 0.8617, F1 Micro: 0.6026, F1 Macro: 0.5661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3677, Accuracy: 0.8763, F1 Micro: 0.6806, F1 Macro: 0.6637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3074, Accuracy: 0.8902, F1 Micro: 0.7433, F1 Macro: 0.7429\n",
      "Epoch 5/10, Train Loss: 0.2486, Accuracy: 0.8872, F1 Micro: 0.6982, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1966, Accuracy: 0.8928, F1 Micro: 0.7482, F1 Macro: 0.746\n",
      "Epoch 7/10, Train Loss: 0.1488, Accuracy: 0.8938, F1 Micro: 0.7391, F1 Macro: 0.7314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.8961, F1 Micro: 0.7512, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.8955, F1 Micro: 0.7523, F1 Macro: 0.7493\n",
      "Epoch 10/10, Train Loss: 0.084, Accuracy: 0.8942, F1 Micro: 0.7278, F1 Macro: 0.7097\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8955, F1 Micro: 0.7523, F1 Macro: 0.7493\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       370\n",
      "                sara       0.65      0.64      0.64       248\n",
      "         radikalisme       0.75      0.77      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.69      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.74      0.75      1365\n",
      "           macro avg       0.76      0.74      0.75      1365\n",
      "        weighted avg       0.76      0.74      0.75      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 87.07362818717957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.833, F1 Micro: 0.4335, F1 Macro: 0.3787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4238, Accuracy: 0.8581, F1 Micro: 0.6232, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3462, Accuracy: 0.8802, F1 Micro: 0.7102, F1 Macro: 0.7068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2961, Accuracy: 0.8856, F1 Micro: 0.7367, F1 Macro: 0.7369\n",
      "Epoch 5/10, Train Loss: 0.24, Accuracy: 0.8897, F1 Micro: 0.7229, F1 Macro: 0.7145\n",
      "Epoch 6/10, Train Loss: 0.1941, Accuracy: 0.8916, F1 Micro: 0.7244, F1 Macro: 0.7174\n",
      "Epoch 7/10, Train Loss: 0.1493, Accuracy: 0.8895, F1 Micro: 0.7207, F1 Macro: 0.708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1209, Accuracy: 0.8916, F1 Micro: 0.7449, F1 Macro: 0.7402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.8928, F1 Micro: 0.745, F1 Macro: 0.742\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.8917, F1 Micro: 0.729, F1 Macro: 0.7188\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8928, F1 Micro: 0.745, F1 Macro: 0.742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.65      0.62      0.64       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.68      0.69       504\n",
      "\n",
      "           micro avg       0.76      0.73      0.74      1365\n",
      "           macro avg       0.75      0.73      0.74      1365\n",
      "        weighted avg       0.76      0.73      0.75      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 86.07847332954407 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8841, F1 Micro: 0.7207, F1 Macro: 0.7176\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 98.20993661880493 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5352, Accuracy: 0.8522, F1 Micro: 0.6015, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3863, Accuracy: 0.8817, F1 Micro: 0.7193, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.333, Accuracy: 0.89, F1 Micro: 0.7231, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2766, Accuracy: 0.8922, F1 Micro: 0.7298, F1 Macro: 0.7312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2283, Accuracy: 0.8963, F1 Micro: 0.7359, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1939, Accuracy: 0.9009, F1 Micro: 0.7556, F1 Macro: 0.7522\n",
      "Epoch 7/10, Train Loss: 0.1402, Accuracy: 0.8995, F1 Micro: 0.7501, F1 Macro: 0.7475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9002, F1 Micro: 0.7645, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9002, F1 Micro: 0.7549, F1 Macro: 0.7438\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.8983, F1 Micro: 0.7541, F1 Macro: 0.7525\n",
      "Model 1 - Iteration 1969: Accuracy: 0.9002, F1 Micro: 0.7645, F1 Macro: 0.7613\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.78      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1365\n",
      "           macro avg       0.77      0.76      0.76      1365\n",
      "        weighted avg       0.77      0.76      0.77      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 100.80288648605347 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5579, Accuracy: 0.8394, F1 Micro: 0.5519, F1 Macro: 0.4646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4156, Accuracy: 0.8789, F1 Micro: 0.7189, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3473, Accuracy: 0.8872, F1 Micro: 0.7191, F1 Macro: 0.7214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2879, Accuracy: 0.8903, F1 Micro: 0.729, F1 Macro: 0.7322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2386, Accuracy: 0.8973, F1 Micro: 0.7371, F1 Macro: 0.7248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1968, Accuracy: 0.897, F1 Micro: 0.7553, F1 Macro: 0.7572\n",
      "Epoch 7/10, Train Loss: 0.1371, Accuracy: 0.8977, F1 Micro: 0.7424, F1 Macro: 0.7415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.9009, F1 Micro: 0.756, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.8995, F1 Micro: 0.7571, F1 Macro: 0.7489\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.8973, F1 Micro: 0.7468, F1 Macro: 0.7408\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8995, F1 Micro: 0.7571, F1 Macro: 0.7489\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.69      0.55      0.61       248\n",
      "         radikalisme       0.78      0.76      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.73      0.76      1365\n",
      "           macro avg       0.78      0.72      0.75      1365\n",
      "        weighted avg       0.78      0.73      0.76      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 103.75867795944214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5235, Accuracy: 0.8556, F1 Micro: 0.6156, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3899, Accuracy: 0.8773, F1 Micro: 0.694, F1 Macro: 0.6891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3346, Accuracy: 0.8858, F1 Micro: 0.6976, F1 Macro: 0.6958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2746, Accuracy: 0.8908, F1 Micro: 0.7331, F1 Macro: 0.728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2333, Accuracy: 0.8966, F1 Micro: 0.7462, F1 Macro: 0.7297\n",
      "Epoch 6/10, Train Loss: 0.1898, Accuracy: 0.8966, F1 Micro: 0.7422, F1 Macro: 0.7351\n",
      "Epoch 7/10, Train Loss: 0.1379, Accuracy: 0.8939, F1 Micro: 0.7296, F1 Macro: 0.7209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.8955, F1 Micro: 0.7538, F1 Macro: 0.7512\n",
      "Epoch 9/10, Train Loss: 0.0811, Accuracy: 0.8961, F1 Micro: 0.7486, F1 Macro: 0.7349\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.8966, F1 Micro: 0.7477, F1 Macro: 0.7471\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8955, F1 Micro: 0.7538, F1 Macro: 0.7512\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.64      0.65      0.64       248\n",
      "         radikalisme       0.75      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.75      0.75      1365\n",
      "           macro avg       0.75      0.75      0.75      1365\n",
      "        weighted avg       0.76      0.75      0.76      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 99.6006109714508 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8876, F1 Micro: 0.7302, F1 Macro: 0.7266\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 88.48330736160278 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.524, Accuracy: 0.8708, F1 Micro: 0.6899, F1 Macro: 0.6798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.887, F1 Micro: 0.717, F1 Macro: 0.7183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3054, Accuracy: 0.8956, F1 Micro: 0.7585, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2457, Accuracy: 0.8997, F1 Micro: 0.7694, F1 Macro: 0.7669\n",
      "Epoch 5/10, Train Loss: 0.2016, Accuracy: 0.9017, F1 Micro: 0.7627, F1 Macro: 0.755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.9028, F1 Micro: 0.7706, F1 Macro: 0.7652\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9006, F1 Micro: 0.751, F1 Macro: 0.7456\n",
      "Epoch 8/10, Train Loss: 0.0916, Accuracy: 0.9009, F1 Micro: 0.7661, F1 Macro: 0.7596\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9034, F1 Micro: 0.7641, F1 Macro: 0.7574\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.902, F1 Micro: 0.7679, F1 Macro: 0.765\n",
      "Model 1 - Iteration 2394: Accuracy: 0.9028, F1 Micro: 0.7706, F1 Macro: 0.7652\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.69      0.60      0.64       248\n",
      "         radikalisme       0.78      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.78      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 111.73827528953552 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.546, Accuracy: 0.8547, F1 Micro: 0.6244, F1 Macro: 0.5909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3945, Accuracy: 0.8853, F1 Micro: 0.6964, F1 Macro: 0.6917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3121, Accuracy: 0.8958, F1 Micro: 0.7534, F1 Macro: 0.7514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2566, Accuracy: 0.8964, F1 Micro: 0.7691, F1 Macro: 0.7685\n",
      "Epoch 5/10, Train Loss: 0.2133, Accuracy: 0.9019, F1 Micro: 0.7681, F1 Macro: 0.7636\n",
      "Epoch 6/10, Train Loss: 0.162, Accuracy: 0.9016, F1 Micro: 0.7672, F1 Macro: 0.7649\n",
      "Epoch 7/10, Train Loss: 0.1277, Accuracy: 0.9, F1 Micro: 0.7557, F1 Macro: 0.7529\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.898, F1 Micro: 0.7665, F1 Macro: 0.7663\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.8992, F1 Micro: 0.7666, F1 Macro: 0.7635\n",
      "Epoch 10/10, Train Loss: 0.0599, Accuracy: 0.9047, F1 Micro: 0.7621, F1 Macro: 0.759\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8964, F1 Micro: 0.7691, F1 Macro: 0.7685\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.71      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.81      0.77      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 109.18096613883972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.8639, F1 Micro: 0.6384, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3836, Accuracy: 0.8875, F1 Micro: 0.715, F1 Macro: 0.7108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3052, Accuracy: 0.8952, F1 Micro: 0.74, F1 Macro: 0.7333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2484, Accuracy: 0.8983, F1 Micro: 0.7604, F1 Macro: 0.7569\n",
      "Epoch 5/10, Train Loss: 0.2007, Accuracy: 0.897, F1 Micro: 0.7425, F1 Macro: 0.7297\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.8977, F1 Micro: 0.7555, F1 Macro: 0.7516\n",
      "Epoch 7/10, Train Loss: 0.1201, Accuracy: 0.8998, F1 Micro: 0.7578, F1 Macro: 0.7529\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9, F1 Micro: 0.757, F1 Macro: 0.7464\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.8981, F1 Micro: 0.7549, F1 Macro: 0.7438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9017, F1 Micro: 0.7647, F1 Macro: 0.7606\n",
      "Model 3 - Iteration 2394: Accuracy: 0.9017, F1 Micro: 0.7647, F1 Macro: 0.7606\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.60      0.64       248\n",
      "         radikalisme       0.78      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.78      0.75      0.76      1365\n",
      "           macro avg       0.78      0.74      0.76      1365\n",
      "        weighted avg       0.78      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.41      1365\n",
      "\n",
      "Training completed in 111.5894410610199 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8902, F1 Micro: 0.7378, F1 Macro: 0.7343\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 80.09355401992798 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.505, Accuracy: 0.8519, F1 Micro: 0.5241, F1 Macro: 0.486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3712, Accuracy: 0.8908, F1 Micro: 0.742, F1 Macro: 0.7334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3031, Accuracy: 0.8988, F1 Micro: 0.7596, F1 Macro: 0.757\n",
      "Epoch 4/10, Train Loss: 0.2437, Accuracy: 0.902, F1 Micro: 0.7584, F1 Macro: 0.7516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1988, Accuracy: 0.9016, F1 Micro: 0.7692, F1 Macro: 0.7686\n",
      "Epoch 6/10, Train Loss: 0.1613, Accuracy: 0.9009, F1 Micro: 0.7539, F1 Macro: 0.7544\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9, F1 Micro: 0.7683, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9013, F1 Micro: 0.7713, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9011, F1 Micro: 0.7725, F1 Macro: 0.7717\n",
      "Epoch 10/10, Train Loss: 0.0577, Accuracy: 0.9041, F1 Micro: 0.7685, F1 Macro: 0.7651\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9011, F1 Micro: 0.7725, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 124.8502037525177 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.8422, F1 Micro: 0.4985, F1 Macro: 0.4211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3838, Accuracy: 0.8898, F1 Micro: 0.7353, F1 Macro: 0.7231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3133, Accuracy: 0.8992, F1 Micro: 0.7643, F1 Macro: 0.7622\n",
      "Epoch 4/10, Train Loss: 0.2487, Accuracy: 0.9019, F1 Micro: 0.7518, F1 Macro: 0.7446\n",
      "Epoch 5/10, Train Loss: 0.2069, Accuracy: 0.8984, F1 Micro: 0.7603, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1593, Accuracy: 0.9039, F1 Micro: 0.7677, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9023, F1 Micro: 0.771, F1 Macro: 0.7653\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9019, F1 Micro: 0.7619, F1 Macro: 0.7553\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9003, F1 Micro: 0.767, F1 Macro: 0.7633\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9008, F1 Micro: 0.7635, F1 Macro: 0.7593\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9023, F1 Micro: 0.771, F1 Macro: 0.7653\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.78      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 122.9875476360321 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5031, Accuracy: 0.8466, F1 Micro: 0.4901, F1 Macro: 0.4307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3792, Accuracy: 0.8886, F1 Micro: 0.7298, F1 Macro: 0.7158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3075, Accuracy: 0.8958, F1 Micro: 0.7516, F1 Macro: 0.7494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2445, Accuracy: 0.9008, F1 Micro: 0.7579, F1 Macro: 0.7503\n",
      "Epoch 5/10, Train Loss: 0.1966, Accuracy: 0.8975, F1 Micro: 0.7539, F1 Macro: 0.7528\n",
      "Epoch 6/10, Train Loss: 0.1563, Accuracy: 0.898, F1 Micro: 0.745, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9031, F1 Micro: 0.777, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9028, F1 Micro: 0.7656, F1 Macro: 0.7566\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9, F1 Micro: 0.7681, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9011, F1 Micro: 0.7625, F1 Macro: 0.7568\n",
      "Model 3 - Iteration 2777: Accuracy: 0.9031, F1 Micro: 0.777, F1 Macro: 0.7756\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.78      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 122.73263120651245 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8922, F1 Micro: 0.7437, F1 Macro: 0.7404\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 72.14444971084595 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.499, Accuracy: 0.8662, F1 Micro: 0.6403, F1 Macro: 0.638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3421, Accuracy: 0.8906, F1 Micro: 0.731, F1 Macro: 0.7159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2937, Accuracy: 0.9028, F1 Micro: 0.7746, F1 Macro: 0.7735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2402, Accuracy: 0.9053, F1 Micro: 0.7783, F1 Macro: 0.7727\n",
      "Epoch 5/10, Train Loss: 0.2035, Accuracy: 0.9031, F1 Micro: 0.758, F1 Macro: 0.7495\n",
      "Epoch 6/10, Train Loss: 0.1472, Accuracy: 0.9023, F1 Micro: 0.7715, F1 Macro: 0.7714\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9031, F1 Micro: 0.7612, F1 Macro: 0.7596\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9016, F1 Micro: 0.774, F1 Macro: 0.7726\n",
      "Epoch 9/10, Train Loss: 0.0662, Accuracy: 0.9019, F1 Micro: 0.7728, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9039, F1 Micro: 0.7652, F1 Macro: 0.7608\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9053, F1 Micro: 0.7783, F1 Macro: 0.7727\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 130.96615862846375 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5237, Accuracy: 0.8611, F1 Micro: 0.6026, F1 Macro: 0.5708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3536, Accuracy: 0.8931, F1 Micro: 0.7339, F1 Macro: 0.7175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2986, Accuracy: 0.9031, F1 Micro: 0.7757, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2443, Accuracy: 0.9038, F1 Micro: 0.7766, F1 Macro: 0.7757\n",
      "Epoch 5/10, Train Loss: 0.205, Accuracy: 0.9028, F1 Micro: 0.7628, F1 Macro: 0.7552\n",
      "Epoch 6/10, Train Loss: 0.1425, Accuracy: 0.897, F1 Micro: 0.7747, F1 Macro: 0.7764\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.8997, F1 Micro: 0.7538, F1 Macro: 0.7496\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9016, F1 Micro: 0.7579, F1 Macro: 0.7485\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.8998, F1 Micro: 0.7656, F1 Macro: 0.7649\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9, F1 Micro: 0.7663, F1 Macro: 0.7649\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9038, F1 Micro: 0.7766, F1 Macro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 130.8911612033844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4966, Accuracy: 0.8675, F1 Micro: 0.6287, F1 Macro: 0.6252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3433, Accuracy: 0.8917, F1 Micro: 0.7362, F1 Macro: 0.7229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2936, Accuracy: 0.8994, F1 Micro: 0.7663, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2412, Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.1961, Accuracy: 0.9003, F1 Micro: 0.7692, F1 Macro: 0.7622\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.8973, F1 Micro: 0.7622, F1 Macro: 0.7598\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9008, F1 Micro: 0.7566, F1 Macro: 0.7529\n",
      "Epoch 8/10, Train Loss: 0.0884, Accuracy: 0.8991, F1 Micro: 0.7759, F1 Macro: 0.7766\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.9009, F1 Micro: 0.7609, F1 Macro: 0.7549\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.8984, F1 Micro: 0.7635, F1 Macro: 0.7565\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.66      0.69      0.68       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.77      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 130.70703673362732 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8939, F1 Micro: 0.7485, F1 Macro: 0.7452\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 65.56703662872314 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4802, Accuracy: 0.8675, F1 Micro: 0.6578, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3312, Accuracy: 0.8933, F1 Micro: 0.7679, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.272, Accuracy: 0.9036, F1 Micro: 0.7687, F1 Macro: 0.7611\n",
      "Epoch 4/10, Train Loss: 0.2284, Accuracy: 0.9031, F1 Micro: 0.752, F1 Macro: 0.7338\n",
      "Epoch 5/10, Train Loss: 0.1915, Accuracy: 0.8995, F1 Micro: 0.7398, F1 Macro: 0.729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1417, Accuracy: 0.9019, F1 Micro: 0.7698, F1 Macro: 0.7654\n",
      "Epoch 7/10, Train Loss: 0.1144, Accuracy: 0.9033, F1 Micro: 0.7622, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9086, F1 Micro: 0.7803, F1 Macro: 0.7804\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.903, F1 Micro: 0.7682, F1 Macro: 0.7634\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9053, F1 Micro: 0.768, F1 Macro: 0.7612\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9086, F1 Micro: 0.7803, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.80      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.75      0.70      0.73       504\n",
      "\n",
      "           micro avg       0.80      0.76      0.78      1365\n",
      "           macro avg       0.80      0.76      0.78      1365\n",
      "        weighted avg       0.80      0.76      0.78      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 142.36113286018372 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5022, Accuracy: 0.8627, F1 Micro: 0.6293, F1 Macro: 0.564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3451, Accuracy: 0.8863, F1 Micro: 0.7607, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2821, Accuracy: 0.9033, F1 Micro: 0.7679, F1 Macro: 0.7612\n",
      "Epoch 4/10, Train Loss: 0.234, Accuracy: 0.907, F1 Micro: 0.7679, F1 Macro: 0.7534\n",
      "Epoch 5/10, Train Loss: 0.1929, Accuracy: 0.9047, F1 Micro: 0.7608, F1 Macro: 0.7539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1441, Accuracy: 0.9066, F1 Micro: 0.781, F1 Macro: 0.78\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9047, F1 Micro: 0.7686, F1 Macro: 0.7594\n",
      "Epoch 8/10, Train Loss: 0.0844, Accuracy: 0.9052, F1 Micro: 0.7639, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.9027, F1 Micro: 0.7784, F1 Macro: 0.7769\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9048, F1 Micro: 0.7767, F1 Macro: 0.7765\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9066, F1 Micro: 0.781, F1 Macro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 141.2790493965149 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4791, Accuracy: 0.8691, F1 Micro: 0.6485, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.337, Accuracy: 0.8908, F1 Micro: 0.7589, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2705, Accuracy: 0.9005, F1 Micro: 0.7642, F1 Macro: 0.7561\n",
      "Epoch 4/10, Train Loss: 0.228, Accuracy: 0.8972, F1 Micro: 0.7389, F1 Macro: 0.7247\n",
      "Epoch 5/10, Train Loss: 0.1864, Accuracy: 0.8961, F1 Micro: 0.7278, F1 Macro: 0.7115\n",
      "Epoch 6/10, Train Loss: 0.1447, Accuracy: 0.9, F1 Micro: 0.7581, F1 Macro: 0.7455\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9013, F1 Micro: 0.7613, F1 Macro: 0.7532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9028, F1 Micro: 0.7648, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9003, F1 Micro: 0.763, F1 Macro: 0.7557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9036, F1 Micro: 0.7722, F1 Macro: 0.7688\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9036, F1 Micro: 0.7722, F1 Macro: 0.7688\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.65      0.68      0.67       248\n",
      "         radikalisme       0.78      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 142.3686966896057 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8955, F1 Micro: 0.7522, F1 Macro: 0.7491\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 60.285335063934326 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4786, Accuracy: 0.8692, F1 Micro: 0.6488, F1 Macro: 0.6364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3273, Accuracy: 0.8931, F1 Micro: 0.7518, F1 Macro: 0.7451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2783, Accuracy: 0.9014, F1 Micro: 0.7625, F1 Macro: 0.7583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2323, Accuracy: 0.9045, F1 Micro: 0.7748, F1 Macro: 0.771\n",
      "Epoch 5/10, Train Loss: 0.1847, Accuracy: 0.9066, F1 Micro: 0.773, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1389, Accuracy: 0.9058, F1 Micro: 0.7772, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.903, F1 Micro: 0.7736, F1 Macro: 0.7709\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.7682\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9042, F1 Micro: 0.7737, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9039, F1 Micro: 0.7773, F1 Macro: 0.7748\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9039, F1 Micro: 0.7773, F1 Macro: 0.7748\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 151.75165557861328 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4974, Accuracy: 0.8609, F1 Micro: 0.586, F1 Macro: 0.538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3355, Accuracy: 0.8919, F1 Micro: 0.7473, F1 Macro: 0.7377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2829, Accuracy: 0.9027, F1 Micro: 0.768, F1 Macro: 0.7656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2369, Accuracy: 0.9025, F1 Micro: 0.7733, F1 Macro: 0.7714\n",
      "Epoch 5/10, Train Loss: 0.1927, Accuracy: 0.9058, F1 Micro: 0.7703, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.1387, Accuracy: 0.9014, F1 Micro: 0.7645, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1105, Accuracy: 0.8994, F1 Micro: 0.7775, F1 Macro: 0.779\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.9008, F1 Micro: 0.7612, F1 Macro: 0.7535\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.8995, F1 Micro: 0.7698, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7693\n",
      "Model 2 - Iteration 3711: Accuracy: 0.8994, F1 Micro: 0.7775, F1 Macro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 150.1955587863922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.474, Accuracy: 0.8755, F1 Micro: 0.6604, F1 Macro: 0.6425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3281, Accuracy: 0.8908, F1 Micro: 0.7483, F1 Macro: 0.7386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2735, Accuracy: 0.8992, F1 Micro: 0.7572, F1 Macro: 0.7519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2257, Accuracy: 0.9022, F1 Micro: 0.7704, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1803, Accuracy: 0.905, F1 Micro: 0.7731, F1 Macro: 0.7638\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.902, F1 Micro: 0.7611, F1 Macro: 0.7551\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.8994, F1 Micro: 0.7629, F1 Macro: 0.7535\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.8992, F1 Micro: 0.7569, F1 Macro: 0.7498\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.8988, F1 Micro: 0.7654, F1 Macro: 0.76\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9027, F1 Micro: 0.7704, F1 Macro: 0.7664\n",
      "Model 3 - Iteration 3711: Accuracy: 0.905, F1 Micro: 0.7731, F1 Macro: 0.7638\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.57      0.62       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.76      0.77      1365\n",
      "           macro avg       0.78      0.75      0.76      1365\n",
      "        weighted avg       0.79      0.76      0.77      1365\n",
      "         samples avg       0.44      0.43      0.42      1365\n",
      "\n",
      "Training completed in 149.90628027915955 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8963, F1 Micro: 0.7548, F1 Macro: 0.7517\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 54.74067401885986 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4675, Accuracy: 0.8709, F1 Micro: 0.65, F1 Macro: 0.6486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3155, Accuracy: 0.8916, F1 Micro: 0.7086, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2633, Accuracy: 0.9036, F1 Micro: 0.777, F1 Macro: 0.7744\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9045, F1 Micro: 0.7728, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7833\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9022, F1 Micro: 0.7652, F1 Macro: 0.7605\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9019, F1 Micro: 0.7618, F1 Macro: 0.7554\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9011, F1 Micro: 0.7778, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0559, Accuracy: 0.905, F1 Micro: 0.7751, F1 Macro: 0.7717\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9013, F1 Micro: 0.7633, F1 Macro: 0.7585\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 153.57198905944824 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4905, Accuracy: 0.8705, F1 Micro: 0.6626, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3309, Accuracy: 0.8892, F1 Micro: 0.6994, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2726, Accuracy: 0.9027, F1 Micro: 0.7825, F1 Macro: 0.7804\n",
      "Epoch 4/10, Train Loss: 0.221, Accuracy: 0.9044, F1 Micro: 0.7732, F1 Macro: 0.7674\n",
      "Epoch 5/10, Train Loss: 0.1894, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7754\n",
      "Epoch 6/10, Train Loss: 0.1311, Accuracy: 0.9023, F1 Micro: 0.7621, F1 Macro: 0.7541\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9006, F1 Micro: 0.7717, F1 Macro: 0.7673\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.902, F1 Micro: 0.7721, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9022, F1 Micro: 0.7767, F1 Macro: 0.7749\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9019, F1 Micro: 0.765, F1 Macro: 0.7572\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9027, F1 Micro: 0.7825, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 152.50480580329895 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4634, Accuracy: 0.8702, F1 Micro: 0.6737, F1 Macro: 0.6762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3192, Accuracy: 0.8952, F1 Micro: 0.7249, F1 Macro: 0.7041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2569, Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7738\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9008, F1 Micro: 0.7547, F1 Macro: 0.7404\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9044, F1 Micro: 0.7691, F1 Macro: 0.7615\n",
      "Epoch 6/10, Train Loss: 0.121, Accuracy: 0.9003, F1 Micro: 0.7602, F1 Macro: 0.7516\n",
      "Epoch 7/10, Train Loss: 0.0958, Accuracy: 0.9009, F1 Micro: 0.7671, F1 Macro: 0.7569\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.8992, F1 Micro: 0.7709, F1 Macro: 0.7634\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9014, F1 Micro: 0.7678, F1 Macro: 0.7611\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9014, F1 Micro: 0.7667, F1 Macro: 0.7559\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7738\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 152.92598271369934 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.897, F1 Micro: 0.7576, F1 Macro: 0.7544\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 49.91255235671997 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4599, Accuracy: 0.8727, F1 Micro: 0.6667, F1 Macro: 0.6664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3175, Accuracy: 0.8966, F1 Micro: 0.7394, F1 Macro: 0.736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2671, Accuracy: 0.9014, F1 Micro: 0.7463, F1 Macro: 0.7303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2249, Accuracy: 0.8983, F1 Micro: 0.7742, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1687, Accuracy: 0.9073, F1 Micro: 0.7862, F1 Macro: 0.7828\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9019, F1 Micro: 0.7686, F1 Macro: 0.7634\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9047, F1 Micro: 0.7749, F1 Macro: 0.7685\n",
      "Epoch 8/10, Train Loss: 0.0716, Accuracy: 0.9044, F1 Micro: 0.7836, F1 Macro: 0.7828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.908, F1 Micro: 0.7864, F1 Macro: 0.7839\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9038, F1 Micro: 0.7789, F1 Macro: 0.7751\n",
      "Model 1 - Iteration 4120: Accuracy: 0.908, F1 Micro: 0.7864, F1 Macro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.67      0.71      0.68       248\n",
      "         radikalisme       0.75      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 165.4543216228485 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4736, Accuracy: 0.8748, F1 Micro: 0.693, F1 Macro: 0.6954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.323, Accuracy: 0.8978, F1 Micro: 0.7419, F1 Macro: 0.7371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.9025, F1 Micro: 0.7532, F1 Macro: 0.7361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2242, Accuracy: 0.9002, F1 Micro: 0.7815, F1 Macro: 0.7815\n",
      "Epoch 5/10, Train Loss: 0.1694, Accuracy: 0.9031, F1 Micro: 0.7685, F1 Macro: 0.7615\n",
      "Epoch 6/10, Train Loss: 0.126, Accuracy: 0.9039, F1 Micro: 0.7682, F1 Macro: 0.7638\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9034, F1 Micro: 0.7736, F1 Macro: 0.7687\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.9016, F1 Micro: 0.7745, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.902, F1 Micro: 0.7689, F1 Macro: 0.7671\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9042, F1 Micro: 0.7775, F1 Macro: 0.7747\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9002, F1 Micro: 0.7815, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.62      0.79      0.69       248\n",
      "         radikalisme       0.72      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 162.91651940345764 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4555, Accuracy: 0.8686, F1 Micro: 0.6594, F1 Macro: 0.6505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.317, Accuracy: 0.8969, F1 Micro: 0.7454, F1 Macro: 0.7413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2666, Accuracy: 0.8991, F1 Micro: 0.7467, F1 Macro: 0.7243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2207, Accuracy: 0.902, F1 Micro: 0.7836, F1 Macro: 0.7808\n",
      "Epoch 5/10, Train Loss: 0.1684, Accuracy: 0.9006, F1 Micro: 0.7707, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9047, F1 Micro: 0.7717, F1 Macro: 0.7665\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9061, F1 Micro: 0.7814, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.9028, F1 Micro: 0.7672, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0583, Accuracy: 0.9028, F1 Micro: 0.7771, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9038, F1 Micro: 0.7735, F1 Macro: 0.7686\n",
      "Model 3 - Iteration 4120: Accuracy: 0.902, F1 Micro: 0.7836, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.71      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 161.89947628974915 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8976, F1 Micro: 0.76, F1 Macro: 0.7569\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 46.135310888290405 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4563, Accuracy: 0.8723, F1 Micro: 0.6636, F1 Macro: 0.6657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3039, Accuracy: 0.8997, F1 Micro: 0.7712, F1 Macro: 0.7682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2557, Accuracy: 0.9031, F1 Micro: 0.7745, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2105, Accuracy: 0.9061, F1 Micro: 0.785, F1 Macro: 0.7828\n",
      "Epoch 5/10, Train Loss: 0.1671, Accuracy: 0.9033, F1 Micro: 0.7718, F1 Macro: 0.7683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9067, F1 Micro: 0.7856, F1 Macro: 0.7859\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9053, F1 Micro: 0.7762, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9042, F1 Micro: 0.7788, F1 Macro: 0.7727\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9052, F1 Micro: 0.7776, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.903, F1 Micro: 0.7796, F1 Macro: 0.7778\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9067, F1 Micro: 0.7856, F1 Macro: 0.7859\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.77      0.71       248\n",
      "         radikalisme       0.78      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 170.63968992233276 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4723, Accuracy: 0.87, F1 Micro: 0.6698, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3109, Accuracy: 0.8972, F1 Micro: 0.765, F1 Macro: 0.7631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2582, Accuracy: 0.9003, F1 Micro: 0.7668, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.214, Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.781\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9027, F1 Micro: 0.7681, F1 Macro: 0.7627\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9033, F1 Micro: 0.7688, F1 Macro: 0.7672\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9036, F1 Micro: 0.7764, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9014, F1 Micro: 0.7703, F1 Macro: 0.765\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9008, F1 Micro: 0.768, F1 Macro: 0.7668\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9008, F1 Micro: 0.7707, F1 Macro: 0.7668\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 168.67287373542786 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4535, Accuracy: 0.8658, F1 Micro: 0.6484, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3073, Accuracy: 0.897, F1 Micro: 0.7592, F1 Macro: 0.753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2583, Accuracy: 0.9016, F1 Micro: 0.7667, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2129, Accuracy: 0.9039, F1 Micro: 0.7773, F1 Macro: 0.7696\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.8981, F1 Micro: 0.7525, F1 Macro: 0.7431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1357, Accuracy: 0.9067, F1 Micro: 0.7812, F1 Macro: 0.779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9039, F1 Micro: 0.7815, F1 Macro: 0.7763\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9019, F1 Micro: 0.7776, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9036, F1 Micro: 0.7732, F1 Macro: 0.7719\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9036, F1 Micro: 0.7762, F1 Macro: 0.7694\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9039, F1 Micro: 0.7815, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.70      0.86      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.95346975326538 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8982, F1 Micro: 0.7619, F1 Macro: 0.759\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 40.99054455757141 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4383, Accuracy: 0.8773, F1 Micro: 0.6954, F1 Macro: 0.6869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3053, Accuracy: 0.8956, F1 Micro: 0.7668, F1 Macro: 0.7637\n",
      "Epoch 3/10, Train Loss: 0.2593, Accuracy: 0.9016, F1 Micro: 0.7573, F1 Macro: 0.7539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2203, Accuracy: 0.9086, F1 Micro: 0.7782, F1 Macro: 0.7745\n",
      "Epoch 5/10, Train Loss: 0.1741, Accuracy: 0.9003, F1 Micro: 0.767, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9031, F1 Micro: 0.7765, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0957, Accuracy: 0.9058, F1 Micro: 0.779, F1 Macro: 0.7741\n",
      "Epoch 8/10, Train Loss: 0.0773, Accuracy: 0.9039, F1 Micro: 0.7781, F1 Macro: 0.7742\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9044, F1 Micro: 0.7698, F1 Macro: 0.7597\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9025, F1 Micro: 0.7744, F1 Macro: 0.7677\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9058, F1 Micro: 0.779, F1 Macro: 0.7741\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.66      0.61      0.64       248\n",
      "         radikalisme       0.78      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.44      0.44      1365\n",
      "\n",
      "Training completed in 173.87487649917603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4569, Accuracy: 0.8686, F1 Micro: 0.6583, F1 Macro: 0.6309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3164, Accuracy: 0.8956, F1 Micro: 0.763, F1 Macro: 0.7601\n",
      "Epoch 3/10, Train Loss: 0.2596, Accuracy: 0.9028, F1 Micro: 0.7538, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2238, Accuracy: 0.9044, F1 Micro: 0.7666, F1 Macro: 0.7629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1702, Accuracy: 0.9033, F1 Micro: 0.7701, F1 Macro: 0.7656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.126, Accuracy: 0.9062, F1 Micro: 0.782, F1 Macro: 0.7747\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9036, F1 Micro: 0.7796, F1 Macro: 0.7716\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9016, F1 Micro: 0.7745, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9042, F1 Micro: 0.7693, F1 Macro: 0.7617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.9039, F1 Micro: 0.782, F1 Macro: 0.7762\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9039, F1 Micro: 0.782, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.64      0.65       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 177.15460968017578 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4366, Accuracy: 0.8712, F1 Micro: 0.6738, F1 Macro: 0.6633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3088, Accuracy: 0.8964, F1 Micro: 0.7689, F1 Macro: 0.7643\n",
      "Epoch 3/10, Train Loss: 0.2569, Accuracy: 0.8994, F1 Micro: 0.7475, F1 Macro: 0.7362\n",
      "Epoch 4/10, Train Loss: 0.2171, Accuracy: 0.9022, F1 Micro: 0.7623, F1 Macro: 0.7583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1647, Accuracy: 0.9, F1 Micro: 0.7756, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.9019, F1 Micro: 0.7803, F1 Macro: 0.7736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9028, F1 Micro: 0.7848, F1 Macro: 0.7783\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.9014, F1 Micro: 0.7739, F1 Macro: 0.7687\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.9023, F1 Micro: 0.7701, F1 Macro: 0.7629\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9036, F1 Micro: 0.7749, F1 Macro: 0.7699\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9028, F1 Micro: 0.7848, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.94      0.91       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 175.95778346061707 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8987, F1 Micro: 0.7635, F1 Macro: 0.7603\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 37.361897230148315 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4409, Accuracy: 0.8813, F1 Micro: 0.7008, F1 Macro: 0.6956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3064, Accuracy: 0.9052, F1 Micro: 0.77, F1 Macro: 0.7677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.9036, F1 Micro: 0.7754, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2056, Accuracy: 0.9056, F1 Micro: 0.7794, F1 Macro: 0.7734\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9023, F1 Micro: 0.7751, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9069, F1 Micro: 0.7855, F1 Macro: 0.7845\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9083, F1 Micro: 0.7794, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9045, F1 Micro: 0.7736, F1 Macro: 0.7694\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9055, F1 Micro: 0.7679, F1 Macro: 0.7601\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9066, F1 Micro: 0.7846, F1 Macro: 0.7799\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9069, F1 Micro: 0.7855, F1 Macro: 0.7845\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 179.4690878391266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.46, Accuracy: 0.8763, F1 Micro: 0.6806, F1 Macro: 0.669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3151, Accuracy: 0.9, F1 Micro: 0.7581, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2494, Accuracy: 0.9039, F1 Micro: 0.7668, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2099, Accuracy: 0.9053, F1 Micro: 0.7804, F1 Macro: 0.7728\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9045, F1 Micro: 0.7766, F1 Macro: 0.7725\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9058, F1 Micro: 0.7744, F1 Macro: 0.769\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9028, F1 Micro: 0.7701, F1 Macro: 0.7682\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.8995, F1 Micro: 0.7534, F1 Macro: 0.7484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9081, F1 Micro: 0.7851, F1 Macro: 0.7845\n",
      "Epoch 10/10, Train Loss: 0.038, Accuracy: 0.9038, F1 Micro: 0.7734, F1 Macro: 0.7723\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9081, F1 Micro: 0.7851, F1 Macro: 0.7845\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       370\n",
      "                sara       0.71      0.68      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 179.321373462677 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4369, Accuracy: 0.8841, F1 Micro: 0.714, F1 Macro: 0.6973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3087, Accuracy: 0.8986, F1 Micro: 0.7507, F1 Macro: 0.745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2447, Accuracy: 0.9011, F1 Micro: 0.7704, F1 Macro: 0.7658\n",
      "Epoch 4/10, Train Loss: 0.2061, Accuracy: 0.9002, F1 Micro: 0.7697, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9023, F1 Micro: 0.7761, F1 Macro: 0.7708\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9027, F1 Micro: 0.7673, F1 Macro: 0.7613\n",
      "Epoch 7/10, Train Loss: 0.0933, Accuracy: 0.9009, F1 Micro: 0.7731, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9033, F1 Micro: 0.7712, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9033, F1 Micro: 0.7742, F1 Macro: 0.769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.8998, F1 Micro: 0.7829, F1 Macro: 0.7828\n",
      "Model 3 - Iteration 4663: Accuracy: 0.8998, F1 Micro: 0.7829, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.61      0.81      0.70       248\n",
      "         radikalisme       0.70      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.78      1365\n",
      "           macro avg       0.73      0.85      0.78      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 179.44761896133423 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8991, F1 Micro: 0.765, F1 Macro: 0.762\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 33.64528560638428 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4261, Accuracy: 0.8791, F1 Micro: 0.6762, F1 Macro: 0.6681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.9003, F1 Micro: 0.7768, F1 Macro: 0.7733\n",
      "Epoch 3/10, Train Loss: 0.2387, Accuracy: 0.9016, F1 Micro: 0.7543, F1 Macro: 0.7455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9086, F1 Micro: 0.7797, F1 Macro: 0.7738\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9062, F1 Micro: 0.7746, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9087, F1 Micro: 0.7819, F1 Macro: 0.7795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9045, F1 Micro: 0.7837, F1 Macro: 0.7829\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9059, F1 Micro: 0.778, F1 Macro: 0.7726\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9067, F1 Micro: 0.7762, F1 Macro: 0.775\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9048, F1 Micro: 0.7826, F1 Macro: 0.779\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9045, F1 Micro: 0.7837, F1 Macro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.76      0.85      0.81       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 185.99009251594543 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4429, Accuracy: 0.8831, F1 Micro: 0.7145, F1 Macro: 0.7132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2964, Accuracy: 0.8983, F1 Micro: 0.7732, F1 Macro: 0.7691\n",
      "Epoch 3/10, Train Loss: 0.2432, Accuracy: 0.8994, F1 Micro: 0.7457, F1 Macro: 0.7379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2028, Accuracy: 0.9059, F1 Micro: 0.7739, F1 Macro: 0.7683\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9025, F1 Micro: 0.7629, F1 Macro: 0.7564\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.9036, F1 Micro: 0.7734, F1 Macro: 0.7686\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.9056, F1 Micro: 0.7691, F1 Macro: 0.7599\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9022, F1 Micro: 0.7707, F1 Macro: 0.7644\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9025, F1 Micro: 0.7703, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9031, F1 Micro: 0.7787, F1 Macro: 0.7739\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9031, F1 Micro: 0.7787, F1 Macro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.90       370\n",
      "                sara       0.65      0.65      0.65       248\n",
      "         radikalisme       0.77      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 183.36073899269104 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4245, Accuracy: 0.8866, F1 Micro: 0.7307, F1 Macro: 0.7259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2925, Accuracy: 0.8983, F1 Micro: 0.772, F1 Macro: 0.766\n",
      "Epoch 3/10, Train Loss: 0.2371, Accuracy: 0.9034, F1 Micro: 0.7595, F1 Macro: 0.7497\n",
      "Epoch 4/10, Train Loss: 0.1994, Accuracy: 0.9034, F1 Micro: 0.7625, F1 Macro: 0.7549\n",
      "Epoch 5/10, Train Loss: 0.1612, Accuracy: 0.9039, F1 Micro: 0.7713, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9041, F1 Micro: 0.7771, F1 Macro: 0.7723\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9045, F1 Micro: 0.7756, F1 Macro: 0.7695\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.902, F1 Micro: 0.7718, F1 Macro: 0.7673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.905, F1 Micro: 0.7778, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9022, F1 Micro: 0.7822, F1 Macro: 0.7807\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9022, F1 Micro: 0.7822, F1 Macro: 0.7807\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.63      0.73      0.68       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 185.2787435054779 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8994, F1 Micro: 0.7661, F1 Macro: 0.7631\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 29.994028329849243 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4174, Accuracy: 0.8827, F1 Micro: 0.6911, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2867, Accuracy: 0.8989, F1 Micro: 0.7619, F1 Macro: 0.7496\n",
      "Epoch 3/10, Train Loss: 0.2339, Accuracy: 0.9033, F1 Micro: 0.7605, F1 Macro: 0.7538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1904, Accuracy: 0.9002, F1 Micro: 0.773, F1 Macro: 0.7698\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.903, F1 Micro: 0.7696, F1 Macro: 0.7677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9053, F1 Micro: 0.7777, F1 Macro: 0.7715\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9036, F1 Micro: 0.7752, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9034, F1 Micro: 0.7833, F1 Macro: 0.7794\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9045, F1 Micro: 0.7793, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9025, F1 Micro: 0.7685, F1 Macro: 0.7623\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9034, F1 Micro: 0.7833, F1 Macro: 0.7794\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.75      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 192.89422941207886 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4299, Accuracy: 0.8834, F1 Micro: 0.6907, F1 Macro: 0.6714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.897, F1 Micro: 0.7627, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2374, Accuracy: 0.9053, F1 Micro: 0.7671, F1 Macro: 0.7608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1898, Accuracy: 0.9031, F1 Micro: 0.7763, F1 Macro: 0.7721\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.9058, F1 Micro: 0.7751, F1 Macro: 0.7709\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9064, F1 Micro: 0.7715, F1 Macro: 0.7633\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9045, F1 Micro: 0.7702, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.9067, F1 Micro: 0.7816, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9033, F1 Micro: 0.7821, F1 Macro: 0.7805\n",
      "Epoch 10/10, Train Loss: 0.0385, Accuracy: 0.9038, F1 Micro: 0.7749, F1 Macro: 0.7714\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9033, F1 Micro: 0.7821, F1 Macro: 0.7805\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.64      0.73      0.69       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.380122423172 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4161, Accuracy: 0.8814, F1 Micro: 0.6841, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2898, Accuracy: 0.898, F1 Micro: 0.7672, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9044, F1 Micro: 0.7691, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1898, Accuracy: 0.9017, F1 Micro: 0.7786, F1 Macro: 0.7718\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.9041, F1 Micro: 0.7726, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9014, F1 Micro: 0.779, F1 Macro: 0.7747\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.902, F1 Micro: 0.7736, F1 Macro: 0.7735\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9058, F1 Micro: 0.772, F1 Macro: 0.7622\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9056, F1 Micro: 0.7753, F1 Macro: 0.7721\n",
      "Epoch 10/10, Train Loss: 0.0381, Accuracy: 0.9039, F1 Micro: 0.7785, F1 Macro: 0.7767\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9014, F1 Micro: 0.779, F1 Macro: 0.7747\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 192.6411907672882 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8996, F1 Micro: 0.767, F1 Macro: 0.7641\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.26827907562256 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.8722, F1 Micro: 0.6396, F1 Macro: 0.6186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.8975, F1 Micro: 0.7411, F1 Macro: 0.7201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9075, F1 Micro: 0.7811, F1 Macro: 0.7684\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.9062, F1 Micro: 0.776, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1431, Accuracy: 0.9031, F1 Micro: 0.7834, F1 Macro: 0.7813\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.9058, F1 Micro: 0.7696, F1 Macro: 0.7567\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9019, F1 Micro: 0.7793, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9036, F1 Micro: 0.7816, F1 Macro: 0.7803\n",
      "Epoch 9/10, Train Loss: 0.0467, Accuracy: 0.9059, F1 Micro: 0.778, F1 Macro: 0.7744\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9045, F1 Micro: 0.7812, F1 Macro: 0.779\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9031, F1 Micro: 0.7834, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.79      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 197.63606476783752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4209, Accuracy: 0.8719, F1 Micro: 0.6339, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.285, Accuracy: 0.8964, F1 Micro: 0.7393, F1 Macro: 0.7184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9066, F1 Micro: 0.7805, F1 Macro: 0.7702\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9062, F1 Micro: 0.7734, F1 Macro: 0.766\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9003, F1 Micro: 0.7769, F1 Macro: 0.7694\n",
      "Epoch 6/10, Train Loss: 0.1078, Accuracy: 0.9038, F1 Micro: 0.7781, F1 Macro: 0.7705\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9033, F1 Micro: 0.774, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9053, F1 Micro: 0.7742, F1 Macro: 0.7678\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9041, F1 Micro: 0.7721, F1 Macro: 0.7633\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9027, F1 Micro: 0.7793, F1 Macro: 0.7765\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9066, F1 Micro: 0.7805, F1 Macro: 0.7702\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.72      0.56      0.63       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 195.6382908821106 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4075, Accuracy: 0.8784, F1 Micro: 0.6641, F1 Macro: 0.6422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.8995, F1 Micro: 0.752, F1 Macro: 0.7371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2299, Accuracy: 0.9034, F1 Micro: 0.7723, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.9052, F1 Micro: 0.7785, F1 Macro: 0.7688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.8995, F1 Micro: 0.7793, F1 Macro: 0.7756\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.903, F1 Micro: 0.7754, F1 Macro: 0.7649\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9009, F1 Micro: 0.7745, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.905, F1 Micro: 0.7825, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.9048, F1 Micro: 0.7739, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9014, F1 Micro: 0.7754, F1 Macro: 0.7705\n",
      "Model 3 - Iteration 5263: Accuracy: 0.905, F1 Micro: 0.7825, F1 Macro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.79      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 200.4410047531128 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8999, F1 Micro: 0.7679, F1 Macro: 0.7648\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 21.070842027664185 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.8873, F1 Micro: 0.7428, F1 Macro: 0.74\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2685, Accuracy: 0.8981, F1 Micro: 0.7564, F1 Macro: 0.7408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.8998, F1 Micro: 0.7803, F1 Macro: 0.7795\n",
      "Epoch 4/10, Train Loss: 0.1799, Accuracy: 0.9019, F1 Micro: 0.7657, F1 Macro: 0.7629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9087, F1 Micro: 0.7895, F1 Macro: 0.7853\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.9038, F1 Micro: 0.7713, F1 Macro: 0.7607\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9013, F1 Micro: 0.7693, F1 Macro: 0.7597\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.8967, F1 Micro: 0.7754, F1 Macro: 0.7718\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.8992, F1 Micro: 0.7766, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.8981, F1 Micro: 0.7766, F1 Macro: 0.7752\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9087, F1 Micro: 0.7895, F1 Macro: 0.7853\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.77      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 203.56755900382996 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4136, Accuracy: 0.8839, F1 Micro: 0.7253, F1 Macro: 0.7226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2748, Accuracy: 0.9023, F1 Micro: 0.7677, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.226, Accuracy: 0.8967, F1 Micro: 0.7783, F1 Macro: 0.7783\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9016, F1 Micro: 0.7651, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1438, Accuracy: 0.9041, F1 Micro: 0.7824, F1 Macro: 0.7789\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9031, F1 Micro: 0.7688, F1 Macro: 0.7592\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.8933, F1 Micro: 0.7742, F1 Macro: 0.7795\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9042, F1 Micro: 0.778, F1 Macro: 0.7736\n",
      "Epoch 9/10, Train Loss: 0.0401, Accuracy: 0.9017, F1 Micro: 0.7788, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0386, Accuracy: 0.9042, F1 Micro: 0.7819, F1 Macro: 0.7794\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9041, F1 Micro: 0.7824, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 202.65549087524414 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3978, Accuracy: 0.8852, F1 Micro: 0.7334, F1 Macro: 0.7306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.269, Accuracy: 0.898, F1 Micro: 0.7529, F1 Macro: 0.7349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.898, F1 Micro: 0.7781, F1 Macro: 0.7783\n",
      "Epoch 4/10, Train Loss: 0.1776, Accuracy: 0.9014, F1 Micro: 0.757, F1 Macro: 0.7461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9062, F1 Micro: 0.7848, F1 Macro: 0.7808\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9014, F1 Micro: 0.7689, F1 Macro: 0.7578\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9013, F1 Micro: 0.7746, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.0586, Accuracy: 0.9017, F1 Micro: 0.7812, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0397, Accuracy: 0.9038, F1 Micro: 0.78, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0355, Accuracy: 0.9016, F1 Micro: 0.7831, F1 Macro: 0.7821\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9062, F1 Micro: 0.7848, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 202.98097014427185 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9003, F1 Micro: 0.7689, F1 Macro: 0.7657\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.043321132659912 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3961, Accuracy: 0.8916, F1 Micro: 0.7463, F1 Macro: 0.7381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9011, F1 Micro: 0.7699, F1 Macro: 0.769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2014, Accuracy: 0.9064, F1 Micro: 0.7807, F1 Macro: 0.774\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9019, F1 Micro: 0.7705, F1 Macro: 0.7686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9058, F1 Micro: 0.787, F1 Macro: 0.7846\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9034, F1 Micro: 0.7785, F1 Macro: 0.7761\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9025, F1 Micro: 0.7776, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9036, F1 Micro: 0.7759, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9, F1 Micro: 0.7778, F1 Macro: 0.7773\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9058, F1 Micro: 0.787, F1 Macro: 0.7846\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.65      0.73      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 208.9853653907776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4159, Accuracy: 0.8898, F1 Micro: 0.7443, F1 Macro: 0.7357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9033, F1 Micro: 0.7663, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.205, Accuracy: 0.9059, F1 Micro: 0.7811, F1 Macro: 0.7743\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.903, F1 Micro: 0.7756, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1314, Accuracy: 0.9, F1 Micro: 0.7686, F1 Macro: 0.7656\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.8973, F1 Micro: 0.7689, F1 Macro: 0.7668\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9013, F1 Micro: 0.774, F1 Macro: 0.7707\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9006, F1 Micro: 0.7749, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9028, F1 Micro: 0.7761, F1 Macro: 0.7739\n",
      "Epoch 10/10, Train Loss: 0.0375, Accuracy: 0.8998, F1 Micro: 0.7734, F1 Macro: 0.7706\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9059, F1 Micro: 0.7811, F1 Macro: 0.7743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.69      0.61      0.65       248\n",
      "         radikalisme       0.72      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 206.8295819759369 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3974, Accuracy: 0.8894, F1 Micro: 0.7376, F1 Macro: 0.7257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9028, F1 Micro: 0.7715, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2007, Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7699\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.9003, F1 Micro: 0.7665, F1 Macro: 0.7637\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9009, F1 Micro: 0.7671, F1 Macro: 0.7633\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9031, F1 Micro: 0.7704, F1 Macro: 0.7641\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.9016, F1 Micro: 0.7752, F1 Macro: 0.7748\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.902, F1 Micro: 0.7732, F1 Macro: 0.7684\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9023, F1 Micro: 0.7689, F1 Macro: 0.7625\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.902, F1 Micro: 0.7685, F1 Macro: 0.7628\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7699\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.72      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 207.0862820148468 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9006, F1 Micro: 0.7695, F1 Macro: 0.7663\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.770872831344604 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3751, Accuracy: 0.8909, F1 Micro: 0.7471, F1 Macro: 0.7431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2504, Accuracy: 0.9, F1 Micro: 0.7668, F1 Macro: 0.7584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9061, F1 Micro: 0.779, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.1653, Accuracy: 0.9048, F1 Micro: 0.7732, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.9052, F1 Micro: 0.7838, F1 Macro: 0.781\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.9044, F1 Micro: 0.777, F1 Macro: 0.7718\n",
      "Epoch 7/10, Train Loss: 0.0701, Accuracy: 0.9047, F1 Micro: 0.7766, F1 Macro: 0.7728\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.9002, F1 Micro: 0.776, F1 Macro: 0.7746\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9066, F1 Micro: 0.783, F1 Macro: 0.7804\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9067, F1 Micro: 0.7755, F1 Macro: 0.7678\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9052, F1 Micro: 0.7838, F1 Macro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 215.4821071624756 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3954, Accuracy: 0.8909, F1 Micro: 0.7413, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2558, Accuracy: 0.9013, F1 Micro: 0.7693, F1 Macro: 0.7614\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9036, F1 Micro: 0.7596, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9084, F1 Micro: 0.7813, F1 Macro: 0.7721\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.9014, F1 Micro: 0.7746, F1 Macro: 0.77\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9041, F1 Micro: 0.7801, F1 Macro: 0.7795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9045, F1 Micro: 0.7826, F1 Macro: 0.7785\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.8995, F1 Micro: 0.7791, F1 Macro: 0.7784\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.9022, F1 Micro: 0.7641, F1 Macro: 0.7589\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9047, F1 Micro: 0.7764, F1 Macro: 0.7682\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9045, F1 Micro: 0.7826, F1 Macro: 0.7785\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.77      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 214.21026754379272 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8898, F1 Micro: 0.7465, F1 Macro: 0.7407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9008, F1 Micro: 0.7623, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2085, Accuracy: 0.9044, F1 Micro: 0.7646, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9027, F1 Micro: 0.7669, F1 Macro: 0.7544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.9025, F1 Micro: 0.7752, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9039, F1 Micro: 0.7792, F1 Macro: 0.7769\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9017, F1 Micro: 0.7784, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.9005, F1 Micro: 0.7721, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9058, F1 Micro: 0.7789, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9052, F1 Micro: 0.7813, F1 Macro: 0.7772\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9052, F1 Micro: 0.7813, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.65      0.66       248\n",
      "         radikalisme       0.79      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 219.73161435127258 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9008, F1 Micro: 0.7702, F1 Macro: 0.7669\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.054455518722534 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3674, Accuracy: 0.8898, F1 Micro: 0.7322, F1 Macro: 0.7233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2431, Accuracy: 0.9013, F1 Micro: 0.7636, F1 Macro: 0.7599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.9044, F1 Micro: 0.7684, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9073, F1 Micro: 0.7755, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9045, F1 Micro: 0.7785, F1 Macro: 0.7764\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.897, F1 Micro: 0.7727, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.0627, Accuracy: 0.9025, F1 Micro: 0.7741, F1 Macro: 0.7677\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9058, F1 Micro: 0.7763, F1 Macro: 0.7751\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9034, F1 Micro: 0.7733, F1 Macro: 0.7713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9033, F1 Micro: 0.7812, F1 Macro: 0.7829\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9033, F1 Micro: 0.7812, F1 Macro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.61      0.75      0.67       248\n",
      "         radikalisme       0.78      0.85      0.81       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 223.57743191719055 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3835, Accuracy: 0.892, F1 Micro: 0.7298, F1 Macro: 0.7192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9013, F1 Micro: 0.7619, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1948, Accuracy: 0.9061, F1 Micro: 0.7709, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.161, Accuracy: 0.9064, F1 Micro: 0.7749, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9009, F1 Micro: 0.7786, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.8966, F1 Micro: 0.7802, F1 Macro: 0.7802\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9061, F1 Micro: 0.7731, F1 Macro: 0.7615\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9053, F1 Micro: 0.777, F1 Macro: 0.7729\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9045, F1 Micro: 0.7759, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9033, F1 Micro: 0.7818, F1 Macro: 0.7808\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9033, F1 Micro: 0.7818, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 225.5449652671814 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3687, Accuracy: 0.8909, F1 Micro: 0.7366, F1 Macro: 0.7315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.8975, F1 Micro: 0.7517, F1 Macro: 0.7397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9056, F1 Micro: 0.7721, F1 Macro: 0.7592\n",
      "Epoch 4/10, Train Loss: 0.1575, Accuracy: 0.9047, F1 Micro: 0.7661, F1 Macro: 0.7566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1194, Accuracy: 0.9019, F1 Micro: 0.7821, F1 Macro: 0.7783\n",
      "Epoch 6/10, Train Loss: 0.0869, Accuracy: 0.8992, F1 Micro: 0.7789, F1 Macro: 0.7772\n",
      "Epoch 7/10, Train Loss: 0.0653, Accuracy: 0.9031, F1 Micro: 0.7757, F1 Macro: 0.7702\n",
      "Epoch 8/10, Train Loss: 0.0516, Accuracy: 0.9056, F1 Micro: 0.7763, F1 Macro: 0.7698\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9036, F1 Micro: 0.7729, F1 Macro: 0.7728\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9006, F1 Micro: 0.7709, F1 Macro: 0.7718\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9019, F1 Micro: 0.7821, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.61      0.78      0.68       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 220.34856581687927 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9009, F1 Micro: 0.7707, F1 Macro: 0.7676\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.643327951431274 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3608, Accuracy: 0.8923, F1 Micro: 0.7426, F1 Macro: 0.7379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.8994, F1 Micro: 0.7799, F1 Macro: 0.7779\n",
      "Epoch 3/10, Train Loss: 0.1955, Accuracy: 0.9044, F1 Micro: 0.7701, F1 Macro: 0.7604\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9036, F1 Micro: 0.7631, F1 Macro: 0.7514\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.9036, F1 Micro: 0.7794, F1 Macro: 0.7779\n",
      "Epoch 6/10, Train Loss: 0.0865, Accuracy: 0.8973, F1 Micro: 0.7763, F1 Macro: 0.778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0646, Accuracy: 0.9069, F1 Micro: 0.7823, F1 Macro: 0.7824\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.7691\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9039, F1 Micro: 0.7803, F1 Macro: 0.779\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9042, F1 Micro: 0.7715, F1 Macro: 0.7646\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9069, F1 Micro: 0.7823, F1 Macro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.69      0.66      0.67       248\n",
      "         radikalisme       0.80      0.84      0.82       243\n",
      "pencemaran_nama_baik       0.71      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.44      0.44      1365\n",
      "\n",
      "Training completed in 225.3573784828186 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3791, Accuracy: 0.8911, F1 Micro: 0.7272, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2348, Accuracy: 0.9009, F1 Micro: 0.7826, F1 Macro: 0.7788\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9047, F1 Micro: 0.77, F1 Macro: 0.7586\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.9022, F1 Micro: 0.7627, F1 Macro: 0.7545\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.9031, F1 Micro: 0.775, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.9031, F1 Micro: 0.7869, F1 Macro: 0.7865\n",
      "Epoch 7/10, Train Loss: 0.0607, Accuracy: 0.903, F1 Micro: 0.7607, F1 Macro: 0.7546\n",
      "Epoch 8/10, Train Loss: 0.047, Accuracy: 0.8992, F1 Micro: 0.776, F1 Macro: 0.7739\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.9011, F1 Micro: 0.7656, F1 Macro: 0.7534\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9042, F1 Micro: 0.7725, F1 Macro: 0.7687\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9031, F1 Micro: 0.7869, F1 Macro: 0.7865\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 224.5201153755188 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3589, Accuracy: 0.8908, F1 Micro: 0.7357, F1 Macro: 0.7297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2316, Accuracy: 0.9006, F1 Micro: 0.7805, F1 Macro: 0.7767\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9056, F1 Micro: 0.7743, F1 Macro: 0.7632\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9016, F1 Micro: 0.7658, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1135, Accuracy: 0.9034, F1 Micro: 0.7838, F1 Macro: 0.7826\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.9003, F1 Micro: 0.778, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0638, Accuracy: 0.902, F1 Micro: 0.7684, F1 Macro: 0.7652\n",
      "Epoch 8/10, Train Loss: 0.0459, Accuracy: 0.9033, F1 Micro: 0.7763, F1 Macro: 0.773\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.902, F1 Micro: 0.7724, F1 Macro: 0.7671\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9036, F1 Micro: 0.7699, F1 Macro: 0.7654\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9034, F1 Micro: 0.7838, F1 Macro: 0.7826\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 224.6764817237854 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.901, F1 Micro: 0.7714, F1 Macro: 0.7683\n",
      "Total sampling time: 1075.61 seconds\n",
      "Total runtime: 11917.58267211914 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4685004",
   "metadata": {
    "papermill": {
     "duration": 0.389835,
     "end_time": "2025-01-28T23:10:18.138519",
     "exception": false,
     "start_time": "2025-01-28T23:10:17.748684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27139099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5781, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.432, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3729, Accuracy: 0.8223, F1 Micro: 0.2889, F1 Macro: 0.2126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3601, Accuracy: 0.83, F1 Micro: 0.3747, F1 Macro: 0.2926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2934, Accuracy: 0.8384, F1 Micro: 0.4405, F1 Macro: 0.3659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2592, Accuracy: 0.8566, F1 Micro: 0.5816, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2344, Accuracy: 0.8592, F1 Micro: 0.588, F1 Macro: 0.5698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1829, Accuracy: 0.8694, F1 Micro: 0.6384, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1544, Accuracy: 0.8728, F1 Micro: 0.6921, F1 Macro: 0.6849\n",
      "Epoch 10/10, Train Loss: 0.1323, Accuracy: 0.8736, F1 Micro: 0.6645, F1 Macro: 0.6516\n",
      "Model 1 - Iteration 388: Accuracy: 0.8728, F1 Micro: 0.6921, F1 Macro: 0.6849\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.82      0.86       370\n",
      "                sara       0.58      0.52      0.55       248\n",
      "         radikalisme       0.67      0.72      0.70       243\n",
      "pencemaran_nama_baik       0.68      0.61      0.64       504\n",
      "\n",
      "           micro avg       0.72      0.67      0.69      1365\n",
      "           macro avg       0.70      0.67      0.68      1365\n",
      "        weighted avg       0.72      0.67      0.69      1365\n",
      "         samples avg       0.38      0.37      0.36      1365\n",
      "\n",
      "Training completed in 55.258695125579834 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5897, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4473, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3959, Accuracy: 0.8066, F1 Micro: 0.1702, F1 Macro: 0.1278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3926, Accuracy: 0.8258, F1 Micro: 0.3197, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3251, Accuracy: 0.8391, F1 Micro: 0.4372, F1 Macro: 0.3627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2762, Accuracy: 0.8562, F1 Micro: 0.5611, F1 Macro: 0.5364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2538, Accuracy: 0.8605, F1 Micro: 0.6061, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1942, Accuracy: 0.8698, F1 Micro: 0.6481, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1598, Accuracy: 0.8733, F1 Micro: 0.6915, F1 Macro: 0.6872\n",
      "Epoch 10/10, Train Loss: 0.1373, Accuracy: 0.8755, F1 Micro: 0.6886, F1 Macro: 0.6828\n",
      "Model 2 - Iteration 388: Accuracy: 0.8733, F1 Micro: 0.6915, F1 Macro: 0.6872\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.60      0.54      0.57       248\n",
      "         radikalisme       0.67      0.74      0.70       243\n",
      "pencemaran_nama_baik       0.68      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.72      0.67      0.69      1365\n",
      "           macro avg       0.71      0.67      0.69      1365\n",
      "        weighted avg       0.72      0.67      0.69      1365\n",
      "         samples avg       0.37      0.37      0.36      1365\n",
      "\n",
      "Training completed in 56.71885085105896 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5583, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.43, Accuracy: 0.7978, F1 Micro: 0.0989, F1 Macro: 0.0805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.363, Accuracy: 0.8225, F1 Micro: 0.2996, F1 Macro: 0.2036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3502, Accuracy: 0.8367, F1 Micro: 0.4397, F1 Macro: 0.3464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2882, Accuracy: 0.85, F1 Micro: 0.5289, F1 Macro: 0.4943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.24, Accuracy: 0.857, F1 Micro: 0.6037, F1 Macro: 0.5962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2211, Accuracy: 0.8647, F1 Micro: 0.6264, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1673, Accuracy: 0.867, F1 Micro: 0.6352, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1339, Accuracy: 0.8692, F1 Micro: 0.6926, F1 Macro: 0.6863\n",
      "Epoch 10/10, Train Loss: 0.1203, Accuracy: 0.8734, F1 Micro: 0.6865, F1 Macro: 0.6788\n",
      "Model 3 - Iteration 388: Accuracy: 0.8692, F1 Micro: 0.6926, F1 Macro: 0.6863\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.85      0.87       370\n",
      "                sara       0.56      0.56      0.56       248\n",
      "         radikalisme       0.67      0.70      0.68       243\n",
      "pencemaran_nama_baik       0.64      0.63      0.64       504\n",
      "\n",
      "           micro avg       0.69      0.69      0.69      1365\n",
      "           macro avg       0.69      0.69      0.69      1365\n",
      "        weighted avg       0.70      0.69      0.69      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 56.45287036895752 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8718, F1 Micro: 0.6921, F1 Macro: 0.6861\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 105.04494524002075 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5575, Accuracy: 0.7958, F1 Micro: 0.0828, F1 Macro: 0.0737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4423, Accuracy: 0.8475, F1 Micro: 0.5091, F1 Macro: 0.4617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3578, Accuracy: 0.8702, F1 Micro: 0.6648, F1 Macro: 0.6615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2758, Accuracy: 0.8834, F1 Micro: 0.6985, F1 Macro: 0.6901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2386, Accuracy: 0.8875, F1 Micro: 0.7134, F1 Macro: 0.7092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1877, Accuracy: 0.8925, F1 Micro: 0.7308, F1 Macro: 0.7267\n",
      "Epoch 7/10, Train Loss: 0.154, Accuracy: 0.8873, F1 Micro: 0.6969, F1 Macro: 0.6879\n",
      "Epoch 8/10, Train Loss: 0.1202, Accuracy: 0.8928, F1 Micro: 0.7258, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.096, Accuracy: 0.8931, F1 Micro: 0.7423, F1 Macro: 0.7362\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.892, F1 Micro: 0.7362, F1 Macro: 0.7303\n",
      "Model 1 - Iteration 971: Accuracy: 0.8931, F1 Micro: 0.7423, F1 Macro: 0.7362\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.83      0.87       370\n",
      "                sara       0.68      0.58      0.63       248\n",
      "         radikalisme       0.73      0.75      0.74       243\n",
      "pencemaran_nama_baik       0.71      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.76      0.72      0.74      1365\n",
      "        weighted avg       0.77      0.72      0.74      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 67.9577989578247 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.573, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.8481, F1 Micro: 0.5277, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3721, Accuracy: 0.8672, F1 Micro: 0.6499, F1 Macro: 0.6471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2872, Accuracy: 0.8714, F1 Micro: 0.652, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2455, Accuracy: 0.8883, F1 Micro: 0.7257, F1 Macro: 0.7242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.196, Accuracy: 0.8922, F1 Micro: 0.7307, F1 Macro: 0.7319\n",
      "Epoch 7/10, Train Loss: 0.1587, Accuracy: 0.8859, F1 Micro: 0.688, F1 Macro: 0.6792\n",
      "Epoch 8/10, Train Loss: 0.1265, Accuracy: 0.8927, F1 Micro: 0.7167, F1 Macro: 0.7081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1027, Accuracy: 0.8916, F1 Micro: 0.7592, F1 Macro: 0.7591\n",
      "Epoch 10/10, Train Loss: 0.0842, Accuracy: 0.8939, F1 Micro: 0.7292, F1 Macro: 0.7245\n",
      "Model 2 - Iteration 971: Accuracy: 0.8916, F1 Micro: 0.7592, F1 Macro: 0.7591\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.83      0.87       370\n",
      "                sara       0.61      0.75      0.67       248\n",
      "         radikalisme       0.72      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.80      0.76      1365\n",
      "           macro avg       0.73      0.80      0.76      1365\n",
      "        weighted avg       0.73      0.80      0.76      1365\n",
      "         samples avg       0.43      0.45      0.43      1365\n",
      "\n",
      "Training completed in 67.89778184890747 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.547, Accuracy: 0.8027, F1 Micro: 0.1483, F1 Macro: 0.1163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4348, Accuracy: 0.8486, F1 Micro: 0.5138, F1 Macro: 0.4671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3483, Accuracy: 0.8686, F1 Micro: 0.6651, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2683, Accuracy: 0.8794, F1 Micro: 0.6737, F1 Macro: 0.6633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2288, Accuracy: 0.8913, F1 Micro: 0.7358, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1835, Accuracy: 0.8928, F1 Micro: 0.7417, F1 Macro: 0.7376\n",
      "Epoch 7/10, Train Loss: 0.1429, Accuracy: 0.8911, F1 Micro: 0.7152, F1 Macro: 0.7076\n",
      "Epoch 8/10, Train Loss: 0.1163, Accuracy: 0.8902, F1 Micro: 0.7191, F1 Macro: 0.7134\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.8928, F1 Micro: 0.7347, F1 Macro: 0.7306\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.8911, F1 Micro: 0.7233, F1 Macro: 0.7183\n",
      "Model 3 - Iteration 971: Accuracy: 0.8928, F1 Micro: 0.7417, F1 Macro: 0.7376\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.82      0.87       370\n",
      "                sara       0.67      0.61      0.64       248\n",
      "         radikalisme       0.71      0.78      0.74       243\n",
      "pencemaran_nama_baik       0.73      0.67      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.76      0.72      0.74      1365\n",
      "        weighted avg       0.77      0.72      0.74      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 66.31933307647705 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8821, F1 Micro: 0.7199, F1 Macro: 0.7152\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 93.53473663330078 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.8317, F1 Micro: 0.5656, F1 Macro: 0.4912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.414, Accuracy: 0.8719, F1 Micro: 0.6787, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3398, Accuracy: 0.8872, F1 Micro: 0.7171, F1 Macro: 0.7108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2692, Accuracy: 0.8927, F1 Micro: 0.7418, F1 Macro: 0.7283\n",
      "Epoch 5/10, Train Loss: 0.2252, Accuracy: 0.8917, F1 Micro: 0.7216, F1 Macro: 0.7107\n",
      "Epoch 6/10, Train Loss: 0.1623, Accuracy: 0.8919, F1 Micro: 0.7159, F1 Macro: 0.7076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1366, Accuracy: 0.8936, F1 Micro: 0.7445, F1 Macro: 0.7376\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.8919, F1 Micro: 0.7387, F1 Macro: 0.7309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.8936, F1 Micro: 0.7537, F1 Macro: 0.7524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.8923, F1 Micro: 0.7578, F1 Macro: 0.7561\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8923, F1 Micro: 0.7578, F1 Macro: 0.7561\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.72      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1365\n",
      "           macro avg       0.73      0.78      0.76      1365\n",
      "        weighted avg       0.73      0.79      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 81.54963874816895 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5581, Accuracy: 0.8173, F1 Micro: 0.4099, F1 Macro: 0.2864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4279, Accuracy: 0.8712, F1 Micro: 0.6831, F1 Macro: 0.6772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.343, Accuracy: 0.8873, F1 Micro: 0.7193, F1 Macro: 0.7103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2701, Accuracy: 0.892, F1 Micro: 0.7263, F1 Macro: 0.71\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.232, Accuracy: 0.8928, F1 Micro: 0.7293, F1 Macro: 0.7215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1692, Accuracy: 0.8944, F1 Micro: 0.7414, F1 Macro: 0.7381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1267, Accuracy: 0.8955, F1 Micro: 0.7514, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1006, Accuracy: 0.8958, F1 Micro: 0.7629, F1 Macro: 0.7578\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.8972, F1 Micro: 0.7568, F1 Macro: 0.7505\n",
      "Epoch 10/10, Train Loss: 0.0583, Accuracy: 0.8958, F1 Micro: 0.7593, F1 Macro: 0.7557\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8958, F1 Micro: 0.7629, F1 Macro: 0.7578\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.89      0.88       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.78      0.74      0.76       243\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.75      0.77      0.76      1365\n",
      "        weighted avg       0.75      0.79      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 82.9183144569397 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5241, Accuracy: 0.8391, F1 Micro: 0.5893, F1 Macro: 0.5101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4103, Accuracy: 0.8716, F1 Micro: 0.6851, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3338, Accuracy: 0.8897, F1 Micro: 0.728, F1 Macro: 0.719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2571, Accuracy: 0.8922, F1 Micro: 0.7292, F1 Macro: 0.7079\n",
      "Epoch 5/10, Train Loss: 0.2208, Accuracy: 0.8902, F1 Micro: 0.7155, F1 Macro: 0.7075\n",
      "Epoch 6/10, Train Loss: 0.1571, Accuracy: 0.893, F1 Micro: 0.7237, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.136, Accuracy: 0.8973, F1 Micro: 0.7488, F1 Macro: 0.7451\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.895, F1 Micro: 0.7419, F1 Macro: 0.7377\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.8923, F1 Micro: 0.7387, F1 Macro: 0.7367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.8923, F1 Micro: 0.7524, F1 Macro: 0.7523\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8923, F1 Micro: 0.7524, F1 Macro: 0.7523\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.87      0.88       370\n",
      "                sara       0.65      0.68      0.67       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.72      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.77      0.75      1365\n",
      "           macro avg       0.74      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 79.02367496490479 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8859, F1 Micro: 0.7325, F1 Macro: 0.7286\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 84.91812753677368 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.8464, F1 Micro: 0.561, F1 Macro: 0.5281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3901, Accuracy: 0.8841, F1 Micro: 0.7252, F1 Macro: 0.716\n",
      "Epoch 3/10, Train Loss: 0.3189, Accuracy: 0.8895, F1 Micro: 0.7157, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2754, Accuracy: 0.8945, F1 Micro: 0.7452, F1 Macro: 0.7386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2179, Accuracy: 0.8944, F1 Micro: 0.761, F1 Macro: 0.7629\n",
      "Epoch 6/10, Train Loss: 0.1724, Accuracy: 0.8958, F1 Micro: 0.7484, F1 Macro: 0.7421\n",
      "Epoch 7/10, Train Loss: 0.1273, Accuracy: 0.8988, F1 Micro: 0.7538, F1 Macro: 0.7499\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.8956, F1 Micro: 0.7289, F1 Macro: 0.7196\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.8983, F1 Micro: 0.7581, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.8991, F1 Micro: 0.7695, F1 Macro: 0.7663\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8991, F1 Micro: 0.7695, F1 Macro: 0.7663\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.70      0.64      0.67       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.44      0.44      0.44      1365\n",
      "\n",
      "Training completed in 88.9833734035492 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5462, Accuracy: 0.85, F1 Micro: 0.5652, F1 Macro: 0.5225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3977, Accuracy: 0.8791, F1 Micro: 0.7068, F1 Macro: 0.7077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3163, Accuracy: 0.8941, F1 Micro: 0.732, F1 Macro: 0.7305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.8983, F1 Micro: 0.7559, F1 Macro: 0.756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2152, Accuracy: 0.8945, F1 Micro: 0.7716, F1 Macro: 0.7727\n",
      "Epoch 6/10, Train Loss: 0.1709, Accuracy: 0.8994, F1 Micro: 0.7588, F1 Macro: 0.7543\n",
      "Epoch 7/10, Train Loss: 0.1269, Accuracy: 0.9003, F1 Micro: 0.7572, F1 Macro: 0.7538\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.8986, F1 Micro: 0.729, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9017, F1 Micro: 0.7643, F1 Macro: 0.7612\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9005, F1 Micro: 0.7688, F1 Macro: 0.7655\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8945, F1 Micro: 0.7716, F1 Macro: 0.7727\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.89       370\n",
      "                sara       0.62      0.77      0.69       248\n",
      "         radikalisme       0.72      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.84      0.77      1365\n",
      "           macro avg       0.72      0.83      0.77      1365\n",
      "        weighted avg       0.73      0.84      0.78      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 89.73425912857056 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5162, Accuracy: 0.8508, F1 Micro: 0.6003, F1 Macro: 0.5422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.384, Accuracy: 0.8852, F1 Micro: 0.7318, F1 Macro: 0.7233\n",
      "Epoch 3/10, Train Loss: 0.3064, Accuracy: 0.8909, F1 Micro: 0.7208, F1 Macro: 0.7199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2659, Accuracy: 0.8964, F1 Micro: 0.7529, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2071, Accuracy: 0.8948, F1 Micro: 0.7654, F1 Macro: 0.7656\n",
      "Epoch 6/10, Train Loss: 0.1672, Accuracy: 0.8947, F1 Micro: 0.7601, F1 Macro: 0.7575\n",
      "Epoch 7/10, Train Loss: 0.1265, Accuracy: 0.8995, F1 Micro: 0.7556, F1 Macro: 0.7477\n",
      "Epoch 8/10, Train Loss: 0.0952, Accuracy: 0.8953, F1 Micro: 0.7296, F1 Macro: 0.7231\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.8963, F1 Micro: 0.7456, F1 Macro: 0.7416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.8981, F1 Micro: 0.7655, F1 Macro: 0.7644\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8981, F1 Micro: 0.7655, F1 Macro: 0.7644\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.76      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 89.60969877243042 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8888, F1 Micro: 0.7416, F1 Macro: 0.7384\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 76.76595902442932 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5175, Accuracy: 0.8503, F1 Micro: 0.5186, F1 Macro: 0.4454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3761, Accuracy: 0.8853, F1 Micro: 0.7228, F1 Macro: 0.7213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.8925, F1 Micro: 0.7394, F1 Macro: 0.7364\n",
      "Epoch 4/10, Train Loss: 0.2559, Accuracy: 0.8891, F1 Micro: 0.6921, F1 Macro: 0.6594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2138, Accuracy: 0.9009, F1 Micro: 0.7617, F1 Macro: 0.7606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1602, Accuracy: 0.902, F1 Micro: 0.7691, F1 Macro: 0.7683\n",
      "Epoch 7/10, Train Loss: 0.1168, Accuracy: 0.897, F1 Micro: 0.7674, F1 Macro: 0.7675\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9019, F1 Micro: 0.7683, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.8992, F1 Micro: 0.7687, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9011, F1 Micro: 0.7706, F1 Macro: 0.771\n",
      "Model 1 - Iteration 2394: Accuracy: 0.9011, F1 Micro: 0.7706, F1 Macro: 0.771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.86      0.89       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.43      0.43      0.43      1365\n",
      "\n",
      "Training completed in 102.17593240737915 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.8511, F1 Micro: 0.5468, F1 Macro: 0.4921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3791, Accuracy: 0.8853, F1 Micro: 0.7213, F1 Macro: 0.7176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3111, Accuracy: 0.8986, F1 Micro: 0.7556, F1 Macro: 0.7525\n",
      "Epoch 4/10, Train Loss: 0.2566, Accuracy: 0.8941, F1 Micro: 0.7065, F1 Macro: 0.682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2074, Accuracy: 0.9034, F1 Micro: 0.7586, F1 Macro: 0.7516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1632, Accuracy: 0.9038, F1 Micro: 0.7763, F1 Macro: 0.7737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.123, Accuracy: 0.9052, F1 Micro: 0.7795, F1 Macro: 0.7762\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.902, F1 Micro: 0.7765, F1 Macro: 0.7757\n",
      "Epoch 9/10, Train Loss: 0.0686, Accuracy: 0.9019, F1 Micro: 0.7585, F1 Macro: 0.7559\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9027, F1 Micro: 0.7619, F1 Macro: 0.759\n",
      "Model 2 - Iteration 2394: Accuracy: 0.9052, F1 Micro: 0.7795, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 103.78093791007996 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5099, Accuracy: 0.8508, F1 Micro: 0.5279, F1 Macro: 0.4579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3693, Accuracy: 0.89, F1 Micro: 0.7335, F1 Macro: 0.7296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3037, Accuracy: 0.898, F1 Micro: 0.7586, F1 Macro: 0.7567\n",
      "Epoch 4/10, Train Loss: 0.2505, Accuracy: 0.8922, F1 Micro: 0.7064, F1 Macro: 0.681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2109, Accuracy: 0.9038, F1 Micro: 0.766, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1616, Accuracy: 0.903, F1 Micro: 0.7754, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1174, Accuracy: 0.8981, F1 Micro: 0.7763, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.8986, F1 Micro: 0.7735, F1 Macro: 0.776\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9022, F1 Micro: 0.7618, F1 Macro: 0.7607\n",
      "Epoch 10/10, Train Loss: 0.0534, Accuracy: 0.898, F1 Micro: 0.7649, F1 Macro: 0.7678\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8981, F1 Micro: 0.7763, F1 Macro: 0.7797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.61      0.84      0.70       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 103.33610582351685 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8913, F1 Micro: 0.7484, F1 Macro: 0.7459\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 69.38455319404602 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4913, Accuracy: 0.86, F1 Micro: 0.5931, F1 Macro: 0.543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3599, Accuracy: 0.8917, F1 Micro: 0.754, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2849, Accuracy: 0.8977, F1 Micro: 0.7586, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2317, Accuracy: 0.9017, F1 Micro: 0.7702, F1 Macro: 0.7677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9033, F1 Micro: 0.7774, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.1456, Accuracy: 0.902, F1 Micro: 0.7684, F1 Macro: 0.7681\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.8966, F1 Micro: 0.7768, F1 Macro: 0.7767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9019, F1 Micro: 0.7816, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9017, F1 Micro: 0.7719, F1 Macro: 0.769\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9014, F1 Micro: 0.7645, F1 Macro: 0.7615\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9019, F1 Micro: 0.7816, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 112.95293879508972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5044, Accuracy: 0.8634, F1 Micro: 0.6074, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3607, Accuracy: 0.8903, F1 Micro: 0.7438, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2834, Accuracy: 0.8994, F1 Micro: 0.7692, F1 Macro: 0.7676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2337, Accuracy: 0.902, F1 Micro: 0.7762, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1953, Accuracy: 0.9052, F1 Micro: 0.7811, F1 Macro: 0.7795\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.9055, F1 Micro: 0.7725, F1 Macro: 0.769\n",
      "Epoch 7/10, Train Loss: 0.1075, Accuracy: 0.9006, F1 Micro: 0.7746, F1 Macro: 0.7724\n",
      "Epoch 8/10, Train Loss: 0.0884, Accuracy: 0.9038, F1 Micro: 0.7805, F1 Macro: 0.7777\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.9005, F1 Micro: 0.7636, F1 Macro: 0.7571\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9031, F1 Micro: 0.7688, F1 Macro: 0.7659\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9052, F1 Micro: 0.7811, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 111.60672664642334 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4814, Accuracy: 0.8656, F1 Micro: 0.6248, F1 Macro: 0.5889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.356, Accuracy: 0.8942, F1 Micro: 0.7559, F1 Macro: 0.755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.8991, F1 Micro: 0.7688, F1 Macro: 0.7663\n",
      "Epoch 4/10, Train Loss: 0.2321, Accuracy: 0.902, F1 Micro: 0.7631, F1 Macro: 0.7595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1887, Accuracy: 0.9038, F1 Micro: 0.7808, F1 Macro: 0.7814\n",
      "Epoch 6/10, Train Loss: 0.1404, Accuracy: 0.9039, F1 Micro: 0.7685, F1 Macro: 0.7678\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9005, F1 Micro: 0.7792, F1 Macro: 0.7808\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.7697\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9019, F1 Micro: 0.7736, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9031, F1 Micro: 0.7707, F1 Macro: 0.7667\n",
      "Model 3 - Iteration 2777: Accuracy: 0.9038, F1 Micro: 0.7808, F1 Macro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.67      0.75      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 110.67192697525024 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8933, F1 Micro: 0.7538, F1 Macro: 0.7517\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 62.489505767822266 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4821, Accuracy: 0.8545, F1 Micro: 0.5871, F1 Macro: 0.5574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3462, Accuracy: 0.8925, F1 Micro: 0.7516, F1 Macro: 0.7484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2823, Accuracy: 0.8977, F1 Micro: 0.7631, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.236, Accuracy: 0.8967, F1 Micro: 0.7739, F1 Macro: 0.7747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.208, Accuracy: 0.9047, F1 Micro: 0.7809, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9064, F1 Micro: 0.7816, F1 Macro: 0.7802\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9017, F1 Micro: 0.7789, F1 Macro: 0.7806\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9022, F1 Micro: 0.7654, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9009, F1 Micro: 0.7529, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9033, F1 Micro: 0.7827, F1 Macro: 0.7818\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9033, F1 Micro: 0.7827, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 124.3726224899292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.492, Accuracy: 0.8637, F1 Micro: 0.6179, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.342, Accuracy: 0.8916, F1 Micro: 0.7465, F1 Macro: 0.7457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2773, Accuracy: 0.9011, F1 Micro: 0.7722, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2353, Accuracy: 0.9042, F1 Micro: 0.7819, F1 Macro: 0.7821\n",
      "Epoch 5/10, Train Loss: 0.19, Accuracy: 0.9042, F1 Micro: 0.7724, F1 Macro: 0.7702\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9052, F1 Micro: 0.7719, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.8977, F1 Micro: 0.7773, F1 Macro: 0.7791\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9042, F1 Micro: 0.7691, F1 Macro: 0.7683\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.9031, F1 Micro: 0.7606, F1 Macro: 0.7553\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9036, F1 Micro: 0.7747, F1 Macro: 0.7708\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9042, F1 Micro: 0.7819, F1 Macro: 0.7821\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.64      0.76      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 119.79903411865234 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4731, Accuracy: 0.8642, F1 Micro: 0.6405, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3395, Accuracy: 0.8942, F1 Micro: 0.7501, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2777, Accuracy: 0.9, F1 Micro: 0.7676, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2271, Accuracy: 0.8986, F1 Micro: 0.7764, F1 Macro: 0.7757\n",
      "Epoch 5/10, Train Loss: 0.1936, Accuracy: 0.9052, F1 Micro: 0.7736, F1 Macro: 0.7703\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.905, F1 Micro: 0.7755, F1 Macro: 0.7751\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9041, F1 Micro: 0.7744, F1 Macro: 0.7719\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.9042, F1 Micro: 0.7661, F1 Macro: 0.7669\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9023, F1 Micro: 0.7649, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9016, F1 Micro: 0.7768, F1 Macro: 0.7793\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9016, F1 Micro: 0.7768, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 121.43870949745178 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8947, F1 Micro: 0.7576, F1 Macro: 0.7559\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 57.06711673736572 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4682, Accuracy: 0.8586, F1 Micro: 0.5869, F1 Macro: 0.5427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3321, Accuracy: 0.893, F1 Micro: 0.7317, F1 Macro: 0.7237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2738, Accuracy: 0.9003, F1 Micro: 0.7721, F1 Macro: 0.766\n",
      "Epoch 4/10, Train Loss: 0.2345, Accuracy: 0.9005, F1 Micro: 0.7679, F1 Macro: 0.7584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1796, Accuracy: 0.9064, F1 Micro: 0.784, F1 Macro: 0.7838\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9034, F1 Micro: 0.7689, F1 Macro: 0.7634\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9023, F1 Micro: 0.7601, F1 Macro: 0.7544\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9022, F1 Micro: 0.7683, F1 Macro: 0.7638\n",
      "Epoch 9/10, Train Loss: 0.0581, Accuracy: 0.9002, F1 Micro: 0.7704, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9006, F1 Micro: 0.758, F1 Macro: 0.7543\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9064, F1 Micro: 0.784, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.70      0.69       248\n",
      "         radikalisme       0.76      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 128.0733778476715 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.8597, F1 Micro: 0.5929, F1 Macro: 0.5519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3318, Accuracy: 0.8966, F1 Micro: 0.746, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2726, Accuracy: 0.9011, F1 Micro: 0.7689, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2319, Accuracy: 0.9008, F1 Micro: 0.7728, F1 Macro: 0.7688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.181, Accuracy: 0.9039, F1 Micro: 0.7743, F1 Macro: 0.7742\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9005, F1 Micro: 0.7597, F1 Macro: 0.7527\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9041, F1 Micro: 0.7669, F1 Macro: 0.7615\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9047, F1 Micro: 0.7652, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.9003, F1 Micro: 0.7788, F1 Macro: 0.7793\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.903, F1 Micro: 0.7733, F1 Macro: 0.7708\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9003, F1 Micro: 0.7788, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 131.04577469825745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4605, Accuracy: 0.8691, F1 Micro: 0.6272, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3278, Accuracy: 0.8934, F1 Micro: 0.7263, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2665, Accuracy: 0.9023, F1 Micro: 0.772, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2272, Accuracy: 0.9056, F1 Micro: 0.7843, F1 Macro: 0.7794\n",
      "Epoch 5/10, Train Loss: 0.1801, Accuracy: 0.905, F1 Micro: 0.7819, F1 Macro: 0.7832\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9055, F1 Micro: 0.7752, F1 Macro: 0.7723\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9038, F1 Micro: 0.7677, F1 Macro: 0.7669\n",
      "Epoch 8/10, Train Loss: 0.0792, Accuracy: 0.9039, F1 Micro: 0.7604, F1 Macro: 0.7538\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9052, F1 Micro: 0.7806, F1 Macro: 0.7822\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9036, F1 Micro: 0.7817, F1 Macro: 0.7818\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9056, F1 Micro: 0.7843, F1 Macro: 0.7794\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.71      0.65      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 128.5399408340454 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8959, F1 Micro: 0.7607, F1 Macro: 0.759\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 50.38427400588989 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4632, Accuracy: 0.8813, F1 Micro: 0.6979, F1 Macro: 0.6945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3368, Accuracy: 0.8911, F1 Micro: 0.731, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2759, Accuracy: 0.9006, F1 Micro: 0.7589, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.228, Accuracy: 0.9086, F1 Micro: 0.7844, F1 Macro: 0.7808\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.9042, F1 Micro: 0.7742, F1 Macro: 0.7723\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.8981, F1 Micro: 0.7411, F1 Macro: 0.727\n",
      "Epoch 7/10, Train Loss: 0.1019, Accuracy: 0.9028, F1 Micro: 0.7771, F1 Macro: 0.7762\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9016, F1 Micro: 0.7539, F1 Macro: 0.7517\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9041, F1 Micro: 0.7841, F1 Macro: 0.7839\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9047, F1 Micro: 0.7686, F1 Macro: 0.7662\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9086, F1 Micro: 0.7844, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.73      0.64      0.68       248\n",
      "         radikalisme       0.78      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1365\n",
      "           macro avg       0.79      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 134.46685004234314 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.476, Accuracy: 0.8686, F1 Micro: 0.6509, F1 Macro: 0.6408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.338, Accuracy: 0.8931, F1 Micro: 0.7334, F1 Macro: 0.7092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2769, Accuracy: 0.903, F1 Micro: 0.7697, F1 Macro: 0.7673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2264, Accuracy: 0.9055, F1 Micro: 0.7704, F1 Macro: 0.7632\n",
      "Epoch 5/10, Train Loss: 0.1781, Accuracy: 0.9023, F1 Micro: 0.756, F1 Macro: 0.7498\n",
      "Epoch 6/10, Train Loss: 0.1377, Accuracy: 0.9019, F1 Micro: 0.7653, F1 Macro: 0.7614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1033, Accuracy: 0.9045, F1 Micro: 0.7748, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0746, Accuracy: 0.9025, F1 Micro: 0.7701, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9033, F1 Micro: 0.7748, F1 Macro: 0.7689\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9033, F1 Micro: 0.7618, F1 Macro: 0.7579\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9033, F1 Micro: 0.7748, F1 Macro: 0.7689\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.66      0.62      0.64       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 137.13067054748535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4567, Accuracy: 0.8817, F1 Micro: 0.7014, F1 Macro: 0.6984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3369, Accuracy: 0.8952, F1 Micro: 0.7386, F1 Macro: 0.72\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2745, Accuracy: 0.9033, F1 Micro: 0.772, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.224, Accuracy: 0.9067, F1 Micro: 0.7775, F1 Macro: 0.771\n",
      "Epoch 5/10, Train Loss: 0.1787, Accuracy: 0.9045, F1 Micro: 0.7729, F1 Macro: 0.7709\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.902, F1 Micro: 0.7604, F1 Macro: 0.752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9013, F1 Micro: 0.7781, F1 Macro: 0.7787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0817, Accuracy: 0.9038, F1 Micro: 0.7809, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0605, Accuracy: 0.9059, F1 Micro: 0.7792, F1 Macro: 0.7766\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9073, F1 Micro: 0.7741, F1 Macro: 0.7734\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9038, F1 Micro: 0.7809, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 137.0312898159027 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8969, F1 Micro: 0.7629, F1 Macro: 0.761\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 46.80260229110718 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4597, Accuracy: 0.8802, F1 Micro: 0.7323, F1 Macro: 0.7317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.324, Accuracy: 0.8939, F1 Micro: 0.7603, F1 Macro: 0.7621\n",
      "Epoch 3/10, Train Loss: 0.2649, Accuracy: 0.9008, F1 Micro: 0.7449, F1 Macro: 0.7332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2208, Accuracy: 0.9016, F1 Micro: 0.7811, F1 Macro: 0.7794\n",
      "Epoch 5/10, Train Loss: 0.1726, Accuracy: 0.8984, F1 Micro: 0.776, F1 Macro: 0.7755\n",
      "Epoch 6/10, Train Loss: 0.1326, Accuracy: 0.9033, F1 Micro: 0.7745, F1 Macro: 0.7712\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9025, F1 Micro: 0.7659, F1 Macro: 0.759\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9022, F1 Micro: 0.7767, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9062, F1 Micro: 0.7753, F1 Macro: 0.7683\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9039, F1 Micro: 0.7689, F1 Macro: 0.7621\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9016, F1 Micro: 0.7811, F1 Macro: 0.7794\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.90       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 137.33528900146484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4734, Accuracy: 0.8755, F1 Micro: 0.7101, F1 Macro: 0.7139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3231, Accuracy: 0.8983, F1 Micro: 0.7652, F1 Macro: 0.7652\n",
      "Epoch 3/10, Train Loss: 0.2641, Accuracy: 0.9013, F1 Micro: 0.7462, F1 Macro: 0.737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2205, Accuracy: 0.9017, F1 Micro: 0.7798, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1703, Accuracy: 0.9008, F1 Micro: 0.7793, F1 Macro: 0.7766\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9013, F1 Micro: 0.7752, F1 Macro: 0.7744\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9045, F1 Micro: 0.7678, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9041, F1 Micro: 0.7787, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9048, F1 Micro: 0.7728, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9036, F1 Micro: 0.7671, F1 Macro: 0.7642\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9017, F1 Micro: 0.7798, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.67      0.73      0.69       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 138.29288172721863 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4533, Accuracy: 0.8747, F1 Micro: 0.7142, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3212, Accuracy: 0.8964, F1 Micro: 0.7673, F1 Macro: 0.7671\n",
      "Epoch 3/10, Train Loss: 0.2627, Accuracy: 0.905, F1 Micro: 0.7669, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2207, Accuracy: 0.8986, F1 Micro: 0.7816, F1 Macro: 0.782\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.898, F1 Micro: 0.7808, F1 Macro: 0.7816\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9019, F1 Micro: 0.7773, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.1026, Accuracy: 0.9028, F1 Micro: 0.7688, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0771, Accuracy: 0.9041, F1 Micro: 0.7832, F1 Macro: 0.7808\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.907, F1 Micro: 0.7792, F1 Macro: 0.7812\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9034, F1 Micro: 0.7779, F1 Macro: 0.7761\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9041, F1 Micro: 0.7832, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 139.7717261314392 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8975, F1 Micro: 0.7647, F1 Macro: 0.7629\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 45.78180003166199 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4512, Accuracy: 0.8736, F1 Micro: 0.6625, F1 Macro: 0.6552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3197, Accuracy: 0.8966, F1 Micro: 0.7563, F1 Macro: 0.746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9002, F1 Micro: 0.7584, F1 Macro: 0.7548\n",
      "Epoch 4/10, Train Loss: 0.2212, Accuracy: 0.9014, F1 Micro: 0.7553, F1 Macro: 0.7429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1681, Accuracy: 0.9019, F1 Micro: 0.7653, F1 Macro: 0.7604\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9028, F1 Micro: 0.757, F1 Macro: 0.7463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0969, Accuracy: 0.9039, F1 Micro: 0.768, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.9027, F1 Micro: 0.7652, F1 Macro: 0.7585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9028, F1 Micro: 0.7698, F1 Macro: 0.7641\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.902, F1 Micro: 0.7565, F1 Macro: 0.75\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9028, F1 Micro: 0.7698, F1 Macro: 0.7641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.60      0.65       248\n",
      "         radikalisme       0.77      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1365\n",
      "           macro avg       0.77      0.76      0.76      1365\n",
      "        weighted avg       0.78      0.76      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 150.9130837917328 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4663, Accuracy: 0.8761, F1 Micro: 0.6722, F1 Macro: 0.6676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3178, Accuracy: 0.9023, F1 Micro: 0.7681, F1 Macro: 0.7607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2535, Accuracy: 0.9038, F1 Micro: 0.7717, F1 Macro: 0.7684\n",
      "Epoch 4/10, Train Loss: 0.2165, Accuracy: 0.903, F1 Micro: 0.7656, F1 Macro: 0.7582\n",
      "Epoch 5/10, Train Loss: 0.1674, Accuracy: 0.9019, F1 Micro: 0.7684, F1 Macro: 0.7674\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9053, F1 Micro: 0.7676, F1 Macro: 0.7609\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9058, F1 Micro: 0.7678, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9025, F1 Micro: 0.7718, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.903, F1 Micro: 0.7761, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9016, F1 Micro: 0.7682, F1 Macro: 0.76\n",
      "Model 2 - Iteration 4120: Accuracy: 0.903, F1 Micro: 0.7761, F1 Macro: 0.7705\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.67      0.62      0.65       248\n",
      "         radikalisme       0.80      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 149.22459292411804 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4448, Accuracy: 0.8806, F1 Micro: 0.6859, F1 Macro: 0.6834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3159, Accuracy: 0.8992, F1 Micro: 0.7619, F1 Macro: 0.7523\n",
      "Epoch 3/10, Train Loss: 0.2541, Accuracy: 0.903, F1 Micro: 0.7592, F1 Macro: 0.7544\n",
      "Epoch 4/10, Train Loss: 0.2164, Accuracy: 0.903, F1 Micro: 0.7618, F1 Macro: 0.756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1694, Accuracy: 0.9023, F1 Micro: 0.7696, F1 Macro: 0.7654\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9036, F1 Micro: 0.7593, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9039, F1 Micro: 0.7748, F1 Macro: 0.7743\n",
      "Epoch 8/10, Train Loss: 0.0751, Accuracy: 0.8978, F1 Micro: 0.7606, F1 Macro: 0.7558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9019, F1 Micro: 0.7771, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9016, F1 Micro: 0.7724, F1 Macro: 0.7712\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9019, F1 Micro: 0.7771, F1 Macro: 0.7747\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.77      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 149.28352856636047 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8979, F1 Micro: 0.7656, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.36605787277222 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4421, Accuracy: 0.8825, F1 Micro: 0.6905, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3057, Accuracy: 0.8972, F1 Micro: 0.7496, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2677, Accuracy: 0.9038, F1 Micro: 0.7663, F1 Macro: 0.7601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2123, Accuracy: 0.9013, F1 Micro: 0.7775, F1 Macro: 0.7755\n",
      "Epoch 5/10, Train Loss: 0.172, Accuracy: 0.9003, F1 Micro: 0.7752, F1 Macro: 0.771\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9048, F1 Micro: 0.7624, F1 Macro: 0.7523\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.8995, F1 Micro: 0.7647, F1 Macro: 0.7624\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9039, F1 Micro: 0.7723, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.9055, F1 Micro: 0.7788, F1 Macro: 0.7751\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9033, F1 Micro: 0.7733, F1 Macro: 0.772\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9055, F1 Micro: 0.7788, F1 Macro: 0.7751\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.79      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 155.05756664276123 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4511, Accuracy: 0.8697, F1 Micro: 0.6345, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3064, Accuracy: 0.8994, F1 Micro: 0.7527, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9058, F1 Micro: 0.7747, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9038, F1 Micro: 0.7809, F1 Macro: 0.7776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9041, F1 Micro: 0.781, F1 Macro: 0.7771\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9025, F1 Micro: 0.7679, F1 Macro: 0.7624\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9031, F1 Micro: 0.77, F1 Macro: 0.7665\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.905, F1 Micro: 0.7786, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9017, F1 Micro: 0.7693, F1 Macro: 0.763\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9013, F1 Micro: 0.7676, F1 Macro: 0.7605\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9041, F1 Micro: 0.781, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.67      0.66       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 156.27627110481262 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4352, Accuracy: 0.8784, F1 Micro: 0.6734, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3025, Accuracy: 0.9, F1 Micro: 0.7517, F1 Macro: 0.7397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2625, Accuracy: 0.9059, F1 Micro: 0.7759, F1 Macro: 0.7682\n",
      "Epoch 4/10, Train Loss: 0.2103, Accuracy: 0.9036, F1 Micro: 0.7721, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9031, F1 Micro: 0.7843, F1 Macro: 0.7819\n",
      "Epoch 6/10, Train Loss: 0.1239, Accuracy: 0.9019, F1 Micro: 0.759, F1 Macro: 0.7512\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9045, F1 Micro: 0.7829, F1 Macro: 0.7828\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.9023, F1 Micro: 0.7784, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9047, F1 Micro: 0.7665, F1 Macro: 0.7625\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.902, F1 Micro: 0.7637, F1 Macro: 0.7567\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9031, F1 Micro: 0.7843, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 154.8989896774292 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8985, F1 Micro: 0.7669, F1 Macro: 0.7647\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.007017850875854 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.8838, F1 Micro: 0.6946, F1 Macro: 0.6796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3027, Accuracy: 0.8984, F1 Micro: 0.7455, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2567, Accuracy: 0.9008, F1 Micro: 0.7588, F1 Macro: 0.7406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7812\n",
      "Epoch 5/10, Train Loss: 0.1643, Accuracy: 0.9025, F1 Micro: 0.7708, F1 Macro: 0.7655\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9041, F1 Micro: 0.7777, F1 Macro: 0.7701\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9047, F1 Micro: 0.7774, F1 Macro: 0.7728\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9058, F1 Micro: 0.7651, F1 Macro: 0.7544\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9062, F1 Micro: 0.7791, F1 Macro: 0.7773\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9044, F1 Micro: 0.7763, F1 Macro: 0.7705\n",
      "Model 1 - Iteration 4530: Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.72      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 157.59040665626526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4449, Accuracy: 0.8808, F1 Micro: 0.6905, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3028, Accuracy: 0.8998, F1 Micro: 0.7491, F1 Macro: 0.7413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2587, Accuracy: 0.9056, F1 Micro: 0.7698, F1 Macro: 0.7548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.206, Accuracy: 0.9066, F1 Micro: 0.7873, F1 Macro: 0.7819\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9031, F1 Micro: 0.7752, F1 Macro: 0.7723\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.903, F1 Micro: 0.7731, F1 Macro: 0.7649\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.902, F1 Micro: 0.7685, F1 Macro: 0.7619\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.8977, F1 Micro: 0.7593, F1 Macro: 0.7538\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9042, F1 Micro: 0.7847, F1 Macro: 0.7822\n",
      "Epoch 10/10, Train Loss: 0.0393, Accuracy: 0.9028, F1 Micro: 0.7735, F1 Macro: 0.7671\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9066, F1 Micro: 0.7873, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 158.3521225452423 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4287, Accuracy: 0.883, F1 Micro: 0.6878, F1 Macro: 0.67\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2995, Accuracy: 0.9002, F1 Micro: 0.7505, F1 Macro: 0.7428\n",
      "Epoch 3/10, Train Loss: 0.2608, Accuracy: 0.9008, F1 Micro: 0.7497, F1 Macro: 0.7354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2051, Accuracy: 0.9038, F1 Micro: 0.7808, F1 Macro: 0.7763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1685, Accuracy: 0.9016, F1 Micro: 0.7853, F1 Macro: 0.7843\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9066, F1 Micro: 0.7779, F1 Macro: 0.7727\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9034, F1 Micro: 0.7746, F1 Macro: 0.7694\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9016, F1 Micro: 0.7748, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9044, F1 Micro: 0.7789, F1 Macro: 0.7804\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.8992, F1 Micro: 0.773, F1 Macro: 0.7722\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9016, F1 Micro: 0.7853, F1 Macro: 0.7843\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.65      0.77      0.71       248\n",
      "         radikalisme       0.69      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 158.30121040344238 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8988, F1 Micro: 0.7684, F1 Macro: 0.7661\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.021963119506836 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4224, Accuracy: 0.8834, F1 Micro: 0.7208, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2918, Accuracy: 0.9011, F1 Micro: 0.7572, F1 Macro: 0.7452\n",
      "Epoch 3/10, Train Loss: 0.2443, Accuracy: 0.8988, F1 Micro: 0.7492, F1 Macro: 0.7463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9053, F1 Micro: 0.7761, F1 Macro: 0.7651\n",
      "Epoch 5/10, Train Loss: 0.1579, Accuracy: 0.9017, F1 Micro: 0.7649, F1 Macro: 0.7567\n",
      "Epoch 6/10, Train Loss: 0.116, Accuracy: 0.9036, F1 Micro: 0.7674, F1 Macro: 0.7608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.902, F1 Micro: 0.7819, F1 Macro: 0.7824\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9027, F1 Micro: 0.7683, F1 Macro: 0.765\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9034, F1 Micro: 0.7699, F1 Macro: 0.7644\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.9038, F1 Micro: 0.7672, F1 Macro: 0.7563\n",
      "Model 1 - Iteration 4663: Accuracy: 0.902, F1 Micro: 0.7819, F1 Macro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.25163674354553 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4309, Accuracy: 0.8839, F1 Micro: 0.7119, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2929, Accuracy: 0.903, F1 Micro: 0.7631, F1 Macro: 0.7435\n",
      "Epoch 3/10, Train Loss: 0.2439, Accuracy: 0.9022, F1 Micro: 0.7616, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2073, Accuracy: 0.9072, F1 Micro: 0.7865, F1 Macro: 0.7791\n",
      "Epoch 5/10, Train Loss: 0.1596, Accuracy: 0.9027, F1 Micro: 0.7699, F1 Macro: 0.7617\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.9064, F1 Micro: 0.781, F1 Macro: 0.7775\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9002, F1 Micro: 0.7664, F1 Macro: 0.7636\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9048, F1 Micro: 0.771, F1 Macro: 0.766\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.903, F1 Micro: 0.7696, F1 Macro: 0.7602\n",
      "Epoch 10/10, Train Loss: 0.0395, Accuracy: 0.9027, F1 Micro: 0.7809, F1 Macro: 0.7805\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9072, F1 Micro: 0.7865, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.71      0.60      0.65       248\n",
      "         radikalisme       0.74      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 159.30683302879333 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4188, Accuracy: 0.8842, F1 Micro: 0.7236, F1 Macro: 0.7202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2918, Accuracy: 0.9014, F1 Micro: 0.7581, F1 Macro: 0.7439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2436, Accuracy: 0.9013, F1 Micro: 0.764, F1 Macro: 0.7606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2091, Accuracy: 0.9059, F1 Micro: 0.7847, F1 Macro: 0.7783\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9023, F1 Micro: 0.7632, F1 Macro: 0.7547\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9041, F1 Micro: 0.7749, F1 Macro: 0.7705\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.903, F1 Micro: 0.78, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.902, F1 Micro: 0.7769, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9022, F1 Micro: 0.7722, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0398, Accuracy: 0.9017, F1 Micro: 0.7769, F1 Macro: 0.7728\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9059, F1 Micro: 0.7847, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.07908940315247 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8993, F1 Micro: 0.7695, F1 Macro: 0.7671\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.4648540019989 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4195, Accuracy: 0.882, F1 Micro: 0.6969, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2888, Accuracy: 0.8955, F1 Micro: 0.7344, F1 Macro: 0.7212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2416, Accuracy: 0.9014, F1 Micro: 0.7679, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9067, F1 Micro: 0.7721, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.8992, F1 Micro: 0.7799, F1 Macro: 0.782\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.8994, F1 Micro: 0.7442, F1 Macro: 0.732\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9017, F1 Micro: 0.7659, F1 Macro: 0.7603\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9023, F1 Micro: 0.7731, F1 Macro: 0.7674\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9033, F1 Micro: 0.7785, F1 Macro: 0.7766\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.768\n",
      "Model 1 - Iteration 4863: Accuracy: 0.8992, F1 Micro: 0.7799, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.79      0.79      0.79       243\n",
      "pencemaran_nama_baik       0.63      0.87      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 168.22603678703308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4323, Accuracy: 0.8817, F1 Micro: 0.6794, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.8966, F1 Micro: 0.7313, F1 Macro: 0.7071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2392, Accuracy: 0.9047, F1 Micro: 0.7707, F1 Macro: 0.7631\n",
      "Epoch 4/10, Train Loss: 0.1967, Accuracy: 0.9062, F1 Micro: 0.7656, F1 Macro: 0.7589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1462, Accuracy: 0.9036, F1 Micro: 0.7827, F1 Macro: 0.7788\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9039, F1 Micro: 0.7612, F1 Macro: 0.7495\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9047, F1 Micro: 0.7803, F1 Macro: 0.773\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9038, F1 Micro: 0.7805, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9067, F1 Micro: 0.7798, F1 Macro: 0.7756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9077, F1 Micro: 0.7876, F1 Macro: 0.7839\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9077, F1 Micro: 0.7876, F1 Macro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.70      0.69      0.69       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 169.3315784931183 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4137, Accuracy: 0.8866, F1 Micro: 0.7199, F1 Macro: 0.701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.287, Accuracy: 0.8998, F1 Micro: 0.7511, F1 Macro: 0.7369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2391, Accuracy: 0.9009, F1 Micro: 0.7624, F1 Macro: 0.7521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.19, Accuracy: 0.9064, F1 Micro: 0.7737, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9005, F1 Micro: 0.7798, F1 Macro: 0.7786\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.9033, F1 Micro: 0.7564, F1 Macro: 0.7433\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9042, F1 Micro: 0.775, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9064, F1 Micro: 0.7826, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.907, F1 Micro: 0.7834, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0401, Accuracy: 0.9008, F1 Micro: 0.7787, F1 Macro: 0.7771\n",
      "Model 3 - Iteration 4863: Accuracy: 0.907, F1 Micro: 0.7834, F1 Macro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.80      0.76      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 172.7453978061676 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8996, F1 Micro: 0.7704, F1 Macro: 0.7681\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 26.682629823684692 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4054, Accuracy: 0.8881, F1 Micro: 0.743, F1 Macro: 0.736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.278, Accuracy: 0.897, F1 Micro: 0.7527, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7795\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9045, F1 Micro: 0.7663, F1 Macro: 0.7636\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.9019, F1 Micro: 0.7688, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.8931, F1 Micro: 0.7718, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9027, F1 Micro: 0.7804, F1 Macro: 0.7788\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.902, F1 Micro: 0.7702, F1 Macro: 0.7654\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9022, F1 Micro: 0.7697, F1 Macro: 0.7649\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9036, F1 Micro: 0.7754, F1 Macro: 0.7706\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 171.6933069229126 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4126, Accuracy: 0.8866, F1 Micro: 0.7495, F1 Macro: 0.7478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2792, Accuracy: 0.8997, F1 Micro: 0.766, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2323, Accuracy: 0.9069, F1 Micro: 0.7906, F1 Macro: 0.7891\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.9045, F1 Micro: 0.7686, F1 Macro: 0.7643\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9077, F1 Micro: 0.7801, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.8977, F1 Micro: 0.782, F1 Macro: 0.7858\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9056, F1 Micro: 0.783, F1 Macro: 0.7788\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9022, F1 Micro: 0.7774, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9044, F1 Micro: 0.7781, F1 Macro: 0.771\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9023, F1 Micro: 0.7757, F1 Macro: 0.7691\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9069, F1 Micro: 0.7906, F1 Macro: 0.7891\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.76      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 171.84688305854797 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3991, Accuracy: 0.8883, F1 Micro: 0.7429, F1 Macro: 0.7354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.9006, F1 Micro: 0.763, F1 Macro: 0.7589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9062, F1 Micro: 0.7883, F1 Macro: 0.7833\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.9081, F1 Micro: 0.7804, F1 Macro: 0.7772\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9061, F1 Micro: 0.776, F1 Macro: 0.768\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.8963, F1 Micro: 0.7758, F1 Macro: 0.7802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.9064, F1 Micro: 0.7886, F1 Macro: 0.7887\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9042, F1 Micro: 0.7778, F1 Macro: 0.7764\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9061, F1 Micro: 0.7757, F1 Macro: 0.7693\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9041, F1 Micro: 0.7749, F1 Macro: 0.7705\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9064, F1 Micro: 0.7886, F1 Macro: 0.7887\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 173.54013347625732 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9001, F1 Micro: 0.7715, F1 Macro: 0.7692\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.311677932739258 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3942, Accuracy: 0.8891, F1 Micro: 0.7349, F1 Macro: 0.7279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.8977, F1 Micro: 0.7495, F1 Macro: 0.7429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.9017, F1 Micro: 0.7676, F1 Macro: 0.7547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1882, Accuracy: 0.9081, F1 Micro: 0.7824, F1 Macro: 0.7776\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9033, F1 Micro: 0.7703, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9027, F1 Micro: 0.7661, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9066, F1 Micro: 0.7832, F1 Macro: 0.7788\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9033, F1 Micro: 0.7663, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.8972, F1 Micro: 0.7795, F1 Macro: 0.7783\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9011, F1 Micro: 0.7794, F1 Macro: 0.773\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9066, F1 Micro: 0.7832, F1 Macro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.78      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 179.946613073349 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4042, Accuracy: 0.8877, F1 Micro: 0.7282, F1 Macro: 0.7258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2746, Accuracy: 0.9036, F1 Micro: 0.7549, F1 Macro: 0.7482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9031, F1 Micro: 0.7695, F1 Macro: 0.7532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1869, Accuracy: 0.907, F1 Micro: 0.7842, F1 Macro: 0.7802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1414, Accuracy: 0.9067, F1 Micro: 0.7856, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.9066, F1 Micro: 0.7833, F1 Macro: 0.7773\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9027, F1 Micro: 0.7792, F1 Macro: 0.7761\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9041, F1 Micro: 0.7795, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9008, F1 Micro: 0.782, F1 Macro: 0.7816\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.9036, F1 Micro: 0.7676, F1 Macro: 0.7587\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9067, F1 Micro: 0.7856, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.69      0.62      0.66       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 179.5296392440796 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8919, F1 Micro: 0.7433, F1 Macro: 0.734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2745, Accuracy: 0.9014, F1 Micro: 0.755, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.905, F1 Micro: 0.7781, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9075, F1 Micro: 0.7784, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1403, Accuracy: 0.9059, F1 Micro: 0.7809, F1 Macro: 0.7752\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9052, F1 Micro: 0.7739, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9039, F1 Micro: 0.7841, F1 Macro: 0.7811\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9062, F1 Micro: 0.7763, F1 Macro: 0.7714\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9027, F1 Micro: 0.7815, F1 Macro: 0.7794\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9052, F1 Micro: 0.7819, F1 Macro: 0.7784\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9039, F1 Micro: 0.7841, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.77      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 182.49337100982666 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9004, F1 Micro: 0.7723, F1 Macro: 0.7698\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 18.48316764831543 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8891, F1 Micro: 0.725, F1 Macro: 0.7205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2695, Accuracy: 0.9, F1 Micro: 0.7729, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.9028, F1 Micro: 0.7821, F1 Macro: 0.7819\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.9036, F1 Micro: 0.7633, F1 Macro: 0.7512\n",
      "Epoch 5/10, Train Loss: 0.1305, Accuracy: 0.9052, F1 Micro: 0.7787, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9066, F1 Micro: 0.7832, F1 Macro: 0.7755\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9048, F1 Micro: 0.7762, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9039, F1 Micro: 0.7844, F1 Macro: 0.7802\n",
      "Epoch 9/10, Train Loss: 0.0441, Accuracy: 0.9062, F1 Micro: 0.7835, F1 Macro: 0.7806\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9052, F1 Micro: 0.7774, F1 Macro: 0.7735\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9039, F1 Micro: 0.7844, F1 Macro: 0.7802\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.73      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 186.5064251422882 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3983, Accuracy: 0.8902, F1 Micro: 0.7242, F1 Macro: 0.7185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2694, Accuracy: 0.9031, F1 Micro: 0.7794, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9073, F1 Micro: 0.7825, F1 Macro: 0.7784\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.9034, F1 Micro: 0.7528, F1 Macro: 0.7341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.908, F1 Micro: 0.7846, F1 Macro: 0.7796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9061, F1 Micro: 0.785, F1 Macro: 0.7792\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9102, F1 Micro: 0.7839, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9066, F1 Micro: 0.7878, F1 Macro: 0.7835\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.9053, F1 Micro: 0.7767, F1 Macro: 0.7701\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.9053, F1 Micro: 0.7809, F1 Macro: 0.7761\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9066, F1 Micro: 0.7878, F1 Macro: 0.7835\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 186.96910429000854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3837, Accuracy: 0.8888, F1 Micro: 0.7186, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2671, Accuracy: 0.9023, F1 Micro: 0.7786, F1 Macro: 0.7757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2186, Accuracy: 0.9044, F1 Micro: 0.7806, F1 Macro: 0.7761\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9059, F1 Micro: 0.7735, F1 Macro: 0.7638\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.9056, F1 Micro: 0.7751, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9069, F1 Micro: 0.7859, F1 Macro: 0.7813\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9067, F1 Micro: 0.7783, F1 Macro: 0.7717\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9048, F1 Micro: 0.7785, F1 Macro: 0.7737\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9047, F1 Micro: 0.7691, F1 Macro: 0.7654\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9036, F1 Micro: 0.7828, F1 Macro: 0.7818\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9069, F1 Micro: 0.7859, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.68      0.68       248\n",
      "         radikalisme       0.78      0.79      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 183.23208689689636 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9007, F1 Micro: 0.7731, F1 Macro: 0.7704\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.692224502563477 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3872, Accuracy: 0.8869, F1 Micro: 0.703, F1 Macro: 0.6772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9014, F1 Micro: 0.7688, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9047, F1 Micro: 0.7872, F1 Macro: 0.7832\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.8989, F1 Micro: 0.7834, F1 Macro: 0.7837\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.902, F1 Micro: 0.7762, F1 Macro: 0.7693\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.8958, F1 Micro: 0.7759, F1 Macro: 0.7714\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9025, F1 Micro: 0.7734, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9005, F1 Micro: 0.7654, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0354, Accuracy: 0.9034, F1 Micro: 0.7821, F1 Macro: 0.7816\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9008, F1 Micro: 0.7722, F1 Macro: 0.7661\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9047, F1 Micro: 0.7872, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 187.93737936019897 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8883, F1 Micro: 0.7099, F1 Macro: 0.6861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.9039, F1 Micro: 0.767, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.211, Accuracy: 0.9066, F1 Micro: 0.7825, F1 Macro: 0.777\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.8994, F1 Micro: 0.782, F1 Macro: 0.7801\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9045, F1 Micro: 0.7803, F1 Macro: 0.7753\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.8975, F1 Micro: 0.7787, F1 Macro: 0.7774\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9048, F1 Micro: 0.7781, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9031, F1 Micro: 0.7707, F1 Macro: 0.7587\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.907, F1 Micro: 0.7794, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9017, F1 Micro: 0.7728, F1 Macro: 0.7689\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9066, F1 Micro: 0.7825, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.76      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 187.93796348571777 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3814, Accuracy: 0.8908, F1 Micro: 0.7125, F1 Macro: 0.6836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.9023, F1 Micro: 0.7677, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9009, F1 Micro: 0.7824, F1 Macro: 0.7767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9052, F1 Micro: 0.7912, F1 Macro: 0.7912\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9014, F1 Micro: 0.7819, F1 Macro: 0.7791\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9022, F1 Micro: 0.7793, F1 Macro: 0.7776\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9052, F1 Micro: 0.7734, F1 Macro: 0.7717\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9053, F1 Micro: 0.7785, F1 Macro: 0.7752\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.9055, F1 Micro: 0.7802, F1 Macro: 0.7781\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9058, F1 Micro: 0.7734, F1 Macro: 0.7658\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9052, F1 Micro: 0.7912, F1 Macro: 0.7912\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.75      0.71       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 188.83016085624695 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9009, F1 Micro: 0.7738, F1 Macro: 0.7712\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.87458086013794 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3739, Accuracy: 0.8888, F1 Micro: 0.7219, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.8981, F1 Micro: 0.7413, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.9033, F1 Micro: 0.7758, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9072, F1 Micro: 0.7848, F1 Macro: 0.7769\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.9047, F1 Micro: 0.7736, F1 Macro: 0.7711\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.8917, F1 Micro: 0.7706, F1 Macro: 0.7686\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9022, F1 Micro: 0.7819, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.9039, F1 Micro: 0.7793, F1 Macro: 0.775\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9059, F1 Micro: 0.7759, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9028, F1 Micro: 0.7831, F1 Macro: 0.7803\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9072, F1 Micro: 0.7848, F1 Macro: 0.7769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.32652497291565 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.887, F1 Micro: 0.719, F1 Macro: 0.7159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.8998, F1 Micro: 0.7433, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9052, F1 Micro: 0.7696, F1 Macro: 0.764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9097, F1 Micro: 0.7915, F1 Macro: 0.7863\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9094, F1 Micro: 0.7776, F1 Macro: 0.7699\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9077, F1 Micro: 0.7823, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9023, F1 Micro: 0.7718, F1 Macro: 0.7652\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.9011, F1 Micro: 0.7795, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9059, F1 Micro: 0.7737, F1 Macro: 0.7667\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.902, F1 Micro: 0.7762, F1 Macro: 0.7704\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9097, F1 Micro: 0.7915, F1 Macro: 0.7863\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.67      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.77      0.76       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.53902626037598 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3683, Accuracy: 0.8895, F1 Micro: 0.7207, F1 Macro: 0.7104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2467, Accuracy: 0.9009, F1 Micro: 0.7571, F1 Macro: 0.7443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9045, F1 Micro: 0.7721, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.9061, F1 Micro: 0.7885, F1 Macro: 0.7854\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9039, F1 Micro: 0.7675, F1 Macro: 0.7607\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.903, F1 Micro: 0.7831, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9031, F1 Micro: 0.7699, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0497, Accuracy: 0.9011, F1 Micro: 0.7822, F1 Macro: 0.7828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9092, F1 Micro: 0.7887, F1 Macro: 0.7832\n",
      "Epoch 10/10, Train Loss: 0.0279, Accuracy: 0.9002, F1 Micro: 0.7776, F1 Macro: 0.7753\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9092, F1 Micro: 0.7887, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.69      0.66      0.67       248\n",
      "         radikalisme       0.81      0.78      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.79      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.47      0.45      0.45      1365\n",
      "\n",
      "Training completed in 195.78727674484253 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9013, F1 Micro: 0.7745, F1 Macro: 0.7717\n",
      "Launching training on 2 GPUs.\n",
      "Cluster 197 has no members, skipping.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 199\n",
      "Sampling duration: 7.744882583618164 seconds\n",
      "New train size: 6040\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3586, Accuracy: 0.8881, F1 Micro: 0.7159, F1 Macro: 0.7079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2392, Accuracy: 0.8988, F1 Micro: 0.7755, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1998, Accuracy: 0.9062, F1 Micro: 0.7788, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9084, F1 Micro: 0.7823, F1 Macro: 0.7754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.12, Accuracy: 0.9066, F1 Micro: 0.7858, F1 Macro: 0.7838\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9061, F1 Micro: 0.7825, F1 Macro: 0.7786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9067, F1 Micro: 0.7867, F1 Macro: 0.7826\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9044, F1 Micro: 0.7822, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9044, F1 Micro: 0.7648, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9036, F1 Micro: 0.7879, F1 Macro: 0.7875\n",
      "Model 1 - Iteration 6040: Accuracy: 0.9036, F1 Micro: 0.7879, F1 Macro: 0.7875\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.77      0.71       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 205.46936202049255 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3647, Accuracy: 0.892, F1 Micro: 0.7393, F1 Macro: 0.7363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.9002, F1 Micro: 0.7805, F1 Macro: 0.7786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.9089, F1 Micro: 0.784, F1 Macro: 0.7778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9061, F1 Micro: 0.7883, F1 Macro: 0.7841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9038, F1 Micro: 0.7905, F1 Macro: 0.7889\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9083, F1 Micro: 0.7731, F1 Macro: 0.7643\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9069, F1 Micro: 0.782, F1 Macro: 0.7765\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.9083, F1 Micro: 0.7838, F1 Macro: 0.7752\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.9083, F1 Micro: 0.7769, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9042, F1 Micro: 0.7907, F1 Macro: 0.7894\n",
      "Model 2 - Iteration 6040: Accuracy: 0.9042, F1 Micro: 0.7907, F1 Macro: 0.7894\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.72      0.89      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 203.40622544288635 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3521, Accuracy: 0.8919, F1 Micro: 0.7265, F1 Macro: 0.721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2357, Accuracy: 0.9006, F1 Micro: 0.7784, F1 Macro: 0.7718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1997, Accuracy: 0.9061, F1 Micro: 0.7831, F1 Macro: 0.778\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9047, F1 Micro: 0.7821, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9048, F1 Micro: 0.7855, F1 Macro: 0.7848\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9023, F1 Micro: 0.7662, F1 Macro: 0.7585\n",
      "Epoch 7/10, Train Loss: 0.0697, Accuracy: 0.9048, F1 Micro: 0.776, F1 Macro: 0.7742\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9008, F1 Micro: 0.7717, F1 Macro: 0.7686\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.903, F1 Micro: 0.778, F1 Macro: 0.7753\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9002, F1 Micro: 0.7833, F1 Macro: 0.7858\n",
      "Model 3 - Iteration 6040: Accuracy: 0.9048, F1 Micro: 0.7855, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 202.5445854663849 s\n",
      "Averaged - Iteration 6040: Accuracy: 0.9015, F1 Micro: 0.7752, F1 Macro: 0.7724\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 178\n",
      "Sampling duration: 4.123073101043701 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3509, Accuracy: 0.8908, F1 Micro: 0.7345, F1 Macro: 0.724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2311, Accuracy: 0.8981, F1 Micro: 0.7739, F1 Macro: 0.7678\n",
      "Epoch 3/10, Train Loss: 0.1931, Accuracy: 0.9019, F1 Micro: 0.766, F1 Macro: 0.7614\n",
      "Epoch 4/10, Train Loss: 0.1492, Accuracy: 0.9042, F1 Micro: 0.7652, F1 Macro: 0.7561\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9019, F1 Micro: 0.7601, F1 Macro: 0.7407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.9034, F1 Micro: 0.7844, F1 Macro: 0.7817\n",
      "Epoch 7/10, Train Loss: 0.0631, Accuracy: 0.9073, F1 Micro: 0.7798, F1 Macro: 0.7729\n",
      "Epoch 8/10, Train Loss: 0.0459, Accuracy: 0.9055, F1 Micro: 0.7802, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0371, Accuracy: 0.9031, F1 Micro: 0.7794, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0279, Accuracy: 0.9075, F1 Micro: 0.7761, F1 Macro: 0.7701\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9034, F1 Micro: 0.7844, F1 Macro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 202.2023115158081 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3554, Accuracy: 0.8906, F1 Micro: 0.7266, F1 Macro: 0.7038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.9011, F1 Micro: 0.7783, F1 Macro: 0.7755\n",
      "Epoch 3/10, Train Loss: 0.1902, Accuracy: 0.9069, F1 Micro: 0.7771, F1 Macro: 0.7707\n",
      "Epoch 4/10, Train Loss: 0.1506, Accuracy: 0.9073, F1 Micro: 0.7758, F1 Macro: 0.7664\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.903, F1 Micro: 0.7665, F1 Macro: 0.7544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9028, F1 Micro: 0.7839, F1 Macro: 0.7827\n",
      "Epoch 7/10, Train Loss: 0.0633, Accuracy: 0.9058, F1 Micro: 0.7787, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0477, Accuracy: 0.9081, F1 Micro: 0.7943, F1 Macro: 0.7891\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.8984, F1 Micro: 0.7738, F1 Macro: 0.7698\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9055, F1 Micro: 0.7656, F1 Macro: 0.7562\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9081, F1 Micro: 0.7943, F1 Macro: 0.7891\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.94      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 204.73999524116516 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3479, Accuracy: 0.8889, F1 Micro: 0.7202, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2293, Accuracy: 0.8986, F1 Micro: 0.7719, F1 Macro: 0.7635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1906, Accuracy: 0.9073, F1 Micro: 0.7775, F1 Macro: 0.7736\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9042, F1 Micro: 0.7696, F1 Macro: 0.7615\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.9019, F1 Micro: 0.7581, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.7793\n",
      "Epoch 7/10, Train Loss: 0.0616, Accuracy: 0.9011, F1 Micro: 0.7709, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0484, Accuracy: 0.9041, F1 Micro: 0.7887, F1 Macro: 0.7875\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.9027, F1 Micro: 0.7801, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9061, F1 Micro: 0.7772, F1 Macro: 0.7715\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9041, F1 Micro: 0.7887, F1 Macro: 0.7875\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 206.19540214538574 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9016, F1 Micro: 0.7758, F1 Macro: 0.7731\n",
      "Total sampling time: 940.95 seconds\n",
      "Total runtime: 10872.555598020554 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6053344,
     "sourceId": 9862714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35706.944245,
   "end_time": "2025-01-28T23:10:22.094491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-28T13:15:15.150246",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "010d1f94431d44659271af79f5b834a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32576cae0d634c1d94c1aa6fdb36066b",
       "placeholder": "​",
       "style": "IPY_MODEL_6f9e1a32616340b2aa8cb2f722d0c908",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "0375ccda7508418b9160ba39fc7d0c57": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0d15c04ad0884724a148f62b962f4261": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0df16bfd198c449eafefa8f27fbf4af8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "197ab0b2f3164f4cb0f82e60e34c27ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1f9d3cab02a84d3ea83959d940d3858f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_96d15f0d97e645529db590965d912a4a",
       "placeholder": "​",
       "style": "IPY_MODEL_0d15c04ad0884724a148f62b962f4261",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "218ddeb6eef1418e92e22a84a5afd119": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0375ccda7508418b9160ba39fc7d0c57",
       "placeholder": "​",
       "style": "IPY_MODEL_50bf3cb903814e0d8fc08cb049fb592f",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/229k [00:00&lt;00:00, 6.10MB/s]"
      }
     },
     "26fe678aa0f94d2db7e4a793b7931e7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3190dd34c2cd48c9a71e79d8ffd624b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "32576cae0d634c1d94c1aa6fdb36066b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "371630b6074748efba244f35e0560480": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6b25a9939ddd4f77b8aab7a326bdb098",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_58956dcbaa1b4dc69864f80230132c00",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "401220e442b24ab69c9f069f5bae60c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_26fe678aa0f94d2db7e4a793b7931e7e",
       "placeholder": "​",
       "style": "IPY_MODEL_45866dd8a7db4dfbbc0b32fbac749428",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "45866dd8a7db4dfbbc0b32fbac749428": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a27f12fbdea4fa684d85764ed135f73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50bf3cb903814e0d8fc08cb049fb592f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "515e0477df2a4463b135fc938b62cc5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53855225d9ae4624b6f138824bf48062": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff9aab01b06e4ec6be67450c64b0a487",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7ae407cd7efd4a5ba841836764e10221",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "56fd6e8d44d04a548dab86ca22ca3f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ba346ea6565840b9b97cf572d32d14ef",
       "placeholder": "​",
       "style": "IPY_MODEL_b05db150df5a40bca24203ceecdbb7ab",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 170B/s]"
      }
     },
     "58956dcbaa1b4dc69864f80230132c00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5a633aec40874ce0ac449b34658e1ab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c27963541ee841d2ac9e8ba9d3bb16f4",
       "placeholder": "​",
       "style": "IPY_MODEL_197ab0b2f3164f4cb0f82e60e34c27ca",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "6b25a9939ddd4f77b8aab7a326bdb098": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f9e1a32616340b2aa8cb2f722d0c908": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "78e47a470b624438be60c6f8b12fb9f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4a27f12fbdea4fa684d85764ed135f73",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3190dd34c2cd48c9a71e79d8ffd624b1",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "7ae407cd7efd4a5ba841836764e10221": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7cf77bde96174324b951a405e1b21bcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "803e594fcc6e48aeb582150501d2a824": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f6e1f205be8406ea50f137d031705d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96d15f0d97e645529db590965d912a4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9add871305894a72ad61aec7f445cdf9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a3356b799dd6412fb91783a4b16e95f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3fb82da1811490093242c3bf5d9064d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a3356b799dd6412fb91783a4b16e95f8",
       "placeholder": "​",
       "style": "IPY_MODEL_d0cade27c3654d9eaa5907b590aaeac7",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 11.8kB/s]"
      }
     },
     "a71caa6213af4970805848fcf5377a57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af0d822dbdd741509478cc0984e6bff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b215993ae6594a78b1cc99c7a98ad989",
       "placeholder": "​",
       "style": "IPY_MODEL_a71caa6213af4970805848fcf5377a57",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/1.53k [00:00&lt;00:00, 169kB/s]"
      }
     },
     "b05db150df5a40bca24203ceecdbb7ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b215993ae6594a78b1cc99c7a98ad989": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5c29bd3bba449fe80d0d3b5b5c90d04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_401220e442b24ab69c9f069f5bae60c9",
        "IPY_MODEL_371630b6074748efba244f35e0560480",
        "IPY_MODEL_56fd6e8d44d04a548dab86ca22ca3f9a"
       ],
       "layout": "IPY_MODEL_8f6e1f205be8406ea50f137d031705d3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b754f31825d243698e599854e3f328c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1f9d3cab02a84d3ea83959d940d3858f",
        "IPY_MODEL_53855225d9ae4624b6f138824bf48062",
        "IPY_MODEL_218ddeb6eef1418e92e22a84a5afd119"
       ],
       "layout": "IPY_MODEL_515e0477df2a4463b135fc938b62cc5a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ba346ea6565840b9b97cf572d32d14ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c27963541ee841d2ac9e8ba9d3bb16f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca6f186e5c064dcaa15c209d9f764266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_803e594fcc6e48aeb582150501d2a824",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9add871305894a72ad61aec7f445cdf9",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "d0cade27c3654d9eaa5907b590aaeac7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d64932cbea9a4a09ae29f5b5af905665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a633aec40874ce0ac449b34658e1ab3",
        "IPY_MODEL_78e47a470b624438be60c6f8b12fb9f9",
        "IPY_MODEL_af0d822dbdd741509478cc0984e6bff8"
       ],
       "layout": "IPY_MODEL_7cf77bde96174324b951a405e1b21bcd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "efa880c7ffba4b16ac6b77f7dd6d1951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_010d1f94431d44659271af79f5b834a8",
        "IPY_MODEL_ca6f186e5c064dcaa15c209d9f764266",
        "IPY_MODEL_a3fb82da1811490093242c3bf5d9064d"
       ],
       "layout": "IPY_MODEL_0df16bfd198c449eafefa8f27fbf4af8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ff9aab01b06e4ec6be67450c64b0a487": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
