{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc49be27",
   "metadata": {
    "papermill": {
     "duration": 0.010704,
     "end_time": "2025-06-07T18:17:50.778185",
     "exception": false,
     "start_time": "2025-06-07T18:17:50.767481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef81dc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:17:50.798820Z",
     "iopub.status.busy": "2025-06-07T18:17:50.798544Z",
     "iopub.status.idle": "2025-06-07T18:18:13.627918Z",
     "shell.execute_reply": "2025-06-07T18:18:13.626770Z"
    },
    "papermill": {
     "duration": 22.841636,
     "end_time": "2025-06-07T18:18:13.629729",
     "exception": false,
     "start_time": "2025-06-07T18:17:50.788093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881fc3c",
   "metadata": {
    "papermill": {
     "duration": 0.009571,
     "end_time": "2025-06-07T18:18:13.649777",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.640206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3d0664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:13.670131Z",
     "iopub.status.busy": "2025-06-07T18:18:13.669652Z",
     "iopub.status.idle": "2025-06-07T18:18:13.672889Z",
     "shell.execute_reply": "2025-06-07T18:18:13.672307Z"
    },
    "papermill": {
     "duration": 0.014727,
     "end_time": "2025-06-07T18:18:13.674130",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.659403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7771959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:13.694317Z",
     "iopub.status.busy": "2025-06-07T18:18:13.694064Z",
     "iopub.status.idle": "2025-06-07T18:18:13.697603Z",
     "shell.execute_reply": "2025-06-07T18:18:13.696944Z"
    },
    "papermill": {
     "duration": 0.014945,
     "end_time": "2025-06-07T18:18:13.698822",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.683877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0174e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:13.719070Z",
     "iopub.status.busy": "2025-06-07T18:18:13.718874Z",
     "iopub.status.idle": "2025-06-07T18:18:13.727033Z",
     "shell.execute_reply": "2025-06-07T18:18:13.726504Z"
    },
    "papermill": {
     "duration": 0.019628,
     "end_time": "2025-06-07T18:18:13.728166",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.708538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ae206",
   "metadata": {
    "papermill": {
     "duration": 0.009551,
     "end_time": "2025-06-07T18:18:13.747814",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.738263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d15f344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:13.768039Z",
     "iopub.status.busy": "2025-06-07T18:18:13.767844Z",
     "iopub.status.idle": "2025-06-07T18:18:13.819150Z",
     "shell.execute_reply": "2025-06-07T18:18:13.817632Z"
    },
    "papermill": {
     "duration": 0.063426,
     "end_time": "2025-06-07T18:18:13.821002",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.757576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-besra'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a439bb9",
   "metadata": {
    "papermill": {
     "duration": 0.009632,
     "end_time": "2025-06-07T18:18:13.840483",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.830851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e68dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:13.861399Z",
     "iopub.status.busy": "2025-06-07T18:18:13.861120Z",
     "iopub.status.idle": "2025-06-07T18:18:14.002842Z",
     "shell.execute_reply": "2025-06-07T18:18:14.002010Z"
    },
    "papermill": {
     "duration": 0.1537,
     "end_time": "2025-06-07T18:18:14.004101",
     "exception": false,
     "start_time": "2025-06-07T18:18:13.850401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732ed418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:14.026694Z",
     "iopub.status.busy": "2025-06-07T18:18:14.026448Z",
     "iopub.status.idle": "2025-06-07T18:18:14.053367Z",
     "shell.execute_reply": "2025-06-07T18:18:14.052449Z"
    },
    "papermill": {
     "duration": 0.039045,
     "end_time": "2025-06-07T18:18:14.054734",
     "exception": false,
     "start_time": "2025-06-07T18:18:14.015689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81dd8652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:14.078673Z",
     "iopub.status.busy": "2025-06-07T18:18:14.078415Z",
     "iopub.status.idle": "2025-06-07T18:18:14.096336Z",
     "shell.execute_reply": "2025-06-07T18:18:14.095366Z"
    },
    "papermill": {
     "duration": 0.032992,
     "end_time": "2025-06-07T18:18:14.098138",
     "exception": false,
     "start_time": "2025-06-07T18:18:14.065146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6218,) (6218, 4)\n",
      "(1555,) (1555, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "train_labels = train_data.columns[2:6]\n",
    "val_labels = val_data.columns[2:6]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['processed_text'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['processed_text'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681e90f",
   "metadata": {
    "papermill": {
     "duration": 0.012922,
     "end_time": "2025-06-07T18:18:14.130563",
     "exception": false,
     "start_time": "2025-06-07T18:18:14.117641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3edeee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:14.159887Z",
     "iopub.status.busy": "2025-06-07T18:18:14.159626Z",
     "iopub.status.idle": "2025-06-07T18:18:15.374521Z",
     "shell.execute_reply": "2025-06-07T18:18:15.373453Z"
    },
    "papermill": {
     "duration": 1.22846,
     "end_time": "2025-06-07T18:18:15.376059",
     "exception": false,
     "start_time": "2025-06-07T18:18:14.147599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af49625722ce449d9c13d3ece0a713d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd0a03470c44b0cb80b73181ba68d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80be937f35bf40ab90b03d65a9688ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd06ed2a78d4a3a80c2d5e5bf673bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f712e812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.399111Z",
     "iopub.status.busy": "2025-06-07T18:18:15.398885Z",
     "iopub.status.idle": "2025-06-07T18:18:15.403255Z",
     "shell.execute_reply": "2025-06-07T18:18:15.402462Z"
    },
    "papermill": {
     "duration": 0.017158,
     "end_time": "2025-06-07T18:18:15.404596",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.387438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25544e",
   "metadata": {
    "papermill": {
     "duration": 0.010404,
     "end_time": "2025-06-07T18:18:15.425731",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.415327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bc924a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.448355Z",
     "iopub.status.busy": "2025-06-07T18:18:15.448084Z",
     "iopub.status.idle": "2025-06-07T18:18:15.451804Z",
     "shell.execute_reply": "2025-06-07T18:18:15.451004Z"
    },
    "papermill": {
     "duration": 0.016722,
     "end_time": "2025-06-07T18:18:15.453040",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.436318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a5daa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.475423Z",
     "iopub.status.busy": "2025-06-07T18:18:15.475176Z",
     "iopub.status.idle": "2025-06-07T18:18:15.479742Z",
     "shell.execute_reply": "2025-06-07T18:18:15.479112Z"
    },
    "papermill": {
     "duration": 0.017064,
     "end_time": "2025-06-07T18:18:15.480870",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.463806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    )  \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9819628a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.503170Z",
     "iopub.status.busy": "2025-06-07T18:18:15.502970Z",
     "iopub.status.idle": "2025-06-07T18:18:15.514984Z",
     "shell.execute_reply": "2025-06-07T18:18:15.514393Z"
    },
    "papermill": {
     "duration": 0.024575,
     "end_time": "2025-06-07T18:18:15.516160",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.491585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(train_labels),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "\n",
    "            nearest_cp = current_train_size\n",
    "            if nearest_cp not in checkpoints:\n",
    "                for cp in checkpoints:\n",
    "                    if cp > current_train_size:\n",
    "                        nearest_cp = cp\n",
    "                        break\n",
    "            percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2b8e6",
   "metadata": {
    "papermill": {
     "duration": 0.010809,
     "end_time": "2025-06-07T18:18:15.537966",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.527157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9c593f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.560845Z",
     "iopub.status.busy": "2025-06-07T18:18:15.560603Z",
     "iopub.status.idle": "2025-06-07T18:18:15.565914Z",
     "shell.execute_reply": "2025-06-07T18:18:15.565272Z"
    },
    "papermill": {
     "duration": 0.018396,
     "end_time": "2025-06-07T18:18:15.567135",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.548739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453b03a",
   "metadata": {
    "papermill": {
     "duration": 0.01056,
     "end_time": "2025-06-07T18:18:15.588650",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.578090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532aab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.611283Z",
     "iopub.status.busy": "2025-06-07T18:18:15.611033Z",
     "iopub.status.idle": "2025-06-07T18:18:15.637982Z",
     "shell.execute_reply": "2025-06-07T18:18:15.637169Z"
    },
    "papermill": {
     "duration": 0.039955,
     "end_time": "2025-06-07T18:18:15.639345",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.599390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(predicted_prob_one, true_outcome, alpha_param=0.1, beta_param=3.0):\n",
    "    epsilon = 1e-9\n",
    "    # Clip probabilities to avoid log(0) or other numerical errors\n",
    "    safe_prob = np.clip(predicted_prob_one, epsilon, 1.0 - epsilon)\n",
    "\n",
    "    if true_outcome == 1:\n",
    "        # Arguments for the betaln function when the true outcome is 1\n",
    "        arg1 = alpha_param\n",
    "        arg2 = beta_param + 1\n",
    "        arg3 = alpha_param + safe_prob\n",
    "        arg4 = beta_param + 1 - safe_prob\n",
    "        if not (arg1 > 0 and arg2 > 0 and arg3 > 0 and arg4 > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1, arg2) + betaln(arg3, arg4)\n",
    "    elif true_outcome == 0:\n",
    "        # Arguments for the betaln function when the true outcome is 0\n",
    "        arg1 = alpha_param + 1\n",
    "        arg2 = beta_param\n",
    "        arg3 = alpha_param + 1 - safe_prob\n",
    "        arg4 = beta_param + safe_prob\n",
    "        if not (arg1 > 0 and arg2 > 0 and arg3 > 0 and arg4 > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1, arg2) + betaln(arg3, arg4)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: true_outcome must be 0 or 1.\")\n",
    "\n",
    "def calculate_expected_beta_score(predicted_prob_one, alpha_param, beta_param):\n",
    "    score_if_one = beta_score(predicted_prob_one, 1, alpha_param, beta_param)\n",
    "    score_if_zero = beta_score(predicted_prob_one, 0, alpha_param, beta_param)\n",
    "    # The expected score is the average of scores for each possible outcome,\n",
    "    # weighted by the predicted probability of that outcome.\n",
    "    expected_score = predicted_prob_one * score_if_one + (1.0 - predicted_prob_one) * score_if_zero\n",
    "    return expected_score\n",
    "\n",
    "def get_ensemble_predictions_for_batch(ensemble_models, input_ids, attention_mask, device):\n",
    "    individual_model_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for model_instance in ensemble_models:\n",
    "            model_instance.eval()\n",
    "            model_instance.to(device)\n",
    "            outputs = model_instance(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            individual_model_probs_list.append(probs)\n",
    "\n",
    "        if not individual_model_probs_list:\n",
    "            return torch.empty(0, device=device), []\n",
    "\n",
    "        individual_probs_tensor = torch.stack(individual_model_probs_list)\n",
    "        average_probs = torch.mean(individual_probs_tensor, dim=0)\n",
    "\n",
    "    return average_probs, individual_model_probs_list\n",
    "\n",
    "def besra_sampling(\n",
    "    ensemble_models,\n",
    "    unlabeled_data,\n",
    "    train_indices,\n",
    "    remaining_indices,\n",
    "    estimation_data,\n",
    "    tokenizer,\n",
    "    num_classes,\n",
    "    sampling_dur,\n",
    "    new_samples,\n",
    "    trials,\n",
    "    alpha_param=0.1,\n",
    "    beta_param=3.0,\n",
    "    n_clusters=100, # min_increment\n",
    "):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    unlabeled_dataset = NetifierDataset(unlabeled_data,\n",
    "                                        np.zeros((len(unlabeled_data), num_classes)),\n",
    "                                        tokenizer,\n",
    "                                        max_length=256) # sequence_length\n",
    "    unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, num_workers=4, pin_memory=True) #batch_size\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "\n",
    "    estimation_dataset = NetifierDataset(estimation_data,\n",
    "                                        np.zeros((len(estimation_data), num_classes)),\n",
    "                                        tokenizer,\n",
    "                                        max_length=256) # sequence_length\n",
    "    estimation_dataloader = DataLoader(estimation_dataset, batch_size=len(estimation_dataset), num_workers=4, pin_memory=True)\n",
    "    estimation_dataloader, unlabeled_dataloader = accelerator.prepare(estimation_dataloader, unlabeled_dataloader)\n",
    "\n",
    "    for model in ensemble_models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    candidate_scores = []\n",
    "    all_estimation_avg_probs = []\n",
    "    all_estimation_individual_probs_by_model = []\n",
    "    all_candidate_avg_probs = []\n",
    "    all_candidate_individual_probs_by_model = []\n",
    "\n",
    "    # --- Pre-computation: Get predictions for all estimation and candidate samples ---\n",
    "    for est_batch in estimation_dataloader:\n",
    "        est_input_ids = est_batch['input_ids']\n",
    "        est_attention_mask = est_batch['attention_mask']\n",
    "        est_avg_probs_batch, est_indiv_probs_batch = get_ensemble_predictions_for_batch(ensemble_models, est_input_ids, est_attention_mask, device)\n",
    "        all_estimation_avg_probs.append(accelerator.gather_for_metrics(est_avg_probs_batch))\n",
    "\n",
    "        if not all_estimation_individual_probs_by_model:\n",
    "            all_estimation_individual_probs_by_model = [[] for _ in est_indiv_probs_batch]\n",
    "        for model_idx, probs in enumerate(est_indiv_probs_batch):\n",
    "            all_estimation_individual_probs_by_model[model_idx].append(accelerator.gather_for_metrics(probs))\n",
    "\n",
    "    for cand_batch in unlabeled_dataloader:\n",
    "        cand_input_ids = cand_batch['input_ids']\n",
    "        cand_attention_mask = cand_batch['attention_mask']\n",
    "        cand_avg_probs_batch, cand_indiv_probs_batch = get_ensemble_predictions_for_batch(ensemble_models, cand_input_ids, cand_attention_mask, device)\n",
    "        all_candidate_avg_probs.append(accelerator.gather_for_metrics(cand_avg_probs_batch))\n",
    "\n",
    "        if not all_candidate_individual_probs_by_model:\n",
    "             all_candidate_individual_probs_by_model = [[] for _ in cand_indiv_probs_batch]\n",
    "        for model_idx, probs in enumerate(cand_indiv_probs_batch):\n",
    "            all_candidate_individual_probs_by_model[model_idx].append(accelerator.gather_for_metrics(probs))\n",
    "\n",
    "    # Concatenate all batch predictions into single tensors\n",
    "    all_estimation_avg_probs = torch.cat(all_estimation_avg_probs, dim=0)\n",
    "    all_estimation_individual_probs = [torch.cat(model_probs, dim=0) for model_probs in all_estimation_individual_probs_by_model]\n",
    "    all_candidate_avg_probs = torch.cat(all_candidate_avg_probs, dim=0)\n",
    "    all_candidate_individual_probs = [torch.cat(model_probs, dim=0) for model_probs in all_candidate_individual_probs_by_model]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Equation 2: Change in Quality Score Î”Q(x|L)\n",
    "    # This section calculates the core acquisition score for each candidate sample 'x'.\n",
    "    # It estimates the expected improvement to the model by looping through all\n",
    "    # estimation samples 'x_prime' and all possible labels 'y' for the candidate.\n",
    "    for candidate_index in range(len(all_candidate_avg_probs)):\n",
    "        candidate_avg_probs_for_sample = all_candidate_avg_probs[candidate_index]\n",
    "        candidate_individual_probs_for_sample = [m_probs[candidate_index] for m_probs in all_candidate_individual_probs]\n",
    "        candidate_delta_q_score = 0.0\n",
    "\n",
    "        for estimation_index in range(len(all_estimation_avg_probs)):\n",
    "            estimation_avg_probs_for_sample = all_estimation_avg_probs[estimation_index]\n",
    "            estimation_individual_probs_for_sample = [m_probs[estimation_index] for m_probs in all_estimation_individual_probs]\n",
    "\n",
    "            # Calculate the initial expected score on the estimation sample before any update\n",
    "            initial_expected_score_on_estimation_sample = 0.0\n",
    "            for estimation_class_index in range(num_classes):\n",
    "                prob_one_avg_for_est_class = estimation_avg_probs_for_sample[estimation_class_index].item()\n",
    "                initial_expected_score_on_estimation_sample += calculate_expected_beta_score(prob_one_avg_for_est_class, alpha_param, beta_param)\n",
    "\n",
    "            delta_q_for_one_estimation_sample = 0.0\n",
    "            for candidate_class_index in range(num_classes):\n",
    "                expected_score_after_update = 0.0\n",
    "                for hypothetical_candidate_label in [0, 1]:\n",
    "                    prob_one_avg_for_candidate_class = candidate_avg_probs_for_sample[candidate_class_index].item()\n",
    "                    prob_of_hypothetical_label_avg = prob_one_avg_for_candidate_class if hypothetical_candidate_label == 1 else (1.0 - prob_one_avg_for_candidate_class)\n",
    "\n",
    "                    # Equations 4 & 3: Bayesian Update and New Prediction\n",
    "                    # This inner block performs the Bayesian update. It re-weights the ensemble\n",
    "                    # models based on how well they predict the hypothetical label for the\n",
    "                    # candidate sample (Eq. 4) and then computes a new, updated prediction\n",
    "                    # for the estimation sample using these weights (Eq. 3).\n",
    "                    model_weights = []\n",
    "                    for model_index in range(len(ensemble_models)):\n",
    "                        prob_one_individual_for_candidate_class = candidate_individual_probs_for_sample[model_index][candidate_class_index].item()\n",
    "                        prob_of_hypothetical_label_individual = prob_one_individual_for_candidate_class if hypothetical_candidate_label == 1 else (1.0 - prob_one_individual_for_candidate_class)\n",
    "                        model_weights.append(prob_of_hypothetical_label_individual)\n",
    "\n",
    "                    sum_model_weights = sum(model_weights)\n",
    "                    if sum_model_weights < 1e-9:\n",
    "                        normalized_model_weights = [1.0 / len(ensemble_models)] * len(ensemble_models)\n",
    "                    else:\n",
    "                        normalized_model_weights = [w / sum_model_weights for w in model_weights]\n",
    "\n",
    "                    reweighted_prob_one_for_est_class = 0.0\n",
    "                    for model_index in range(len(ensemble_models)):\n",
    "                        prob_one_individual_for_est_class = estimation_individual_probs_for_sample[model_index][candidate_class_index].item()\n",
    "                        reweighted_prob_one_for_est_class += normalized_model_weights[model_index] * prob_one_individual_for_est_class\n",
    "\n",
    "                    updated_expected_score_on_estimation_sample = calculate_expected_beta_score(reweighted_prob_one_for_est_class, alpha_param, beta_param)\n",
    "                    expected_score_after_update += prob_of_hypothetical_label_avg * updated_expected_score_on_estimation_sample\n",
    "\n",
    "                delta_q_for_one_estimation_sample += (expected_score_after_update - initial_expected_score_on_estimation_sample)\n",
    "            candidate_delta_q_score += delta_q_for_one_estimation_sample\n",
    "        candidate_scores.append(candidate_delta_q_score)\n",
    "                \n",
    "    # K-Means Clustering and Selection\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        candidate_scores = np.array(candidate_scores)\n",
    "        candidate_scores = candidate_scores.reshape(-1, 1)\n",
    "    \n",
    "        accelerator.print(f\"BESRA Uncertainty Score Threshold {np.percentile(candidate_scores, 90)}\")\n",
    "    \n",
    "        target_samples = math.ceil(0.1 * len(unlabeled_data)) \n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "    \n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "    \n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'pornografi': [y_train[i][0] for i in temp],\n",
    "                'sara': [y_train[i][1] for i in temp],\n",
    "                'radikalisme': [y_train[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "    \n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(candidate_scores)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(candidate_scores[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "    \n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'pornografi': [y_train[i][0] for i in temp],\n",
    "                    'sara': [y_train[i][1] for i in temp],\n",
    "                    'radikalisme': [y_train[i][2] for i in temp],\n",
    "                    'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "    \n",
    "            # threshold_data = pd.DataFrame({\n",
    "            #     'Threshold': thresholds\n",
    "            # })\n",
    "            # threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe3fcb",
   "metadata": {
    "papermill": {
     "duration": 0.01096,
     "end_time": "2025-06-07T18:18:15.661627",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.650667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4047c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.684755Z",
     "iopub.status.busy": "2025-06-07T18:18:15.684535Z",
     "iopub.status.idle": "2025-06-07T18:18:15.695523Z",
     "shell.execute_reply": "2025-06-07T18:18:15.694853Z"
    },
    "papermill": {
     "duration": 0.023945,
     "end_time": "2025-06-07T18:18:15.696735",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.672790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "\n",
    "        nearest_cp = current_train_size\n",
    "        if nearest_cp not in checkpoints:\n",
    "            for cp in checkpoints:\n",
    "                if cp > current_train_size:\n",
    "                    nearest_cp = cp\n",
    "                    break\n",
    "        percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}-{percentage}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        estimation_pool_indices = np.random.choice(remaining_indices, size=min(32, int(0.1 * len(remaining_indices))), replace=False).tolist()\n",
    "        estimation_pool_data = [X_train[i] for i in estimation_pool_indices]\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            models, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            estimation_pool_data,\n",
    "            tokenizer, \n",
    "            4,\n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a32e89e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.719511Z",
     "iopub.status.busy": "2025-06-07T18:18:15.719307Z",
     "iopub.status.idle": "2025-06-07T18:18:15.722334Z",
     "shell.execute_reply": "2025-06-07T18:18:15.721748Z"
    },
    "papermill": {
     "duration": 0.015621,
     "end_time": "2025-06-07T18:18:15.723501",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.707880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b555aa",
   "metadata": {
    "papermill": {
     "duration": 0.010726,
     "end_time": "2025-06-07T18:18:15.745044",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.734318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a32d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4486, Accuracy: 0.7886, F1 Micro: 0.0174, F1 Macro: 0.0157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3911, Accuracy: 0.8078, F1 Micro: 0.18, F1 Macro: 0.1337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.381, Accuracy: 0.8263, F1 Micro: 0.3293, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3204, Accuracy: 0.8355, F1 Micro: 0.416, F1 Macro: 0.3338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.281, Accuracy: 0.8527, F1 Micro: 0.5464, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2621, Accuracy: 0.8564, F1 Micro: 0.5767, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2083, Accuracy: 0.8581, F1 Micro: 0.5979, F1 Macro: 0.5834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1662, Accuracy: 0.8673, F1 Micro: 0.6473, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.149, Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "Model 1 - Iteration 388: Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.84      0.86       370\n",
      "                sara       0.63      0.47      0.54       248\n",
      "         radikalisme       0.67      0.62      0.64       243\n",
      "pencemaran_nama_baik       0.66      0.48      0.56       504\n",
      "\n",
      "           micro avg       0.73      0.60      0.66      1365\n",
      "           macro avg       0.71      0.60      0.65      1365\n",
      "        weighted avg       0.72      0.60      0.65      1365\n",
      "         samples avg       0.34      0.33      0.33      1365\n",
      "\n",
      "Training completed in 66.93093276023865 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4335, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3767, Accuracy: 0.8175, F1 Micro: 0.2561, F1 Macro: 0.1748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3582, Accuracy: 0.8288, F1 Micro: 0.3613, F1 Macro: 0.2669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2972, Accuracy: 0.8455, F1 Micro: 0.5013, F1 Macro: 0.467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2541, Accuracy: 0.855, F1 Micro: 0.5739, F1 Macro: 0.5572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2294, Accuracy: 0.8559, F1 Micro: 0.582, F1 Macro: 0.5669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1796, Accuracy: 0.8684, F1 Micro: 0.6509, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1463, Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "Epoch 10/10, Train Loss: 0.132, Accuracy: 0.8741, F1 Micro: 0.6753, F1 Macro: 0.6652\n",
      "Model 2 - Iteration 388: Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.57      0.52      0.54       248\n",
      "         radikalisme       0.65      0.79      0.71       243\n",
      "pencemaran_nama_baik       0.66      0.55      0.60       504\n",
      "\n",
      "           micro avg       0.70      0.67      0.68      1365\n",
      "           macro avg       0.69      0.68      0.68      1365\n",
      "        weighted avg       0.70      0.67      0.68      1365\n",
      "         samples avg       0.36      0.37      0.36      1365\n",
      "\n",
      "Training completed in 59.06886100769043 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5779, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.456, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4117, Accuracy: 0.7941, F1 Micro: 0.0666, F1 Macro: 0.0564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3928, Accuracy: 0.8244, F1 Micro: 0.3237, F1 Macro: 0.2387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3221, Accuracy: 0.8383, F1 Micro: 0.4396, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2818, Accuracy: 0.8533, F1 Micro: 0.5802, F1 Macro: 0.5753\n",
      "Epoch 7/10, Train Loss: 0.2525, Accuracy: 0.8531, F1 Micro: 0.5528, F1 Macro: 0.53\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1925, Accuracy: 0.8631, F1 Micro: 0.6244, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1636, Accuracy: 0.8678, F1 Micro: 0.6672, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1389, Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "Model 3 - Iteration 388: Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.84      0.86       370\n",
      "                sara       0.64      0.52      0.57       248\n",
      "         radikalisme       0.69      0.65      0.67       243\n",
      "pencemaran_nama_baik       0.70      0.52      0.59       504\n",
      "\n",
      "           micro avg       0.74      0.63      0.68      1365\n",
      "           macro avg       0.73      0.63      0.67      1365\n",
      "        weighted avg       0.74      0.63      0.68      1365\n",
      "         samples avg       0.35      0.35      0.34      1365\n",
      "\n",
      "Training completed in 60.06448459625244 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8697, F1 Micro: 0.6738, F1 Macro: 0.6678\n",
      "Launching training on 2 GPUs.\n",
      "5830\n",
      "BESRA Uncertainty Score Threshold 155.11871233995083\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 280.89488649368286 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.532, Accuracy: 0.7981, F1 Micro: 0.1015, F1 Macro: 0.0824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4118, Accuracy: 0.8308, F1 Micro: 0.3843, F1 Macro: 0.2879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3471, Accuracy: 0.8617, F1 Micro: 0.5972, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2774, Accuracy: 0.8712, F1 Micro: 0.637, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2288, Accuracy: 0.8834, F1 Micro: 0.7065, F1 Macro: 0.7004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1799, Accuracy: 0.8867, F1 Micro: 0.7372, F1 Macro: 0.7354\n",
      "Epoch 7/10, Train Loss: 0.1401, Accuracy: 0.8877, F1 Micro: 0.7123, F1 Macro: 0.7076\n",
      "Epoch 8/10, Train Loss: 0.1011, Accuracy: 0.888, F1 Micro: 0.731, F1 Macro: 0.7266\n",
      "Epoch 9/10, Train Loss: 0.0878, Accuracy: 0.8873, F1 Micro: 0.7193, F1 Macro: 0.7118\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.8878, F1 Micro: 0.713, F1 Macro: 0.7045\n",
      "Model 1 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7372, F1 Macro: 0.7354\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.64      0.63      0.64       248\n",
      "         radikalisme       0.67      0.82      0.74       243\n",
      "pencemaran_nama_baik       0.68      0.69      0.68       504\n",
      "\n",
      "           micro avg       0.73      0.75      0.74      1365\n",
      "           macro avg       0.73      0.75      0.74      1365\n",
      "        weighted avg       0.74      0.75      0.74      1365\n",
      "         samples avg       0.40      0.41      0.40      1365\n",
      "\n",
      "Training completed in 72.74191975593567 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5074, Accuracy: 0.8017, F1 Micro: 0.1314, F1 Macro: 0.103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3926, Accuracy: 0.84, F1 Micro: 0.4459, F1 Macro: 0.3691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.325, Accuracy: 0.8633, F1 Micro: 0.6006, F1 Macro: 0.5828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.258, Accuracy: 0.8809, F1 Micro: 0.6856, F1 Macro: 0.6724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2017, Accuracy: 0.8842, F1 Micro: 0.7265, F1 Macro: 0.728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1532, Accuracy: 0.8902, F1 Micro: 0.735, F1 Macro: 0.7342\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.8883, F1 Micro: 0.7264, F1 Macro: 0.725\n",
      "Epoch 8/10, Train Loss: 0.0844, Accuracy: 0.8886, F1 Micro: 0.7327, F1 Macro: 0.7262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.892, F1 Micro: 0.7486, F1 Macro: 0.7476\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.8914, F1 Micro: 0.7318, F1 Macro: 0.7271\n",
      "Model 2 - Iteration 971: Accuracy: 0.892, F1 Micro: 0.7486, F1 Macro: 0.7476\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.68      0.64      0.66       248\n",
      "         radikalisme       0.70      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.69      0.68       504\n",
      "\n",
      "           micro avg       0.74      0.75      0.75      1365\n",
      "           macro avg       0.74      0.76      0.75      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.41      0.42      0.41      1365\n",
      "\n",
      "Training completed in 74.98468351364136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4263, Accuracy: 0.8305, F1 Micro: 0.3922, F1 Macro: 0.3092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3489, Accuracy: 0.8575, F1 Micro: 0.5714, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2725, Accuracy: 0.8761, F1 Micro: 0.6647, F1 Macro: 0.6564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2218, Accuracy: 0.8823, F1 Micro: 0.7224, F1 Macro: 0.7241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.173, Accuracy: 0.8873, F1 Micro: 0.7364, F1 Macro: 0.7354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.8884, F1 Micro: 0.7369, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.8872, F1 Micro: 0.7395, F1 Macro: 0.7372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.8892, F1 Micro: 0.7469, F1 Macro: 0.7438\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.8883, F1 Micro: 0.741, F1 Macro: 0.7369\n",
      "Model 3 - Iteration 971: Accuracy: 0.8892, F1 Micro: 0.7469, F1 Macro: 0.7438\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.90      0.89       370\n",
      "                sara       0.67      0.61      0.64       248\n",
      "         radikalisme       0.71      0.80      0.75       243\n",
      "pencemaran_nama_baik       0.65      0.73      0.69       504\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1365\n",
      "           macro avg       0.73      0.76      0.74      1365\n",
      "        weighted avg       0.73      0.77      0.75      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 77.9945878982544 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8893, F1 Micro: 0.7442, F1 Macro: 0.7423\n",
      "Launching training on 2 GPUs.\n",
      "5247\n",
      "BESRA Uncertainty Score Threshold 235.5194360391016\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 253.98298573493958 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.8231, F1 Micro: 0.3172, F1 Macro: 0.212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3901, Accuracy: 0.8675, F1 Micro: 0.6251, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3058, Accuracy: 0.8836, F1 Micro: 0.7229, F1 Macro: 0.7205\n",
      "Epoch 4/10, Train Loss: 0.243, Accuracy: 0.8886, F1 Micro: 0.7192, F1 Macro: 0.7087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1946, Accuracy: 0.8939, F1 Micro: 0.7544, F1 Macro: 0.7499\n",
      "Epoch 6/10, Train Loss: 0.1627, Accuracy: 0.8872, F1 Micro: 0.7148, F1 Macro: 0.708\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.8953, F1 Micro: 0.7493, F1 Macro: 0.7442\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.8919, F1 Micro: 0.7511, F1 Macro: 0.7464\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.8888, F1 Micro: 0.7323, F1 Macro: 0.73\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.8898, F1 Micro: 0.7458, F1 Macro: 0.7443\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8939, F1 Micro: 0.7544, F1 Macro: 0.7499\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.89      0.88       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.73      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.76      0.75      1365\n",
      "           macro avg       0.74      0.76      0.75      1365\n",
      "        weighted avg       0.74      0.76      0.75      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 85.34823942184448 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4966, Accuracy: 0.8278, F1 Micro: 0.344, F1 Macro: 0.2355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3697, Accuracy: 0.8673, F1 Micro: 0.6659, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2927, Accuracy: 0.885, F1 Micro: 0.7385, F1 Macro: 0.7336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2283, Accuracy: 0.8916, F1 Micro: 0.7563, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1785, Accuracy: 0.8959, F1 Micro: 0.7652, F1 Macro: 0.7606\n",
      "Epoch 6/10, Train Loss: 0.1483, Accuracy: 0.8938, F1 Micro: 0.734, F1 Macro: 0.7265\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.8956, F1 Micro: 0.7487, F1 Macro: 0.747\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.8934, F1 Micro: 0.7533, F1 Macro: 0.7505\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.8931, F1 Micro: 0.7524, F1 Macro: 0.7473\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.8878, F1 Micro: 0.7484, F1 Macro: 0.747\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8959, F1 Micro: 0.7652, F1 Macro: 0.7606\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.90      0.89       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.77      0.76      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.77      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 86.94151520729065 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5285, Accuracy: 0.8086, F1 Micro: 0.2081, F1 Macro: 0.1581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3965, Accuracy: 0.8639, F1 Micro: 0.6342, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3045, Accuracy: 0.8816, F1 Micro: 0.7182, F1 Macro: 0.7134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.245, Accuracy: 0.8877, F1 Micro: 0.7221, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1945, Accuracy: 0.8947, F1 Micro: 0.756, F1 Macro: 0.7515\n",
      "Epoch 6/10, Train Loss: 0.1627, Accuracy: 0.8892, F1 Micro: 0.7225, F1 Macro: 0.7143\n",
      "Epoch 7/10, Train Loss: 0.1164, Accuracy: 0.8942, F1 Micro: 0.7411, F1 Macro: 0.7367\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.8914, F1 Micro: 0.7366, F1 Macro: 0.7353\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.8892, F1 Micro: 0.7389, F1 Macro: 0.7359\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.8884, F1 Micro: 0.7413, F1 Macro: 0.7416\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8947, F1 Micro: 0.756, F1 Macro: 0.7515\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.90      0.88       370\n",
      "                sara       0.69      0.62      0.65       248\n",
      "         radikalisme       0.75      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.73      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.76      0.76      1365\n",
      "           macro avg       0.75      0.76      0.75      1365\n",
      "        weighted avg       0.75      0.76      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 86.5999743938446 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8948, F1 Micro: 0.7585, F1 Macro: 0.754\n",
      "Launching training on 2 GPUs.\n",
      "4722\n",
      "BESRA Uncertainty Score Threshold 131.90491466682326\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 227.47129702568054 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.483, Accuracy: 0.8456, F1 Micro: 0.5517, F1 Macro: 0.4861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.347, Accuracy: 0.87, F1 Micro: 0.6408, F1 Macro: 0.636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2779, Accuracy: 0.8881, F1 Micro: 0.7143, F1 Macro: 0.7047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2336, Accuracy: 0.8922, F1 Micro: 0.7669, F1 Macro: 0.7681\n",
      "Epoch 5/10, Train Loss: 0.1925, Accuracy: 0.9, F1 Micro: 0.7603, F1 Macro: 0.7557\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.8959, F1 Micro: 0.7642, F1 Macro: 0.7573\n",
      "Epoch 7/10, Train Loss: 0.1232, Accuracy: 0.8956, F1 Micro: 0.7628, F1 Macro: 0.7583\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.8973, F1 Micro: 0.7562, F1 Macro: 0.7507\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.8959, F1 Micro: 0.7524, F1 Macro: 0.748\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.8972, F1 Micro: 0.7665, F1 Macro: 0.7611\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8922, F1 Micro: 0.7669, F1 Macro: 0.7681\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.61      0.76      0.68       248\n",
      "         radikalisme       0.70      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.71      0.83      0.77      1365\n",
      "           macro avg       0.72      0.83      0.77      1365\n",
      "        weighted avg       0.72      0.83      0.77      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 99.04506969451904 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4646, Accuracy: 0.8517, F1 Micro: 0.6218, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3336, Accuracy: 0.8777, F1 Micro: 0.6969, F1 Macro: 0.7015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2619, Accuracy: 0.8916, F1 Micro: 0.7272, F1 Macro: 0.7142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2166, Accuracy: 0.8903, F1 Micro: 0.763, F1 Macro: 0.7621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1779, Accuracy: 0.8994, F1 Micro: 0.7632, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.8969, F1 Micro: 0.7663, F1 Macro: 0.7629\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.8964, F1 Micro: 0.7551, F1 Macro: 0.7491\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.8945, F1 Micro: 0.7562, F1 Macro: 0.7454\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.8966, F1 Micro: 0.7469, F1 Macro: 0.741\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.8975, F1 Micro: 0.7492, F1 Macro: 0.7364\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8969, F1 Micro: 0.7663, F1 Macro: 0.7629\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.67      0.62      0.65       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 101.84810400009155 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4907, Accuracy: 0.8431, F1 Micro: 0.5742, F1 Macro: 0.578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3475, Accuracy: 0.8695, F1 Micro: 0.6602, F1 Macro: 0.6631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2774, Accuracy: 0.8884, F1 Micro: 0.7189, F1 Macro: 0.7121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2261, Accuracy: 0.8913, F1 Micro: 0.7565, F1 Macro: 0.7545\n",
      "Epoch 5/10, Train Loss: 0.187, Accuracy: 0.8969, F1 Micro: 0.7489, F1 Macro: 0.7446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.8958, F1 Micro: 0.7654, F1 Macro: 0.7626\n",
      "Epoch 7/10, Train Loss: 0.11, Accuracy: 0.8948, F1 Micro: 0.7521, F1 Macro: 0.7476\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.8934, F1 Micro: 0.7554, F1 Macro: 0.7479\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.8956, F1 Micro: 0.7519, F1 Macro: 0.7474\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.8992, F1 Micro: 0.7624, F1 Macro: 0.7566\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8958, F1 Micro: 0.7654, F1 Macro: 0.7626\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.64      0.66      0.65       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.78      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 100.52193474769592 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8949, F1 Micro: 0.7662, F1 Macro: 0.7646\n",
      "Launching training on 2 GPUs.\n",
      "4249\n",
      "BESRA Uncertainty Score Threshold 254.70636302323072\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 205.98517990112305 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4721, Accuracy: 0.8483, F1 Micro: 0.5069, F1 Macro: 0.4375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3348, Accuracy: 0.875, F1 Micro: 0.6619, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2589, Accuracy: 0.8913, F1 Micro: 0.7205, F1 Macro: 0.7049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2155, Accuracy: 0.8995, F1 Micro: 0.7598, F1 Macro: 0.7566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.9002, F1 Micro: 0.7735, F1 Macro: 0.7733\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.8981, F1 Micro: 0.7631, F1 Macro: 0.7582\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.902, F1 Micro: 0.7679, F1 Macro: 0.7645\n",
      "Epoch 8/10, Train Loss: 0.0771, Accuracy: 0.8958, F1 Micro: 0.772, F1 Macro: 0.7702\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.8992, F1 Micro: 0.7598, F1 Macro: 0.7481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9006, F1 Micro: 0.7773, F1 Macro: 0.7738\n",
      "Model 1 - Iteration 2394: Accuracy: 0.9006, F1 Micro: 0.7773, F1 Macro: 0.7738\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.67      0.66      0.67       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 115.76110482215881 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4564, Accuracy: 0.8562, F1 Micro: 0.5784, F1 Macro: 0.5512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3207, Accuracy: 0.8814, F1 Micro: 0.7055, F1 Macro: 0.7051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2498, Accuracy: 0.8948, F1 Micro: 0.737, F1 Macro: 0.7294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2077, Accuracy: 0.8964, F1 Micro: 0.7501, F1 Macro: 0.7473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.8992, F1 Micro: 0.7674, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.8956, F1 Micro: 0.7553, F1 Macro: 0.7515\n",
      "Epoch 7/10, Train Loss: 0.093, Accuracy: 0.8972, F1 Micro: 0.7567, F1 Macro: 0.7511\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.8991, F1 Micro: 0.7637, F1 Macro: 0.761\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.8959, F1 Micro: 0.7585, F1 Macro: 0.7518\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.8967, F1 Micro: 0.7514, F1 Macro: 0.7445\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8992, F1 Micro: 0.7674, F1 Macro: 0.7684\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 114.85122799873352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4877, Accuracy: 0.8502, F1 Micro: 0.5396, F1 Macro: 0.5233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3332, Accuracy: 0.8761, F1 Micro: 0.6775, F1 Macro: 0.6755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2594, Accuracy: 0.8923, F1 Micro: 0.7365, F1 Macro: 0.7281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2117, Accuracy: 0.8941, F1 Micro: 0.7487, F1 Macro: 0.7477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1709, Accuracy: 0.8995, F1 Micro: 0.7699, F1 Macro: 0.7654\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9005, F1 Micro: 0.7662, F1 Macro: 0.7612\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.8988, F1 Micro: 0.7674, F1 Macro: 0.7664\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.8961, F1 Micro: 0.7677, F1 Macro: 0.7665\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.8952, F1 Micro: 0.7566, F1 Macro: 0.749\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.8989, F1 Micro: 0.762, F1 Macro: 0.7539\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8995, F1 Micro: 0.7699, F1 Macro: 0.7654\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 114.78814578056335 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8998, F1 Micro: 0.7716, F1 Macro: 0.7692\n",
      "Launching training on 2 GPUs.\n",
      "3824\n",
      "BESRA Uncertainty Score Threshold 195.85152376087945\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 184.6428406238556 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4609, Accuracy: 0.8455, F1 Micro: 0.5154, F1 Macro: 0.4297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2995, Accuracy: 0.8791, F1 Micro: 0.7012, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.8942, F1 Micro: 0.758, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2155, Accuracy: 0.9003, F1 Micro: 0.7765, F1 Macro: 0.7746\n",
      "Epoch 5/10, Train Loss: 0.1672, Accuracy: 0.8992, F1 Micro: 0.7705, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.8992, F1 Micro: 0.7769, F1 Macro: 0.7763\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.9009, F1 Micro: 0.7734, F1 Macro: 0.7711\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.8933, F1 Micro: 0.7659, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.8995, F1 Micro: 0.7675, F1 Macro: 0.7637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.903, F1 Micro: 0.778, F1 Macro: 0.7798\n",
      "Model 1 - Iteration 2777: Accuracy: 0.903, F1 Micro: 0.778, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.67      0.74      0.70       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 127.78791546821594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4411, Accuracy: 0.8544, F1 Micro: 0.5817, F1 Macro: 0.5043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2854, Accuracy: 0.8853, F1 Micro: 0.7211, F1 Macro: 0.7243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2434, Accuracy: 0.8928, F1 Micro: 0.755, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2007, Accuracy: 0.9019, F1 Micro: 0.7789, F1 Macro: 0.7788\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.8998, F1 Micro: 0.7606, F1 Macro: 0.7557\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.8981, F1 Micro: 0.7696, F1 Macro: 0.7685\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.8978, F1 Micro: 0.7661, F1 Macro: 0.7604\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.8977, F1 Micro: 0.7619, F1 Macro: 0.7569\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.897, F1 Micro: 0.7569, F1 Macro: 0.7453\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.8992, F1 Micro: 0.7563, F1 Macro: 0.7522\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9019, F1 Micro: 0.7789, F1 Macro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.89       370\n",
      "                sara       0.69      0.71      0.70       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 124.31814932823181 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4683, Accuracy: 0.8453, F1 Micro: 0.5434, F1 Macro: 0.4694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3002, Accuracy: 0.882, F1 Micro: 0.7091, F1 Macro: 0.7104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2511, Accuracy: 0.8945, F1 Micro: 0.7562, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.8998, F1 Micro: 0.776, F1 Macro: 0.7745\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.8989, F1 Micro: 0.7645, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.1176, Accuracy: 0.8973, F1 Micro: 0.774, F1 Macro: 0.7761\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.8964, F1 Micro: 0.7665, F1 Macro: 0.766\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.8969, F1 Micro: 0.7639, F1 Macro: 0.7602\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.8978, F1 Micro: 0.7611, F1 Macro: 0.7593\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.8955, F1 Micro: 0.7608, F1 Macro: 0.7624\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8998, F1 Micro: 0.776, F1 Macro: 0.7745\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.73      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 124.35719752311707 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.9016, F1 Micro: 0.7776, F1 Macro: 0.7777\n",
      "Launching training on 2 GPUs.\n",
      "3441\n",
      "BESRA Uncertainty Score Threshold 158.95307148751493\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 166.93740129470825 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4619, Accuracy: 0.8542, F1 Micro: 0.5521, F1 Macro: 0.529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3064, Accuracy: 0.8894, F1 Micro: 0.7328, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2499, Accuracy: 0.8942, F1 Micro: 0.7632, F1 Macro: 0.7629\n",
      "Epoch 4/10, Train Loss: 0.2013, Accuracy: 0.8984, F1 Micro: 0.7596, F1 Macro: 0.7516\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9003, F1 Micro: 0.758, F1 Macro: 0.7466\n",
      "Epoch 6/10, Train Loss: 0.1252, Accuracy: 0.9027, F1 Micro: 0.7569, F1 Macro: 0.7511\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9013, F1 Micro: 0.7611, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.9028, F1 Micro: 0.7706, F1 Macro: 0.7679\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9019, F1 Micro: 0.7706, F1 Macro: 0.768\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9003, F1 Micro: 0.7653, F1 Macro: 0.7599\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9028, F1 Micro: 0.7706, F1 Macro: 0.7679\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.91       370\n",
      "                sara       0.65      0.68      0.66       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.75      0.70      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 134.5125012397766 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4429, Accuracy: 0.86, F1 Micro: 0.6018, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.8863, F1 Micro: 0.7245, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.8959, F1 Micro: 0.7621, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9014, F1 Micro: 0.7674, F1 Macro: 0.7623\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.8956, F1 Micro: 0.7345, F1 Macro: 0.7102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.9045, F1 Micro: 0.7707, F1 Macro: 0.7679\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.8984, F1 Micro: 0.7687, F1 Macro: 0.7634\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9023, F1 Micro: 0.7688, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.8998, F1 Micro: 0.7718, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0387, Accuracy: 0.8988, F1 Micro: 0.772, F1 Macro: 0.7729\n",
      "Model 2 - Iteration 3122: Accuracy: 0.8988, F1 Micro: 0.772, F1 Macro: 0.7729\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.62      0.76      0.68       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 139.84177708625793 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4671, Accuracy: 0.8564, F1 Micro: 0.5743, F1 Macro: 0.569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3065, Accuracy: 0.8903, F1 Micro: 0.7375, F1 Macro: 0.7343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2491, Accuracy: 0.8938, F1 Micro: 0.7663, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1971, Accuracy: 0.8984, F1 Micro: 0.7677, F1 Macro: 0.7626\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9011, F1 Micro: 0.7646, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9033, F1 Micro: 0.7626, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.8989, F1 Micro: 0.7708, F1 Macro: 0.7673\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.8984, F1 Micro: 0.7662, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9008, F1 Micro: 0.7754, F1 Macro: 0.7733\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.8997, F1 Micro: 0.7638, F1 Macro: 0.7639\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9008, F1 Micro: 0.7754, F1 Macro: 0.7733\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 138.326984167099 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.9008, F1 Micro: 0.7727, F1 Macro: 0.7713\n",
      "Launching training on 2 GPUs.\n",
      "3096\n",
      "BESRA Uncertainty Score Threshold 250.3918930892702\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 150.08974647521973 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4437, Accuracy: 0.8558, F1 Micro: 0.5548, F1 Macro: 0.5182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2962, Accuracy: 0.893, F1 Micro: 0.7451, F1 Macro: 0.7351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2362, Accuracy: 0.8986, F1 Micro: 0.7594, F1 Macro: 0.7558\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9009, F1 Micro: 0.7589, F1 Macro: 0.7541\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.9003, F1 Micro: 0.7572, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1115, Accuracy: 0.9056, F1 Micro: 0.7719, F1 Macro: 0.7687\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9022, F1 Micro: 0.769, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9013, F1 Micro: 0.7768, F1 Macro: 0.7732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9036, F1 Micro: 0.7796, F1 Macro: 0.7784\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9055, F1 Micro: 0.7711, F1 Macro: 0.7692\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9036, F1 Micro: 0.7796, F1 Macro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.81      0.79      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 148.5790286064148 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4303, Accuracy: 0.8573, F1 Micro: 0.5825, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2849, Accuracy: 0.897, F1 Micro: 0.7505, F1 Macro: 0.7387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2263, Accuracy: 0.8973, F1 Micro: 0.7525, F1 Macro: 0.7485\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.8995, F1 Micro: 0.7471, F1 Macro: 0.7377\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9019, F1 Micro: 0.7522, F1 Macro: 0.7498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9038, F1 Micro: 0.7805, F1 Macro: 0.7807\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9039, F1 Micro: 0.775, F1 Macro: 0.7729\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9022, F1 Micro: 0.7719, F1 Macro: 0.7672\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.8995, F1 Micro: 0.7722, F1 Macro: 0.7682\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.9019, F1 Micro: 0.7683, F1 Macro: 0.7622\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9038, F1 Micro: 0.7805, F1 Macro: 0.7807\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.78      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 144.59970712661743 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4496, Accuracy: 0.8527, F1 Micro: 0.5546, F1 Macro: 0.522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2998, Accuracy: 0.8941, F1 Micro: 0.7453, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.8991, F1 Micro: 0.7623, F1 Macro: 0.7585\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9019, F1 Micro: 0.7614, F1 Macro: 0.7534\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.8983, F1 Micro: 0.7579, F1 Macro: 0.7554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.8984, F1 Micro: 0.7688, F1 Macro: 0.7678\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9008, F1 Micro: 0.7626, F1 Macro: 0.7559\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.8944, F1 Micro: 0.762, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.8986, F1 Micro: 0.7741, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0381, Accuracy: 0.9022, F1 Micro: 0.7659, F1 Macro: 0.7632\n",
      "Model 3 - Iteration 3432: Accuracy: 0.8986, F1 Micro: 0.7741, F1 Macro: 0.7731\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.65      0.83      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 145.70255732536316 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.902, F1 Micro: 0.778, F1 Macro: 0.7774\n",
      "Launching training on 2 GPUs.\n",
      "2786\n",
      "BESRA Uncertainty Score Threshold 206.8654662103629\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 136.4855194091797 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4404, Accuracy: 0.8691, F1 Micro: 0.6703, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2909, Accuracy: 0.8953, F1 Micro: 0.7419, F1 Macro: 0.7277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2355, Accuracy: 0.9017, F1 Micro: 0.7769, F1 Macro: 0.7714\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.8977, F1 Micro: 0.7723, F1 Macro: 0.7712\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9005, F1 Micro: 0.7439, F1 Macro: 0.7302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1186, Accuracy: 0.9014, F1 Micro: 0.7788, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9033, F1 Micro: 0.7693, F1 Macro: 0.767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.903, F1 Micro: 0.7796, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9023, F1 Micro: 0.7749, F1 Macro: 0.7728\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9034, F1 Micro: 0.7733, F1 Macro: 0.7714\n",
      "Model 1 - Iteration 3711: Accuracy: 0.903, F1 Micro: 0.7796, F1 Macro: 0.7779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.73      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 154.32100653648376 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.427, Accuracy: 0.8692, F1 Micro: 0.6574, F1 Macro: 0.6588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2834, Accuracy: 0.8934, F1 Micro: 0.7243, F1 Macro: 0.709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2329, Accuracy: 0.8959, F1 Micro: 0.7758, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9033, F1 Micro: 0.7781, F1 Macro: 0.7741\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9, F1 Micro: 0.7358, F1 Macro: 0.718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.8988, F1 Micro: 0.7782, F1 Macro: 0.7761\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9016, F1 Micro: 0.7661, F1 Macro: 0.7635\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.8978, F1 Micro: 0.7731, F1 Macro: 0.7737\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.9002, F1 Micro: 0.7684, F1 Macro: 0.7645\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.8995, F1 Micro: 0.7647, F1 Macro: 0.7577\n",
      "Model 2 - Iteration 3711: Accuracy: 0.8988, F1 Micro: 0.7782, F1 Macro: 0.7761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 154.43196034431458 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4484, Accuracy: 0.8677, F1 Micro: 0.6434, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2955, Accuracy: 0.8928, F1 Micro: 0.7252, F1 Macro: 0.712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.8945, F1 Micro: 0.772, F1 Macro: 0.7707\n",
      "Epoch 4/10, Train Loss: 0.1897, Accuracy: 0.9034, F1 Micro: 0.7691, F1 Macro: 0.7607\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9003, F1 Micro: 0.7421, F1 Macro: 0.7223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1163, Accuracy: 0.8983, F1 Micro: 0.7786, F1 Macro: 0.7781\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.9, F1 Micro: 0.7669, F1 Macro: 0.7662\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.8992, F1 Micro: 0.7701, F1 Macro: 0.7691\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9003, F1 Micro: 0.7628, F1 Macro: 0.7569\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.9023, F1 Micro: 0.7753, F1 Macro: 0.774\n",
      "Model 3 - Iteration 3711: Accuracy: 0.8983, F1 Micro: 0.7786, F1 Macro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.65      0.75      0.69       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.65      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 153.21933960914612 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.9, F1 Micro: 0.7788, F1 Macro: 0.7774\n",
      "Launching training on 2 GPUs.\n",
      "2507\n",
      "BESRA Uncertainty Score Threshold 197.36228390649617\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 122.76440620422363 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4359, Accuracy: 0.8719, F1 Micro: 0.6626, F1 Macro: 0.6591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.8947, F1 Micro: 0.7522, F1 Macro: 0.7455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.8991, F1 Micro: 0.767, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9038, F1 Micro: 0.7715, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9042, F1 Micro: 0.7805, F1 Macro: 0.7778\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.8978, F1 Micro: 0.7791, F1 Macro: 0.779\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.8995, F1 Micro: 0.7683, F1 Macro: 0.7647\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9028, F1 Micro: 0.7746, F1 Macro: 0.7706\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9031, F1 Micro: 0.7775, F1 Macro: 0.7722\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9022, F1 Micro: 0.779, F1 Macro: 0.7776\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9042, F1 Micro: 0.7805, F1 Macro: 0.7778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.65      0.70      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 160.4803822040558 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4214, Accuracy: 0.8773, F1 Micro: 0.6797, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.268, Accuracy: 0.8955, F1 Micro: 0.7558, F1 Macro: 0.7503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.903, F1 Micro: 0.7739, F1 Macro: 0.772\n",
      "Epoch 4/10, Train Loss: 0.1847, Accuracy: 0.9041, F1 Micro: 0.7638, F1 Macro: 0.7537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9039, F1 Micro: 0.7769, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.8964, F1 Micro: 0.7744, F1 Macro: 0.7727\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.8991, F1 Micro: 0.776, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9033, F1 Micro: 0.7784, F1 Macro: 0.7752\n",
      "Epoch 9/10, Train Loss: 0.0445, Accuracy: 0.9002, F1 Micro: 0.7759, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.8995, F1 Micro: 0.7793, F1 Macro: 0.7804\n",
      "Model 2 - Iteration 3886: Accuracy: 0.8995, F1 Micro: 0.7793, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.60      0.81      0.69       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 161.3218719959259 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4465, Accuracy: 0.8733, F1 Micro: 0.6664, F1 Macro: 0.6608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2773, Accuracy: 0.8942, F1 Micro: 0.7473, F1 Macro: 0.7402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2325, Accuracy: 0.8989, F1 Micro: 0.7629, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.905, F1 Micro: 0.767, F1 Macro: 0.7574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9033, F1 Micro: 0.7735, F1 Macro: 0.768\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.8906, F1 Micro: 0.7726, F1 Macro: 0.7737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.087, Accuracy: 0.9053, F1 Micro: 0.7796, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9023, F1 Micro: 0.7706, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0445, Accuracy: 0.8998, F1 Micro: 0.7747, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9022, F1 Micro: 0.7825, F1 Macro: 0.782\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9022, F1 Micro: 0.7825, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.60      0.79      0.68       248\n",
      "         radikalisme       0.76      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 162.48932194709778 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.902, F1 Micro: 0.7808, F1 Macro: 0.7801\n",
      "Launching training on 2 GPUs.\n",
      "2332\n",
      "BESRA Uncertainty Score Threshold 189.3639007166652\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 113.68263387680054 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4225, Accuracy: 0.8702, F1 Micro: 0.6325, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2825, Accuracy: 0.8975, F1 Micro: 0.7569, F1 Macro: 0.75\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.8969, F1 Micro: 0.7304, F1 Macro: 0.7152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1871, Accuracy: 0.9047, F1 Micro: 0.7696, F1 Macro: 0.7548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9041, F1 Micro: 0.7833, F1 Macro: 0.7828\n",
      "Epoch 6/10, Train Loss: 0.1138, Accuracy: 0.8964, F1 Micro: 0.7393, F1 Macro: 0.7307\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9061, F1 Micro: 0.7768, F1 Macro: 0.7742\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9038, F1 Micro: 0.7677, F1 Macro: 0.7632\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9008, F1 Micro: 0.7698, F1 Macro: 0.7654\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9027, F1 Micro: 0.7653, F1 Macro: 0.7575\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9041, F1 Micro: 0.7833, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.85      0.90       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.75      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 166.3345239162445 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4079, Accuracy: 0.8739, F1 Micro: 0.6392, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.8986, F1 Micro: 0.7561, F1 Macro: 0.7473\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.9003, F1 Micro: 0.7504, F1 Macro: 0.7399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1855, Accuracy: 0.9047, F1 Micro: 0.777, F1 Macro: 0.7681\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9009, F1 Micro: 0.7737, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9016, F1 Micro: 0.7623, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.9034, F1 Micro: 0.7783, F1 Macro: 0.7754\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7715\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.8994, F1 Micro: 0.7644, F1 Macro: 0.763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9033, F1 Micro: 0.7787, F1 Macro: 0.7761\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9033, F1 Micro: 0.7787, F1 Macro: 0.7761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.64      0.70      0.67       248\n",
      "         radikalisme       0.78      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 166.83682703971863 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4288, Accuracy: 0.8691, F1 Micro: 0.6279, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.8986, F1 Micro: 0.7658, F1 Macro: 0.7593\n",
      "Epoch 3/10, Train Loss: 0.219, Accuracy: 0.8967, F1 Micro: 0.7346, F1 Macro: 0.7208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9038, F1 Micro: 0.7742, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1433, Accuracy: 0.9031, F1 Micro: 0.7763, F1 Macro: 0.7746\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9002, F1 Micro: 0.7658, F1 Macro: 0.7614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9016, F1 Micro: 0.7766, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9005, F1 Micro: 0.7673, F1 Macro: 0.7671\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.8994, F1 Micro: 0.7643, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.8997, F1 Micro: 0.7768, F1 Macro: 0.7771\n",
      "Model 3 - Iteration 4120: Accuracy: 0.8997, F1 Micro: 0.7768, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.62      0.79      0.70       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 168.6274917125702 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.9023, F1 Micro: 0.7796, F1 Macro: 0.7787\n",
      "Launching training on 2 GPUs.\n",
      "2098\n",
      "BESRA Uncertainty Score Threshold 278.37810662758403\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 102.59556579589844 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4168, Accuracy: 0.872, F1 Micro: 0.6456, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8966, F1 Micro: 0.7624, F1 Macro: 0.7581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9011, F1 Micro: 0.7824, F1 Macro: 0.781\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9014, F1 Micro: 0.7755, F1 Macro: 0.7724\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.8875, F1 Micro: 0.7685, F1 Macro: 0.7716\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.8992, F1 Micro: 0.7712, F1 Macro: 0.7677\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.902, F1 Micro: 0.767, F1 Macro: 0.7614\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.9002, F1 Micro: 0.7672, F1 Macro: 0.7585\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9044, F1 Micro: 0.7687, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.9038, F1 Micro: 0.7745, F1 Macro: 0.7719\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9011, F1 Micro: 0.7824, F1 Macro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 170.6968858242035 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3997, Accuracy: 0.8808, F1 Micro: 0.6952, F1 Macro: 0.6685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.8984, F1 Micro: 0.7633, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2245, Accuracy: 0.9002, F1 Micro: 0.7796, F1 Macro: 0.7766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9027, F1 Micro: 0.7822, F1 Macro: 0.7815\n",
      "Epoch 5/10, Train Loss: 0.1364, Accuracy: 0.8973, F1 Micro: 0.7789, F1 Macro: 0.7827\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.8959, F1 Micro: 0.7779, F1 Macro: 0.7772\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9028, F1 Micro: 0.7669, F1 Macro: 0.761\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.8998, F1 Micro: 0.7634, F1 Macro: 0.7603\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.9059, F1 Micro: 0.782, F1 Macro: 0.7795\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9005, F1 Micro: 0.7748, F1 Macro: 0.7711\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9027, F1 Micro: 0.7822, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.75      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.259033203125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4188, Accuracy: 0.8764, F1 Micro: 0.6741, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.8986, F1 Micro: 0.7634, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2258, Accuracy: 0.8992, F1 Micro: 0.7783, F1 Macro: 0.7765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9058, F1 Micro: 0.7819, F1 Macro: 0.7767\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.8964, F1 Micro: 0.7758, F1 Macro: 0.7754\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.8966, F1 Micro: 0.7751, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9014, F1 Micro: 0.7814, F1 Macro: 0.7813\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.9019, F1 Micro: 0.7711, F1 Macro: 0.7652\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9, F1 Micro: 0.7659, F1 Macro: 0.7606\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9005, F1 Micro: 0.7755, F1 Macro: 0.7731\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9058, F1 Micro: 0.7819, F1 Macro: 0.7767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.71      0.62      0.66       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 172.25073194503784 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.9032, F1 Micro: 0.7822, F1 Macro: 0.7797\n",
      "Launching training on 2 GPUs.\n",
      "1888\n",
      "BESRA Uncertainty Score Threshold 194.4991749824987\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 93.82533073425293 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.414, Accuracy: 0.8722, F1 Micro: 0.712, F1 Macro: 0.7098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2756, Accuracy: 0.893, F1 Micro: 0.7594, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.221, Accuracy: 0.9038, F1 Micro: 0.7681, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.9006, F1 Micro: 0.7805, F1 Macro: 0.7801\n",
      "Epoch 5/10, Train Loss: 0.1387, Accuracy: 0.9013, F1 Micro: 0.7743, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9028, F1 Micro: 0.7759, F1 Macro: 0.7737\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9005, F1 Micro: 0.7703, F1 Macro: 0.7655\n",
      "Epoch 8/10, Train Loss: 0.0586, Accuracy: 0.9, F1 Micro: 0.7787, F1 Macro: 0.7787\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9003, F1 Micro: 0.7731, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.038, Accuracy: 0.9047, F1 Micro: 0.7769, F1 Macro: 0.774\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9006, F1 Micro: 0.7805, F1 Macro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.66      0.76      0.70       248\n",
      "         radikalisme       0.69      0.87      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 178.70323848724365 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.8778, F1 Micro: 0.7307, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.8959, F1 Micro: 0.7703, F1 Macro: 0.7672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2181, Accuracy: 0.9038, F1 Micro: 0.7784, F1 Macro: 0.7728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.8963, F1 Micro: 0.7809, F1 Macro: 0.7801\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.9014, F1 Micro: 0.7701, F1 Macro: 0.7599\n",
      "Epoch 6/10, Train Loss: 0.1105, Accuracy: 0.9041, F1 Micro: 0.7788, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.7724\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9016, F1 Micro: 0.7766, F1 Macro: 0.7756\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9038, F1 Micro: 0.7798, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.9008, F1 Micro: 0.7744, F1 Macro: 0.7701\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.7724\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.72      0.57      0.63       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 179.95186758041382 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4196, Accuracy: 0.8733, F1 Micro: 0.7131, F1 Macro: 0.7108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2785, Accuracy: 0.8938, F1 Micro: 0.7636, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9044, F1 Micro: 0.7737, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.8963, F1 Micro: 0.7806, F1 Macro: 0.7811\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.9017, F1 Micro: 0.7687, F1 Macro: 0.7565\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9003, F1 Micro: 0.7688, F1 Macro: 0.7609\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.8981, F1 Micro: 0.7756, F1 Macro: 0.7732\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.8972, F1 Micro: 0.7768, F1 Macro: 0.7771\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9025, F1 Micro: 0.7804, F1 Macro: 0.7788\n",
      "Epoch 10/10, Train Loss: 0.0386, Accuracy: 0.903, F1 Micro: 0.7754, F1 Macro: 0.7679\n",
      "Model 3 - Iteration 4530: Accuracy: 0.8963, F1 Micro: 0.7806, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.60      0.83      0.70       248\n",
      "         radikalisme       0.69      0.88      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.86      0.75       504\n",
      "\n",
      "           micro avg       0.71      0.87      0.78      1365\n",
      "           macro avg       0.72      0.86      0.78      1365\n",
      "        weighted avg       0.73      0.87      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 178.46107959747314 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.9007, F1 Micro: 0.7808, F1 Macro: 0.7779\n",
      "Launching training on 2 GPUs.\n",
      "1688\n",
      "BESRA Uncertainty Score Threshold 207.774991591464\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 82.92341876029968 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4095, Accuracy: 0.8794, F1 Micro: 0.6818, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.8969, F1 Micro: 0.76, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2142, Accuracy: 0.8977, F1 Micro: 0.7739, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.9005, F1 Micro: 0.7792, F1 Macro: 0.777\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9038, F1 Micro: 0.776, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9013, F1 Micro: 0.774, F1 Macro: 0.7693\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.903, F1 Micro: 0.7748, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9052, F1 Micro: 0.7813, F1 Macro: 0.7808\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9036, F1 Micro: 0.7688, F1 Macro: 0.7682\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.9044, F1 Micro: 0.7762, F1 Macro: 0.7731\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9052, F1 Micro: 0.7813, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 183.80765628814697 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.394, Accuracy: 0.8873, F1 Micro: 0.7158, F1 Macro: 0.7098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.8991, F1 Micro: 0.7658, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.8989, F1 Micro: 0.7735, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9017, F1 Micro: 0.7806, F1 Macro: 0.7768\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.9022, F1 Micro: 0.7676, F1 Macro: 0.7548\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.8997, F1 Micro: 0.7677, F1 Macro: 0.7644\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9022, F1 Micro: 0.7788, F1 Macro: 0.7772\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9016, F1 Micro: 0.7785, F1 Macro: 0.7754\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9042, F1 Micro: 0.7688, F1 Macro: 0.7605\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9031, F1 Micro: 0.7775, F1 Macro: 0.777\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9017, F1 Micro: 0.7806, F1 Macro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.65      0.71      0.67       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.6519069671631 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4149, Accuracy: 0.8806, F1 Micro: 0.6817, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.8981, F1 Micro: 0.7587, F1 Macro: 0.7585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.8983, F1 Micro: 0.7692, F1 Macro: 0.7672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9011, F1 Micro: 0.7791, F1 Macro: 0.7762\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.8997, F1 Micro: 0.7596, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9013, F1 Micro: 0.7819, F1 Macro: 0.7815\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.8967, F1 Micro: 0.7704, F1 Macro: 0.7707\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9044, F1 Micro: 0.7797, F1 Macro: 0.7799\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.8997, F1 Micro: 0.7681, F1 Macro: 0.7654\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.9048, F1 Micro: 0.7763, F1 Macro: 0.7731\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9013, F1 Micro: 0.7819, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.67      0.76      0.71       248\n",
      "         radikalisme       0.69      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 184.38000965118408 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.9027, F1 Micro: 0.7813, F1 Macro: 0.7797\n",
      "Launching training on 2 GPUs.\n",
      "1555\n",
      "BESRA Uncertainty Score Threshold 278.7956315354088\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 77.4624514579773 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8745, F1 Micro: 0.7253, F1 Macro: 0.7208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2656, Accuracy: 0.8936, F1 Micro: 0.726, F1 Macro: 0.7182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2163, Accuracy: 0.9034, F1 Micro: 0.7677, F1 Macro: 0.7585\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9039, F1 Micro: 0.7643, F1 Macro: 0.7529\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9038, F1 Micro: 0.7567, F1 Macro: 0.7435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.9058, F1 Micro: 0.7882, F1 Macro: 0.7858\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9033, F1 Micro: 0.7698, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.9084, F1 Micro: 0.7907, F1 Macro: 0.7899\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9055, F1 Micro: 0.7763, F1 Macro: 0.7717\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9048, F1 Micro: 0.7873, F1 Macro: 0.7849\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9084, F1 Micro: 0.7907, F1 Macro: 0.7899\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.68      0.73      0.70       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 189.26270270347595 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3855, Accuracy: 0.8806, F1 Micro: 0.7401, F1 Macro: 0.7356\n",
      "Epoch 2/10, Train Loss: 0.2595, Accuracy: 0.8959, F1 Micro: 0.733, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.21, Accuracy: 0.9047, F1 Micro: 0.7665, F1 Macro: 0.7575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.184, Accuracy: 0.9052, F1 Micro: 0.7679, F1 Macro: 0.7608\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.8998, F1 Micro: 0.7395, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9064, F1 Micro: 0.7835, F1 Macro: 0.7803\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.903, F1 Micro: 0.7697, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9045, F1 Micro: 0.7812, F1 Macro: 0.7789\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9028, F1 Micro: 0.7696, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.9038, F1 Micro: 0.7823, F1 Macro: 0.7781\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9064, F1 Micro: 0.7835, F1 Macro: 0.7803\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.68      0.65      0.66       248\n",
      "         radikalisme       0.79      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 187.80181860923767 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.407, Accuracy: 0.8734, F1 Micro: 0.726, F1 Macro: 0.7238\n",
      "Epoch 2/10, Train Loss: 0.269, Accuracy: 0.8928, F1 Micro: 0.7175, F1 Macro: 0.7074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9069, F1 Micro: 0.7779, F1 Macro: 0.7709\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9009, F1 Micro: 0.7498, F1 Macro: 0.7302\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.898, F1 Micro: 0.732, F1 Macro: 0.717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9027, F1 Micro: 0.7822, F1 Macro: 0.7813\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.903, F1 Micro: 0.778, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.9045, F1 Micro: 0.7806, F1 Macro: 0.7739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.9041, F1 Micro: 0.783, F1 Macro: 0.7789\n",
      "Epoch 10/10, Train Loss: 0.0363, Accuracy: 0.9005, F1 Micro: 0.7783, F1 Macro: 0.7769\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9041, F1 Micro: 0.783, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 187.98076581954956 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.9063, F1 Micro: 0.7858, F1 Macro: 0.783\n",
      "Launching training on 2 GPUs.\n",
      "1355\n",
      "BESRA Uncertainty Score Threshold 188.6776053358966\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 67.47980809211731 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4048, Accuracy: 0.8778, F1 Micro: 0.6689, F1 Macro: 0.6637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2641, Accuracy: 0.8991, F1 Micro: 0.7676, F1 Macro: 0.7608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.902, F1 Micro: 0.7726, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.905, F1 Micro: 0.7819, F1 Macro: 0.7755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9002, F1 Micro: 0.7843, F1 Macro: 0.7866\n",
      "Epoch 6/10, Train Loss: 0.0997, Accuracy: 0.9036, F1 Micro: 0.7808, F1 Macro: 0.7776\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9073, F1 Micro: 0.7793, F1 Macro: 0.776\n",
      "Epoch 8/10, Train Loss: 0.0538, Accuracy: 0.9045, F1 Micro: 0.7828, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.8964, F1 Micro: 0.7743, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.907, F1 Micro: 0.7839, F1 Macro: 0.7824\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9002, F1 Micro: 0.7843, F1 Macro: 0.7866\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.61      0.82      0.70       248\n",
      "         radikalisme       0.73      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.78      1365\n",
      "           macro avg       0.73      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 197.58106637001038 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3921, Accuracy: 0.8811, F1 Micro: 0.6905, F1 Macro: 0.6837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.8983, F1 Micro: 0.7687, F1 Macro: 0.7612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.9031, F1 Micro: 0.7721, F1 Macro: 0.7676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9038, F1 Micro: 0.7771, F1 Macro: 0.7748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1257, Accuracy: 0.9036, F1 Micro: 0.7846, F1 Macro: 0.7826\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9045, F1 Micro: 0.7779, F1 Macro: 0.772\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9025, F1 Micro: 0.777, F1 Macro: 0.7758\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9011, F1 Micro: 0.7717, F1 Macro: 0.7648\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9036, F1 Micro: 0.7727, F1 Macro: 0.7695\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.8991, F1 Micro: 0.7768, F1 Macro: 0.7788\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9036, F1 Micro: 0.7846, F1 Macro: 0.7826\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.63      0.73      0.67       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 197.60700035095215 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4149, Accuracy: 0.8745, F1 Micro: 0.6587, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.268, Accuracy: 0.9003, F1 Micro: 0.776, F1 Macro: 0.7699\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9, F1 Micro: 0.7679, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9056, F1 Micro: 0.7763, F1 Macro: 0.7683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.9011, F1 Micro: 0.7812, F1 Macro: 0.7821\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9045, F1 Micro: 0.7712, F1 Macro: 0.7608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9048, F1 Micro: 0.7829, F1 Macro: 0.7819\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9038, F1 Micro: 0.782, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9036, F1 Micro: 0.7791, F1 Macro: 0.7748\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.8992, F1 Micro: 0.7801, F1 Macro: 0.7792\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9048, F1 Micro: 0.7829, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.77      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 197.83744478225708 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9029, F1 Micro: 0.784, F1 Macro: 0.7837\n",
      "Launching training on 2 GPUs.\n",
      "1155\n",
      "BESRA Uncertainty Score Threshold 174.99735133618753\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 58.13283705711365 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3921, Accuracy: 0.883, F1 Micro: 0.7038, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.8963, F1 Micro: 0.7741, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9017, F1 Micro: 0.7773, F1 Macro: 0.7716\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9044, F1 Micro: 0.773, F1 Macro: 0.7686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9053, F1 Micro: 0.7834, F1 Macro: 0.7818\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9034, F1 Micro: 0.7809, F1 Macro: 0.7787\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.9013, F1 Micro: 0.7712, F1 Macro: 0.7671\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9028, F1 Micro: 0.7808, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.9052, F1 Micro: 0.7779, F1 Macro: 0.7754\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.9034, F1 Micro: 0.7636, F1 Macro: 0.756\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9053, F1 Micro: 0.7834, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 201.22246885299683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3815, Accuracy: 0.883, F1 Micro: 0.7036, F1 Macro: 0.6753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2595, Accuracy: 0.8988, F1 Micro: 0.775, F1 Macro: 0.7712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9058, F1 Micro: 0.78, F1 Macro: 0.7771\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9023, F1 Micro: 0.7748, F1 Macro: 0.7706\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9062, F1 Micro: 0.7773, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9059, F1 Micro: 0.7833, F1 Macro: 0.7782\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9009, F1 Micro: 0.7678, F1 Macro: 0.7624\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9053, F1 Micro: 0.7822, F1 Macro: 0.7783\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9083, F1 Micro: 0.782, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.0355, Accuracy: 0.9044, F1 Micro: 0.7778, F1 Macro: 0.7695\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9059, F1 Micro: 0.7833, F1 Macro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 200.82512092590332 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3971, Accuracy: 0.8791, F1 Micro: 0.6926, F1 Macro: 0.6715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.8967, F1 Micro: 0.7746, F1 Macro: 0.7729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2208, Accuracy: 0.9041, F1 Micro: 0.7824, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.1697, Accuracy: 0.9048, F1 Micro: 0.7804, F1 Macro: 0.7766\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9056, F1 Micro: 0.7808, F1 Macro: 0.7777\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.8997, F1 Micro: 0.7769, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9028, F1 Micro: 0.7788, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9038, F1 Micro: 0.7763, F1 Macro: 0.7705\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9031, F1 Micro: 0.7614, F1 Macro: 0.7533\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.9034, F1 Micro: 0.7794, F1 Macro: 0.7757\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9041, F1 Micro: 0.7824, F1 Macro: 0.7752\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.66      0.67      0.66       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.63961696624756 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9051, F1 Micro: 0.783, F1 Macro: 0.7784\n",
      "Launching training on 2 GPUs.\n",
      "955\n",
      "BESRA Uncertainty Score Threshold 118.8898856572406\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 47.70579957962036 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3898, Accuracy: 0.8777, F1 Micro: 0.6573, F1 Macro: 0.6348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.8963, F1 Micro: 0.7485, F1 Macro: 0.7381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9062, F1 Micro: 0.7676, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1795, Accuracy: 0.9045, F1 Micro: 0.7777, F1 Macro: 0.7738\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9014, F1 Micro: 0.7718, F1 Macro: 0.7653\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.902, F1 Micro: 0.7661, F1 Macro: 0.756\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9025, F1 Micro: 0.7687, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9059, F1 Micro: 0.7801, F1 Macro: 0.7795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9064, F1 Micro: 0.7813, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9042, F1 Micro: 0.7764, F1 Macro: 0.7749\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9064, F1 Micro: 0.7813, F1 Macro: 0.7767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.69      0.64      0.66       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 210.85272240638733 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.875, F1 Micro: 0.6307, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9017, F1 Micro: 0.7657, F1 Macro: 0.7589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.9084, F1 Micro: 0.7805, F1 Macro: 0.7742\n",
      "Epoch 4/10, Train Loss: 0.1743, Accuracy: 0.9031, F1 Micro: 0.7734, F1 Macro: 0.7705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9034, F1 Micro: 0.783, F1 Macro: 0.78\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.9053, F1 Micro: 0.7804, F1 Macro: 0.7736\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.905, F1 Micro: 0.7825, F1 Macro: 0.7796\n",
      "Epoch 8/10, Train Loss: 0.054, Accuracy: 0.9044, F1 Micro: 0.7733, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9058, F1 Micro: 0.7769, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.907, F1 Micro: 0.7864, F1 Macro: 0.7847\n",
      "Model 2 - Iteration 5441: Accuracy: 0.907, F1 Micro: 0.7864, F1 Macro: 0.7847\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 208.87373423576355 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.401, Accuracy: 0.8681, F1 Micro: 0.603, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.8989, F1 Micro: 0.7634, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2089, Accuracy: 0.9078, F1 Micro: 0.7784, F1 Macro: 0.7697\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9034, F1 Micro: 0.7684, F1 Macro: 0.7638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9038, F1 Micro: 0.7825, F1 Macro: 0.7781\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9013, F1 Micro: 0.7635, F1 Macro: 0.7514\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.9016, F1 Micro: 0.7806, F1 Macro: 0.7793\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.9028, F1 Micro: 0.7759, F1 Macro: 0.7729\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9038, F1 Micro: 0.7798, F1 Macro: 0.7773\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9014, F1 Micro: 0.7728, F1 Macro: 0.7664\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9038, F1 Micro: 0.7825, F1 Macro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.66      0.67       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 206.94249415397644 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9057, F1 Micro: 0.7834, F1 Macro: 0.7798\n",
      "Launching training on 2 GPUs.\n",
      "777\n",
      "BESRA Uncertainty Score Threshold 179.300533033139\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 39.331218957901 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3851, Accuracy: 0.8861, F1 Micro: 0.7436, F1 Macro: 0.7365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2549, Accuracy: 0.8983, F1 Micro: 0.744, F1 Macro: 0.7241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9047, F1 Micro: 0.7837, F1 Macro: 0.7775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.8992, F1 Micro: 0.7864, F1 Macro: 0.7851\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9075, F1 Micro: 0.7798, F1 Macro: 0.773\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9091, F1 Micro: 0.7846, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9098, F1 Micro: 0.7838, F1 Macro: 0.7815\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9038, F1 Micro: 0.7758, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.9062, F1 Micro: 0.7901, F1 Macro: 0.7871\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.9064, F1 Micro: 0.7799, F1 Macro: 0.7778\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9062, F1 Micro: 0.7901, F1 Macro: 0.7871\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.74      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 215.08718705177307 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3684, Accuracy: 0.8894, F1 Micro: 0.744, F1 Macro: 0.7327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2485, Accuracy: 0.9002, F1 Micro: 0.7445, F1 Macro: 0.7299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2091, Accuracy: 0.9073, F1 Micro: 0.786, F1 Macro: 0.7811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1706, Accuracy: 0.9031, F1 Micro: 0.7904, F1 Macro: 0.7885\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9028, F1 Micro: 0.7689, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9081, F1 Micro: 0.7819, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.9073, F1 Micro: 0.7878, F1 Macro: 0.7869\n",
      "Epoch 8/10, Train Loss: 0.0546, Accuracy: 0.9042, F1 Micro: 0.7764, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9061, F1 Micro: 0.7811, F1 Macro: 0.775\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.9089, F1 Micro: 0.7827, F1 Macro: 0.7765\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9031, F1 Micro: 0.7904, F1 Macro: 0.7885\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.70      0.90      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.84      0.76       504\n",
      "\n",
      "           micro avg       0.73      0.86      0.79      1365\n",
      "           macro avg       0.73      0.86      0.79      1365\n",
      "        weighted avg       0.74      0.86      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 213.04713988304138 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8864, F1 Micro: 0.7436, F1 Macro: 0.7371\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.8977, F1 Micro: 0.7381, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2119, Accuracy: 0.9053, F1 Micro: 0.7872, F1 Macro: 0.782\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9013, F1 Micro: 0.7865, F1 Macro: 0.786\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9014, F1 Micro: 0.762, F1 Macro: 0.7508\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9056, F1 Micro: 0.7834, F1 Macro: 0.7796\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.9064, F1 Micro: 0.7756, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.9036, F1 Micro: 0.783, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9005, F1 Micro: 0.7792, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8995, F1 Micro: 0.7741, F1 Macro: 0.7709\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9053, F1 Micro: 0.7872, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 210.60573363304138 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9049, F1 Micro: 0.7892, F1 Macro: 0.7859\n",
      "Launching training on 2 GPUs.\n",
      "577\n",
      "BESRA Uncertainty Score Threshold 174.2759358720726\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.58511233329773 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3862, Accuracy: 0.8886, F1 Micro: 0.7252, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2517, Accuracy: 0.8998, F1 Micro: 0.7543, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.9013, F1 Micro: 0.7804, F1 Macro: 0.7757\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.8988, F1 Micro: 0.7664, F1 Macro: 0.7617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1257, Accuracy: 0.9069, F1 Micro: 0.7877, F1 Macro: 0.7849\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9008, F1 Micro: 0.7822, F1 Macro: 0.7794\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9045, F1 Micro: 0.7842, F1 Macro: 0.7825\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.905, F1 Micro: 0.7849, F1 Macro: 0.7809\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9031, F1 Micro: 0.779, F1 Macro: 0.7732\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9062, F1 Micro: 0.7868, F1 Macro: 0.7845\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9069, F1 Micro: 0.7877, F1 Macro: 0.7849\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 219.069491147995 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3673, Accuracy: 0.8922, F1 Micro: 0.7366, F1 Macro: 0.733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9023, F1 Micro: 0.7637, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.9087, F1 Micro: 0.7879, F1 Macro: 0.7802\n",
      "Epoch 4/10, Train Loss: 0.1607, Accuracy: 0.9038, F1 Micro: 0.7674, F1 Macro: 0.7626\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9081, F1 Micro: 0.7768, F1 Macro: 0.7689\n",
      "Epoch 6/10, Train Loss: 0.0888, Accuracy: 0.9073, F1 Micro: 0.7814, F1 Macro: 0.7702\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9075, F1 Micro: 0.7827, F1 Macro: 0.7779\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9083, F1 Micro: 0.7819, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9033, F1 Micro: 0.7812, F1 Macro: 0.7774\n",
      "Epoch 10/10, Train Loss: 0.027, Accuracy: 0.9072, F1 Micro: 0.7813, F1 Macro: 0.7761\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9087, F1 Micro: 0.7879, F1 Macro: 0.7802\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.64      0.65       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.78      0.76       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 217.82063126564026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3898, Accuracy: 0.8909, F1 Micro: 0.7356, F1 Macro: 0.7337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9006, F1 Micro: 0.7573, F1 Macro: 0.7521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.9064, F1 Micro: 0.7871, F1 Macro: 0.7813\n",
      "Epoch 4/10, Train Loss: 0.1624, Accuracy: 0.9048, F1 Micro: 0.771, F1 Macro: 0.7613\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9053, F1 Micro: 0.7865, F1 Macro: 0.7813\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9059, F1 Micro: 0.7865, F1 Macro: 0.7862\n",
      "Epoch 8/10, Train Loss: 0.0516, Accuracy: 0.9062, F1 Micro: 0.7809, F1 Macro: 0.7776\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9017, F1 Micro: 0.7791, F1 Macro: 0.772\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.905, F1 Micro: 0.7803, F1 Macro: 0.7767\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9064, F1 Micro: 0.7871, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.80      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 218.07142686843872 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9073, F1 Micro: 0.7876, F1 Macro: 0.7821\n",
      "Launching training on 2 GPUs.\n",
      "377\n",
      "BESRA Uncertainty Score Threshold 180.397671480472\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.79730796813965 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3679, Accuracy: 0.8883, F1 Micro: 0.7319, F1 Macro: 0.7288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.8969, F1 Micro: 0.7387, F1 Macro: 0.7353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2004, Accuracy: 0.907, F1 Micro: 0.7867, F1 Macro: 0.7836\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9083, F1 Micro: 0.7792, F1 Macro: 0.7661\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9055, F1 Micro: 0.7802, F1 Macro: 0.7731\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.9048, F1 Micro: 0.7843, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9048, F1 Micro: 0.7827, F1 Macro: 0.7816\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.9059, F1 Micro: 0.7839, F1 Macro: 0.7818\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9052, F1 Micro: 0.7808, F1 Macro: 0.7783\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9036, F1 Micro: 0.7716, F1 Macro: 0.7655\n",
      "Model 1 - Iteration 6041: Accuracy: 0.907, F1 Micro: 0.7867, F1 Macro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 223.88736867904663 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3532, Accuracy: 0.8902, F1 Micro: 0.7348, F1 Macro: 0.7314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.8978, F1 Micro: 0.7453, F1 Macro: 0.7402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1973, Accuracy: 0.9066, F1 Micro: 0.7852, F1 Macro: 0.7819\n",
      "Epoch 4/10, Train Loss: 0.1625, Accuracy: 0.9069, F1 Micro: 0.7701, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9073, F1 Micro: 0.792, F1 Macro: 0.7885\n",
      "Epoch 6/10, Train Loss: 0.0888, Accuracy: 0.9075, F1 Micro: 0.7874, F1 Macro: 0.7831\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9055, F1 Micro: 0.7783, F1 Macro: 0.7753\n",
      "Epoch 8/10, Train Loss: 0.0505, Accuracy: 0.9056, F1 Micro: 0.7869, F1 Macro: 0.7854\n",
      "Epoch 9/10, Train Loss: 0.0373, Accuracy: 0.9047, F1 Micro: 0.7823, F1 Macro: 0.7781\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9034, F1 Micro: 0.767, F1 Macro: 0.7585\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9073, F1 Micro: 0.792, F1 Macro: 0.7885\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.67      0.71      0.68       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 225.8011839389801 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3696, Accuracy: 0.8872, F1 Micro: 0.7332, F1 Macro: 0.73\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2411, Accuracy: 0.8977, F1 Micro: 0.7424, F1 Macro: 0.7337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1999, Accuracy: 0.9078, F1 Micro: 0.7924, F1 Macro: 0.7889\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9069, F1 Micro: 0.7713, F1 Macro: 0.7628\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9052, F1 Micro: 0.7834, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.905, F1 Micro: 0.7725, F1 Macro: 0.761\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9039, F1 Micro: 0.776, F1 Macro: 0.7693\n",
      "Epoch 8/10, Train Loss: 0.0482, Accuracy: 0.9034, F1 Micro: 0.7805, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9023, F1 Micro: 0.7829, F1 Macro: 0.7816\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9014, F1 Micro: 0.7664, F1 Macro: 0.7613\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9078, F1 Micro: 0.7924, F1 Macro: 0.7889\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.77      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.80      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 223.39602136611938 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9074, F1 Micro: 0.7904, F1 Macro: 0.787\n",
      "Launching training on 2 GPUs.\n",
      "177\n",
      "BESRA Uncertainty Score Threshold 85.43168332650896\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 8.13508677482605 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3586, Accuracy: 0.8833, F1 Micro: 0.6987, F1 Macro: 0.6862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2416, Accuracy: 0.8992, F1 Micro: 0.7558, F1 Macro: 0.7473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9031, F1 Micro: 0.7787, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1554, Accuracy: 0.9084, F1 Micro: 0.7906, F1 Macro: 0.7867\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9062, F1 Micro: 0.7774, F1 Macro: 0.7712\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9091, F1 Micro: 0.7833, F1 Macro: 0.7778\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9077, F1 Micro: 0.7802, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0506, Accuracy: 0.9069, F1 Micro: 0.7822, F1 Macro: 0.7804\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9042, F1 Micro: 0.7679, F1 Macro: 0.7646\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.9044, F1 Micro: 0.7811, F1 Macro: 0.778\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9084, F1 Micro: 0.7906, F1 Macro: 0.7867\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.76       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 231.61348390579224 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3414, Accuracy: 0.8881, F1 Micro: 0.7136, F1 Macro: 0.7009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2361, Accuracy: 0.8992, F1 Micro: 0.7565, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.9064, F1 Micro: 0.7854, F1 Macro: 0.7792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1494, Accuracy: 0.9072, F1 Micro: 0.79, F1 Macro: 0.7862\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9033, F1 Micro: 0.7859, F1 Macro: 0.7861\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.9002, F1 Micro: 0.777, F1 Macro: 0.7751\n",
      "Epoch 7/10, Train Loss: 0.0576, Accuracy: 0.9058, F1 Micro: 0.7756, F1 Macro: 0.7665\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9039, F1 Micro: 0.7733, F1 Macro: 0.7714\n",
      "Epoch 9/10, Train Loss: 0.0337, Accuracy: 0.9019, F1 Micro: 0.7796, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9045, F1 Micro: 0.7771, F1 Macro: 0.7747\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9072, F1 Micro: 0.79, F1 Macro: 0.7862\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.79      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 230.53621816635132 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3611, Accuracy: 0.882, F1 Micro: 0.6861, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2408, Accuracy: 0.9003, F1 Micro: 0.7607, F1 Macro: 0.7504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1962, Accuracy: 0.9059, F1 Micro: 0.7728, F1 Macro: 0.7632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9038, F1 Micro: 0.7757, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9062, F1 Micro: 0.789, F1 Macro: 0.7872\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.8984, F1 Micro: 0.7719, F1 Macro: 0.7659\n",
      "Epoch 7/10, Train Loss: 0.0597, Accuracy: 0.903, F1 Micro: 0.7711, F1 Macro: 0.7611\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9023, F1 Micro: 0.7788, F1 Macro: 0.7788\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.9042, F1 Micro: 0.7804, F1 Macro: 0.7776\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9027, F1 Micro: 0.7748, F1 Macro: 0.771\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9062, F1 Micro: 0.789, F1 Macro: 0.7872\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.75      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 231.81300902366638 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9073, F1 Micro: 0.7899, F1 Macro: 0.7867\n",
      "Total sampling time: 2470.91 seconds\n",
      "Total runtime: 13548.496273517609 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc79271",
   "metadata": {
    "papermill": {
     "duration": 0.010795,
     "end_time": "2025-06-07T18:18:15.793189",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.782394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6045, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3839, Accuracy: 0.8116, F1 Micro: 0.2087, F1 Macro: 0.1506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3762, Accuracy: 0.8338, F1 Micro: 0.4095, F1 Macro: 0.342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3143, Accuracy: 0.8447, F1 Micro: 0.4918, F1 Macro: 0.4438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2621, Accuracy: 0.8547, F1 Micro: 0.565, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2351, Accuracy: 0.8594, F1 Micro: 0.6053, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1834, Accuracy: 0.8698, F1 Micro: 0.6445, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1462, Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "Epoch 10/10, Train Loss: 0.1326, Accuracy: 0.877, F1 Micro: 0.6883, F1 Macro: 0.6815\n",
      "Model 1 - Iteration 388: Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.81      0.85       370\n",
      "                sara       0.58      0.56      0.57       248\n",
      "         radikalisme       0.66      0.74      0.70       243\n",
      "pencemaran_nama_baik       0.66      0.63      0.64       504\n",
      "\n",
      "           micro avg       0.71      0.69      0.70      1365\n",
      "           macro avg       0.70      0.69      0.69      1365\n",
      "        weighted avg       0.71      0.69      0.70      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 57.381505727767944 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5934, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.439, Accuracy: 0.7909, F1 Micro: 0.0388, F1 Macro: 0.034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3734, Accuracy: 0.8158, F1 Micro: 0.2398, F1 Macro: 0.1687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3544, Accuracy: 0.8344, F1 Micro: 0.4137, F1 Macro: 0.3332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2904, Accuracy: 0.8481, F1 Micro: 0.5272, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2499, Accuracy: 0.8531, F1 Micro: 0.5595, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2281, Accuracy: 0.8608, F1 Micro: 0.607, F1 Macro: 0.5951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1774, Accuracy: 0.8648, F1 Micro: 0.6211, F1 Macro: 0.6062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1432, Accuracy: 0.8706, F1 Micro: 0.662, F1 Macro: 0.6503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1304, Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "Model 2 - Iteration 388: Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.85      0.87       370\n",
      "                sara       0.57      0.51      0.54       248\n",
      "         radikalisme       0.66      0.68      0.67       243\n",
      "pencemaran_nama_baik       0.66      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.67      1365\n",
      "           macro avg       0.70      0.64      0.67      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.37      0.36      0.35      1365\n",
      "\n",
      "Training completed in 59.636526107788086 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6052, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4376, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3737, Accuracy: 0.8203, F1 Micro: 0.274, F1 Macro: 0.1872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3629, Accuracy: 0.8316, F1 Micro: 0.3896, F1 Macro: 0.2908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3015, Accuracy: 0.842, F1 Micro: 0.4648, F1 Macro: 0.4003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2601, Accuracy: 0.8594, F1 Micro: 0.5917, F1 Macro: 0.565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2378, Accuracy: 0.8631, F1 Micro: 0.6158, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1836, Accuracy: 0.8686, F1 Micro: 0.6453, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1511, Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "Epoch 10/10, Train Loss: 0.13, Accuracy: 0.8755, F1 Micro: 0.6876, F1 Macro: 0.6794\n",
      "Model 3 - Iteration 388: Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.86      0.86       370\n",
      "                sara       0.60      0.54      0.57       248\n",
      "         radikalisme       0.67      0.71      0.69       243\n",
      "pencemaran_nama_baik       0.67      0.60      0.63       504\n",
      "\n",
      "           micro avg       0.71      0.68      0.70      1365\n",
      "           macro avg       0.70      0.68      0.69      1365\n",
      "        weighted avg       0.71      0.68      0.69      1365\n",
      "         samples avg       0.37      0.37      0.36      1365\n",
      "\n",
      "Training completed in 57.98148989677429 s\n",
      "Averaged - Iteration 388: Accuracy: 0.871, F1 Micro: 0.6888, F1 Macro: 0.6828\n",
      "Launching training on 2 GPUs.\n",
      "5830\n",
      "BESRA Uncertainty Score Threshold 190.33248119542415\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 287.8979022502899 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5479, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4222, Accuracy: 0.8434, F1 Micro: 0.4765, F1 Macro: 0.412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3269, Accuracy: 0.8598, F1 Micro: 0.5883, F1 Macro: 0.5746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2563, Accuracy: 0.8695, F1 Micro: 0.6393, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2222, Accuracy: 0.8834, F1 Micro: 0.6958, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1754, Accuracy: 0.8911, F1 Micro: 0.7371, F1 Macro: 0.7332\n",
      "Epoch 7/10, Train Loss: 0.1293, Accuracy: 0.8863, F1 Micro: 0.7102, F1 Macro: 0.7098\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.8872, F1 Micro: 0.7146, F1 Macro: 0.711\n",
      "Epoch 9/10, Train Loss: 0.0914, Accuracy: 0.8845, F1 Micro: 0.6914, F1 Macro: 0.6747\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.8834, F1 Micro: 0.6955, F1 Macro: 0.684\n",
      "Model 1 - Iteration 971: Accuracy: 0.8911, F1 Micro: 0.7371, F1 Macro: 0.7332\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.83      0.87       370\n",
      "                sara       0.69      0.60      0.65       248\n",
      "         radikalisme       0.71      0.75      0.73       243\n",
      "pencemaran_nama_baik       0.71      0.67      0.69       504\n",
      "\n",
      "           micro avg       0.76      0.72      0.74      1365\n",
      "           macro avg       0.76      0.71      0.73      1365\n",
      "        weighted avg       0.76      0.72      0.74      1365\n",
      "         samples avg       0.41      0.40      0.39      1365\n",
      "\n",
      "Training completed in 70.4877245426178 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.538, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4036, Accuracy: 0.8534, F1 Micro: 0.5653, F1 Macro: 0.5528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3166, Accuracy: 0.857, F1 Micro: 0.6078, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2582, Accuracy: 0.8662, F1 Micro: 0.6222, F1 Macro: 0.6159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2261, Accuracy: 0.8875, F1 Micro: 0.7243, F1 Macro: 0.7229\n",
      "Epoch 6/10, Train Loss: 0.1751, Accuracy: 0.8867, F1 Micro: 0.7182, F1 Macro: 0.7145\n",
      "Epoch 7/10, Train Loss: 0.1318, Accuracy: 0.8836, F1 Micro: 0.6945, F1 Macro: 0.6874\n",
      "Epoch 8/10, Train Loss: 0.1103, Accuracy: 0.8789, F1 Micro: 0.6729, F1 Macro: 0.6675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0963, Accuracy: 0.8898, F1 Micro: 0.742, F1 Macro: 0.7355\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.8833, F1 Micro: 0.7023, F1 Macro: 0.6952\n",
      "Model 2 - Iteration 971: Accuracy: 0.8898, F1 Micro: 0.742, F1 Macro: 0.7355\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.65      0.62      0.63       248\n",
      "         radikalisme       0.68      0.76      0.72       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.74      0.74      1365\n",
      "           macro avg       0.74      0.74      0.74      1365\n",
      "        weighted avg       0.74      0.74      0.74      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 70.69622230529785 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.7956, F1 Micro: 0.0802, F1 Macro: 0.0667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4125, Accuracy: 0.8411, F1 Micro: 0.4518, F1 Macro: 0.3798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3243, Accuracy: 0.8619, F1 Micro: 0.6126, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2592, Accuracy: 0.8683, F1 Micro: 0.6311, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2169, Accuracy: 0.8839, F1 Micro: 0.6981, F1 Macro: 0.6908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1819, Accuracy: 0.8842, F1 Micro: 0.7021, F1 Macro: 0.6963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1288, Accuracy: 0.8844, F1 Micro: 0.7026, F1 Macro: 0.6956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.8886, F1 Micro: 0.7156, F1 Macro: 0.7083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.8889, F1 Micro: 0.7168, F1 Macro: 0.7059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.8884, F1 Micro: 0.7213, F1 Macro: 0.7189\n",
      "Model 3 - Iteration 971: Accuracy: 0.8884, F1 Micro: 0.7213, F1 Macro: 0.7189\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.84      0.88       370\n",
      "                sara       0.67      0.59      0.63       248\n",
      "         radikalisme       0.69      0.76      0.73       243\n",
      "pencemaran_nama_baik       0.75      0.56      0.64       504\n",
      "\n",
      "           micro avg       0.77      0.68      0.72      1365\n",
      "           macro avg       0.76      0.69      0.72      1365\n",
      "        weighted avg       0.77      0.68      0.72      1365\n",
      "         samples avg       0.39      0.38      0.38      1365\n",
      "\n",
      "Training completed in 76.83238983154297 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8898, F1 Micro: 0.7335, F1 Macro: 0.7292\n",
      "Launching training on 2 GPUs.\n",
      "5247\n",
      "BESRA Uncertainty Score Threshold 160.71585469933774\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 253.50546884536743 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5231, Accuracy: 0.8184, F1 Micro: 0.2923, F1 Macro: 0.2291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3758, Accuracy: 0.8594, F1 Micro: 0.5872, F1 Macro: 0.5724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2913, Accuracy: 0.8695, F1 Micro: 0.6237, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2293, Accuracy: 0.888, F1 Micro: 0.7122, F1 Macro: 0.6989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.8895, F1 Micro: 0.7576, F1 Macro: 0.7546\n",
      "Epoch 6/10, Train Loss: 0.1601, Accuracy: 0.892, F1 Micro: 0.7356, F1 Macro: 0.7288\n",
      "Epoch 7/10, Train Loss: 0.1106, Accuracy: 0.8955, F1 Micro: 0.7572, F1 Macro: 0.7536\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.8967, F1 Micro: 0.7514, F1 Macro: 0.7421\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.8961, F1 Micro: 0.7453, F1 Macro: 0.7387\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.8928, F1 Micro: 0.7297, F1 Macro: 0.7226\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8895, F1 Micro: 0.7576, F1 Macro: 0.7546\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.61      0.71      0.65       248\n",
      "         radikalisme       0.70      0.81      0.75       243\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.71      0.81      0.76      1365\n",
      "           macro avg       0.72      0.80      0.75      1365\n",
      "        weighted avg       0.72      0.81      0.76      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 83.60516571998596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5144, Accuracy: 0.83, F1 Micro: 0.3696, F1 Macro: 0.3043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3672, Accuracy: 0.8616, F1 Micro: 0.6117, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.296, Accuracy: 0.8752, F1 Micro: 0.6647, F1 Macro: 0.6611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2352, Accuracy: 0.8914, F1 Micro: 0.7413, F1 Macro: 0.7348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1921, Accuracy: 0.8955, F1 Micro: 0.7632, F1 Macro: 0.757\n",
      "Epoch 6/10, Train Loss: 0.1635, Accuracy: 0.8917, F1 Micro: 0.7298, F1 Macro: 0.7166\n",
      "Epoch 7/10, Train Loss: 0.1167, Accuracy: 0.893, F1 Micro: 0.7424, F1 Macro: 0.7378\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.8944, F1 Micro: 0.7474, F1 Macro: 0.7376\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.8925, F1 Micro: 0.7388, F1 Macro: 0.7293\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.8936, F1 Micro: 0.7454, F1 Macro: 0.7395\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8955, F1 Micro: 0.7632, F1 Macro: 0.757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.70      0.80      0.74       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.73      0.78      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 82.3337709903717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5145, Accuracy: 0.8406, F1 Micro: 0.4864, F1 Macro: 0.3807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3749, Accuracy: 0.8683, F1 Micro: 0.648, F1 Macro: 0.6365\n",
      "Epoch 3/10, Train Loss: 0.2945, Accuracy: 0.8733, F1 Micro: 0.6416, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2355, Accuracy: 0.8922, F1 Micro: 0.7378, F1 Macro: 0.7312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1918, Accuracy: 0.8969, F1 Micro: 0.7621, F1 Macro: 0.7534\n",
      "Epoch 6/10, Train Loss: 0.1611, Accuracy: 0.8938, F1 Micro: 0.7338, F1 Macro: 0.72\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.8939, F1 Micro: 0.7465, F1 Macro: 0.7404\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.8938, F1 Micro: 0.747, F1 Macro: 0.7412\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.8934, F1 Micro: 0.7281, F1 Macro: 0.7163\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.8928, F1 Micro: 0.7349, F1 Macro: 0.7281\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8969, F1 Micro: 0.7621, F1 Macro: 0.7534\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.65      0.63      0.64       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1365\n",
      "           macro avg       0.74      0.76      0.75      1365\n",
      "        weighted avg       0.75      0.77      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 80.49964666366577 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.894, F1 Micro: 0.761, F1 Macro: 0.755\n",
      "Launching training on 2 GPUs.\n",
      "4722\n",
      "BESRA Uncertainty Score Threshold 178.13294989813394\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 228.58098459243774 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4956, Accuracy: 0.8397, F1 Micro: 0.4617, F1 Macro: 0.3958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3404, Accuracy: 0.873, F1 Micro: 0.6945, F1 Macro: 0.6987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2724, Accuracy: 0.8916, F1 Micro: 0.7204, F1 Macro: 0.7164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2208, Accuracy: 0.8953, F1 Micro: 0.7639, F1 Macro: 0.7592\n",
      "Epoch 5/10, Train Loss: 0.1703, Accuracy: 0.8959, F1 Micro: 0.7566, F1 Macro: 0.7538\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.8955, F1 Micro: 0.7549, F1 Macro: 0.7537\n",
      "Epoch 7/10, Train Loss: 0.1184, Accuracy: 0.8897, F1 Micro: 0.6996, F1 Macro: 0.6817\n",
      "Epoch 8/10, Train Loss: 0.0952, Accuracy: 0.8942, F1 Micro: 0.7393, F1 Macro: 0.7348\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.8969, F1 Micro: 0.7621, F1 Macro: 0.7594\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.8928, F1 Micro: 0.7559, F1 Macro: 0.7493\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8953, F1 Micro: 0.7639, F1 Macro: 0.7592\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.90      0.89       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.73      0.78      0.75       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.76      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 94.0510618686676 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.484, Accuracy: 0.843, F1 Micro: 0.4785, F1 Macro: 0.4284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3333, Accuracy: 0.8684, F1 Micro: 0.6881, F1 Macro: 0.6926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2755, Accuracy: 0.89, F1 Micro: 0.7254, F1 Macro: 0.7251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.226, Accuracy: 0.8933, F1 Micro: 0.7577, F1 Macro: 0.7523\n",
      "Epoch 5/10, Train Loss: 0.1764, Accuracy: 0.8891, F1 Micro: 0.7577, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.8958, F1 Micro: 0.7619, F1 Macro: 0.7604\n",
      "Epoch 7/10, Train Loss: 0.12, Accuracy: 0.8952, F1 Micro: 0.7408, F1 Macro: 0.7273\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.8944, F1 Micro: 0.7472, F1 Macro: 0.7392\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.8944, F1 Micro: 0.7577, F1 Macro: 0.7514\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.8961, F1 Micro: 0.7507, F1 Macro: 0.742\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8958, F1 Micro: 0.7619, F1 Macro: 0.7604\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.84      0.89       370\n",
      "                sara       0.62      0.72      0.67       248\n",
      "         radikalisme       0.69      0.84      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 96.24919962882996 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4878, Accuracy: 0.8356, F1 Micro: 0.407, F1 Macro: 0.3347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3373, Accuracy: 0.8756, F1 Micro: 0.6971, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.274, Accuracy: 0.892, F1 Micro: 0.7517, F1 Macro: 0.7501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.223, Accuracy: 0.8908, F1 Micro: 0.7644, F1 Macro: 0.7621\n",
      "Epoch 5/10, Train Loss: 0.1776, Accuracy: 0.8952, F1 Micro: 0.7643, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1434, Accuracy: 0.8938, F1 Micro: 0.7674, F1 Macro: 0.7694\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.8938, F1 Micro: 0.7308, F1 Macro: 0.7195\n",
      "Epoch 8/10, Train Loss: 0.0882, Accuracy: 0.8966, F1 Micro: 0.7622, F1 Macro: 0.7593\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.8958, F1 Micro: 0.7525, F1 Macro: 0.7453\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.8958, F1 Micro: 0.7538, F1 Macro: 0.7495\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8938, F1 Micro: 0.7674, F1 Macro: 0.7694\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.82      0.89       370\n",
      "                sara       0.61      0.81      0.69       248\n",
      "         radikalisme       0.68      0.87      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.82      0.77      1365\n",
      "           macro avg       0.73      0.83      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.77      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 94.87934589385986 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8949, F1 Micro: 0.7644, F1 Macro: 0.763\n",
      "Launching training on 2 GPUs.\n",
      "4249\n",
      "BESRA Uncertainty Score Threshold 245.0348778595517\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 207.49924087524414 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4783, Accuracy: 0.8537, F1 Micro: 0.5329, F1 Macro: 0.5034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3148, Accuracy: 0.877, F1 Micro: 0.6681, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2535, Accuracy: 0.8906, F1 Micro: 0.7036, F1 Macro: 0.6784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2043, Accuracy: 0.8927, F1 Micro: 0.7224, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.8963, F1 Micro: 0.7454, F1 Macro: 0.7275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.8986, F1 Micro: 0.7671, F1 Macro: 0.765\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.8959, F1 Micro: 0.7582, F1 Macro: 0.7548\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.8939, F1 Micro: 0.7539, F1 Macro: 0.7442\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.8969, F1 Micro: 0.7566, F1 Macro: 0.7533\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.8973, F1 Micro: 0.7608, F1 Macro: 0.7579\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8986, F1 Micro: 0.7671, F1 Macro: 0.765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.86      0.91       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.69      0.81      0.75       243\n",
      "pencemaran_nama_baik       0.71      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 109.89202570915222 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4723, Accuracy: 0.8553, F1 Micro: 0.55, F1 Macro: 0.5348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3119, Accuracy: 0.8834, F1 Micro: 0.6992, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2575, Accuracy: 0.892, F1 Micro: 0.7206, F1 Macro: 0.7018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2058, Accuracy: 0.8983, F1 Micro: 0.7452, F1 Macro: 0.7385\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.8919, F1 Micro: 0.7383, F1 Macro: 0.7218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.897, F1 Micro: 0.7547, F1 Macro: 0.7487\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.8988, F1 Micro: 0.7484, F1 Macro: 0.7401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.8989, F1 Micro: 0.7605, F1 Macro: 0.7509\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.8992, F1 Micro: 0.7596, F1 Macro: 0.7554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.8998, F1 Micro: 0.7607, F1 Macro: 0.7549\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8998, F1 Micro: 0.7607, F1 Macro: 0.7549\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.75      0.72      0.73       243\n",
      "pencemaran_nama_baik       0.73      0.70      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.75      0.76      1365\n",
      "           macro avg       0.77      0.74      0.75      1365\n",
      "        weighted avg       0.78      0.75      0.76      1365\n",
      "         samples avg       0.44      0.43      0.42      1365\n",
      "\n",
      "Training completed in 111.5486011505127 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4695, Accuracy: 0.8436, F1 Micro: 0.4615, F1 Macro: 0.3929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3126, Accuracy: 0.8847, F1 Micro: 0.7015, F1 Macro: 0.6945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2585, Accuracy: 0.8927, F1 Micro: 0.7107, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.8959, F1 Micro: 0.7405, F1 Macro: 0.7324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1626, Accuracy: 0.8944, F1 Micro: 0.7507, F1 Macro: 0.7368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1214, Accuracy: 0.898, F1 Micro: 0.7695, F1 Macro: 0.7665\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.8998, F1 Micro: 0.7455, F1 Macro: 0.7371\n",
      "Epoch 8/10, Train Loss: 0.0785, Accuracy: 0.8948, F1 Micro: 0.7444, F1 Macro: 0.7317\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9006, F1 Micro: 0.7554, F1 Macro: 0.748\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9009, F1 Micro: 0.7604, F1 Macro: 0.7563\n",
      "Model 3 - Iteration 2394: Accuracy: 0.898, F1 Micro: 0.7695, F1 Macro: 0.7665\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.67      0.86      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 109.54127430915833 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8988, F1 Micro: 0.7658, F1 Macro: 0.7621\n",
      "Launching training on 2 GPUs.\n",
      "3824\n",
      "BESRA Uncertainty Score Threshold 210.40910415239608\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 186.22606945037842 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4505, Accuracy: 0.8717, F1 Micro: 0.672, F1 Macro: 0.6522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3027, Accuracy: 0.8884, F1 Micro: 0.7176, F1 Macro: 0.7121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.8972, F1 Micro: 0.7424, F1 Macro: 0.7328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.8998, F1 Micro: 0.7499, F1 Macro: 0.7422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1455, Accuracy: 0.9028, F1 Micro: 0.7722, F1 Macro: 0.7658\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9016, F1 Micro: 0.7581, F1 Macro: 0.7514\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.8944, F1 Micro: 0.7666, F1 Macro: 0.7641\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.8998, F1 Micro: 0.7649, F1 Macro: 0.7658\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9, F1 Micro: 0.7578, F1 Macro: 0.7554\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.8956, F1 Micro: 0.7594, F1 Macro: 0.7565\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9028, F1 Micro: 0.7722, F1 Macro: 0.7658\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 121.31606960296631 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.8708, F1 Micro: 0.6606, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3004, Accuracy: 0.8891, F1 Micro: 0.7351, F1 Macro: 0.7326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2403, Accuracy: 0.8967, F1 Micro: 0.7393, F1 Macro: 0.733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.8995, F1 Micro: 0.7532, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.154, Accuracy: 0.9011, F1 Micro: 0.7594, F1 Macro: 0.7498\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9002, F1 Micro: 0.7491, F1 Macro: 0.741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.8958, F1 Micro: 0.7629, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.8984, F1 Micro: 0.7721, F1 Macro: 0.773\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9019, F1 Micro: 0.7706, F1 Macro: 0.766\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.902, F1 Micro: 0.7672, F1 Macro: 0.7623\n",
      "Model 2 - Iteration 2777: Accuracy: 0.8984, F1 Micro: 0.7721, F1 Macro: 0.773\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.73      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.75      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 123.14979290962219 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4463, Accuracy: 0.8653, F1 Micro: 0.6031, F1 Macro: 0.5368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3066, Accuracy: 0.8911, F1 Micro: 0.7272, F1 Macro: 0.7185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2425, Accuracy: 0.8991, F1 Micro: 0.7527, F1 Macro: 0.7473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1942, Accuracy: 0.8981, F1 Micro: 0.7562, F1 Macro: 0.7542\n",
      "Epoch 5/10, Train Loss: 0.1512, Accuracy: 0.8972, F1 Micro: 0.7418, F1 Macro: 0.7292\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.8959, F1 Micro: 0.7483, F1 Macro: 0.7413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.8905, F1 Micro: 0.7664, F1 Macro: 0.7675\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.8983, F1 Micro: 0.7652, F1 Macro: 0.7653\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.8973, F1 Micro: 0.7608, F1 Macro: 0.7569\n",
      "Epoch 10/10, Train Loss: 0.0387, Accuracy: 0.8961, F1 Micro: 0.7604, F1 Macro: 0.7589\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8905, F1 Micro: 0.7664, F1 Macro: 0.7675\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.60      0.79      0.68       248\n",
      "         radikalisme       0.73      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.64      0.84      0.72       504\n",
      "\n",
      "           micro avg       0.70      0.84      0.77      1365\n",
      "           macro avg       0.71      0.84      0.77      1365\n",
      "        weighted avg       0.72      0.84      0.77      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 120.49023079872131 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8972, F1 Micro: 0.7702, F1 Macro: 0.7688\n",
      "Launching training on 2 GPUs.\n",
      "3441\n",
      "BESRA Uncertainty Score Threshold 214.30970230890256\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 169.0000514984131 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4537, Accuracy: 0.8741, F1 Micro: 0.6747, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2896, Accuracy: 0.8953, F1 Micro: 0.746, F1 Macro: 0.7377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.8992, F1 Micro: 0.7649, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.9, F1 Micro: 0.7706, F1 Macro: 0.7642\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.8991, F1 Micro: 0.7658, F1 Macro: 0.7612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.8991, F1 Micro: 0.7729, F1 Macro: 0.771\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.8988, F1 Micro: 0.772, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.8983, F1 Micro: 0.763, F1 Macro: 0.7601\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.8975, F1 Micro: 0.7644, F1 Macro: 0.7624\n",
      "Epoch 10/10, Train Loss: 0.0378, Accuracy: 0.8975, F1 Micro: 0.7645, F1 Macro: 0.7612\n",
      "Model 1 - Iteration 3122: Accuracy: 0.8991, F1 Micro: 0.7729, F1 Macro: 0.771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.61      0.76      0.68       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 130.47034621238708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.447, Accuracy: 0.8703, F1 Micro: 0.6623, F1 Macro: 0.6662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2863, Accuracy: 0.8894, F1 Micro: 0.7289, F1 Macro: 0.7236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2384, Accuracy: 0.8947, F1 Micro: 0.7586, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1907, Accuracy: 0.8989, F1 Micro: 0.7708, F1 Macro: 0.766\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.8963, F1 Micro: 0.7543, F1 Macro: 0.7432\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.892, F1 Micro: 0.768, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.898, F1 Micro: 0.7645, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9003, F1 Micro: 0.7675, F1 Macro: 0.7658\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.898, F1 Micro: 0.7588, F1 Macro: 0.7525\n",
      "Epoch 10/10, Train Loss: 0.0385, Accuracy: 0.8983, F1 Micro: 0.7694, F1 Macro: 0.7667\n",
      "Model 2 - Iteration 3122: Accuracy: 0.8989, F1 Micro: 0.7708, F1 Macro: 0.766\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       370\n",
      "                sara       0.64      0.67      0.66       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 128.2847180366516 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4488, Accuracy: 0.8747, F1 Micro: 0.6843, F1 Macro: 0.6801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.8939, F1 Micro: 0.7357, F1 Macro: 0.7233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2417, Accuracy: 0.8978, F1 Micro: 0.7632, F1 Macro: 0.7581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.8998, F1 Micro: 0.7726, F1 Macro: 0.7695\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.8977, F1 Micro: 0.7444, F1 Macro: 0.738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.8997, F1 Micro: 0.7762, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.8989, F1 Micro: 0.7754, F1 Macro: 0.7749\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9027, F1 Micro: 0.7752, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9, F1 Micro: 0.7635, F1 Macro: 0.7614\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.9016, F1 Micro: 0.768, F1 Macro: 0.7641\n",
      "Model 3 - Iteration 3122: Accuracy: 0.8997, F1 Micro: 0.7762, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.59      0.77      0.67       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 129.75753116607666 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8992, F1 Micro: 0.7733, F1 Macro: 0.7711\n",
      "Launching training on 2 GPUs.\n",
      "3096\n",
      "BESRA Uncertainty Score Threshold 157.02549631870863\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 151.2312343120575 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.45, Accuracy: 0.8622, F1 Micro: 0.5698, F1 Macro: 0.5138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2968, Accuracy: 0.893, F1 Micro: 0.7354, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2292, Accuracy: 0.8981, F1 Micro: 0.7545, F1 Macro: 0.7527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.8986, F1 Micro: 0.7634, F1 Macro: 0.7557\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.8988, F1 Micro: 0.7575, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.1158, Accuracy: 0.8989, F1 Micro: 0.7489, F1 Macro: 0.7369\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.8983, F1 Micro: 0.7472, F1 Macro: 0.737\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.8989, F1 Micro: 0.7561, F1 Macro: 0.7494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.898, F1 Micro: 0.7688, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9006, F1 Micro: 0.7573, F1 Macro: 0.7479\n",
      "Model 1 - Iteration 3432: Accuracy: 0.898, F1 Micro: 0.7688, F1 Macro: 0.7669\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.63      0.68      0.65       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 139.56241941452026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4425, Accuracy: 0.8711, F1 Micro: 0.6325, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.8955, F1 Micro: 0.7475, F1 Macro: 0.7427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.8991, F1 Micro: 0.7542, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1911, Accuracy: 0.8969, F1 Micro: 0.7678, F1 Macro: 0.7639\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.8973, F1 Micro: 0.7478, F1 Macro: 0.7431\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.8967, F1 Micro: 0.7547, F1 Macro: 0.7504\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.8973, F1 Micro: 0.7522, F1 Macro: 0.7401\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.8983, F1 Micro: 0.7647, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9013, F1 Micro: 0.7692, F1 Macro: 0.7661\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.8998, F1 Micro: 0.7672, F1 Macro: 0.7578\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9013, F1 Micro: 0.7692, F1 Macro: 0.7661\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.64      0.66      0.65       248\n",
      "         radikalisme       0.76      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.76      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 138.86219215393066 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4433, Accuracy: 0.883, F1 Micro: 0.7022, F1 Macro: 0.6963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2918, Accuracy: 0.8961, F1 Micro: 0.7482, F1 Macro: 0.746\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.8964, F1 Micro: 0.7482, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.8941, F1 Micro: 0.7664, F1 Macro: 0.7624\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.8988, F1 Micro: 0.7612, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1161, Accuracy: 0.9034, F1 Micro: 0.7689, F1 Macro: 0.7625\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9013, F1 Micro: 0.7685, F1 Macro: 0.7654\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.8991, F1 Micro: 0.7673, F1 Macro: 0.7617\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9006, F1 Micro: 0.7637, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9023, F1 Micro: 0.7698, F1 Macro: 0.762\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9023, F1 Micro: 0.7698, F1 Macro: 0.762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.61      0.63       248\n",
      "         radikalisme       0.76      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 138.61769151687622 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.9005, F1 Micro: 0.7693, F1 Macro: 0.765\n",
      "Launching training on 2 GPUs.\n",
      "2786\n",
      "BESRA Uncertainty Score Threshold 179.09353875592763\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 136.534419298172 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.8789, F1 Micro: 0.7179, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.273, Accuracy: 0.9009, F1 Micro: 0.7633, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2259, Accuracy: 0.9011, F1 Micro: 0.7719, F1 Macro: 0.7749\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.9038, F1 Micro: 0.7645, F1 Macro: 0.7537\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9039, F1 Micro: 0.7652, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1072, Accuracy: 0.9042, F1 Micro: 0.7813, F1 Macro: 0.7748\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.8947, F1 Micro: 0.7752, F1 Macro: 0.7749\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9002, F1 Micro: 0.7805, F1 Macro: 0.7808\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9003, F1 Micro: 0.7761, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9006, F1 Micro: 0.7711, F1 Macro: 0.7672\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9042, F1 Micro: 0.7813, F1 Macro: 0.7748\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       370\n",
      "                sara       0.66      0.63      0.65       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 145.70871210098267 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4208, Accuracy: 0.872, F1 Micro: 0.7129, F1 Macro: 0.7152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2742, Accuracy: 0.8966, F1 Micro: 0.7593, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2307, Accuracy: 0.9009, F1 Micro: 0.7664, F1 Macro: 0.7652\n",
      "Epoch 4/10, Train Loss: 0.1901, Accuracy: 0.8997, F1 Micro: 0.7438, F1 Macro: 0.73\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.8995, F1 Micro: 0.7511, F1 Macro: 0.7438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9022, F1 Micro: 0.7777, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.8942, F1 Micro: 0.7715, F1 Macro: 0.7691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.8994, F1 Micro: 0.7643, F1 Macro: 0.7599\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.9, F1 Micro: 0.7679, F1 Macro: 0.7646\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 145.99336409568787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4239, Accuracy: 0.8772, F1 Micro: 0.7177, F1 Macro: 0.717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2748, Accuracy: 0.8995, F1 Micro: 0.7637, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9028, F1 Micro: 0.7748, F1 Macro: 0.7733\n",
      "Epoch 4/10, Train Loss: 0.19, Accuracy: 0.9038, F1 Micro: 0.7563, F1 Macro: 0.7447\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9031, F1 Micro: 0.7538, F1 Macro: 0.7366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.9008, F1 Micro: 0.7765, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9019, F1 Micro: 0.7776, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9039, F1 Micro: 0.7815, F1 Macro: 0.7815\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9019, F1 Micro: 0.7674, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9019, F1 Micro: 0.7619, F1 Macro: 0.7574\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9039, F1 Micro: 0.7815, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.73      0.70       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 148.12411236763 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.9035, F1 Micro: 0.7807, F1 Macro: 0.7777\n",
      "Launching training on 2 GPUs.\n",
      "2507\n",
      "BESRA Uncertainty Score Threshold 182.87356146329225\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 122.99342823028564 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4235, Accuracy: 0.8833, F1 Micro: 0.7108, F1 Macro: 0.7057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.276, Accuracy: 0.8978, F1 Micro: 0.7525, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9019, F1 Micro: 0.7716, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.184, Accuracy: 0.9033, F1 Micro: 0.7765, F1 Macro: 0.7716\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9014, F1 Micro: 0.775, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1055, Accuracy: 0.9033, F1 Micro: 0.7804, F1 Macro: 0.7796\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.8964, F1 Micro: 0.7749, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9023, F1 Micro: 0.7693, F1 Macro: 0.7642\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.8981, F1 Micro: 0.7753, F1 Macro: 0.7765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.903, F1 Micro: 0.7817, F1 Macro: 0.7813\n",
      "Model 1 - Iteration 3886: Accuracy: 0.903, F1 Micro: 0.7817, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 157.36444902420044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4182, Accuracy: 0.878, F1 Micro: 0.7049, F1 Macro: 0.7006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.8936, F1 Micro: 0.739, F1 Macro: 0.7333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2334, Accuracy: 0.8983, F1 Micro: 0.7674, F1 Macro: 0.7641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9025, F1 Micro: 0.7762, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9006, F1 Micro: 0.7813, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.8977, F1 Micro: 0.7731, F1 Macro: 0.7732\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.8994, F1 Micro: 0.7648, F1 Macro: 0.7611\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.8975, F1 Micro: 0.769, F1 Macro: 0.7642\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.8973, F1 Micro: 0.7707, F1 Macro: 0.7698\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9019, F1 Micro: 0.7793, F1 Macro: 0.7766\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9006, F1 Micro: 0.7813, F1 Macro: 0.7797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.62      0.76      0.68       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 152.71810460090637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4173, Accuracy: 0.8823, F1 Micro: 0.7224, F1 Macro: 0.7171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.8945, F1 Micro: 0.7385, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9003, F1 Micro: 0.7752, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9044, F1 Micro: 0.7825, F1 Macro: 0.78\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9042, F1 Micro: 0.783, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9048, F1 Micro: 0.7891, F1 Macro: 0.7874\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.8997, F1 Micro: 0.7777, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9042, F1 Micro: 0.7767, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.907, F1 Micro: 0.785, F1 Macro: 0.7836\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.9048, F1 Micro: 0.782, F1 Macro: 0.7804\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9048, F1 Micro: 0.7891, F1 Macro: 0.7874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.71      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 153.39811635017395 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.9028, F1 Micro: 0.784, F1 Macro: 0.7828\n",
      "Launching training on 2 GPUs.\n",
      "2332\n",
      "BESRA Uncertainty Score Threshold 222.1441862192576\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 114.05582571029663 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4105, Accuracy: 0.8739, F1 Micro: 0.6347, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.8975, F1 Micro: 0.7578, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.213, Accuracy: 0.9027, F1 Micro: 0.7818, F1 Macro: 0.7809\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9011, F1 Micro: 0.7707, F1 Macro: 0.7654\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9033, F1 Micro: 0.7681, F1 Macro: 0.7637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9064, F1 Micro: 0.7823, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9028, F1 Micro: 0.7628, F1 Macro: 0.7588\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9033, F1 Micro: 0.7649, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9009, F1 Micro: 0.7788, F1 Macro: 0.7745\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9002, F1 Micro: 0.7743, F1 Macro: 0.773\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9064, F1 Micro: 0.7823, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 158.29257917404175 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.403, Accuracy: 0.8747, F1 Micro: 0.6504, F1 Macro: 0.6314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2746, Accuracy: 0.8981, F1 Micro: 0.7599, F1 Macro: 0.749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2112, Accuracy: 0.9, F1 Micro: 0.7758, F1 Macro: 0.7738\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9034, F1 Micro: 0.7671, F1 Macro: 0.7587\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9027, F1 Micro: 0.771, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9041, F1 Micro: 0.782, F1 Macro: 0.778\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9014, F1 Micro: 0.7726, F1 Macro: 0.7701\n",
      "Epoch 8/10, Train Loss: 0.0572, Accuracy: 0.9019, F1 Micro: 0.7744, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0411, Accuracy: 0.8994, F1 Micro: 0.7672, F1 Macro: 0.7606\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.9014, F1 Micro: 0.7774, F1 Macro: 0.7759\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9041, F1 Micro: 0.782, F1 Macro: 0.778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 157.70032048225403 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4074, Accuracy: 0.8797, F1 Micro: 0.6652, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.272, Accuracy: 0.8977, F1 Micro: 0.7568, F1 Macro: 0.7507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9, F1 Micro: 0.7773, F1 Macro: 0.7751\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9019, F1 Micro: 0.7674, F1 Macro: 0.7596\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9025, F1 Micro: 0.7652, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9031, F1 Micro: 0.7784, F1 Macro: 0.7775\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9023, F1 Micro: 0.7705, F1 Macro: 0.767\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9041, F1 Micro: 0.7761, F1 Macro: 0.7747\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9014, F1 Micro: 0.7664, F1 Macro: 0.7622\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.8992, F1 Micro: 0.7624, F1 Macro: 0.7597\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9031, F1 Micro: 0.7784, F1 Macro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 157.59317111968994 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.9045, F1 Micro: 0.7809, F1 Macro: 0.7789\n",
      "Launching training on 2 GPUs.\n",
      "2098\n",
      "BESRA Uncertainty Score Threshold 296.1961033904852\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 102.16537046432495 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8767, F1 Micro: 0.6689, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.8986, F1 Micro: 0.7653, F1 Macro: 0.758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9023, F1 Micro: 0.7834, F1 Macro: 0.7826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9073, F1 Micro: 0.7862, F1 Macro: 0.7828\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.8995, F1 Micro: 0.7443, F1 Macro: 0.7313\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9017, F1 Micro: 0.7712, F1 Macro: 0.7633\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.9045, F1 Micro: 0.7731, F1 Macro: 0.7653\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9009, F1 Micro: 0.7758, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9033, F1 Micro: 0.7776, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0401, Accuracy: 0.9033, F1 Micro: 0.7737, F1 Macro: 0.7692\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9073, F1 Micro: 0.7862, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 165.13368439674377 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4123, Accuracy: 0.8789, F1 Micro: 0.6901, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.9, F1 Micro: 0.7651, F1 Macro: 0.7545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9008, F1 Micro: 0.7804, F1 Macro: 0.78\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9059, F1 Micro: 0.7809, F1 Macro: 0.7757\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9003, F1 Micro: 0.7502, F1 Macro: 0.7388\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9038, F1 Micro: 0.7727, F1 Macro: 0.7656\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9034, F1 Micro: 0.7759, F1 Macro: 0.7718\n",
      "Epoch 8/10, Train Loss: 0.0584, Accuracy: 0.8997, F1 Micro: 0.7633, F1 Macro: 0.7605\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9025, F1 Micro: 0.7687, F1 Macro: 0.7617\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.9013, F1 Micro: 0.7748, F1 Macro: 0.7697\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9059, F1 Micro: 0.7809, F1 Macro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 163.36596250534058 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4146, Accuracy: 0.8794, F1 Micro: 0.6764, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2642, Accuracy: 0.8991, F1 Micro: 0.7661, F1 Macro: 0.7574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9034, F1 Micro: 0.7836, F1 Macro: 0.7831\n",
      "Epoch 4/10, Train Loss: 0.1872, Accuracy: 0.9069, F1 Micro: 0.7815, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9014, F1 Micro: 0.7526, F1 Macro: 0.741\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9019, F1 Micro: 0.7725, F1 Macro: 0.7665\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9038, F1 Micro: 0.7677, F1 Macro: 0.7601\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9023, F1 Micro: 0.7762, F1 Macro: 0.7702\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9047, F1 Micro: 0.7686, F1 Macro: 0.7605\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.9044, F1 Micro: 0.7813, F1 Macro: 0.7764\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9034, F1 Micro: 0.7836, F1 Macro: 0.7831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.64      0.78      0.70       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 162.07788348197937 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.9056, F1 Micro: 0.7836, F1 Macro: 0.7805\n",
      "Launching training on 2 GPUs.\n",
      "1888\n",
      "BESRA Uncertainty Score Threshold 188.11434958309616\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 92.59873390197754 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4105, Accuracy: 0.8833, F1 Micro: 0.6979, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.898, F1 Micro: 0.7444, F1 Macro: 0.7328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9028, F1 Micro: 0.7653, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1712, Accuracy: 0.9016, F1 Micro: 0.7735, F1 Macro: 0.7703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.765\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9028, F1 Micro: 0.7705, F1 Macro: 0.7614\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.8978, F1 Micro: 0.7485, F1 Macro: 0.7418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9031, F1 Micro: 0.7755, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9047, F1 Micro: 0.7818, F1 Macro: 0.781\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9036, F1 Micro: 0.7731, F1 Macro: 0.7713\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9047, F1 Micro: 0.7818, F1 Macro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       370\n",
      "                sara       0.64      0.76      0.70       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 176.4596664905548 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4053, Accuracy: 0.8809, F1 Micro: 0.7007, F1 Macro: 0.6919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.8994, F1 Micro: 0.7593, F1 Macro: 0.7492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9013, F1 Micro: 0.7602, F1 Macro: 0.7576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1708, Accuracy: 0.9038, F1 Micro: 0.7765, F1 Macro: 0.7706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.9042, F1 Micro: 0.7768, F1 Macro: 0.7691\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9022, F1 Micro: 0.7676, F1 Macro: 0.7604\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9017, F1 Micro: 0.7613, F1 Macro: 0.754\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9005, F1 Micro: 0.752, F1 Macro: 0.7451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9034, F1 Micro: 0.7782, F1 Macro: 0.7759\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.903, F1 Micro: 0.7743, F1 Macro: 0.7705\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9034, F1 Micro: 0.7782, F1 Macro: 0.7759\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 173.9393503665924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4063, Accuracy: 0.8855, F1 Micro: 0.7173, F1 Macro: 0.7131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2682, Accuracy: 0.898, F1 Micro: 0.7579, F1 Macro: 0.7507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2179, Accuracy: 0.9061, F1 Micro: 0.779, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9017, F1 Micro: 0.7727, F1 Macro: 0.7701\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.9031, F1 Micro: 0.7779, F1 Macro: 0.7713\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.903, F1 Micro: 0.7564, F1 Macro: 0.7465\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9019, F1 Micro: 0.763, F1 Macro: 0.7579\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.9038, F1 Micro: 0.7712, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9005, F1 Micro: 0.7798, F1 Macro: 0.7794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9041, F1 Micro: 0.7826, F1 Macro: 0.7807\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9041, F1 Micro: 0.7826, F1 Macro: 0.7807\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.06930804252625 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.9041, F1 Micro: 0.7809, F1 Macro: 0.7792\n",
      "Launching training on 2 GPUs.\n",
      "1688\n",
      "BESRA Uncertainty Score Threshold 251.33077881530812\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 83.17211055755615 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4102, Accuracy: 0.8786, F1 Micro: 0.6703, F1 Macro: 0.6361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.8953, F1 Micro: 0.7331, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2315, Accuracy: 0.9048, F1 Micro: 0.7796, F1 Macro: 0.7742\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9022, F1 Micro: 0.7759, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9033, F1 Micro: 0.7826, F1 Macro: 0.7768\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9039, F1 Micro: 0.7751, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9053, F1 Micro: 0.7828, F1 Macro: 0.7769\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.8972, F1 Micro: 0.7704, F1 Macro: 0.7676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.9059, F1 Micro: 0.7848, F1 Macro: 0.7823\n",
      "Epoch 10/10, Train Loss: 0.0354, Accuracy: 0.9011, F1 Micro: 0.7775, F1 Macro: 0.7733\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9059, F1 Micro: 0.7848, F1 Macro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 177.67867946624756 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4069, Accuracy: 0.873, F1 Micro: 0.6491, F1 Macro: 0.6115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2614, Accuracy: 0.8952, F1 Micro: 0.7412, F1 Macro: 0.734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2326, Accuracy: 0.9036, F1 Micro: 0.7792, F1 Macro: 0.7753\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9017, F1 Micro: 0.7719, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9027, F1 Micro: 0.7876, F1 Macro: 0.7856\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9027, F1 Micro: 0.7784, F1 Macro: 0.7739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9042, F1 Micro: 0.7881, F1 Macro: 0.7891\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9056, F1 Micro: 0.7804, F1 Macro: 0.7766\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9011, F1 Micro: 0.7789, F1 Macro: 0.7741\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9042, F1 Micro: 0.7881, F1 Macro: 0.7891\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.64      0.78      0.70       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 175.575581073761 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4093, Accuracy: 0.8769, F1 Micro: 0.6538, F1 Macro: 0.6185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.8983, F1 Micro: 0.7432, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9042, F1 Micro: 0.7781, F1 Macro: 0.7708\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9013, F1 Micro: 0.7693, F1 Macro: 0.7589\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9031, F1 Micro: 0.7749, F1 Macro: 0.7688\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9044, F1 Micro: 0.7733, F1 Macro: 0.7646\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9061, F1 Micro: 0.7775, F1 Macro: 0.7719\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9019, F1 Micro: 0.7746, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7788\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9013, F1 Micro: 0.7765, F1 Macro: 0.7714\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 173.80783414840698 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.9048, F1 Micro: 0.7841, F1 Macro: 0.7834\n",
      "Launching training on 2 GPUs.\n",
      "1555\n",
      "BESRA Uncertainty Score Threshold 265.6241478409179\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 76.59924530982971 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4024, Accuracy: 0.8872, F1 Micro: 0.7117, F1 Macro: 0.6908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.8934, F1 Micro: 0.7451, F1 Macro: 0.74\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9033, F1 Micro: 0.7703, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9106, F1 Micro: 0.7888, F1 Macro: 0.7832\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9013, F1 Micro: 0.7697, F1 Macro: 0.7575\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.9039, F1 Micro: 0.7756, F1 Macro: 0.7678\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9014, F1 Micro: 0.7636, F1 Macro: 0.7502\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.902, F1 Micro: 0.7608, F1 Macro: 0.7491\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9038, F1 Micro: 0.7717, F1 Macro: 0.7647\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.9009, F1 Micro: 0.7732, F1 Macro: 0.7696\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9106, F1 Micro: 0.7888, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.86      0.91       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.75      0.76      0.76       504\n",
      "\n",
      "           micro avg       0.80      0.78      0.79      1365\n",
      "           macro avg       0.79      0.78      0.78      1365\n",
      "        weighted avg       0.80      0.78      0.79      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 180.19722151756287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3944, Accuracy: 0.8828, F1 Micro: 0.7068, F1 Macro: 0.6944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.8955, F1 Micro: 0.7561, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2112, Accuracy: 0.9036, F1 Micro: 0.7726, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1748, Accuracy: 0.9028, F1 Micro: 0.7767, F1 Macro: 0.7729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9042, F1 Micro: 0.7786, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.1072, Accuracy: 0.9036, F1 Micro: 0.7752, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.8994, F1 Micro: 0.767, F1 Macro: 0.7604\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9047, F1 Micro: 0.7612, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7777\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9036, F1 Micro: 0.7773, F1 Macro: 0.7743\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 182.48239850997925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3978, Accuracy: 0.8858, F1 Micro: 0.7174, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.8948, F1 Micro: 0.7541, F1 Macro: 0.7523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9039, F1 Micro: 0.7756, F1 Macro: 0.7739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1777, Accuracy: 0.9048, F1 Micro: 0.7781, F1 Macro: 0.7761\n",
      "Epoch 5/10, Train Loss: 0.1344, Accuracy: 0.9048, F1 Micro: 0.7696, F1 Macro: 0.7567\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9022, F1 Micro: 0.7772, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9027, F1 Micro: 0.7787, F1 Macro: 0.7717\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9044, F1 Micro: 0.776, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.902, F1 Micro: 0.7706, F1 Macro: 0.7645\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9044, F1 Micro: 0.7781, F1 Macro: 0.7776\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9027, F1 Micro: 0.7787, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.94      0.91       370\n",
      "                sara       0.64      0.65      0.64       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.50719022750854 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.906, F1 Micro: 0.7824, F1 Macro: 0.7775\n",
      "Launching training on 2 GPUs.\n",
      "1355\n",
      "BESRA Uncertainty Score Threshold 266.0571219290492\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 66.83519697189331 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3905, Accuracy: 0.8814, F1 Micro: 0.6747, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2554, Accuracy: 0.8964, F1 Micro: 0.7275, F1 Macro: 0.7138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2072, Accuracy: 0.9047, F1 Micro: 0.7729, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9067, F1 Micro: 0.7891, F1 Macro: 0.7852\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.9081, F1 Micro: 0.7889, F1 Macro: 0.7837\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9064, F1 Micro: 0.7813, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9005, F1 Micro: 0.7803, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9056, F1 Micro: 0.7829, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9017, F1 Micro: 0.7683, F1 Macro: 0.7661\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9039, F1 Micro: 0.7764, F1 Macro: 0.7709\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9067, F1 Micro: 0.7891, F1 Macro: 0.7852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.63      0.69      0.66       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 188.62160634994507 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3855, Accuracy: 0.8809, F1 Micro: 0.6854, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.8936, F1 Micro: 0.7235, F1 Macro: 0.7132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2097, Accuracy: 0.9044, F1 Micro: 0.7662, F1 Macro: 0.753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1763, Accuracy: 0.9025, F1 Micro: 0.777, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.126, Accuracy: 0.9036, F1 Micro: 0.7789, F1 Macro: 0.7712\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9017, F1 Micro: 0.7746, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9025, F1 Micro: 0.7832, F1 Macro: 0.7824\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.9017, F1 Micro: 0.7773, F1 Macro: 0.7737\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9016, F1 Micro: 0.7752, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9041, F1 Micro: 0.7697, F1 Macro: 0.7653\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9025, F1 Micro: 0.7832, F1 Macro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.70      0.68       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 190.95829510688782 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.387, Accuracy: 0.8792, F1 Micro: 0.6681, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.8977, F1 Micro: 0.7319, F1 Macro: 0.7182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9052, F1 Micro: 0.7739, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.9019, F1 Micro: 0.7831, F1 Macro: 0.7832\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9036, F1 Micro: 0.7828, F1 Macro: 0.7778\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9033, F1 Micro: 0.7787, F1 Macro: 0.7767\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9038, F1 Micro: 0.7739, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9052, F1 Micro: 0.7802, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9003, F1 Micro: 0.7725, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9055, F1 Micro: 0.7781, F1 Macro: 0.7738\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9019, F1 Micro: 0.7831, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.91       370\n",
      "                sara       0.62      0.76      0.69       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 187.2560272216797 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9037, F1 Micro: 0.7852, F1 Macro: 0.7836\n",
      "Launching training on 2 GPUs.\n",
      "1155\n",
      "BESRA Uncertainty Score Threshold 243.82751430471959\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200Sampling duration: 58.02833580970764 seconds\n",
      "\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8845, F1 Micro: 0.698, F1 Macro: 0.6915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2689, Accuracy: 0.8992, F1 Micro: 0.7758, F1 Macro: 0.7747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.213, Accuracy: 0.9058, F1 Micro: 0.7776, F1 Macro: 0.7688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9031, F1 Micro: 0.7849, F1 Macro: 0.7815\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9023, F1 Micro: 0.7754, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9016, F1 Micro: 0.778, F1 Macro: 0.7772\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9009, F1 Micro: 0.7812, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.905, F1 Micro: 0.7821, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0467, Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.7742\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.9084, F1 Micro: 0.7802, F1 Macro: 0.7753\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9031, F1 Micro: 0.7849, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.84      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 193.60233783721924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.882, F1 Micro: 0.6853, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.8992, F1 Micro: 0.7722, F1 Macro: 0.7719\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.902, F1 Micro: 0.7692, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9003, F1 Micro: 0.7821, F1 Macro: 0.7806\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9016, F1 Micro: 0.7768, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9044, F1 Micro: 0.7802, F1 Macro: 0.7779\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9036, F1 Micro: 0.7796, F1 Macro: 0.7771\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.903, F1 Micro: 0.7811, F1 Macro: 0.7782\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9058, F1 Micro: 0.7706, F1 Macro: 0.7648\n",
      "Epoch 10/10, Train Loss: 0.0375, Accuracy: 0.9019, F1 Micro: 0.7795, F1 Macro: 0.7769\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9003, F1 Micro: 0.7821, F1 Macro: 0.7806\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.86      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 190.6710970401764 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8856, F1 Micro: 0.7174, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2707, Accuracy: 0.8989, F1 Micro: 0.7735, F1 Macro: 0.773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2167, Accuracy: 0.9038, F1 Micro: 0.7742, F1 Macro: 0.7625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.9066, F1 Micro: 0.7916, F1 Macro: 0.7906\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.9059, F1 Micro: 0.7839, F1 Macro: 0.7802\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.9048, F1 Micro: 0.7759, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9064, F1 Micro: 0.7848, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9045, F1 Micro: 0.7885, F1 Macro: 0.788\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.9078, F1 Micro: 0.7905, F1 Macro: 0.7879\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9023, F1 Micro: 0.7784, F1 Macro: 0.7768\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9066, F1 Micro: 0.7916, F1 Macro: 0.7906\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.88      0.92       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.77      0.83      0.80      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 193.59864163398743 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9033, F1 Micro: 0.7862, F1 Macro: 0.7842\n",
      "Launching training on 2 GPUs.\n",
      "955\n",
      "BESRA Uncertainty Score Threshold 251.10492173707635\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 47.79276657104492 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3911, Accuracy: 0.8839, F1 Micro: 0.7005, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2563, Accuracy: 0.9016, F1 Micro: 0.7624, F1 Macro: 0.7509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.9041, F1 Micro: 0.7729, F1 Macro: 0.7685\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9039, F1 Micro: 0.7696, F1 Macro: 0.7632\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9036, F1 Micro: 0.7674, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9061, F1 Micro: 0.7822, F1 Macro: 0.7784\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.9031, F1 Micro: 0.7775, F1 Macro: 0.7735\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.8991, F1 Micro: 0.7703, F1 Macro: 0.7644\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9013, F1 Micro: 0.7778, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9025, F1 Micro: 0.7824, F1 Macro: 0.7848\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9025, F1 Micro: 0.7824, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.77      0.70       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 200.74860262870789 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3831, Accuracy: 0.8873, F1 Micro: 0.7272, F1 Macro: 0.7177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.9011, F1 Micro: 0.7589, F1 Macro: 0.7476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9066, F1 Micro: 0.7733, F1 Macro: 0.7642\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9023, F1 Micro: 0.7676, F1 Macro: 0.7607\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.902, F1 Micro: 0.7584, F1 Macro: 0.7476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.907, F1 Micro: 0.7817, F1 Macro: 0.7776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.9067, F1 Micro: 0.7864, F1 Macro: 0.7823\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9002, F1 Micro: 0.766, F1 Macro: 0.756\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.8986, F1 Micro: 0.7769, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9036, F1 Micro: 0.7854, F1 Macro: 0.7837\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9067, F1 Micro: 0.7864, F1 Macro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.70      0.64      0.67       248\n",
      "         radikalisme       0.78      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 199.84626841545105 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3853, Accuracy: 0.8852, F1 Micro: 0.7047, F1 Macro: 0.6865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2558, Accuracy: 0.8995, F1 Micro: 0.7611, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2099, Accuracy: 0.9047, F1 Micro: 0.7696, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9034, F1 Micro: 0.7733, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9075, F1 Micro: 0.7761, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9066, F1 Micro: 0.7844, F1 Macro: 0.7835\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.9048, F1 Micro: 0.7755, F1 Macro: 0.766\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.9033, F1 Micro: 0.7745, F1 Macro: 0.7684\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9028, F1 Micro: 0.7828, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.031, Accuracy: 0.9041, F1 Micro: 0.7829, F1 Macro: 0.7816\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9066, F1 Micro: 0.7844, F1 Macro: 0.7835\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.63      0.73      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 201.28885340690613 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9053, F1 Micro: 0.7844, F1 Macro: 0.7835\n",
      "Launching training on 2 GPUs.\n",
      "777\n",
      "BESRA Uncertainty Score Threshold 139.587986332107\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 40.087860345840454 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3761, Accuracy: 0.8909, F1 Micro: 0.7336, F1 Macro: 0.7261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.9005, F1 Micro: 0.7732, F1 Macro: 0.7702\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9034, F1 Micro: 0.7638, F1 Macro: 0.7601\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9002, F1 Micro: 0.7803, F1 Macro: 0.7772\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9041, F1 Micro: 0.7748, F1 Macro: 0.7655\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.9008, F1 Micro: 0.7743, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9044, F1 Micro: 0.7814, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9019, F1 Micro: 0.7779, F1 Macro: 0.7763\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.903, F1 Micro: 0.7789, F1 Macro: 0.778\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9044, F1 Micro: 0.7814, F1 Macro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 205.40761756896973 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3744, Accuracy: 0.8903, F1 Micro: 0.7339, F1 Macro: 0.7223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.247, Accuracy: 0.9005, F1 Micro: 0.7723, F1 Macro: 0.7686\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9017, F1 Micro: 0.7517, F1 Macro: 0.7466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1674, Accuracy: 0.9083, F1 Micro: 0.7812, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9005, F1 Micro: 0.778, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.9033, F1 Micro: 0.7829, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.9034, F1 Micro: 0.7764, F1 Macro: 0.7692\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.9009, F1 Micro: 0.7777, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9009, F1 Micro: 0.7761, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9036, F1 Micro: 0.7772, F1 Macro: 0.7745\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9033, F1 Micro: 0.7829, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 204.0812439918518 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3739, Accuracy: 0.892, F1 Micro: 0.7354, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2481, Accuracy: 0.8995, F1 Micro: 0.7703, F1 Macro: 0.768\n",
      "Epoch 3/10, Train Loss: 0.2115, Accuracy: 0.903, F1 Micro: 0.7654, F1 Macro: 0.7625\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9028, F1 Micro: 0.7695, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9041, F1 Micro: 0.7827, F1 Macro: 0.7775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9064, F1 Micro: 0.7834, F1 Macro: 0.7743\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9017, F1 Micro: 0.7821, F1 Macro: 0.78\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9025, F1 Micro: 0.7723, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.044, Accuracy: 0.9041, F1 Micro: 0.7861, F1 Macro: 0.7873\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9064, F1 Micro: 0.7843, F1 Macro: 0.7843\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9041, F1 Micro: 0.7861, F1 Macro: 0.7873\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.79      0.71       248\n",
      "         radikalisme       0.72      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 205.91147017478943 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9039, F1 Micro: 0.7835, F1 Macro: 0.7812\n",
      "Launching training on 2 GPUs.\n",
      "577\n",
      "BESRA Uncertainty Score Threshold 252.6288865588043\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.981999158859253 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3697, Accuracy: 0.893, F1 Micro: 0.7319, F1 Macro: 0.724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2432, Accuracy: 0.9017, F1 Micro: 0.7709, F1 Macro: 0.7671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.9059, F1 Micro: 0.7805, F1 Macro: 0.7763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.9064, F1 Micro: 0.783, F1 Macro: 0.7786\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.9, F1 Micro: 0.7663, F1 Macro: 0.7637\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9052, F1 Micro: 0.7688, F1 Macro: 0.7663\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9025, F1 Micro: 0.7787, F1 Macro: 0.7778\n",
      "Epoch 8/10, Train Loss: 0.0521, Accuracy: 0.8991, F1 Micro: 0.7727, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9036, F1 Micro: 0.7783, F1 Macro: 0.7759\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9003, F1 Micro: 0.7734, F1 Macro: 0.7717\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9064, F1 Micro: 0.783, F1 Macro: 0.7786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.69      0.63      0.66       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 211.39057612419128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3633, Accuracy: 0.8897, F1 Micro: 0.7283, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2402, Accuracy: 0.9006, F1 Micro: 0.7738, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2038, Accuracy: 0.9055, F1 Micro: 0.7818, F1 Macro: 0.7776\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9033, F1 Micro: 0.7796, F1 Macro: 0.7755\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.902, F1 Micro: 0.7721, F1 Macro: 0.7674\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.9061, F1 Micro: 0.7799, F1 Macro: 0.7774\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.9027, F1 Micro: 0.7735, F1 Macro: 0.7687\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9017, F1 Micro: 0.773, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9019, F1 Micro: 0.7821, F1 Macro: 0.783\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8991, F1 Micro: 0.7701, F1 Macro: 0.7665\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9019, F1 Micro: 0.7821, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 210.27708959579468 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3657, Accuracy: 0.8884, F1 Micro: 0.7295, F1 Macro: 0.7234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9013, F1 Micro: 0.7733, F1 Macro: 0.7665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.9073, F1 Micro: 0.7817, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9075, F1 Micro: 0.7825, F1 Macro: 0.7766\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9028, F1 Micro: 0.774, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9087, F1 Micro: 0.7864, F1 Macro: 0.7831\n",
      "Epoch 7/10, Train Loss: 0.0682, Accuracy: 0.9042, F1 Micro: 0.7851, F1 Macro: 0.7812\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9, F1 Micro: 0.7743, F1 Macro: 0.7701\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9042, F1 Micro: 0.7808, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9003, F1 Micro: 0.7817, F1 Macro: 0.7795\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9087, F1 Micro: 0.7864, F1 Macro: 0.7831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 211.30510783195496 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9057, F1 Micro: 0.7838, F1 Macro: 0.7816\n",
      "Launching training on 2 GPUs.\n",
      "377\n",
      "BESRA Uncertainty Score Threshold 175.7565688499354\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.0720694065094 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3633, Accuracy: 0.8894, F1 Micro: 0.7395, F1 Macro: 0.7311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2427, Accuracy: 0.9005, F1 Micro: 0.7806, F1 Macro: 0.7764\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9034, F1 Micro: 0.7761, F1 Macro: 0.7657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1587, Accuracy: 0.9033, F1 Micro: 0.7871, F1 Macro: 0.7865\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.907, F1 Micro: 0.7842, F1 Macro: 0.7817\n",
      "Epoch 6/10, Train Loss: 0.0831, Accuracy: 0.9023, F1 Micro: 0.7705, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0643, Accuracy: 0.9022, F1 Micro: 0.7843, F1 Macro: 0.7846\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.9045, F1 Micro: 0.7806, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.9078, F1 Micro: 0.7864, F1 Macro: 0.7841\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.905, F1 Micro: 0.785, F1 Macro: 0.7815\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9033, F1 Micro: 0.7871, F1 Macro: 0.7865\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.74      0.70       248\n",
      "         radikalisme       0.71      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 214.77169466018677 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3596, Accuracy: 0.8892, F1 Micro: 0.7348, F1 Macro: 0.7222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.9014, F1 Micro: 0.7814, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.9036, F1 Micro: 0.784, F1 Macro: 0.7793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9023, F1 Micro: 0.7843, F1 Macro: 0.7854\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.9038, F1 Micro: 0.7652, F1 Macro: 0.7561\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9003, F1 Micro: 0.7721, F1 Macro: 0.7674\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9033, F1 Micro: 0.783, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9044, F1 Micro: 0.7817, F1 Macro: 0.7772\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.9052, F1 Micro: 0.7759, F1 Macro: 0.7681\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9003, F1 Micro: 0.7749, F1 Macro: 0.7649\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9023, F1 Micro: 0.7843, F1 Macro: 0.7854\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.64      0.77      0.70       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 214.8513753414154 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3605, Accuracy: 0.8917, F1 Micro: 0.7438, F1 Macro: 0.7334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2416, Accuracy: 0.9008, F1 Micro: 0.78, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1988, Accuracy: 0.9034, F1 Micro: 0.7812, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.9036, F1 Micro: 0.7852, F1 Macro: 0.7833\n",
      "Epoch 5/10, Train Loss: 0.1184, Accuracy: 0.9073, F1 Micro: 0.7798, F1 Macro: 0.7781\n",
      "Epoch 6/10, Train Loss: 0.0869, Accuracy: 0.9038, F1 Micro: 0.776, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.0679, Accuracy: 0.9028, F1 Micro: 0.779, F1 Macro: 0.7792\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.9027, F1 Micro: 0.7816, F1 Macro: 0.7822\n",
      "Epoch 9/10, Train Loss: 0.0377, Accuracy: 0.9053, F1 Micro: 0.7749, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9047, F1 Micro: 0.7848, F1 Macro: 0.7852\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9036, F1 Micro: 0.7852, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 214.66224479675293 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9031, F1 Micro: 0.7855, F1 Macro: 0.785\n",
      "Launching training on 2 GPUs.\n",
      "177\n",
      "BESRA Uncertainty Score Threshold 92.65628082876688\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 7.808932304382324 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3559, Accuracy: 0.8917, F1 Micro: 0.7562, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2304, Accuracy: 0.9028, F1 Micro: 0.7642, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.9039, F1 Micro: 0.7902, F1 Macro: 0.7903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9072, F1 Micro: 0.7914, F1 Macro: 0.7892\n",
      "Epoch 5/10, Train Loss: 0.1123, Accuracy: 0.9045, F1 Micro: 0.7784, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.7786\n",
      "Epoch 7/10, Train Loss: 0.0672, Accuracy: 0.9027, F1 Micro: 0.7699, F1 Macro: 0.7666\n",
      "Epoch 8/10, Train Loss: 0.048, Accuracy: 0.9025, F1 Micro: 0.7781, F1 Macro: 0.7775\n",
      "Epoch 9/10, Train Loss: 0.0362, Accuracy: 0.9039, F1 Micro: 0.7703, F1 Macro: 0.7649\n",
      "Epoch 10/10, Train Loss: 0.0274, Accuracy: 0.9044, F1 Micro: 0.7784, F1 Macro: 0.7724\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9072, F1 Micro: 0.7914, F1 Macro: 0.7892\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.73      0.90      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.77      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 222.31149005889893 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3539, Accuracy: 0.8895, F1 Micro: 0.7515, F1 Macro: 0.748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2312, Accuracy: 0.9041, F1 Micro: 0.7726, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1898, Accuracy: 0.9028, F1 Micro: 0.7851, F1 Macro: 0.7811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9078, F1 Micro: 0.7891, F1 Macro: 0.7877\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.905, F1 Micro: 0.7743, F1 Macro: 0.7675\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9069, F1 Micro: 0.7842, F1 Macro: 0.7782\n",
      "Epoch 7/10, Train Loss: 0.0631, Accuracy: 0.9062, F1 Micro: 0.7781, F1 Macro: 0.7717\n",
      "Epoch 8/10, Train Loss: 0.051, Accuracy: 0.8977, F1 Micro: 0.7719, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0333, Accuracy: 0.9055, F1 Micro: 0.7742, F1 Macro: 0.7691\n",
      "Epoch 10/10, Train Loss: 0.0268, Accuracy: 0.9009, F1 Micro: 0.7648, F1 Macro: 0.7613\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9078, F1 Micro: 0.7891, F1 Macro: 0.7877\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.71      0.69       248\n",
      "         radikalisme       0.75      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 221.01930832862854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3568, Accuracy: 0.8925, F1 Micro: 0.7567, F1 Macro: 0.7545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2318, Accuracy: 0.902, F1 Micro: 0.7654, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9075, F1 Micro: 0.7898, F1 Macro: 0.7871\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.9044, F1 Micro: 0.7857, F1 Macro: 0.7827\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9019, F1 Micro: 0.7681, F1 Macro: 0.7575\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9034, F1 Micro: 0.781, F1 Macro: 0.7779\n",
      "Epoch 7/10, Train Loss: 0.0618, Accuracy: 0.9014, F1 Micro: 0.7802, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0453, Accuracy: 0.903, F1 Micro: 0.7757, F1 Macro: 0.7699\n",
      "Epoch 9/10, Train Loss: 0.0335, Accuracy: 0.903, F1 Micro: 0.7792, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0294, Accuracy: 0.9008, F1 Micro: 0.7692, F1 Macro: 0.7612\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9075, F1 Micro: 0.7898, F1 Macro: 0.7871\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.69      0.70      0.70       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.46      1365\n",
      "\n",
      "Training completed in 218.03319263458252 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9075, F1 Micro: 0.7901, F1 Macro: 0.788\n",
      "Total sampling time: 2483.67 seconds\n",
      "Total runtime: 13106.063152074814 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceeb05",
   "metadata": {
    "papermill": {
     "duration": 0.010843,
     "end_time": "2025-06-07T18:18:15.841454",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.830611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5968, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5341, Accuracy: 0.7977, F1 Micro: 0.0988, F1 Macro: 0.0805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4743, Accuracy: 0.8219, F1 Micro: 0.2893, F1 Macro: 0.1949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8345, F1 Micro: 0.4638, F1 Macro: 0.3804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3545, Accuracy: 0.8441, F1 Micro: 0.5035, F1 Macro: 0.4353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3005, Accuracy: 0.8581, F1 Micro: 0.6093, F1 Macro: 0.5602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2678, Accuracy: 0.8648, F1 Micro: 0.628, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2021, Accuracy: 0.8675, F1 Micro: 0.657, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1931, Accuracy: 0.8689, F1 Micro: 0.6917, F1 Macro: 0.6904\n",
      "Epoch 10/10, Train Loss: 0.1319, Accuracy: 0.8711, F1 Micro: 0.6833, F1 Macro: 0.6768\n",
      "Model 1 - Iteration 388: Accuracy: 0.8689, F1 Micro: 0.6917, F1 Macro: 0.6904\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.81      0.85       370\n",
      "                sara       0.56      0.54      0.55       248\n",
      "         radikalisme       0.69      0.78      0.73       243\n",
      "pencemaran_nama_baik       0.62      0.63      0.63       504\n",
      "\n",
      "           micro avg       0.69      0.69      0.69      1365\n",
      "           macro avg       0.69      0.69      0.69      1365\n",
      "        weighted avg       0.70      0.69      0.69      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 57.67043399810791 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5958, Accuracy: 0.7864, F1 Micro: 0.0015, F1 Macro: 0.001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5369, Accuracy: 0.7869, F1 Micro: 0.0116, F1 Macro: 0.0089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5006, Accuracy: 0.825, F1 Micro: 0.3112, F1 Macro: 0.2205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4319, Accuracy: 0.8308, F1 Micro: 0.4199, F1 Macro: 0.2911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3838, Accuracy: 0.8431, F1 Micro: 0.4825, F1 Macro: 0.3976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3223, Accuracy: 0.8555, F1 Micro: 0.5898, F1 Macro: 0.5118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2886, Accuracy: 0.8622, F1 Micro: 0.6358, F1 Macro: 0.5966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2259, Accuracy: 0.8672, F1 Micro: 0.6556, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2022, Accuracy: 0.8661, F1 Micro: 0.6697, F1 Macro: 0.6559\n",
      "Epoch 10/10, Train Loss: 0.1446, Accuracy: 0.8667, F1 Micro: 0.6562, F1 Macro: 0.6345\n",
      "Model 2 - Iteration 388: Accuracy: 0.8661, F1 Micro: 0.6697, F1 Macro: 0.6559\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.84      0.86       370\n",
      "                sara       0.61      0.41      0.49       248\n",
      "         radikalisme       0.65      0.69      0.67       243\n",
      "pencemaran_nama_baik       0.64      0.57      0.60       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.67      1365\n",
      "           macro avg       0.69      0.63      0.66      1365\n",
      "        weighted avg       0.70      0.64      0.66      1365\n",
      "         samples avg       0.38      0.36      0.36      1365\n",
      "\n",
      "Training completed in 57.91219711303711 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5975, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5331, Accuracy: 0.7891, F1 Micro: 0.0217, F1 Macro: 0.0195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4706, Accuracy: 0.8233, F1 Micro: 0.2988, F1 Macro: 0.2114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4013, Accuracy: 0.8339, F1 Micro: 0.4337, F1 Macro: 0.3452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3537, Accuracy: 0.8436, F1 Micro: 0.5086, F1 Macro: 0.441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2988, Accuracy: 0.8544, F1 Micro: 0.6077, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2637, Accuracy: 0.8564, F1 Micro: 0.6114, F1 Macro: 0.5816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2036, Accuracy: 0.8642, F1 Micro: 0.6393, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1917, Accuracy: 0.8637, F1 Micro: 0.6602, F1 Macro: 0.6554\n",
      "Epoch 10/10, Train Loss: 0.1299, Accuracy: 0.8645, F1 Micro: 0.65, F1 Macro: 0.6377\n",
      "Model 3 - Iteration 388: Accuracy: 0.8637, F1 Micro: 0.6602, F1 Macro: 0.6554\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.78      0.84       370\n",
      "                sara       0.59      0.47      0.53       248\n",
      "         radikalisme       0.64      0.70      0.67       243\n",
      "pencemaran_nama_baik       0.64      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.71      0.62      0.66      1365\n",
      "           macro avg       0.70      0.62      0.66      1365\n",
      "        weighted avg       0.71      0.62      0.66      1365\n",
      "         samples avg       0.36      0.35      0.34      1365\n",
      "\n",
      "Training completed in 58.3275842666626 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8662, F1 Micro: 0.6739, F1 Macro: 0.6673\n",
      "Launching training on 2 GPUs.\n",
      "5830\n",
      "BESRA Uncertainty Score Threshold 145.35214177186967\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 282.5393331050873 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7928, F1 Micro: 0.0569, F1 Macro: 0.0488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4382, Accuracy: 0.8392, F1 Micro: 0.4939, F1 Macro: 0.403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3514, Accuracy: 0.8669, F1 Micro: 0.6627, F1 Macro: 0.6602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2786, Accuracy: 0.8798, F1 Micro: 0.7106, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2143, Accuracy: 0.8905, F1 Micro: 0.7401, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.186, Accuracy: 0.8869, F1 Micro: 0.7427, F1 Macro: 0.7439\n",
      "Epoch 7/10, Train Loss: 0.1486, Accuracy: 0.8895, F1 Micro: 0.7394, F1 Macro: 0.7363\n",
      "Epoch 8/10, Train Loss: 0.1108, Accuracy: 0.888, F1 Micro: 0.7271, F1 Macro: 0.7101\n",
      "Epoch 9/10, Train Loss: 0.0918, Accuracy: 0.8895, F1 Micro: 0.739, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.8922, F1 Micro: 0.7498, F1 Macro: 0.7462\n",
      "Model 1 - Iteration 971: Accuracy: 0.8922, F1 Micro: 0.7498, F1 Macro: 0.7462\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.88       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.74      0.75      0.74       243\n",
      "pencemaran_nama_baik       0.68      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.76      0.75      1365\n",
      "           macro avg       0.74      0.75      0.75      1365\n",
      "        weighted avg       0.75      0.76      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 71.90046405792236 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5437, Accuracy: 0.7877, F1 Micro: 0.0131, F1 Macro: 0.0087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4655, Accuracy: 0.8436, F1 Micro: 0.4778, F1 Macro: 0.3972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3657, Accuracy: 0.8711, F1 Micro: 0.6634, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.288, Accuracy: 0.877, F1 Micro: 0.712, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2154, Accuracy: 0.8872, F1 Micro: 0.7255, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1858, Accuracy: 0.8898, F1 Micro: 0.7443, F1 Macro: 0.7406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1465, Accuracy: 0.8908, F1 Micro: 0.749, F1 Macro: 0.7469\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.8902, F1 Micro: 0.7465, F1 Macro: 0.7349\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 0.8873, F1 Micro: 0.7435, F1 Macro: 0.7351\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.8927, F1 Micro: 0.7426, F1 Macro: 0.7354\n",
      "Model 2 - Iteration 971: Accuracy: 0.8908, F1 Micro: 0.749, F1 Macro: 0.7469\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.68      0.81      0.74       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.76      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.76      0.75      1365\n",
      "         samples avg       0.41      0.43      0.41      1365\n",
      "\n",
      "Training completed in 71.84607911109924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5399, Accuracy: 0.7873, F1 Micro: 0.0058, F1 Macro: 0.005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.441, Accuracy: 0.8378, F1 Micro: 0.4866, F1 Macro: 0.4049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3542, Accuracy: 0.8691, F1 Micro: 0.6698, F1 Macro: 0.657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2825, Accuracy: 0.8784, F1 Micro: 0.704, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2212, Accuracy: 0.8861, F1 Micro: 0.7219, F1 Macro: 0.7167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1869, Accuracy: 0.8836, F1 Micro: 0.7329, F1 Macro: 0.7306\n",
      "Epoch 7/10, Train Loss: 0.1515, Accuracy: 0.8852, F1 Micro: 0.7273, F1 Macro: 0.7228\n",
      "Epoch 8/10, Train Loss: 0.1197, Accuracy: 0.8856, F1 Micro: 0.7116, F1 Macro: 0.6886\n",
      "Epoch 9/10, Train Loss: 0.0997, Accuracy: 0.8848, F1 Micro: 0.712, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.074, Accuracy: 0.8819, F1 Micro: 0.737, F1 Macro: 0.7329\n",
      "Model 3 - Iteration 971: Accuracy: 0.8819, F1 Micro: 0.737, F1 Macro: 0.7329\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.86      0.86       370\n",
      "                sara       0.63      0.66      0.64       248\n",
      "         radikalisme       0.70      0.75      0.73       243\n",
      "pencemaran_nama_baik       0.64      0.79      0.70       504\n",
      "\n",
      "           micro avg       0.70      0.78      0.74      1365\n",
      "           macro avg       0.71      0.76      0.73      1365\n",
      "        weighted avg       0.71      0.78      0.74      1365\n",
      "         samples avg       0.43      0.44      0.42      1365\n",
      "\n",
      "Training completed in 71.8174500465393 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8883, F1 Micro: 0.7453, F1 Macro: 0.742\n",
      "Launching training on 2 GPUs.\n",
      "5247\n",
      "BESRA Uncertainty Score Threshold 166.31779147995007\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 253.3680341243744 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5255, Accuracy: 0.8272, F1 Micro: 0.3471, F1 Macro: 0.2495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3781, Accuracy: 0.8578, F1 Micro: 0.5724, F1 Macro: 0.5526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2972, Accuracy: 0.8794, F1 Micro: 0.688, F1 Macro: 0.6684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2399, Accuracy: 0.89, F1 Micro: 0.7416, F1 Macro: 0.7427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1909, Accuracy: 0.8923, F1 Micro: 0.742, F1 Macro: 0.7401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.8909, F1 Micro: 0.7573, F1 Macro: 0.7559\n",
      "Epoch 7/10, Train Loss: 0.1121, Accuracy: 0.8941, F1 Micro: 0.7487, F1 Macro: 0.7408\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.8961, F1 Micro: 0.7512, F1 Macro: 0.7486\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.898, F1 Micro: 0.7526, F1 Macro: 0.7461\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.8967, F1 Micro: 0.7562, F1 Macro: 0.7542\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8909, F1 Micro: 0.7573, F1 Macro: 0.7559\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.67      0.66      0.66       248\n",
      "         radikalisme       0.71      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.64      0.82      0.72       504\n",
      "\n",
      "           micro avg       0.72      0.80      0.76      1365\n",
      "           macro avg       0.73      0.79      0.76      1365\n",
      "        weighted avg       0.73      0.80      0.76      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 86.73306131362915 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5353, Accuracy: 0.8256, F1 Micro: 0.3579, F1 Macro: 0.255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.385, Accuracy: 0.8597, F1 Micro: 0.5736, F1 Macro: 0.5506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2985, Accuracy: 0.8791, F1 Micro: 0.6887, F1 Macro: 0.6659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2395, Accuracy: 0.892, F1 Micro: 0.7423, F1 Macro: 0.7421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1864, Accuracy: 0.8931, F1 Micro: 0.7444, F1 Macro: 0.7359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.8897, F1 Micro: 0.7531, F1 Macro: 0.7508\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.8897, F1 Micro: 0.7436, F1 Macro: 0.7408\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.8939, F1 Micro: 0.751, F1 Macro: 0.7468\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.8931, F1 Micro: 0.7463, F1 Macro: 0.7421\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.8922, F1 Micro: 0.7518, F1 Macro: 0.7471\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8897, F1 Micro: 0.7531, F1 Macro: 0.7508\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.66      0.61      0.63       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.63      0.80      0.71       504\n",
      "\n",
      "           micro avg       0.72      0.79      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.73      0.79      0.75      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 85.16765284538269 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5252, Accuracy: 0.8295, F1 Micro: 0.369, F1 Macro: 0.266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3778, Accuracy: 0.8511, F1 Micro: 0.5425, F1 Macro: 0.5023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2998, Accuracy: 0.877, F1 Micro: 0.6836, F1 Macro: 0.6664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2365, Accuracy: 0.8888, F1 Micro: 0.7394, F1 Macro: 0.7368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.186, Accuracy: 0.8911, F1 Micro: 0.7416, F1 Macro: 0.7385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1513, Accuracy: 0.8898, F1 Micro: 0.7544, F1 Macro: 0.7518\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.8898, F1 Micro: 0.7445, F1 Macro: 0.7417\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.8889, F1 Micro: 0.7412, F1 Macro: 0.7391\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.8905, F1 Micro: 0.7297, F1 Macro: 0.7225\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.8925, F1 Micro: 0.7429, F1 Macro: 0.739\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8898, F1 Micro: 0.7544, F1 Macro: 0.7518\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       370\n",
      "                sara       0.64      0.67      0.66       248\n",
      "         radikalisme       0.70      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.66      0.80      0.72       504\n",
      "\n",
      "           micro avg       0.72      0.79      0.75      1365\n",
      "           macro avg       0.72      0.79      0.75      1365\n",
      "        weighted avg       0.73      0.79      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 85.50439643859863 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8902, F1 Micro: 0.755, F1 Macro: 0.7528\n",
      "Launching training on 2 GPUs.\n",
      "4722\n",
      "BESRA Uncertainty Score Threshold 102.8615196911147\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 230.10024237632751 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5001, Accuracy: 0.8345, F1 Micro: 0.4061, F1 Macro: 0.3156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3317, Accuracy: 0.8672, F1 Micro: 0.6215, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2831, Accuracy: 0.8883, F1 Micro: 0.7278, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.229, Accuracy: 0.897, F1 Micro: 0.751, F1 Macro: 0.747\n",
      "Epoch 5/10, Train Loss: 0.1691, Accuracy: 0.8955, F1 Micro: 0.7505, F1 Macro: 0.7446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.8972, F1 Micro: 0.7552, F1 Macro: 0.7528\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.8995, F1 Micro: 0.7532, F1 Macro: 0.7488\n",
      "Epoch 8/10, Train Loss: 0.0871, Accuracy: 0.8942, F1 Micro: 0.7435, F1 Macro: 0.7362\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.8938, F1 Micro: 0.744, F1 Macro: 0.7361\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.8963, F1 Micro: 0.7541, F1 Macro: 0.7508\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8972, F1 Micro: 0.7552, F1 Macro: 0.7528\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.86      0.90       370\n",
      "                sara       0.67      0.62      0.64       248\n",
      "         radikalisme       0.76      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       504\n",
      "\n",
      "           micro avg       0.77      0.74      0.76      1365\n",
      "           macro avg       0.77      0.74      0.75      1365\n",
      "        weighted avg       0.77      0.74      0.76      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 96.50425672531128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5135, Accuracy: 0.8302, F1 Micro: 0.3579, F1 Macro: 0.2567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.342, Accuracy: 0.8666, F1 Micro: 0.609, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2797, Accuracy: 0.8864, F1 Micro: 0.7454, F1 Macro: 0.7358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.235, Accuracy: 0.8967, F1 Micro: 0.7599, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1717, Accuracy: 0.8984, F1 Micro: 0.7612, F1 Macro: 0.755\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.8953, F1 Micro: 0.7586, F1 Macro: 0.756\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.8984, F1 Micro: 0.76, F1 Macro: 0.7561\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.8997, F1 Micro: 0.7588, F1 Macro: 0.7472\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.8944, F1 Micro: 0.7479, F1 Macro: 0.7371\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.8956, F1 Micro: 0.7611, F1 Macro: 0.7588\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8984, F1 Micro: 0.7612, F1 Macro: 0.755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.74      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.76      0.75      0.76      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 96.61788296699524 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5061, Accuracy: 0.8316, F1 Micro: 0.4051, F1 Macro: 0.3103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3329, Accuracy: 0.8711, F1 Micro: 0.6341, F1 Macro: 0.6115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2894, Accuracy: 0.8838, F1 Micro: 0.7199, F1 Macro: 0.7017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2341, Accuracy: 0.8945, F1 Micro: 0.7395, F1 Macro: 0.7348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.8963, F1 Micro: 0.753, F1 Macro: 0.7478\n",
      "Epoch 6/10, Train Loss: 0.1422, Accuracy: 0.8945, F1 Micro: 0.735, F1 Macro: 0.727\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.892, F1 Micro: 0.7285, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.8958, F1 Micro: 0.7559, F1 Macro: 0.7501\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.893, F1 Micro: 0.7439, F1 Macro: 0.7383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.8972, F1 Micro: 0.7607, F1 Macro: 0.7562\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8972, F1 Micro: 0.7607, F1 Macro: 0.7562\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.67      0.68      0.67       248\n",
      "         radikalisme       0.76      0.72      0.74       243\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.77      0.76      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.77      0.76      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 99.94033479690552 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8976, F1 Micro: 0.759, F1 Macro: 0.7547\n",
      "Launching training on 2 GPUs.\n",
      "4249\n",
      "BESRA Uncertainty Score Threshold 201.20259701542793\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 207.11620426177979 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4649, Accuracy: 0.855, F1 Micro: 0.5521, F1 Macro: 0.5173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3219, Accuracy: 0.887, F1 Micro: 0.7293, F1 Macro: 0.7278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2589, Accuracy: 0.8969, F1 Micro: 0.7633, F1 Macro: 0.7609\n",
      "Epoch 4/10, Train Loss: 0.2029, Accuracy: 0.8956, F1 Micro: 0.7472, F1 Macro: 0.7289\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.8998, F1 Micro: 0.7595, F1 Macro: 0.7557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.8964, F1 Micro: 0.7735, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0943, Accuracy: 0.8923, F1 Micro: 0.7561, F1 Macro: 0.7548\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.8967, F1 Micro: 0.7726, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.8973, F1 Micro: 0.7627, F1 Macro: 0.7594\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.8984, F1 Micro: 0.7669, F1 Macro: 0.7643\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8964, F1 Micro: 0.7735, F1 Macro: 0.775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.63      0.79      0.70       248\n",
      "         radikalisme       0.71      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.83      0.77      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 108.59708642959595 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4816, Accuracy: 0.85, F1 Micro: 0.5067, F1 Macro: 0.4364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3249, Accuracy: 0.8852, F1 Micro: 0.7219, F1 Macro: 0.7149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2555, Accuracy: 0.8952, F1 Micro: 0.742, F1 Macro: 0.7346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2021, Accuracy: 0.8969, F1 Micro: 0.7566, F1 Macro: 0.7452\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.898, F1 Micro: 0.744, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1233, Accuracy: 0.8942, F1 Micro: 0.7665, F1 Macro: 0.7657\n",
      "Epoch 7/10, Train Loss: 0.0923, Accuracy: 0.8931, F1 Micro: 0.7649, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.8969, F1 Micro: 0.7752, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.8995, F1 Micro: 0.7633, F1 Macro: 0.7558\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9014, F1 Micro: 0.7659, F1 Macro: 0.76\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8969, F1 Micro: 0.7752, F1 Macro: 0.7755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.90       370\n",
      "                sara       0.66      0.76      0.71       248\n",
      "         radikalisme       0.72      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.65      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.83      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.73      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 111.93519282341003 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.472, Accuracy: 0.8452, F1 Micro: 0.4894, F1 Macro: 0.4206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3266, Accuracy: 0.8819, F1 Micro: 0.7171, F1 Macro: 0.7126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2614, Accuracy: 0.8939, F1 Micro: 0.7625, F1 Macro: 0.7596\n",
      "Epoch 4/10, Train Loss: 0.2045, Accuracy: 0.8975, F1 Micro: 0.7567, F1 Macro: 0.7419\n",
      "Epoch 5/10, Train Loss: 0.1695, Accuracy: 0.8973, F1 Micro: 0.7527, F1 Macro: 0.749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.8953, F1 Micro: 0.771, F1 Macro: 0.7736\n",
      "Epoch 7/10, Train Loss: 0.093, Accuracy: 0.8981, F1 Micro: 0.7678, F1 Macro: 0.7667\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.8964, F1 Micro: 0.7679, F1 Macro: 0.7648\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.8998, F1 Micro: 0.7687, F1 Macro: 0.7667\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.8989, F1 Micro: 0.7608, F1 Macro: 0.7563\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8953, F1 Micro: 0.771, F1 Macro: 0.7736\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.65      0.79      0.72       248\n",
      "         radikalisme       0.71      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.83      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.83      0.77      1365\n",
      "           macro avg       0.73      0.82      0.77      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 108.01945614814758 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8962, F1 Micro: 0.7732, F1 Macro: 0.7747\n",
      "Launching training on 2 GPUs.\n",
      "3824\n",
      "BESRA Uncertainty Score Threshold 257.67891208999174\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 186.724018573761 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4507, Accuracy: 0.8564, F1 Micro: 0.5728, F1 Macro: 0.5407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2989, Accuracy: 0.8852, F1 Micro: 0.7103, F1 Macro: 0.7132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2476, Accuracy: 0.8928, F1 Micro: 0.7618, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1939, Accuracy: 0.8972, F1 Micro: 0.7717, F1 Macro: 0.7725\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.8984, F1 Micro: 0.7582, F1 Macro: 0.7479\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9006, F1 Micro: 0.7611, F1 Macro: 0.7541\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.895, F1 Micro: 0.77, F1 Macro: 0.7681\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.9008, F1 Micro: 0.7624, F1 Macro: 0.7561\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.8988, F1 Micro: 0.7704, F1 Macro: 0.7672\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.8994, F1 Micro: 0.7613, F1 Macro: 0.757\n",
      "Model 1 - Iteration 2777: Accuracy: 0.8972, F1 Micro: 0.7717, F1 Macro: 0.7725\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.82      0.89       370\n",
      "                sara       0.66      0.73      0.69       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1365\n",
      "           macro avg       0.75      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 119.68708229064941 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4652, Accuracy: 0.8647, F1 Micro: 0.6261, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.302, Accuracy: 0.8884, F1 Micro: 0.7217, F1 Macro: 0.7204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2441, Accuracy: 0.8961, F1 Micro: 0.7671, F1 Macro: 0.7642\n",
      "Epoch 4/10, Train Loss: 0.19, Accuracy: 0.8992, F1 Micro: 0.7635, F1 Macro: 0.7573\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.8975, F1 Micro: 0.7541, F1 Macro: 0.7417\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.8983, F1 Micro: 0.7642, F1 Macro: 0.7579\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.8905, F1 Micro: 0.7613, F1 Macro: 0.7595\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.902, F1 Micro: 0.7668, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.898, F1 Micro: 0.7722, F1 Macro: 0.7744\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.8981, F1 Micro: 0.7492, F1 Macro: 0.74\n",
      "Model 2 - Iteration 2777: Accuracy: 0.898, F1 Micro: 0.7722, F1 Macro: 0.7744\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.75      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 120.10831427574158 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4575, Accuracy: 0.8527, F1 Micro: 0.5371, F1 Macro: 0.4839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3054, Accuracy: 0.887, F1 Micro: 0.7331, F1 Macro: 0.7349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.8905, F1 Micro: 0.7595, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.8955, F1 Micro: 0.7655, F1 Macro: 0.7657\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.8983, F1 Micro: 0.7514, F1 Macro: 0.7369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.8994, F1 Micro: 0.7658, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.8928, F1 Micro: 0.7659, F1 Macro: 0.7633\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.8983, F1 Micro: 0.7615, F1 Macro: 0.7579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.8978, F1 Micro: 0.7694, F1 Macro: 0.7678\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9014, F1 Micro: 0.7672, F1 Macro: 0.7632\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8978, F1 Micro: 0.7694, F1 Macro: 0.7678\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 124.79029417037964 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8977, F1 Micro: 0.7711, F1 Macro: 0.7716\n",
      "Launching training on 2 GPUs.\n",
      "3441\n",
      "BESRA Uncertainty Score Threshold 285.40874396129806\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 168.53032755851746 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4428, Accuracy: 0.8628, F1 Micro: 0.6508, F1 Macro: 0.6535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2932, Accuracy: 0.8861, F1 Micro: 0.7113, F1 Macro: 0.7035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2524, Accuracy: 0.897, F1 Micro: 0.77, F1 Macro: 0.7651\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.897, F1 Micro: 0.7578, F1 Macro: 0.7484\n",
      "Epoch 5/10, Train Loss: 0.1503, Accuracy: 0.9003, F1 Micro: 0.767, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1123, Accuracy: 0.9002, F1 Micro: 0.7744, F1 Macro: 0.7706\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.8942, F1 Micro: 0.766, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9041, F1 Micro: 0.779, F1 Macro: 0.775\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9027, F1 Micro: 0.775, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0401, Accuracy: 0.8994, F1 Micro: 0.7692, F1 Macro: 0.7653\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9041, F1 Micro: 0.779, F1 Macro: 0.775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.77      0.75      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 131.08982968330383 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4545, Accuracy: 0.8692, F1 Micro: 0.6732, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.8919, F1 Micro: 0.7293, F1 Macro: 0.7148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.8986, F1 Micro: 0.7631, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.8981, F1 Micro: 0.7631, F1 Macro: 0.7577\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.895, F1 Micro: 0.762, F1 Macro: 0.7552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9013, F1 Micro: 0.7775, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.8983, F1 Micro: 0.7673, F1 Macro: 0.7639\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.8995, F1 Micro: 0.769, F1 Macro: 0.7627\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.8997, F1 Micro: 0.766, F1 Macro: 0.7599\n",
      "Epoch 10/10, Train Loss: 0.0383, Accuracy: 0.8998, F1 Micro: 0.7705, F1 Macro: 0.7679\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9013, F1 Micro: 0.7775, F1 Macro: 0.7726\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 130.50025606155396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4467, Accuracy: 0.8566, F1 Micro: 0.6036, F1 Macro: 0.5958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2939, Accuracy: 0.8889, F1 Micro: 0.7247, F1 Macro: 0.7154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2525, Accuracy: 0.898, F1 Micro: 0.7652, F1 Macro: 0.758\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.8945, F1 Micro: 0.7467, F1 Macro: 0.7365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.8995, F1 Micro: 0.7777, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.9053, F1 Micro: 0.7792, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.8975, F1 Micro: 0.7635, F1 Macro: 0.7574\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9003, F1 Micro: 0.7766, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9022, F1 Micro: 0.7767, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9022, F1 Micro: 0.7712, F1 Macro: 0.7675\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9053, F1 Micro: 0.7792, F1 Macro: 0.7749\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 130.0884187221527 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.9035, F1 Micro: 0.7785, F1 Macro: 0.7742\n",
      "Launching training on 2 GPUs.\n",
      "3096\n",
      "BESRA Uncertainty Score Threshold 228.2828633945162\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 152.25053358078003 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4378, Accuracy: 0.8658, F1 Micro: 0.6368, F1 Macro: 0.6323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.287, Accuracy: 0.8922, F1 Micro: 0.7546, F1 Macro: 0.7566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9019, F1 Micro: 0.7579, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.8942, F1 Micro: 0.7679, F1 Macro: 0.7667\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.8963, F1 Micro: 0.761, F1 Macro: 0.7583\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9003, F1 Micro: 0.7663, F1 Macro: 0.7589\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.9, F1 Micro: 0.7666, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9009, F1 Micro: 0.77, F1 Macro: 0.7649\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9016, F1 Micro: 0.7696, F1 Macro: 0.7682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.8995, F1 Micro: 0.7746, F1 Macro: 0.7718\n",
      "Model 1 - Iteration 3432: Accuracy: 0.8995, F1 Micro: 0.7746, F1 Macro: 0.7718\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.66      0.69      0.68       248\n",
      "         radikalisme       0.75      0.78      0.76       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 142.05401802062988 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4538, Accuracy: 0.8695, F1 Micro: 0.6505, F1 Macro: 0.6435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2948, Accuracy: 0.8916, F1 Micro: 0.7516, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2339, Accuracy: 0.9011, F1 Micro: 0.7612, F1 Macro: 0.7511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1938, Accuracy: 0.8944, F1 Micro: 0.7638, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.8994, F1 Micro: 0.7673, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.8992, F1 Micro: 0.7677, F1 Macro: 0.7628\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9008, F1 Micro: 0.767, F1 Macro: 0.7591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.8977, F1 Micro: 0.7737, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9038, F1 Micro: 0.7734, F1 Macro: 0.7705\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.8989, F1 Micro: 0.7677, F1 Macro: 0.7575\n",
      "Model 2 - Iteration 3432: Accuracy: 0.8977, F1 Micro: 0.7737, F1 Macro: 0.7722\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.77      0.77      0.77       243\n",
      "pencemaran_nama_baik       0.65      0.83      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 143.22134280204773 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4474, Accuracy: 0.8644, F1 Micro: 0.6233, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2937, Accuracy: 0.8894, F1 Micro: 0.7459, F1 Macro: 0.7476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2367, Accuracy: 0.8994, F1 Micro: 0.7517, F1 Macro: 0.7426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1949, Accuracy: 0.8922, F1 Micro: 0.764, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1559, Accuracy: 0.8978, F1 Micro: 0.7704, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.902, F1 Micro: 0.7685, F1 Macro: 0.7629\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.902, F1 Micro: 0.7629, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.8995, F1 Micro: 0.7708, F1 Macro: 0.7682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.8983, F1 Micro: 0.771, F1 Macro: 0.7703\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.8988, F1 Micro: 0.7657, F1 Macro: 0.7579\n",
      "Model 3 - Iteration 3432: Accuracy: 0.8983, F1 Micro: 0.771, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.63      0.73      0.67       248\n",
      "         radikalisme       0.75      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 143.1430377960205 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8985, F1 Micro: 0.7731, F1 Macro: 0.7714\n",
      "Launching training on 2 GPUs.\n",
      "2786\n",
      "BESRA Uncertainty Score Threshold 331.2577920111179\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 138.6534857749939 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4285, Accuracy: 0.8742, F1 Micro: 0.7122, F1 Macro: 0.7088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.8895, F1 Micro: 0.7492, F1 Macro: 0.7514\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.9, F1 Micro: 0.7411, F1 Macro: 0.7366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.195, Accuracy: 0.8947, F1 Micro: 0.772, F1 Macro: 0.7683\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9019, F1 Micro: 0.7664, F1 Macro: 0.7571\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9014, F1 Micro: 0.7481, F1 Macro: 0.7406\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9003, F1 Micro: 0.7607, F1 Macro: 0.7515\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.8948, F1 Micro: 0.768, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9008, F1 Micro: 0.7743, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.8972, F1 Micro: 0.7723, F1 Macro: 0.7667\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9008, F1 Micro: 0.7743, F1 Macro: 0.7679\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 146.49609518051147 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4333, Accuracy: 0.8709, F1 Micro: 0.6843, F1 Macro: 0.6674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2754, Accuracy: 0.892, F1 Micro: 0.7333, F1 Macro: 0.7245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2269, Accuracy: 0.9014, F1 Micro: 0.7614, F1 Macro: 0.758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1849, Accuracy: 0.8942, F1 Micro: 0.7687, F1 Macro: 0.7661\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9005, F1 Micro: 0.7528, F1 Macro: 0.7375\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.8995, F1 Micro: 0.7351, F1 Macro: 0.7287\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.8984, F1 Micro: 0.768, F1 Macro: 0.7638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.8958, F1 Micro: 0.7707, F1 Macro: 0.769\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.8984, F1 Micro: 0.7697, F1 Macro: 0.7661\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.8959, F1 Micro: 0.7702, F1 Macro: 0.7675\n",
      "Model 2 - Iteration 3711: Accuracy: 0.8958, F1 Micro: 0.7707, F1 Macro: 0.769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.71      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.65      0.81      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1365\n",
      "           macro avg       0.73      0.81      0.77      1365\n",
      "        weighted avg       0.73      0.82      0.77      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 147.75141954421997 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.432, Accuracy: 0.8719, F1 Micro: 0.6936, F1 Macro: 0.689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2837, Accuracy: 0.8913, F1 Micro: 0.746, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2343, Accuracy: 0.8994, F1 Micro: 0.7465, F1 Macro: 0.7416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.8967, F1 Micro: 0.772, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9011, F1 Micro: 0.7623, F1 Macro: 0.7547\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9, F1 Micro: 0.749, F1 Macro: 0.7451\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.9013, F1 Micro: 0.7628, F1 Macro: 0.7557\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.8948, F1 Micro: 0.7705, F1 Macro: 0.769\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9016, F1 Micro: 0.7665, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9002, F1 Micro: 0.7732, F1 Macro: 0.7699\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9002, F1 Micro: 0.7732, F1 Macro: 0.7699\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.73      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 146.99660778045654 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8989, F1 Micro: 0.7727, F1 Macro: 0.7689\n",
      "Launching training on 2 GPUs.\n",
      "2507\n",
      "BESRA Uncertainty Score Threshold 268.6255036082059\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 123.61461067199707 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4289, Accuracy: 0.8748, F1 Micro: 0.6691, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2846, Accuracy: 0.8905, F1 Micro: 0.7626, F1 Macro: 0.7624\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.9005, F1 Micro: 0.7473, F1 Macro: 0.7359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1857, Accuracy: 0.8995, F1 Micro: 0.776, F1 Macro: 0.7704\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9022, F1 Micro: 0.7735, F1 Macro: 0.7666\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9011, F1 Micro: 0.7623, F1 Macro: 0.7464\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9023, F1 Micro: 0.773, F1 Macro: 0.7634\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.8964, F1 Micro: 0.7756, F1 Macro: 0.7726\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9013, F1 Micro: 0.7759, F1 Macro: 0.7719\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9028, F1 Micro: 0.7755, F1 Macro: 0.7678\n",
      "Model 1 - Iteration 3886: Accuracy: 0.8995, F1 Micro: 0.776, F1 Macro: 0.7704\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.90       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.70      0.86      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 150.08651542663574 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4435, Accuracy: 0.8759, F1 Micro: 0.6743, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2851, Accuracy: 0.893, F1 Micro: 0.7622, F1 Macro: 0.7585\n",
      "Epoch 3/10, Train Loss: 0.2284, Accuracy: 0.8961, F1 Micro: 0.7244, F1 Macro: 0.7121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1815, Accuracy: 0.9009, F1 Micro: 0.7721, F1 Macro: 0.7672\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.9017, F1 Micro: 0.7656, F1 Macro: 0.7599\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9005, F1 Micro: 0.7615, F1 Macro: 0.7531\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9002, F1 Micro: 0.7684, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.8923, F1 Micro: 0.7623, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.8995, F1 Micro: 0.7749, F1 Macro: 0.7703\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.9005, F1 Micro: 0.7674, F1 Macro: 0.7596\n",
      "Model 2 - Iteration 3886: Accuracy: 0.8995, F1 Micro: 0.7749, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       370\n",
      "                sara       0.63      0.69      0.66       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 151.6790850162506 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4332, Accuracy: 0.8755, F1 Micro: 0.665, F1 Macro: 0.6583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.8902, F1 Micro: 0.7606, F1 Macro: 0.7585\n",
      "Epoch 3/10, Train Loss: 0.2315, Accuracy: 0.9006, F1 Micro: 0.7521, F1 Macro: 0.7432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9017, F1 Micro: 0.78, F1 Macro: 0.7781\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9, F1 Micro: 0.7721, F1 Macro: 0.7703\n",
      "Epoch 6/10, Train Loss: 0.1043, Accuracy: 0.8981, F1 Micro: 0.7382, F1 Macro: 0.7212\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.9016, F1 Micro: 0.7656, F1 Macro: 0.7571\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.8959, F1 Micro: 0.7745, F1 Macro: 0.7753\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.8956, F1 Micro: 0.7763, F1 Macro: 0.7774\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.8978, F1 Micro: 0.7731, F1 Macro: 0.7705\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9017, F1 Micro: 0.78, F1 Macro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 149.50439643859863 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.9003, F1 Micro: 0.777, F1 Macro: 0.7729\n",
      "Launching training on 2 GPUs.\n",
      "2332\n",
      "BESRA Uncertainty Score Threshold 277.3448494837067\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 115.01564288139343 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4182, Accuracy: 0.8752, F1 Micro: 0.6534, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2738, Accuracy: 0.8941, F1 Micro: 0.7681, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.8972, F1 Micro: 0.7694, F1 Macro: 0.7621\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.8998, F1 Micro: 0.7501, F1 Macro: 0.7358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.8981, F1 Micro: 0.7694, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.8997, F1 Micro: 0.7629, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9027, F1 Micro: 0.7709, F1 Macro: 0.764\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.8997, F1 Micro: 0.7542, F1 Macro: 0.7431\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.9016, F1 Micro: 0.7692, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9031, F1 Micro: 0.7737, F1 Macro: 0.7687\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9031, F1 Micro: 0.7737, F1 Macro: 0.7687\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.68      0.63      0.65       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 162.8467185497284 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4303, Accuracy: 0.8722, F1 Micro: 0.6309, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2789, Accuracy: 0.8928, F1 Micro: 0.7665, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2267, Accuracy: 0.8953, F1 Micro: 0.7743, F1 Macro: 0.7717\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9014, F1 Micro: 0.7638, F1 Macro: 0.7603\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.8972, F1 Micro: 0.7717, F1 Macro: 0.7679\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.8955, F1 Micro: 0.7514, F1 Macro: 0.7439\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.8989, F1 Micro: 0.762, F1 Macro: 0.7529\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.8991, F1 Micro: 0.7564, F1 Macro: 0.7437\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.8977, F1 Micro: 0.7612, F1 Macro: 0.7548\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9013, F1 Micro: 0.7658, F1 Macro: 0.7584\n",
      "Model 2 - Iteration 4120: Accuracy: 0.8953, F1 Micro: 0.7743, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.93      0.90       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.70      0.87      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.84      0.77      1365\n",
      "           macro avg       0.72      0.84      0.77      1365\n",
      "        weighted avg       0.72      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 156.55127048492432 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4226, Accuracy: 0.8766, F1 Micro: 0.6638, F1 Macro: 0.6531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2773, Accuracy: 0.893, F1 Micro: 0.765, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2273, Accuracy: 0.8973, F1 Micro: 0.7687, F1 Macro: 0.7629\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.8992, F1 Micro: 0.7495, F1 Macro: 0.7366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.8972, F1 Micro: 0.7698, F1 Macro: 0.7677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1057, Accuracy: 0.9011, F1 Micro: 0.7738, F1 Macro: 0.7714\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.8997, F1 Micro: 0.7492, F1 Macro: 0.736\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.8981, F1 Micro: 0.7636, F1 Macro: 0.7577\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9006, F1 Micro: 0.7696, F1 Macro: 0.7649\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7693\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9011, F1 Micro: 0.7738, F1 Macro: 0.7714\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.77      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 160.322407245636 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8998, F1 Micro: 0.7739, F1 Macro: 0.7706\n",
      "Launching training on 2 GPUs.\n",
      "2098\n",
      "BESRA Uncertainty Score Threshold 143.60849003202605\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 102.30637454986572 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.412, Accuracy: 0.8725, F1 Micro: 0.6437, F1 Macro: 0.6249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.8964, F1 Micro: 0.7435, F1 Macro: 0.7274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2307, Accuracy: 0.9006, F1 Micro: 0.7699, F1 Macro: 0.7641\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.8984, F1 Micro: 0.7305, F1 Macro: 0.7072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9041, F1 Micro: 0.7741, F1 Macro: 0.7715\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9023, F1 Micro: 0.7721, F1 Macro: 0.7689\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.897, F1 Micro: 0.7682, F1 Macro: 0.7621\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.8911, F1 Micro: 0.763, F1 Macro: 0.7585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9025, F1 Micro: 0.7746, F1 Macro: 0.7663\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9006, F1 Micro: 0.7691, F1 Macro: 0.7662\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9025, F1 Micro: 0.7746, F1 Macro: 0.7663\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.66      0.63      0.64       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 167.21482229232788 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4229, Accuracy: 0.8734, F1 Micro: 0.6438, F1 Macro: 0.6298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2742, Accuracy: 0.8977, F1 Micro: 0.7476, F1 Macro: 0.7287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2248, Accuracy: 0.9016, F1 Micro: 0.7696, F1 Macro: 0.7622\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9006, F1 Micro: 0.7486, F1 Macro: 0.7293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.9059, F1 Micro: 0.7798, F1 Macro: 0.7761\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.903, F1 Micro: 0.7723, F1 Macro: 0.7682\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.8953, F1 Micro: 0.7669, F1 Macro: 0.7634\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9009, F1 Micro: 0.7713, F1 Macro: 0.7669\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.8983, F1 Micro: 0.7756, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9019, F1 Micro: 0.7768, F1 Macro: 0.7744\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9059, F1 Micro: 0.7798, F1 Macro: 0.7761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.78      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 165.16533255577087 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.415, Accuracy: 0.8714, F1 Micro: 0.6389, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2774, Accuracy: 0.897, F1 Micro: 0.7429, F1 Macro: 0.7301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2284, Accuracy: 0.9011, F1 Micro: 0.7686, F1 Macro: 0.7632\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.9002, F1 Micro: 0.7431, F1 Macro: 0.7215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9025, F1 Micro: 0.7704, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.9013, F1 Micro: 0.7725, F1 Macro: 0.7711\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.8959, F1 Micro: 0.7632, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.9003, F1 Micro: 0.7744, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9011, F1 Micro: 0.7775, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.8978, F1 Micro: 0.7671, F1 Macro: 0.7634\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9011, F1 Micro: 0.7775, F1 Macro: 0.7725\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.63      0.67      0.65       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 169.19376397132874 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.9032, F1 Micro: 0.7773, F1 Macro: 0.7716\n",
      "Launching training on 2 GPUs.\n",
      "1888\n",
      "BESRA Uncertainty Score Threshold 242.36464350702929\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 93.86233949661255 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.401, Accuracy: 0.8816, F1 Micro: 0.7178, F1 Macro: 0.7218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.8953, F1 Micro: 0.7616, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.211, Accuracy: 0.8994, F1 Micro: 0.7644, F1 Macro: 0.755\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.8992, F1 Micro: 0.7636, F1 Macro: 0.7515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9028, F1 Micro: 0.7656, F1 Macro: 0.75\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9022, F1 Micro: 0.7714, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.9025, F1 Micro: 0.7734, F1 Macro: 0.7682\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.905, F1 Micro: 0.7699, F1 Macro: 0.7613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.9017, F1 Micro: 0.7792, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.898, F1 Micro: 0.7698, F1 Macro: 0.7661\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9017, F1 Micro: 0.7792, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.62      0.72      0.67       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 175.78041744232178 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4073, Accuracy: 0.8797, F1 Micro: 0.6903, F1 Macro: 0.6895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.8966, F1 Micro: 0.7657, F1 Macro: 0.7601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.9, F1 Micro: 0.7718, F1 Macro: 0.767\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.897, F1 Micro: 0.7697, F1 Macro: 0.7653\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9028, F1 Micro: 0.7688, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9027, F1 Micro: 0.7777, F1 Macro: 0.7742\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9016, F1 Micro: 0.7668, F1 Macro: 0.7585\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9038, F1 Micro: 0.775, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.9036, F1 Micro: 0.7669, F1 Macro: 0.7592\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.8981, F1 Micro: 0.7681, F1 Macro: 0.7624\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9027, F1 Micro: 0.7777, F1 Macro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.63      0.73      0.67       248\n",
      "         radikalisme       0.75      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 169.8356351852417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4048, Accuracy: 0.8773, F1 Micro: 0.7041, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.8959, F1 Micro: 0.766, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.9014, F1 Micro: 0.7705, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9027, F1 Micro: 0.776, F1 Macro: 0.7717\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.903, F1 Micro: 0.7754, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9025, F1 Micro: 0.7636, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9033, F1 Micro: 0.7613, F1 Macro: 0.7502\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.905, F1 Micro: 0.7693, F1 Macro: 0.7618\n",
      "Epoch 9/10, Train Loss: 0.0445, Accuracy: 0.902, F1 Micro: 0.7718, F1 Macro: 0.7653\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.9003, F1 Micro: 0.77, F1 Macro: 0.7657\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9027, F1 Micro: 0.776, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.67      0.68      0.67       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 169.86028599739075 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.9023, F1 Micro: 0.7776, F1 Macro: 0.774\n",
      "Launching training on 2 GPUs.\n",
      "1688\n",
      "BESRA Uncertainty Score Threshold 256.08260928835017\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 83.31528043746948 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3941, Accuracy: 0.8836, F1 Micro: 0.7125, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.8927, F1 Micro: 0.7615, F1 Macro: 0.7582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.9039, F1 Micro: 0.7741, F1 Macro: 0.7646\n",
      "Epoch 4/10, Train Loss: 0.1798, Accuracy: 0.8991, F1 Micro: 0.7685, F1 Macro: 0.7601\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.8989, F1 Micro: 0.7619, F1 Macro: 0.7519\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.8989, F1 Micro: 0.7652, F1 Macro: 0.7543\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9039, F1 Micro: 0.7634, F1 Macro: 0.7548\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.8967, F1 Micro: 0.7725, F1 Macro: 0.7732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9019, F1 Micro: 0.7771, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.9005, F1 Micro: 0.7753, F1 Macro: 0.7733\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9019, F1 Micro: 0.7771, F1 Macro: 0.7714\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.65      0.65       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 175.04095816612244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8819, F1 Micro: 0.7, F1 Macro: 0.6887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2651, Accuracy: 0.8931, F1 Micro: 0.7633, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9042, F1 Micro: 0.7719, F1 Macro: 0.7654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.9025, F1 Micro: 0.7723, F1 Macro: 0.7627\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.903, F1 Micro: 0.7697, F1 Macro: 0.7571\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9008, F1 Micro: 0.7659, F1 Macro: 0.7554\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.9005, F1 Micro: 0.7517, F1 Macro: 0.7393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.8981, F1 Micro: 0.7756, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.8973, F1 Micro: 0.772, F1 Macro: 0.7698\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.8998, F1 Micro: 0.7677, F1 Macro: 0.7652\n",
      "Model 2 - Iteration 4663: Accuracy: 0.8981, F1 Micro: 0.7756, F1 Macro: 0.7749\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.75      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.77      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 175.31823325157166 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.8841, F1 Micro: 0.72, F1 Macro: 0.7098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2657, Accuracy: 0.8955, F1 Micro: 0.7649, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2184, Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7682\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9008, F1 Micro: 0.7619, F1 Macro: 0.7524\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.9034, F1 Micro: 0.7723, F1 Macro: 0.7617\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9017, F1 Micro: 0.7638, F1 Macro: 0.7559\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9009, F1 Micro: 0.7713, F1 Macro: 0.7659\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.8986, F1 Micro: 0.7747, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.8969, F1 Micro: 0.7719, F1 Macro: 0.7718\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9006, F1 Micro: 0.774, F1 Macro: 0.7717\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 171.81468963623047 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.9015, F1 Micro: 0.7761, F1 Macro: 0.7715\n",
      "Launching training on 2 GPUs.\n",
      "1555\n",
      "BESRA Uncertainty Score Threshold 242.80000757116588\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 76.89201831817627 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8816, F1 Micro: 0.7076, F1 Macro: 0.7007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2654, Accuracy: 0.8978, F1 Micro: 0.7707, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2196, Accuracy: 0.9014, F1 Micro: 0.771, F1 Macro: 0.7628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.8967, F1 Micro: 0.7793, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9009, F1 Micro: 0.775, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.7772\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9009, F1 Micro: 0.7772, F1 Macro: 0.7777\n",
      "Epoch 8/10, Train Loss: 0.0546, Accuracy: 0.9045, F1 Micro: 0.7743, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9025, F1 Micro: 0.7737, F1 Macro: 0.7694\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.8975, F1 Micro: 0.762, F1 Macro: 0.7531\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.66      0.66       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 182.87517476081848 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4121, Accuracy: 0.8775, F1 Micro: 0.6836, F1 Macro: 0.6742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.8983, F1 Micro: 0.7731, F1 Macro: 0.7682\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.9027, F1 Micro: 0.7697, F1 Macro: 0.7607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9056, F1 Micro: 0.7849, F1 Macro: 0.7791\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.9003, F1 Micro: 0.7828, F1 Macro: 0.7817\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9033, F1 Micro: 0.779, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.8988, F1 Micro: 0.7759, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9042, F1 Micro: 0.7768, F1 Macro: 0.7733\n",
      "Epoch 9/10, Train Loss: 0.0448, Accuracy: 0.9019, F1 Micro: 0.766, F1 Macro: 0.7566\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9025, F1 Micro: 0.7775, F1 Macro: 0.7735\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9056, F1 Micro: 0.7849, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.93      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 178.06696772575378 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4017, Accuracy: 0.8788, F1 Micro: 0.6983, F1 Macro: 0.6912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.8961, F1 Micro: 0.7689, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2181, Accuracy: 0.9006, F1 Micro: 0.7729, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.8998, F1 Micro: 0.7847, F1 Macro: 0.7813\n",
      "Epoch 5/10, Train Loss: 0.1441, Accuracy: 0.902, F1 Micro: 0.7788, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.1036, Accuracy: 0.9022, F1 Micro: 0.7734, F1 Macro: 0.7682\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.8969, F1 Micro: 0.774, F1 Macro: 0.7716\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9008, F1 Micro: 0.7735, F1 Macro: 0.77\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.9028, F1 Micro: 0.7743, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9006, F1 Micro: 0.7662, F1 Macro: 0.7583\n",
      "Model 3 - Iteration 4863: Accuracy: 0.8998, F1 Micro: 0.7847, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.94      0.91       370\n",
      "                sara       0.62      0.76      0.68       248\n",
      "         radikalisme       0.70      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.72      0.86      0.78      1365\n",
      "           macro avg       0.72      0.85      0.78      1365\n",
      "        weighted avg       0.73      0.86      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 179.80108737945557 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.9036, F1 Micro: 0.7837, F1 Macro: 0.7792\n",
      "Launching training on 2 GPUs.\n",
      "1355\n",
      "BESRA Uncertainty Score Threshold 248.59814689535705\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 67.43763017654419 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4017, Accuracy: 0.8828, F1 Micro: 0.7086, F1 Macro: 0.7034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.9, F1 Micro: 0.7732, F1 Macro: 0.7712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9083, F1 Micro: 0.7784, F1 Macro: 0.7703\n",
      "Epoch 4/10, Train Loss: 0.1789, Accuracy: 0.9036, F1 Micro: 0.7705, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9028, F1 Micro: 0.7824, F1 Macro: 0.7799\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9008, F1 Micro: 0.7785, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9042, F1 Micro: 0.7767, F1 Macro: 0.7678\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9002, F1 Micro: 0.777, F1 Macro: 0.7718\n",
      "Epoch 9/10, Train Loss: 0.0458, Accuracy: 0.9006, F1 Micro: 0.7743, F1 Macro: 0.7718\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9006, F1 Micro: 0.7738, F1 Macro: 0.7708\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9028, F1 Micro: 0.7824, F1 Macro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.64      0.66       248\n",
      "         radikalisme       0.79      0.82      0.81       243\n",
      "pencemaran_nama_baik       0.66      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 189.31931281089783 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.409, Accuracy: 0.8783, F1 Micro: 0.6582, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.8964, F1 Micro: 0.7671, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.9027, F1 Micro: 0.768, F1 Macro: 0.7563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1731, Accuracy: 0.9003, F1 Micro: 0.77, F1 Macro: 0.7582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9016, F1 Micro: 0.7768, F1 Macro: 0.7711\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.8978, F1 Micro: 0.7765, F1 Macro: 0.7759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.8988, F1 Micro: 0.7809, F1 Macro: 0.7805\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.8967, F1 Micro: 0.7611, F1 Macro: 0.7546\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9017, F1 Micro: 0.7754, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.8952, F1 Micro: 0.7638, F1 Macro: 0.7617\n",
      "Model 2 - Iteration 5063: Accuracy: 0.8988, F1 Micro: 0.7809, F1 Macro: 0.7805\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.63      0.72      0.67       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.65      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.85      0.78      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 191.0892617702484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4058, Accuracy: 0.8794, F1 Micro: 0.697, F1 Macro: 0.6952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2643, Accuracy: 0.8966, F1 Micro: 0.7667, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2125, Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7734\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.9055, F1 Micro: 0.7777, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9039, F1 Micro: 0.7841, F1 Macro: 0.7808\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.9009, F1 Micro: 0.7805, F1 Macro: 0.781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7854\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.908, F1 Micro: 0.7801, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9017, F1 Micro: 0.7764, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9066, F1 Micro: 0.7742, F1 Macro: 0.7697\n",
      "Model 3 - Iteration 5063: Accuracy: 0.902, F1 Micro: 0.7846, F1 Macro: 0.7854\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.76      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.79      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 189.03845810890198 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9012, F1 Micro: 0.7826, F1 Macro: 0.7819\n",
      "Launching training on 2 GPUs.\n",
      "1155\n",
      "BESRA Uncertainty Score Threshold 151.6473923368628\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 57.83497595787048 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8884, F1 Micro: 0.7426, F1 Macro: 0.7403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2591, Accuracy: 0.8986, F1 Micro: 0.7651, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.9003, F1 Micro: 0.7521, F1 Macro: 0.7442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9045, F1 Micro: 0.7759, F1 Macro: 0.7696\n",
      "Epoch 5/10, Train Loss: 0.129, Accuracy: 0.9048, F1 Micro: 0.7732, F1 Macro: 0.7679\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9017, F1 Micro: 0.7668, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0701, Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9022, F1 Micro: 0.78, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9019, F1 Micro: 0.777, F1 Macro: 0.7761\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9014, F1 Micro: 0.772, F1 Macro: 0.7677\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9022, F1 Micro: 0.78, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 195.86364006996155 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.8853, F1 Micro: 0.7358, F1 Macro: 0.7319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2616, Accuracy: 0.8991, F1 Micro: 0.7722, F1 Macro: 0.768\n",
      "Epoch 3/10, Train Loss: 0.2122, Accuracy: 0.8995, F1 Micro: 0.7362, F1 Macro: 0.724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9062, F1 Micro: 0.7753, F1 Macro: 0.7643\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9019, F1 Micro: 0.7612, F1 Macro: 0.7526\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9028, F1 Micro: 0.7646, F1 Macro: 0.7593\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9028, F1 Micro: 0.7693, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.8998, F1 Micro: 0.7749, F1 Macro: 0.7729\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9017, F1 Micro: 0.7626, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.0354, Accuracy: 0.9009, F1 Micro: 0.7752, F1 Macro: 0.7737\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9062, F1 Micro: 0.7753, F1 Macro: 0.7643\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.69      0.56      0.61       248\n",
      "         radikalisme       0.80      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.76      0.78      1365\n",
      "           macro avg       0.79      0.74      0.76      1365\n",
      "        weighted avg       0.79      0.76      0.77      1365\n",
      "         samples avg       0.45      0.43      0.43      1365\n",
      "\n",
      "Training completed in 191.90519165992737 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8822, F1 Micro: 0.7256, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.8958, F1 Micro: 0.7682, F1 Macro: 0.7644\n",
      "Epoch 3/10, Train Loss: 0.2112, Accuracy: 0.8997, F1 Micro: 0.7492, F1 Macro: 0.7474\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9025, F1 Micro: 0.7604, F1 Macro: 0.7474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9055, F1 Micro: 0.7762, F1 Macro: 0.7732\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9038, F1 Micro: 0.7658, F1 Macro: 0.7621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.903, F1 Micro: 0.7781, F1 Macro: 0.7775\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9013, F1 Micro: 0.7779, F1 Macro: 0.7756\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.9002, F1 Micro: 0.7643, F1 Macro: 0.756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9009, F1 Micro: 0.7796, F1 Macro: 0.7784\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9009, F1 Micro: 0.7796, F1 Macro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.61      0.74      0.67       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.56176567077637 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9031, F1 Micro: 0.7783, F1 Macro: 0.7741\n",
      "Launching training on 2 GPUs.\n",
      "955\n",
      "BESRA Uncertainty Score Threshold 159.87481873244496\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 48.10110878944397 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3821, Accuracy: 0.8884, F1 Micro: 0.7139, F1 Macro: 0.7007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.8956, F1 Micro: 0.7403, F1 Macro: 0.7357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9078, F1 Micro: 0.7878, F1 Macro: 0.7832\n",
      "Epoch 4/10, Train Loss: 0.1688, Accuracy: 0.9013, F1 Micro: 0.7778, F1 Macro: 0.7749\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9023, F1 Micro: 0.7751, F1 Macro: 0.7737\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.8998, F1 Micro: 0.7718, F1 Macro: 0.766\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9013, F1 Micro: 0.7712, F1 Macro: 0.7664\n",
      "Epoch 8/10, Train Loss: 0.0489, Accuracy: 0.9003, F1 Micro: 0.7698, F1 Macro: 0.7642\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9059, F1 Micro: 0.7775, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9039, F1 Micro: 0.7708, F1 Macro: 0.7663\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9078, F1 Micro: 0.7878, F1 Macro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.70      0.67      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 197.6651110649109 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3912, Accuracy: 0.8863, F1 Micro: 0.7206, F1 Macro: 0.7115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2541, Accuracy: 0.9003, F1 Micro: 0.7583, F1 Macro: 0.7513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9087, F1 Micro: 0.7819, F1 Macro: 0.7739\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9013, F1 Micro: 0.7813, F1 Macro: 0.7794\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9036, F1 Micro: 0.77, F1 Macro: 0.7593\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.8931, F1 Micro: 0.7662, F1 Macro: 0.7645\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9028, F1 Micro: 0.7683, F1 Macro: 0.7639\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.9042, F1 Micro: 0.7785, F1 Macro: 0.7736\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9039, F1 Micro: 0.7735, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9045, F1 Micro: 0.7755, F1 Macro: 0.7701\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9087, F1 Micro: 0.7819, F1 Macro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.73      0.58      0.65       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.76      0.72      0.74       504\n",
      "\n",
      "           micro avg       0.80      0.77      0.78      1365\n",
      "           macro avg       0.79      0.76      0.77      1365\n",
      "        weighted avg       0.80      0.77      0.78      1365\n",
      "         samples avg       0.45      0.43      0.43      1365\n",
      "\n",
      "Training completed in 196.55052065849304 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.383, Accuracy: 0.8873, F1 Micro: 0.7266, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.8984, F1 Micro: 0.7547, F1 Macro: 0.7497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2065, Accuracy: 0.9077, F1 Micro: 0.7809, F1 Macro: 0.7762\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9013, F1 Micro: 0.7768, F1 Macro: 0.7728\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9056, F1 Micro: 0.7733, F1 Macro: 0.7655\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.8998, F1 Micro: 0.7786, F1 Macro: 0.7776\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.9025, F1 Micro: 0.7553, F1 Macro: 0.7459\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9031, F1 Micro: 0.7731, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9044, F1 Micro: 0.7811, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.902, F1 Micro: 0.7749, F1 Macro: 0.7736\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9044, F1 Micro: 0.7811, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 197.8788959980011 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.907, F1 Micro: 0.7836, F1 Macro: 0.7778\n",
      "Launching training on 2 GPUs.\n",
      "777\n",
      "BESRA Uncertainty Score Threshold 169.95337744810453\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 40.79176735877991 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3684, Accuracy: 0.888, F1 Micro: 0.7167, F1 Macro: 0.7142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9011, F1 Micro: 0.7722, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.9022, F1 Micro: 0.7756, F1 Macro: 0.773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9045, F1 Micro: 0.7854, F1 Macro: 0.7821\n",
      "Epoch 5/10, Train Loss: 0.1289, Accuracy: 0.9044, F1 Micro: 0.7742, F1 Macro: 0.7683\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9042, F1 Micro: 0.7793, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.8994, F1 Micro: 0.7683, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.8998, F1 Micro: 0.7737, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.8992, F1 Micro: 0.7755, F1 Macro: 0.774\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.8998, F1 Micro: 0.7627, F1 Macro: 0.7576\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9045, F1 Micro: 0.7854, F1 Macro: 0.7821\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 205.3731653690338 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3778, Accuracy: 0.887, F1 Micro: 0.7256, F1 Macro: 0.7229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.8983, F1 Micro: 0.7696, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.9041, F1 Micro: 0.7729, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1674, Accuracy: 0.9073, F1 Micro: 0.7869, F1 Macro: 0.783\n",
      "Epoch 5/10, Train Loss: 0.1293, Accuracy: 0.9014, F1 Micro: 0.7664, F1 Macro: 0.7628\n",
      "Epoch 6/10, Train Loss: 0.0939, Accuracy: 0.9034, F1 Micro: 0.7793, F1 Macro: 0.7745\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9038, F1 Micro: 0.7822, F1 Macro: 0.7804\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.8972, F1 Micro: 0.7736, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.9011, F1 Micro: 0.7818, F1 Macro: 0.7825\n",
      "Epoch 10/10, Train Loss: 0.03, Accuracy: 0.9003, F1 Micro: 0.7688, F1 Macro: 0.764\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9073, F1 Micro: 0.7869, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.77      0.88      0.82       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 203.82476115226746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.372, Accuracy: 0.8836, F1 Micro: 0.7012, F1 Macro: 0.695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2503, Accuracy: 0.8998, F1 Micro: 0.7712, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2072, Accuracy: 0.9058, F1 Micro: 0.7776, F1 Macro: 0.7721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9078, F1 Micro: 0.79, F1 Macro: 0.789\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.907, F1 Micro: 0.7789, F1 Macro: 0.7753\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.7678\n",
      "Epoch 7/10, Train Loss: 0.0695, Accuracy: 0.9069, F1 Micro: 0.7749, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9055, F1 Micro: 0.7825, F1 Macro: 0.7792\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9039, F1 Micro: 0.7809, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.8995, F1 Micro: 0.7722, F1 Macro: 0.7676\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9078, F1 Micro: 0.79, F1 Macro: 0.789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.70      0.69      0.69       248\n",
      "         radikalisme       0.75      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 204.02728939056396 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9066, F1 Micro: 0.7874, F1 Macro: 0.7847\n",
      "Launching training on 2 GPUs.\n",
      "577\n",
      "BESRA Uncertainty Score Threshold 194.62323818656733\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 29.858471632003784 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3629, Accuracy: 0.8908, F1 Micro: 0.7395, F1 Macro: 0.7395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2416, Accuracy: 0.8978, F1 Micro: 0.7634, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7818\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9036, F1 Micro: 0.7729, F1 Macro: 0.7697\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.8967, F1 Micro: 0.7737, F1 Macro: 0.7728\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.9052, F1 Micro: 0.7798, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9023, F1 Micro: 0.7711, F1 Macro: 0.768\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.902, F1 Micro: 0.7798, F1 Macro: 0.7783\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.9005, F1 Micro: 0.7748, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9067, F1 Micro: 0.7822, F1 Macro: 0.7775\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 209.33390831947327 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3733, Accuracy: 0.8855, F1 Micro: 0.7043, F1 Macro: 0.6978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2411, Accuracy: 0.9005, F1 Micro: 0.7706, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1973, Accuracy: 0.9059, F1 Micro: 0.7828, F1 Macro: 0.7771\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.9047, F1 Micro: 0.7689, F1 Macro: 0.7637\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.903, F1 Micro: 0.7699, F1 Macro: 0.7622\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9047, F1 Micro: 0.7817, F1 Macro: 0.7793\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.902, F1 Micro: 0.7753, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.898, F1 Micro: 0.779, F1 Macro: 0.7776\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.903, F1 Micro: 0.7777, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.9072, F1 Micro: 0.7789, F1 Macro: 0.7748\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9059, F1 Micro: 0.7828, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.65      0.66      0.65       248\n",
      "         radikalisme       0.73      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 208.38882517814636 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.366, Accuracy: 0.8869, F1 Micro: 0.7211, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.8953, F1 Micro: 0.7607, F1 Macro: 0.7595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1963, Accuracy: 0.9041, F1 Micro: 0.7785, F1 Macro: 0.7743\n",
      "Epoch 4/10, Train Loss: 0.1685, Accuracy: 0.9017, F1 Micro: 0.7611, F1 Macro: 0.7584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9058, F1 Micro: 0.7892, F1 Macro: 0.7871\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9039, F1 Micro: 0.7795, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9014, F1 Micro: 0.7829, F1 Macro: 0.7834\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.9016, F1 Micro: 0.7734, F1 Macro: 0.7706\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.8998, F1 Micro: 0.7781, F1 Macro: 0.7776\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.9075, F1 Micro: 0.7825, F1 Macro: 0.777\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9058, F1 Micro: 0.7892, F1 Macro: 0.7871\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.75      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 209.6658899784088 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9056, F1 Micro: 0.7856, F1 Macro: 0.782\n",
      "Launching training on 2 GPUs.\n",
      "377\n",
      "BESRA Uncertainty Score Threshold 88.69203081450331\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.988845348358154 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3744, Accuracy: 0.8913, F1 Micro: 0.7544, F1 Macro: 0.7523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2379, Accuracy: 0.9025, F1 Micro: 0.7635, F1 Macro: 0.7517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.905, F1 Micro: 0.7799, F1 Macro: 0.776\n",
      "Epoch 4/10, Train Loss: 0.1585, Accuracy: 0.9033, F1 Micro: 0.77, F1 Macro: 0.7576\n",
      "Epoch 5/10, Train Loss: 0.1266, Accuracy: 0.9009, F1 Micro: 0.7652, F1 Macro: 0.7569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9073, F1 Micro: 0.7814, F1 Macro: 0.7769\n",
      "Epoch 7/10, Train Loss: 0.0643, Accuracy: 0.9084, F1 Micro: 0.7772, F1 Macro: 0.768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.905, F1 Micro: 0.7816, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9061, F1 Micro: 0.7822, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.9053, F1 Micro: 0.7869, F1 Macro: 0.7879\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9053, F1 Micro: 0.7869, F1 Macro: 0.7879\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.78      0.86      0.82       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 221.72874546051025 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3823, Accuracy: 0.8881, F1 Micro: 0.7533, F1 Macro: 0.7524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2399, Accuracy: 0.902, F1 Micro: 0.7663, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1962, Accuracy: 0.9036, F1 Micro: 0.7821, F1 Macro: 0.7795\n",
      "Epoch 4/10, Train Loss: 0.1559, Accuracy: 0.9052, F1 Micro: 0.7721, F1 Macro: 0.7626\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9059, F1 Micro: 0.7793, F1 Macro: 0.7708\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9025, F1 Micro: 0.7589, F1 Macro: 0.7558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.9053, F1 Micro: 0.783, F1 Macro: 0.7811\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9027, F1 Micro: 0.7787, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.0367, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.9052, F1 Micro: 0.7822, F1 Macro: 0.7829\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9053, F1 Micro: 0.783, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 215.0842957496643 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3768, Accuracy: 0.8886, F1 Micro: 0.7429, F1 Macro: 0.7391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2403, Accuracy: 0.9013, F1 Micro: 0.7597, F1 Macro: 0.7523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1969, Accuracy: 0.9002, F1 Micro: 0.7613, F1 Macro: 0.7571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1548, Accuracy: 0.9055, F1 Micro: 0.7723, F1 Macro: 0.7646\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.8989, F1 Micro: 0.7631, F1 Macro: 0.7568\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9036, F1 Micro: 0.7716, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9055, F1 Micro: 0.778, F1 Macro: 0.775\n",
      "Epoch 8/10, Train Loss: 0.0477, Accuracy: 0.9011, F1 Micro: 0.7761, F1 Macro: 0.775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.9034, F1 Micro: 0.7821, F1 Macro: 0.7838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9033, F1 Micro: 0.783, F1 Macro: 0.7828\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9033, F1 Micro: 0.783, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.78      0.79      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 220.60790252685547 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9046, F1 Micro: 0.7843, F1 Macro: 0.7839\n",
      "Launching training on 2 GPUs.\n",
      "177\n",
      "BESRA Uncertainty Score Threshold 90.2408193966641\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 7.540940046310425 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3536, Accuracy: 0.8919, F1 Micro: 0.742, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2301, Accuracy: 0.9023, F1 Micro: 0.7711, F1 Macro: 0.7682\n",
      "Epoch 3/10, Train Loss: 0.1877, Accuracy: 0.9028, F1 Micro: 0.7696, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.903, F1 Micro: 0.7716, F1 Macro: 0.7633\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.8994, F1 Micro: 0.7566, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0816, Accuracy: 0.8988, F1 Micro: 0.7741, F1 Macro: 0.7715\n",
      "Epoch 7/10, Train Loss: 0.062, Accuracy: 0.8975, F1 Micro: 0.7689, F1 Macro: 0.7657\n",
      "Epoch 8/10, Train Loss: 0.0445, Accuracy: 0.9036, F1 Micro: 0.7737, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.8992, F1 Micro: 0.7733, F1 Macro: 0.7714\n",
      "Epoch 10/10, Train Loss: 0.0285, Accuracy: 0.9047, F1 Micro: 0.7666, F1 Macro: 0.7588\n",
      "Model 1 - Iteration 6218: Accuracy: 0.8988, F1 Micro: 0.7741, F1 Macro: 0.7715\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.89       370\n",
      "                sara       0.63      0.72      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.73      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.81      0.77      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 222.5608251094818 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3566, Accuracy: 0.8888, F1 Micro: 0.7403, F1 Macro: 0.7355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.9005, F1 Micro: 0.7701, F1 Macro: 0.7683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1855, Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.7739\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9053, F1 Micro: 0.7732, F1 Macro: 0.7651\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.9055, F1 Micro: 0.7728, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9027, F1 Micro: 0.7768, F1 Macro: 0.7744\n",
      "Epoch 7/10, Train Loss: 0.0624, Accuracy: 0.9039, F1 Micro: 0.7741, F1 Macro: 0.7696\n",
      "Epoch 8/10, Train Loss: 0.0483, Accuracy: 0.903, F1 Micro: 0.7736, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.0347, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.0271, Accuracy: 0.9061, F1 Micro: 0.7757, F1 Macro: 0.7692\n",
      "Model 2 - Iteration 6218: Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.66      0.65      0.66       248\n",
      "         radikalisme       0.78      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.44      0.44      1365\n",
      "\n",
      "Training completed in 219.18567490577698 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3569, Accuracy: 0.8855, F1 Micro: 0.7416, F1 Macro: 0.7368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2315, Accuracy: 0.9044, F1 Micro: 0.7784, F1 Macro: 0.7731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9064, F1 Micro: 0.7816, F1 Macro: 0.7797\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9044, F1 Micro: 0.7747, F1 Macro: 0.7693\n",
      "Epoch 5/10, Train Loss: 0.1181, Accuracy: 0.9036, F1 Micro: 0.7644, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.0841, Accuracy: 0.907, F1 Micro: 0.7802, F1 Macro: 0.7807\n",
      "Epoch 7/10, Train Loss: 0.0604, Accuracy: 0.9003, F1 Micro: 0.7771, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.902, F1 Micro: 0.7753, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.0367, Accuracy: 0.9033, F1 Micro: 0.7705, F1 Macro: 0.7641\n",
      "Epoch 10/10, Train Loss: 0.0243, Accuracy: 0.9048, F1 Micro: 0.7704, F1 Macro: 0.7667\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9064, F1 Micro: 0.7816, F1 Macro: 0.7797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.84      0.89       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 219.29748558998108 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9034, F1 Micro: 0.7776, F1 Macro: 0.775\n",
      "Total sampling time: 2485.84 seconds\n",
      "Total runtime: 13146.087568283081 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b290b2",
   "metadata": {
    "papermill": {
     "duration": 0.010962,
     "end_time": "2025-06-07T18:18:15.889914",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.878952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c3a52e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T18:18:15.912421Z",
     "iopub.status.busy": "2025-06-07T18:18:15.912179Z",
     "iopub.status.idle": "2025-06-07T21:56:31.172994Z",
     "shell.execute_reply": "2025-06-07T21:56:31.172165Z"
    },
    "papermill": {
     "duration": 13095.273562,
     "end_time": "2025-06-07T21:56:31.174388",
     "exception": false,
     "start_time": "2025-06-07T18:18:15.900826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6257, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4449, Accuracy: 0.7873, F1 Micro: 0.0058, F1 Macro: 0.0053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3901, Accuracy: 0.8056, F1 Micro: 0.1629, F1 Macro: 0.1232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3715, Accuracy: 0.8298, F1 Micro: 0.3702, F1 Macro: 0.2792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3107, Accuracy: 0.8478, F1 Micro: 0.5173, F1 Macro: 0.4639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.269, Accuracy: 0.858, F1 Micro: 0.5821, F1 Macro: 0.5628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2396, Accuracy: 0.8644, F1 Micro: 0.6341, F1 Macro: 0.6306\n",
      "Epoch 8/10, Train Loss: 0.1893, Accuracy: 0.8645, F1 Micro: 0.6199, F1 Macro: 0.6103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1519, Accuracy: 0.8764, F1 Micro: 0.6801, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1377, Accuracy: 0.8766, F1 Micro: 0.7037, F1 Macro: 0.7025\n",
      "Model 1 - Iteration 388: Accuracy: 0.8766, F1 Micro: 0.7037, F1 Macro: 0.7025\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       370\n",
      "                sara       0.60      0.59      0.60       248\n",
      "         radikalisme       0.67      0.79      0.72       243\n",
      "pencemaran_nama_baik       0.68      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.72      0.69      0.70      1365\n",
      "           macro avg       0.71      0.70      0.70      1365\n",
      "        weighted avg       0.72      0.69      0.70      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 58.074225664138794 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.641, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4523, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.7945, F1 Micro: 0.0707, F1 Macro: 0.0595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3985, Accuracy: 0.8206, F1 Micro: 0.2743, F1 Macro: 0.1959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3282, Accuracy: 0.8359, F1 Micro: 0.4312, F1 Macro: 0.3544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.291, Accuracy: 0.8492, F1 Micro: 0.5304, F1 Macro: 0.4884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2575, Accuracy: 0.8558, F1 Micro: 0.5733, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2031, Accuracy: 0.8644, F1 Micro: 0.6331, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1705, Accuracy: 0.8687, F1 Micro: 0.6754, F1 Macro: 0.6686\n",
      "Epoch 10/10, Train Loss: 0.1513, Accuracy: 0.8719, F1 Micro: 0.6743, F1 Macro: 0.6681\n",
      "Model 2 - Iteration 388: Accuracy: 0.8687, F1 Micro: 0.6754, F1 Macro: 0.6686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.80      0.85       370\n",
      "                sara       0.58      0.50      0.54       248\n",
      "         radikalisme       0.66      0.67      0.67       243\n",
      "pencemaran_nama_baik       0.67      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.68      1365\n",
      "           macro avg       0.70      0.64      0.67      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.36      0.36      0.35      1365\n",
      "\n",
      "Training completed in 57.31601166725159 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5978, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4434, Accuracy: 0.7872, F1 Micro: 0.0044, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3804, Accuracy: 0.8202, F1 Micro: 0.2738, F1 Macro: 0.1854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3666, Accuracy: 0.8278, F1 Micro: 0.3464, F1 Macro: 0.2469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3085, Accuracy: 0.8375, F1 Micro: 0.4372, F1 Macro: 0.3739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2698, Accuracy: 0.8508, F1 Micro: 0.538, F1 Macro: 0.5159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2493, Accuracy: 0.853, F1 Micro: 0.5597, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1995, Accuracy: 0.8572, F1 Micro: 0.5981, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1665, Accuracy: 0.8678, F1 Micro: 0.6513, F1 Macro: 0.6482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1477, Accuracy: 0.873, F1 Micro: 0.6661, F1 Macro: 0.6587\n",
      "Model 3 - Iteration 388: Accuracy: 0.873, F1 Micro: 0.6661, F1 Macro: 0.6587\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.83      0.87       370\n",
      "                sara       0.67      0.46      0.54       248\n",
      "         radikalisme       0.68      0.67      0.67       243\n",
      "pencemaran_nama_baik       0.71      0.45      0.55       504\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1365\n",
      "           macro avg       0.74      0.60      0.66      1365\n",
      "        weighted avg       0.75      0.59      0.66      1365\n",
      "         samples avg       0.35      0.33      0.33      1365\n",
      "\n",
      "Training completed in 60.30164623260498 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8728, F1 Micro: 0.6817, F1 Macro: 0.6766\n",
      "Launching training on 2 GPUs.\n",
      "5830\n",
      "BESRA Uncertainty Score Threshold 151.4084556516312\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 282.1463813781738 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5672, Accuracy: 0.7902, F1 Micro: 0.0317, F1 Macro: 0.0281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4105, Accuracy: 0.8487, F1 Micro: 0.5145, F1 Macro: 0.4529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3408, Accuracy: 0.8694, F1 Micro: 0.64, F1 Macro: 0.629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2571, Accuracy: 0.8766, F1 Micro: 0.6914, F1 Macro: 0.6905\n",
      "Epoch 5/10, Train Loss: 0.2278, Accuracy: 0.8758, F1 Micro: 0.6512, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1929, Accuracy: 0.8888, F1 Micro: 0.7103, F1 Macro: 0.703\n",
      "Epoch 7/10, Train Loss: 0.1444, Accuracy: 0.8836, F1 Micro: 0.6734, F1 Macro: 0.6617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1187, Accuracy: 0.8872, F1 Micro: 0.7432, F1 Macro: 0.7388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.8895, F1 Micro: 0.746, F1 Macro: 0.7418\n",
      "Epoch 10/10, Train Loss: 0.0743, Accuracy: 0.8873, F1 Micro: 0.7282, F1 Macro: 0.7218\n",
      "Model 1 - Iteration 971: Accuracy: 0.8895, F1 Micro: 0.746, F1 Macro: 0.7418\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       370\n",
      "                sara       0.64      0.60      0.62       248\n",
      "         radikalisme       0.76      0.76      0.76       243\n",
      "pencemaran_nama_baik       0.65      0.77      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.76      0.75      1365\n",
      "           macro avg       0.74      0.75      0.74      1365\n",
      "        weighted avg       0.74      0.76      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 71.20655965805054 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5813, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4324, Accuracy: 0.8286, F1 Micro: 0.3566, F1 Macro: 0.293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3666, Accuracy: 0.8542, F1 Micro: 0.5499, F1 Macro: 0.5183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2728, Accuracy: 0.8723, F1 Micro: 0.6725, F1 Macro: 0.6749\n",
      "Epoch 5/10, Train Loss: 0.2461, Accuracy: 0.8697, F1 Micro: 0.6099, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2051, Accuracy: 0.8814, F1 Micro: 0.7055, F1 Macro: 0.7042\n",
      "Epoch 7/10, Train Loss: 0.1592, Accuracy: 0.8825, F1 Micro: 0.6775, F1 Macro: 0.6681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1271, Accuracy: 0.8872, F1 Micro: 0.7251, F1 Macro: 0.7175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1046, Accuracy: 0.8839, F1 Micro: 0.7403, F1 Macro: 0.7354\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.8891, F1 Micro: 0.7327, F1 Macro: 0.7238\n",
      "Model 2 - Iteration 971: Accuracy: 0.8839, F1 Micro: 0.7403, F1 Macro: 0.7354\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       370\n",
      "                sara       0.63      0.61      0.62       248\n",
      "         radikalisme       0.74      0.74      0.74       243\n",
      "pencemaran_nama_baik       0.63      0.80      0.70       504\n",
      "\n",
      "           micro avg       0.71      0.78      0.74      1365\n",
      "           macro avg       0.72      0.76      0.74      1365\n",
      "        weighted avg       0.72      0.78      0.74      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 70.66373586654663 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5503, Accuracy: 0.7922, F1 Micro: 0.05, F1 Macro: 0.0432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.413, Accuracy: 0.8311, F1 Micro: 0.3733, F1 Macro: 0.3135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3541, Accuracy: 0.865, F1 Micro: 0.6094, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2695, Accuracy: 0.8752, F1 Micro: 0.6785, F1 Macro: 0.678\n",
      "Epoch 5/10, Train Loss: 0.24, Accuracy: 0.8783, F1 Micro: 0.6591, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2002, Accuracy: 0.8872, F1 Micro: 0.7155, F1 Macro: 0.7113\n",
      "Epoch 7/10, Train Loss: 0.1617, Accuracy: 0.8839, F1 Micro: 0.6916, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1226, Accuracy: 0.8841, F1 Micro: 0.7369, F1 Macro: 0.7277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0984, Accuracy: 0.885, F1 Micro: 0.7421, F1 Macro: 0.7341\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.8852, F1 Micro: 0.7232, F1 Macro: 0.7114\n",
      "Model 3 - Iteration 971: Accuracy: 0.885, F1 Micro: 0.7421, F1 Macro: 0.7341\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.88      0.87       370\n",
      "                sara       0.66      0.58      0.62       248\n",
      "         radikalisme       0.71      0.77      0.74       243\n",
      "pencemaran_nama_baik       0.64      0.80      0.71       504\n",
      "\n",
      "           micro avg       0.71      0.78      0.74      1365\n",
      "           macro avg       0.72      0.76      0.73      1365\n",
      "        weighted avg       0.72      0.78      0.74      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 70.9318118095398 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8861, F1 Micro: 0.7428, F1 Macro: 0.7371\n",
      "Launching training on 2 GPUs.\n",
      "5247\n",
      "BESRA Uncertainty Score Threshold 194.42712505926502\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 252.99029207229614 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5251, Accuracy: 0.8173, F1 Micro: 0.253, F1 Macro: 0.1741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3728, Accuracy: 0.8658, F1 Micro: 0.6257, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2855, Accuracy: 0.8741, F1 Micro: 0.6705, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2358, Accuracy: 0.8884, F1 Micro: 0.7191, F1 Macro: 0.7153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.185, Accuracy: 0.8913, F1 Micro: 0.7558, F1 Macro: 0.7548\n",
      "Epoch 6/10, Train Loss: 0.1524, Accuracy: 0.8905, F1 Micro: 0.7338, F1 Macro: 0.7304\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.8934, F1 Micro: 0.7309, F1 Macro: 0.7229\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.8948, F1 Micro: 0.7521, F1 Macro: 0.7477\n",
      "Epoch 9/10, Train Loss: 0.0668, Accuracy: 0.8877, F1 Micro: 0.752, F1 Macro: 0.755\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.8898, F1 Micro: 0.7458, F1 Macro: 0.7433\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8913, F1 Micro: 0.7558, F1 Macro: 0.7548\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       370\n",
      "                sara       0.63      0.67      0.65       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.77      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1365\n",
      "           macro avg       0.73      0.79      0.75      1365\n",
      "        weighted avg       0.73      0.79      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 82.58324122428894 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5377, Accuracy: 0.7998, F1 Micro: 0.1159, F1 Macro: 0.0928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3982, Accuracy: 0.8534, F1 Micro: 0.5571, F1 Macro: 0.5165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3064, Accuracy: 0.8741, F1 Micro: 0.6599, F1 Macro: 0.6606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2513, Accuracy: 0.8858, F1 Micro: 0.6988, F1 Macro: 0.6933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2028, Accuracy: 0.8925, F1 Micro: 0.7228, F1 Macro: 0.7154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1679, Accuracy: 0.8883, F1 Micro: 0.7335, F1 Macro: 0.7311\n",
      "Epoch 7/10, Train Loss: 0.1265, Accuracy: 0.8939, F1 Micro: 0.7276, F1 Macro: 0.7228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1068, Accuracy: 0.8966, F1 Micro: 0.75, F1 Macro: 0.743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.8963, F1 Micro: 0.755, F1 Macro: 0.7565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0588, Accuracy: 0.8955, F1 Micro: 0.7559, F1 Macro: 0.7555\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8955, F1 Micro: 0.7559, F1 Macro: 0.7555\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       370\n",
      "                sara       0.65      0.66      0.65       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.70       504\n",
      "\n",
      "           micro avg       0.75      0.76      0.76      1365\n",
      "           macro avg       0.75      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 88.70439648628235 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5073, Accuracy: 0.8239, F1 Micro: 0.3207, F1 Macro: 0.208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3779, Accuracy: 0.8641, F1 Micro: 0.613, F1 Macro: 0.6014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.292, Accuracy: 0.8703, F1 Micro: 0.6447, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.239, Accuracy: 0.8861, F1 Micro: 0.7104, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.193, Accuracy: 0.89, F1 Micro: 0.7231, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1603, Accuracy: 0.8875, F1 Micro: 0.7335, F1 Macro: 0.7266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1207, Accuracy: 0.8927, F1 Micro: 0.7344, F1 Macro: 0.728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.8952, F1 Micro: 0.7345, F1 Macro: 0.7217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.8939, F1 Micro: 0.7506, F1 Macro: 0.7479\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.8913, F1 Micro: 0.7348, F1 Macro: 0.7255\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8939, F1 Micro: 0.7506, F1 Macro: 0.7479\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       370\n",
      "                sara       0.65      0.62      0.64       248\n",
      "         radikalisme       0.75      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.70      0.70       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.75      0.75      0.75      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 89.26684546470642 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8935, F1 Micro: 0.7541, F1 Macro: 0.7528\n",
      "Launching training on 2 GPUs.\n",
      "4722\n",
      "BESRA Uncertainty Score Threshold 198.56447318592777\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 229.66592264175415 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4962, Accuracy: 0.8447, F1 Micro: 0.4866, F1 Macro: 0.4018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3293, Accuracy: 0.8817, F1 Micro: 0.7181, F1 Macro: 0.7186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2552, Accuracy: 0.8909, F1 Micro: 0.7342, F1 Macro: 0.7325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2047, Accuracy: 0.893, F1 Micro: 0.742, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.8955, F1 Micro: 0.7482, F1 Macro: 0.7438\n",
      "Epoch 6/10, Train Loss: 0.131, Accuracy: 0.8942, F1 Micro: 0.7306, F1 Macro: 0.7128\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.8955, F1 Micro: 0.729, F1 Macro: 0.7208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.897, F1 Micro: 0.7576, F1 Macro: 0.756\n",
      "Epoch 9/10, Train Loss: 0.0668, Accuracy: 0.8958, F1 Micro: 0.7527, F1 Macro: 0.7489\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.895, F1 Micro: 0.7511, F1 Macro: 0.7466\n",
      "Model 1 - Iteration 1969: Accuracy: 0.897, F1 Micro: 0.7576, F1 Macro: 0.756\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.66      0.64      0.65       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.75      0.76      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 95.9345953464508 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5197, Accuracy: 0.828, F1 Micro: 0.3443, F1 Macro: 0.2485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3563, Accuracy: 0.8803, F1 Micro: 0.6958, F1 Macro: 0.689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2756, Accuracy: 0.8925, F1 Micro: 0.7283, F1 Macro: 0.7201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.221, Accuracy: 0.8928, F1 Micro: 0.7396, F1 Macro: 0.7348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1711, Accuracy: 0.8961, F1 Micro: 0.7447, F1 Macro: 0.7406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.8991, F1 Micro: 0.7455, F1 Macro: 0.7313\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.8963, F1 Micro: 0.7384, F1 Macro: 0.7307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0761, Accuracy: 0.9003, F1 Micro: 0.7632, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.8995, F1 Micro: 0.764, F1 Macro: 0.7605\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.8973, F1 Micro: 0.7593, F1 Macro: 0.7539\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8995, F1 Micro: 0.764, F1 Macro: 0.7605\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.63      0.67      0.65       248\n",
      "         radikalisme       0.76      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.68      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.77      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 100.0608069896698 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4884, Accuracy: 0.8397, F1 Micro: 0.4375, F1 Macro: 0.3813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3356, Accuracy: 0.878, F1 Micro: 0.6922, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2672, Accuracy: 0.8894, F1 Micro: 0.7336, F1 Macro: 0.728\n",
      "Epoch 4/10, Train Loss: 0.2129, Accuracy: 0.8867, F1 Micro: 0.7213, F1 Macro: 0.7168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.8948, F1 Micro: 0.7506, F1 Macro: 0.7454\n",
      "Epoch 6/10, Train Loss: 0.1334, Accuracy: 0.8938, F1 Micro: 0.7397, F1 Macro: 0.7272\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.8959, F1 Micro: 0.7409, F1 Macro: 0.7337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.8972, F1 Micro: 0.759, F1 Macro: 0.7546\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.8942, F1 Micro: 0.7462, F1 Macro: 0.7401\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.8969, F1 Micro: 0.755, F1 Macro: 0.7485\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8972, F1 Micro: 0.759, F1 Macro: 0.7546\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.84      0.89       370\n",
      "                sara       0.64      0.66      0.65       248\n",
      "         radikalisme       0.72      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.76      0.76      0.75      1365\n",
      "        weighted avg       0.77      0.76      0.76      1365\n",
      "         samples avg       0.42      0.42      0.42      1365\n",
      "\n",
      "Training completed in 94.85062193870544 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8979, F1 Micro: 0.7602, F1 Macro: 0.757\n",
      "Launching training on 2 GPUs.\n",
      "4249\n",
      "BESRA Uncertainty Score Threshold 292.3212329571269\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 205.9431540966034 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4906, Accuracy: 0.8539, F1 Micro: 0.5503, F1 Macro: 0.5072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3084, Accuracy: 0.8867, F1 Micro: 0.7314, F1 Macro: 0.7242\n",
      "Epoch 3/10, Train Loss: 0.245, Accuracy: 0.8895, F1 Micro: 0.7046, F1 Macro: 0.6926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2056, Accuracy: 0.8961, F1 Micro: 0.7649, F1 Macro: 0.7639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.8994, F1 Micro: 0.7685, F1 Macro: 0.7635\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.8956, F1 Micro: 0.7648, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.8958, F1 Micro: 0.7534, F1 Macro: 0.7506\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.8983, F1 Micro: 0.7606, F1 Macro: 0.7566\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.8973, F1 Micro: 0.7634, F1 Macro: 0.7615\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.8989, F1 Micro: 0.7605, F1 Macro: 0.7572\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8994, F1 Micro: 0.7685, F1 Macro: 0.7635\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 107.89892840385437 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5141, Accuracy: 0.8411, F1 Micro: 0.447, F1 Macro: 0.3773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3317, Accuracy: 0.8788, F1 Micro: 0.7061, F1 Macro: 0.6925\n",
      "Epoch 3/10, Train Loss: 0.2572, Accuracy: 0.8891, F1 Micro: 0.7044, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.212, Accuracy: 0.8925, F1 Micro: 0.7613, F1 Macro: 0.758\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.8984, F1 Micro: 0.7475, F1 Macro: 0.7321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.8994, F1 Micro: 0.7634, F1 Macro: 0.758\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.8998, F1 Micro: 0.76, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9033, F1 Micro: 0.7644, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.8984, F1 Micro: 0.7542, F1 Macro: 0.7497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9, F1 Micro: 0.7663, F1 Macro: 0.7639\n",
      "Model 2 - Iteration 2394: Accuracy: 0.9, F1 Micro: 0.7663, F1 Macro: 0.7639\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.65      0.66      0.66       248\n",
      "         radikalisme       0.79      0.78      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.77      0.77      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 110.23315382003784 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4868, Accuracy: 0.8367, F1 Micro: 0.4274, F1 Macro: 0.3498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3207, Accuracy: 0.8806, F1 Micro: 0.6994, F1 Macro: 0.6875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2505, Accuracy: 0.8909, F1 Micro: 0.7267, F1 Macro: 0.7095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2098, Accuracy: 0.8966, F1 Micro: 0.769, F1 Macro: 0.7666\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.8948, F1 Micro: 0.7403, F1 Macro: 0.7263\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.8989, F1 Micro: 0.7645, F1 Macro: 0.7586\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.8983, F1 Micro: 0.7649, F1 Macro: 0.7628\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.8961, F1 Micro: 0.7358, F1 Macro: 0.7282\n",
      "Epoch 9/10, Train Loss: 0.0559, Accuracy: 0.8972, F1 Micro: 0.7543, F1 Macro: 0.7501\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.898, F1 Micro: 0.7566, F1 Macro: 0.7526\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8966, F1 Micro: 0.769, F1 Macro: 0.7666\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.89       370\n",
      "                sara       0.65      0.75      0.70       248\n",
      "         radikalisme       0.68      0.83      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1365\n",
      "           macro avg       0.73      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.81      0.77      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 106.99819493293762 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8986, F1 Micro: 0.7679, F1 Macro: 0.7647\n",
      "Launching training on 2 GPUs.\n",
      "3824\n",
      "BESRA Uncertainty Score Threshold 226.60263804949273\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 185.6836597919464 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4689, Accuracy: 0.8627, F1 Micro: 0.586, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2999, Accuracy: 0.8889, F1 Micro: 0.7471, F1 Macro: 0.7505\n",
      "Epoch 3/10, Train Loss: 0.237, Accuracy: 0.8961, F1 Micro: 0.7411, F1 Macro: 0.7322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.8981, F1 Micro: 0.7554, F1 Macro: 0.7443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1573, Accuracy: 0.8984, F1 Micro: 0.7571, F1 Macro: 0.7453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1126, Accuracy: 0.9036, F1 Micro: 0.7646, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9005, F1 Micro: 0.7737, F1 Macro: 0.7714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9027, F1 Micro: 0.7747, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.8988, F1 Micro: 0.7551, F1 Macro: 0.7498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.9033, F1 Micro: 0.775, F1 Macro: 0.7725\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9033, F1 Micro: 0.775, F1 Macro: 0.7725\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.68      0.64      0.66       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 125.58072280883789 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4856, Accuracy: 0.8534, F1 Micro: 0.5366, F1 Macro: 0.4686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3124, Accuracy: 0.8859, F1 Micro: 0.7324, F1 Macro: 0.733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2462, Accuracy: 0.8983, F1 Micro: 0.7489, F1 Macro: 0.738\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.8958, F1 Micro: 0.735, F1 Macro: 0.7193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1599, Accuracy: 0.8989, F1 Micro: 0.7535, F1 Macro: 0.7454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9047, F1 Micro: 0.7746, F1 Macro: 0.7693\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9027, F1 Micro: 0.7737, F1 Macro: 0.7676\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9041, F1 Micro: 0.7716, F1 Macro: 0.7696\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9034, F1 Micro: 0.7663, F1 Macro: 0.761\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9, F1 Micro: 0.7735, F1 Macro: 0.7723\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9047, F1 Micro: 0.7746, F1 Macro: 0.7693\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.78      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 120.27307200431824 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4651, Accuracy: 0.8537, F1 Micro: 0.5239, F1 Macro: 0.4962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3059, Accuracy: 0.8872, F1 Micro: 0.732, F1 Macro: 0.7292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2444, Accuracy: 0.8942, F1 Micro: 0.7403, F1 Macro: 0.7326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.8973, F1 Micro: 0.748, F1 Macro: 0.7332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.8973, F1 Micro: 0.7663, F1 Macro: 0.7614\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9019, F1 Micro: 0.7646, F1 Macro: 0.7533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.903, F1 Micro: 0.7765, F1 Macro: 0.7705\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9055, F1 Micro: 0.7753, F1 Macro: 0.7691\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9, F1 Micro: 0.7671, F1 Macro: 0.7647\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9031, F1 Micro: 0.7676, F1 Macro: 0.7598\n",
      "Model 3 - Iteration 2777: Accuracy: 0.903, F1 Micro: 0.7765, F1 Macro: 0.7705\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.70      0.68      0.69       248\n",
      "         radikalisme       0.71      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 121.96660137176514 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.9036, F1 Micro: 0.7754, F1 Macro: 0.7708\n",
      "Launching training on 2 GPUs.\n",
      "3441\n",
      "BESRA Uncertainty Score Threshold 265.2062614979681\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 167.73692107200623 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.453, Accuracy: 0.8694, F1 Micro: 0.6294, F1 Macro: 0.597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2898, Accuracy: 0.8933, F1 Micro: 0.7492, F1 Macro: 0.7401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.8977, F1 Micro: 0.7568, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9016, F1 Micro: 0.7675, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9052, F1 Micro: 0.7724, F1 Macro: 0.7668\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9031, F1 Micro: 0.7652, F1 Macro: 0.7617\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.8997, F1 Micro: 0.7446, F1 Macro: 0.7377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.905, F1 Micro: 0.777, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9062, F1 Micro: 0.7815, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.0371, Accuracy: 0.9041, F1 Micro: 0.7798, F1 Macro: 0.7774\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9062, F1 Micro: 0.7815, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 133.13705444335938 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4672, Accuracy: 0.8731, F1 Micro: 0.6503, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3017, Accuracy: 0.893, F1 Micro: 0.7488, F1 Macro: 0.7408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2338, Accuracy: 0.8994, F1 Micro: 0.761, F1 Macro: 0.7554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1799, Accuracy: 0.9038, F1 Micro: 0.7768, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9038, F1 Micro: 0.7631, F1 Macro: 0.7573\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9044, F1 Micro: 0.7708, F1 Macro: 0.7635\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.9008, F1 Micro: 0.7536, F1 Macro: 0.7441\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9045, F1 Micro: 0.7731, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.9033, F1 Micro: 0.771, F1 Macro: 0.7681\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9058, F1 Micro: 0.7756, F1 Macro: 0.7724\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9038, F1 Micro: 0.7768, F1 Macro: 0.7743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.74      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.44      1365\n",
      "\n",
      "Training completed in 127.96752834320068 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4492, Accuracy: 0.8692, F1 Micro: 0.6201, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2987, Accuracy: 0.8931, F1 Micro: 0.75, F1 Macro: 0.7426\n",
      "Epoch 3/10, Train Loss: 0.229, Accuracy: 0.8939, F1 Micro: 0.7409, F1 Macro: 0.7385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.8983, F1 Micro: 0.763, F1 Macro: 0.7586\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.8995, F1 Micro: 0.7534, F1 Macro: 0.7449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1089, Accuracy: 0.9023, F1 Micro: 0.766, F1 Macro: 0.7599\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.8975, F1 Micro: 0.7451, F1 Macro: 0.7367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9017, F1 Micro: 0.7737, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.9016, F1 Micro: 0.7711, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.772\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.77      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 131.04997301101685 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.904, F1 Micro: 0.7775, F1 Macro: 0.7757\n",
      "Launching training on 2 GPUs.\n",
      "3096\n",
      "BESRA Uncertainty Score Threshold 266.87849322316936\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 152.49048829078674 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4402, Accuracy: 0.8727, F1 Micro: 0.658, F1 Macro: 0.6421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.289, Accuracy: 0.8856, F1 Micro: 0.693, F1 Macro: 0.6825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.229, Accuracy: 0.8952, F1 Micro: 0.7293, F1 Macro: 0.7192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2004, Accuracy: 0.9045, F1 Micro: 0.7726, F1 Macro: 0.7675\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9064, F1 Micro: 0.7685, F1 Macro: 0.7629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9091, F1 Micro: 0.7739, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.9039, F1 Micro: 0.7773, F1 Macro: 0.7731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9077, F1 Micro: 0.7823, F1 Macro: 0.7796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9056, F1 Micro: 0.7857, F1 Macro: 0.7849\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9062, F1 Micro: 0.7848, F1 Macro: 0.7838\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9056, F1 Micro: 0.7857, F1 Macro: 0.7849\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.68      0.72      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 144.0591471195221 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4593, Accuracy: 0.8673, F1 Micro: 0.6153, F1 Macro: 0.56\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.295, Accuracy: 0.8886, F1 Micro: 0.7023, F1 Macro: 0.6894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2325, Accuracy: 0.8969, F1 Micro: 0.7324, F1 Macro: 0.7187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1981, Accuracy: 0.9041, F1 Micro: 0.7706, F1 Macro: 0.7665\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.9062, F1 Micro: 0.7698, F1 Macro: 0.7671\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9036, F1 Micro: 0.7598, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0912, Accuracy: 0.8991, F1 Micro: 0.7757, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9055, F1 Micro: 0.7848, F1 Macro: 0.7849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9041, F1 Micro: 0.7856, F1 Macro: 0.784\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9055, F1 Micro: 0.7753, F1 Macro: 0.7723\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9041, F1 Micro: 0.7856, F1 Macro: 0.784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.70      0.67      0.68       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 143.445166349411 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4402, Accuracy: 0.8714, F1 Micro: 0.6405, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2913, Accuracy: 0.8867, F1 Micro: 0.7015, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2295, Accuracy: 0.8919, F1 Micro: 0.7182, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1943, Accuracy: 0.9, F1 Micro: 0.7645, F1 Macro: 0.7611\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.8998, F1 Micro: 0.7511, F1 Macro: 0.7459\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9019, F1 Micro: 0.7543, F1 Macro: 0.7456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.9019, F1 Micro: 0.7839, F1 Macro: 0.782\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9036, F1 Micro: 0.7789, F1 Macro: 0.7766\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9023, F1 Micro: 0.774, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0395, Accuracy: 0.9041, F1 Micro: 0.7791, F1 Macro: 0.7753\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9019, F1 Micro: 0.7839, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.75      0.69       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 139.80691051483154 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.9039, F1 Micro: 0.7851, F1 Macro: 0.7836\n",
      "Launching training on 2 GPUs.\n",
      "2786\n",
      "BESRA Uncertainty Score Threshold 184.09312957136996\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 137.89449954032898 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.8781, F1 Micro: 0.6847, F1 Macro: 0.6795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.282, Accuracy: 0.8922, F1 Micro: 0.7229, F1 Macro: 0.7062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.9022, F1 Micro: 0.7727, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9036, F1 Micro: 0.7749, F1 Macro: 0.7674\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.8997, F1 Micro: 0.7538, F1 Macro: 0.7308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.9062, F1 Micro: 0.7812, F1 Macro: 0.7783\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9052, F1 Micro: 0.7738, F1 Macro: 0.7725\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9061, F1 Micro: 0.7798, F1 Macro: 0.7777\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.903, F1 Micro: 0.7752, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0363, Accuracy: 0.9031, F1 Micro: 0.7805, F1 Macro: 0.7797\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9062, F1 Micro: 0.7812, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.69      0.65      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 147.1662940979004 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4582, Accuracy: 0.8788, F1 Micro: 0.6804, F1 Macro: 0.6747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2916, Accuracy: 0.8958, F1 Micro: 0.7348, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2322, Accuracy: 0.9033, F1 Micro: 0.7691, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9045, F1 Micro: 0.7761, F1 Macro: 0.7672\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9028, F1 Micro: 0.7639, F1 Macro: 0.7497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.9039, F1 Micro: 0.7787, F1 Macro: 0.7737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9025, F1 Micro: 0.7811, F1 Macro: 0.7794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.9055, F1 Micro: 0.787, F1 Macro: 0.7855\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9023, F1 Micro: 0.7765, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.7711\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9055, F1 Micro: 0.787, F1 Macro: 0.7855\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.77      0.70       248\n",
      "         radikalisme       0.76      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 150.40846371650696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4356, Accuracy: 0.8777, F1 Micro: 0.6869, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2846, Accuracy: 0.8886, F1 Micro: 0.7131, F1 Macro: 0.6895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.8994, F1 Micro: 0.7708, F1 Macro: 0.7656\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.8981, F1 Micro: 0.7564, F1 Macro: 0.7402\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9013, F1 Micro: 0.7615, F1 Macro: 0.7472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9017, F1 Micro: 0.771, F1 Macro: 0.7635\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9034, F1 Micro: 0.7678, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7828\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.8988, F1 Micro: 0.7607, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.0389, Accuracy: 0.9038, F1 Micro: 0.7783, F1 Macro: 0.7745\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9052, F1 Micro: 0.7847, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 146.33575344085693 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.9056, F1 Micro: 0.7843, F1 Macro: 0.7822\n",
      "Launching training on 2 GPUs.\n",
      "2507\n",
      "BESRA Uncertainty Score Threshold 202.29959717760318\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 123.2005877494812 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.435, Accuracy: 0.8772, F1 Micro: 0.7121, F1 Macro: 0.7096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2854, Accuracy: 0.898, F1 Micro: 0.7579, F1 Macro: 0.7495\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.9022, F1 Micro: 0.756, F1 Macro: 0.7437\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9014, F1 Micro: 0.7576, F1 Macro: 0.7483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.9045, F1 Micro: 0.7781, F1 Macro: 0.7743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9038, F1 Micro: 0.7895, F1 Macro: 0.7909\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9053, F1 Micro: 0.7782, F1 Macro: 0.7692\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9056, F1 Micro: 0.7804, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9058, F1 Micro: 0.7787, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9047, F1 Micro: 0.7769, F1 Macro: 0.7734\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9038, F1 Micro: 0.7895, F1 Macro: 0.7909\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.62      0.81      0.70       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 151.97161388397217 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.451, Accuracy: 0.8767, F1 Micro: 0.7022, F1 Macro: 0.6938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.8938, F1 Micro: 0.7442, F1 Macro: 0.7356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.9009, F1 Micro: 0.752, F1 Macro: 0.7387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9028, F1 Micro: 0.7672, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1471, Accuracy: 0.9069, F1 Micro: 0.7844, F1 Macro: 0.779\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9102, F1 Micro: 0.7828, F1 Macro: 0.7782\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.8998, F1 Micro: 0.7823, F1 Macro: 0.7858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9073, F1 Micro: 0.793, F1 Macro: 0.7926\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.905, F1 Micro: 0.7884, F1 Macro: 0.7877\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9042, F1 Micro: 0.7802, F1 Macro: 0.7752\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9073, F1 Micro: 0.793, F1 Macro: 0.7926\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.78      0.72       248\n",
      "         radikalisme       0.71      0.90      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.80      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 154.18956851959229 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.8753, F1 Micro: 0.6919, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2897, Accuracy: 0.8902, F1 Micro: 0.7227, F1 Macro: 0.7098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9008, F1 Micro: 0.7581, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9008, F1 Micro: 0.7661, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9033, F1 Micro: 0.776, F1 Macro: 0.7671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1043, Accuracy: 0.9069, F1 Micro: 0.7856, F1 Macro: 0.7826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9034, F1 Micro: 0.7865, F1 Macro: 0.7833\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9048, F1 Micro: 0.7823, F1 Macro: 0.7824\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9048, F1 Micro: 0.772, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9048, F1 Micro: 0.7801, F1 Macro: 0.78\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9034, F1 Micro: 0.7865, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.93      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.79      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 155.193430185318 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.9048, F1 Micro: 0.7896, F1 Macro: 0.7889\n",
      "Launching training on 2 GPUs.\n",
      "2332\n",
      "BESRA Uncertainty Score Threshold 241.3730054620893\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 113.87369751930237 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4249, Accuracy: 0.878, F1 Micro: 0.6865, F1 Macro: 0.6857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2819, Accuracy: 0.8997, F1 Micro: 0.7647, F1 Macro: 0.7579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2262, Accuracy: 0.9039, F1 Micro: 0.7689, F1 Macro: 0.7689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9064, F1 Micro: 0.7837, F1 Macro: 0.7779\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9061, F1 Micro: 0.7819, F1 Macro: 0.7805\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.9053, F1 Micro: 0.7803, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9075, F1 Micro: 0.7858, F1 Macro: 0.7814\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.9073, F1 Micro: 0.7848, F1 Macro: 0.7794\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9066, F1 Micro: 0.7775, F1 Macro: 0.7766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9042, F1 Micro: 0.7877, F1 Macro: 0.7882\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9042, F1 Micro: 0.7877, F1 Macro: 0.7882\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.64      0.79      0.71       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 161.80968189239502 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4442, Accuracy: 0.8761, F1 Micro: 0.6757, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2904, Accuracy: 0.8977, F1 Micro: 0.758, F1 Macro: 0.7484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2299, Accuracy: 0.9062, F1 Micro: 0.7768, F1 Macro: 0.7755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9109, F1 Micro: 0.793, F1 Macro: 0.7846\n",
      "Epoch 5/10, Train Loss: 0.1449, Accuracy: 0.9034, F1 Micro: 0.779, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9009, F1 Micro: 0.7661, F1 Macro: 0.7537\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.9027, F1 Micro: 0.7575, F1 Macro: 0.7415\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9047, F1 Micro: 0.7785, F1 Macro: 0.77\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9064, F1 Micro: 0.7799, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9066, F1 Micro: 0.784, F1 Macro: 0.7835\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9109, F1 Micro: 0.793, F1 Macro: 0.7846\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.71      0.62      0.66       248\n",
      "         radikalisme       0.75      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.75      0.78      0.76       504\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.79      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 157.63505125045776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.425, Accuracy: 0.8745, F1 Micro: 0.6627, F1 Macro: 0.6592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2878, Accuracy: 0.8972, F1 Micro: 0.7519, F1 Macro: 0.7474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2273, Accuracy: 0.9036, F1 Micro: 0.7644, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9052, F1 Micro: 0.7844, F1 Macro: 0.7762\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.903, F1 Micro: 0.7806, F1 Macro: 0.7775\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9028, F1 Micro: 0.7591, F1 Macro: 0.7486\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9058, F1 Micro: 0.7779, F1 Macro: 0.7709\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9048, F1 Micro: 0.7713, F1 Macro: 0.7627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.905, F1 Micro: 0.7868, F1 Macro: 0.7844\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.8994, F1 Micro: 0.7839, F1 Macro: 0.7858\n",
      "Model 3 - Iteration 4120: Accuracy: 0.905, F1 Micro: 0.7868, F1 Macro: 0.7844\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 159.3838450908661 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.9067, F1 Micro: 0.7892, F1 Macro: 0.7857\n",
      "Launching training on 2 GPUs.\n",
      "2098\n",
      "BESRA Uncertainty Score Threshold 244.39384881133114\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 102.47952032089233 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4172, Accuracy: 0.8773, F1 Micro: 0.6831, F1 Macro: 0.6815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2802, Accuracy: 0.9006, F1 Micro: 0.7611, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.9042, F1 Micro: 0.7818, F1 Macro: 0.7789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9072, F1 Micro: 0.7872, F1 Macro: 0.7887\n",
      "Epoch 5/10, Train Loss: 0.1432, Accuracy: 0.9033, F1 Micro: 0.7727, F1 Macro: 0.7592\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9038, F1 Micro: 0.7863, F1 Macro: 0.786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.907, F1 Micro: 0.7882, F1 Macro: 0.7852\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9053, F1 Micro: 0.7769, F1 Macro: 0.7743\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.9067, F1 Micro: 0.7799, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0371, Accuracy: 0.9056, F1 Micro: 0.7857, F1 Macro: 0.7847\n",
      "Model 1 - Iteration 4330: Accuracy: 0.907, F1 Micro: 0.7882, F1 Macro: 0.7852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.68      0.71      0.70       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 166.08678674697876 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4357, Accuracy: 0.8808, F1 Micro: 0.7071, F1 Macro: 0.7022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2871, Accuracy: 0.9017, F1 Micro: 0.7595, F1 Macro: 0.7461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9006, F1 Micro: 0.7743, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9077, F1 Micro: 0.7864, F1 Macro: 0.7855\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.905, F1 Micro: 0.7738, F1 Macro: 0.7611\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9025, F1 Micro: 0.7815, F1 Macro: 0.7755\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.9022, F1 Micro: 0.781, F1 Macro: 0.7803\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9022, F1 Micro: 0.7807, F1 Macro: 0.7782\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9044, F1 Micro: 0.7725, F1 Macro: 0.7683\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9041, F1 Micro: 0.778, F1 Macro: 0.7751\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9077, F1 Micro: 0.7864, F1 Macro: 0.7855\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.66      0.75      0.70       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.75      0.74      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 164.13673901557922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4183, Accuracy: 0.8766, F1 Micro: 0.6969, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2819, Accuracy: 0.8981, F1 Micro: 0.753, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2251, Accuracy: 0.9023, F1 Micro: 0.7812, F1 Macro: 0.7763\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9042, F1 Micro: 0.772, F1 Macro: 0.769\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9023, F1 Micro: 0.7809, F1 Macro: 0.7745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9033, F1 Micro: 0.7829, F1 Macro: 0.7794\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9058, F1 Micro: 0.7739, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9038, F1 Micro: 0.7795, F1 Macro: 0.775\n",
      "Epoch 9/10, Train Loss: 0.0426, Accuracy: 0.9038, F1 Micro: 0.7681, F1 Macro: 0.7611\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.9033, F1 Micro: 0.7743, F1 Macro: 0.767\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9033, F1 Micro: 0.7829, F1 Macro: 0.7794\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 165.2616639137268 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.906, F1 Micro: 0.7858, F1 Macro: 0.7834\n",
      "Launching training on 2 GPUs.\n",
      "1888\n",
      "BESRA Uncertainty Score Threshold 196.8682900802869\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 93.06513237953186 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.42, Accuracy: 0.8773, F1 Micro: 0.6813, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9005, F1 Micro: 0.7721, F1 Macro: 0.7631\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9003, F1 Micro: 0.768, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9056, F1 Micro: 0.7755, F1 Macro: 0.7669\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9014, F1 Micro: 0.7591, F1 Macro: 0.7537\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9052, F1 Micro: 0.7641, F1 Macro: 0.7505\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9027, F1 Micro: 0.7717, F1 Macro: 0.7633\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9023, F1 Micro: 0.7664, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9039, F1 Micro: 0.7768, F1 Macro: 0.776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0386, Accuracy: 0.9034, F1 Micro: 0.7836, F1 Macro: 0.7821\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9034, F1 Micro: 0.7836, F1 Macro: 0.7821\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.42250609397888 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4359, Accuracy: 0.8763, F1 Micro: 0.6937, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2731, Accuracy: 0.897, F1 Micro: 0.7632, F1 Macro: 0.7534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9019, F1 Micro: 0.7741, F1 Macro: 0.7635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9008, F1 Micro: 0.7621, F1 Macro: 0.7609\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9066, F1 Micro: 0.7759, F1 Macro: 0.7646\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9044, F1 Micro: 0.7735, F1 Macro: 0.7667\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.9053, F1 Micro: 0.7725, F1 Macro: 0.762\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9044, F1 Micro: 0.7743, F1 Macro: 0.7694\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9041, F1 Micro: 0.7741, F1 Macro: 0.7658\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7708\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.71      0.58      0.64       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.78      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 170.3050103187561 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4147, Accuracy: 0.8772, F1 Micro: 0.6843, F1 Macro: 0.6622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.8969, F1 Micro: 0.7674, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.8988, F1 Micro: 0.7702, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.9053, F1 Micro: 0.7724, F1 Macro: 0.7604\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9003, F1 Micro: 0.7619, F1 Macro: 0.759\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.8966, F1 Micro: 0.735, F1 Macro: 0.7145\n",
      "Epoch 7/10, Train Loss: 0.0808, Accuracy: 0.9, F1 Micro: 0.7617, F1 Macro: 0.756\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.903, F1 Micro: 0.7623, F1 Macro: 0.7538\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9039, F1 Micro: 0.7677, F1 Macro: 0.7612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9023, F1 Micro: 0.7808, F1 Macro: 0.7764\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9023, F1 Micro: 0.7808, F1 Macro: 0.7764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.94      0.90       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 171.58404207229614 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.9041, F1 Micro: 0.7812, F1 Macro: 0.7764\n",
      "Launching training on 2 GPUs.\n",
      "1688\n",
      "BESRA Uncertainty Score Threshold 198.18413708752163\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 83.45054221153259 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4093, Accuracy: 0.8836, F1 Micro: 0.7302, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.8963, F1 Micro: 0.7564, F1 Macro: 0.7493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2167, Accuracy: 0.9045, F1 Micro: 0.7846, F1 Macro: 0.7838\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9059, F1 Micro: 0.7806, F1 Macro: 0.7768\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9048, F1 Micro: 0.771, F1 Macro: 0.7646\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.9003, F1 Micro: 0.7771, F1 Macro: 0.7734\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9011, F1 Micro: 0.7843, F1 Macro: 0.7855\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9014, F1 Micro: 0.7814, F1 Macro: 0.7822\n",
      "Epoch 9/10, Train Loss: 0.0393, Accuracy: 0.9045, F1 Micro: 0.7721, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.9014, F1 Micro: 0.7796, F1 Macro: 0.7779\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9045, F1 Micro: 0.7846, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.65      0.75      0.69       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.78      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 172.3519012928009 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4297, Accuracy: 0.8784, F1 Micro: 0.6966, F1 Macro: 0.6811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2782, Accuracy: 0.8986, F1 Micro: 0.7624, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.9047, F1 Micro: 0.7764, F1 Macro: 0.7727\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9058, F1 Micro: 0.7715, F1 Macro: 0.7597\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9041, F1 Micro: 0.7692, F1 Macro: 0.758\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.8977, F1 Micro: 0.7694, F1 Macro: 0.765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.8981, F1 Micro: 0.7796, F1 Macro: 0.78\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9039, F1 Micro: 0.7741, F1 Macro: 0.7729\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.9006, F1 Micro: 0.7737, F1 Macro: 0.7708\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.905, F1 Micro: 0.7711, F1 Macro: 0.7666\n",
      "Model 2 - Iteration 4663: Accuracy: 0.8981, F1 Micro: 0.7796, F1 Macro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.65      0.86      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 173.34050393104553 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4134, Accuracy: 0.8834, F1 Micro: 0.7193, F1 Macro: 0.7141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2743, Accuracy: 0.8964, F1 Micro: 0.7525, F1 Macro: 0.7408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2187, Accuracy: 0.9039, F1 Micro: 0.7738, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.903, F1 Micro: 0.7822, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9047, F1 Micro: 0.7703, F1 Macro: 0.7581\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9014, F1 Micro: 0.777, F1 Macro: 0.7703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9028, F1 Micro: 0.7833, F1 Macro: 0.7791\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9028, F1 Micro: 0.7635, F1 Macro: 0.7595\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9033, F1 Micro: 0.7818, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9031, F1 Micro: 0.7747, F1 Macro: 0.7716\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9028, F1 Micro: 0.7833, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 175.14466786384583 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.9018, F1 Micro: 0.7825, F1 Macro: 0.781\n",
      "Launching training on 2 GPUs.\n",
      "1555\n",
      "BESRA Uncertainty Score Threshold 253.17519504510093\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 77.37817859649658 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4028, Accuracy: 0.8825, F1 Micro: 0.713, F1 Macro: 0.7151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.8977, F1 Micro: 0.7389, F1 Macro: 0.7316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9011, F1 Micro: 0.773, F1 Macro: 0.7734\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9036, F1 Micro: 0.7695, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9064, F1 Micro: 0.7827, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9019, F1 Micro: 0.7588, F1 Macro: 0.7485\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.9017, F1 Micro: 0.7738, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9097, F1 Micro: 0.785, F1 Macro: 0.7792\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9027, F1 Micro: 0.7819, F1 Macro: 0.7818\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9097, F1 Micro: 0.785, F1 Macro: 0.7792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.71      0.64      0.67       248\n",
      "         radikalisme       0.81      0.77      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.80      0.77      0.78      1365\n",
      "           macro avg       0.80      0.76      0.78      1365\n",
      "        weighted avg       0.80      0.77      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 183.917142868042 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4238, Accuracy: 0.8777, F1 Micro: 0.695, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.898, F1 Micro: 0.7347, F1 Macro: 0.7158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.9047, F1 Micro: 0.7689, F1 Macro: 0.7611\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9009, F1 Micro: 0.7661, F1 Macro: 0.7622\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9055, F1 Micro: 0.767, F1 Macro: 0.7529\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9009, F1 Micro: 0.7533, F1 Macro: 0.746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9056, F1 Micro: 0.775, F1 Macro: 0.7698\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.8997, F1 Micro: 0.761, F1 Macro: 0.7549\n",
      "Epoch 9/10, Train Loss: 0.0458, Accuracy: 0.9016, F1 Micro: 0.7608, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9041, F1 Micro: 0.7779, F1 Macro: 0.7686\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9041, F1 Micro: 0.7779, F1 Macro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.69      0.61      0.65       248\n",
      "         radikalisme       0.76      0.77      0.77       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 180.8861448764801 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.406, Accuracy: 0.8797, F1 Micro: 0.7146, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2636, Accuracy: 0.8998, F1 Micro: 0.752, F1 Macro: 0.7458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.902, F1 Micro: 0.7667, F1 Macro: 0.7597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.9031, F1 Micro: 0.7681, F1 Macro: 0.7617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9033, F1 Micro: 0.7809, F1 Macro: 0.775\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9014, F1 Micro: 0.7596, F1 Macro: 0.7516\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9034, F1 Micro: 0.7761, F1 Macro: 0.7706\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9036, F1 Micro: 0.7687, F1 Macro: 0.7628\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.9056, F1 Micro: 0.7796, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.9031, F1 Micro: 0.7792, F1 Macro: 0.7764\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9033, F1 Micro: 0.7809, F1 Macro: 0.775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.8098840713501 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.9057, F1 Micro: 0.7812, F1 Macro: 0.7743\n",
      "Launching training on 2 GPUs.\n",
      "1355\n",
      "BESRA Uncertainty Score Threshold 326.910761277544\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 68.0886640548706 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3942, Accuracy: 0.8891, F1 Micro: 0.7343, F1 Macro: 0.7296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.9031, F1 Micro: 0.7727, F1 Macro: 0.7672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9006, F1 Micro: 0.7792, F1 Macro: 0.776\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9017, F1 Micro: 0.7515, F1 Macro: 0.7416\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9039, F1 Micro: 0.7785, F1 Macro: 0.772\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9047, F1 Micro: 0.7762, F1 Macro: 0.7689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0667, Accuracy: 0.905, F1 Micro: 0.7844, F1 Macro: 0.7833\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9058, F1 Micro: 0.7798, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.0452, Accuracy: 0.902, F1 Micro: 0.7696, F1 Macro: 0.7617\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9056, F1 Micro: 0.781, F1 Macro: 0.7783\n",
      "Model 1 - Iteration 5063: Accuracy: 0.905, F1 Micro: 0.7844, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.79      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 187.9529869556427 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4169, Accuracy: 0.8811, F1 Micro: 0.6888, F1 Macro: 0.6805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9005, F1 Micro: 0.7684, F1 Macro: 0.7629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2164, Accuracy: 0.9047, F1 Micro: 0.7835, F1 Macro: 0.7788\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9033, F1 Micro: 0.7552, F1 Macro: 0.751\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9045, F1 Micro: 0.7823, F1 Macro: 0.7771\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9028, F1 Micro: 0.779, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.0683, Accuracy: 0.9022, F1 Micro: 0.78, F1 Macro: 0.7772\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.9027, F1 Micro: 0.7812, F1 Macro: 0.7785\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.9031, F1 Micro: 0.7831, F1 Macro: 0.7806\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9008, F1 Micro: 0.7765, F1 Macro: 0.7752\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9047, F1 Micro: 0.7835, F1 Macro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 185.01272821426392 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3938, Accuracy: 0.8816, F1 Micro: 0.6865, F1 Macro: 0.6769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2632, Accuracy: 0.8991, F1 Micro: 0.7616, F1 Macro: 0.7545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9006, F1 Micro: 0.7751, F1 Macro: 0.7714\n",
      "Epoch 4/10, Train Loss: 0.1701, Accuracy: 0.8994, F1 Micro: 0.7424, F1 Macro: 0.7344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9052, F1 Micro: 0.7841, F1 Macro: 0.7761\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9034, F1 Micro: 0.7753, F1 Macro: 0.7674\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9014, F1 Micro: 0.7747, F1 Macro: 0.7698\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9055, F1 Micro: 0.7832, F1 Macro: 0.7798\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.902, F1 Micro: 0.766, F1 Macro: 0.7537\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9048, F1 Micro: 0.7653, F1 Macro: 0.7535\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9052, F1 Micro: 0.7841, F1 Macro: 0.7761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 187.1778688430786 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9049, F1 Micro: 0.784, F1 Macro: 0.7794\n",
      "Launching training on 2 GPUs.\n",
      "1155\n",
      "BESRA Uncertainty Score Threshold 155.8331424974831\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 58.19256591796875 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8838, F1 Micro: 0.7107, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9009, F1 Micro: 0.7775, F1 Macro: 0.7728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2113, Accuracy: 0.9034, F1 Micro: 0.7787, F1 Macro: 0.7659\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.9036, F1 Micro: 0.7505, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.9058, F1 Micro: 0.7833, F1 Macro: 0.7814\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9052, F1 Micro: 0.7756, F1 Macro: 0.7775\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.8995, F1 Micro: 0.7788, F1 Macro: 0.7778\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9044, F1 Micro: 0.7748, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9072, F1 Micro: 0.7868, F1 Macro: 0.7813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9062, F1 Micro: 0.7875, F1 Macro: 0.787\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9062, F1 Micro: 0.7875, F1 Macro: 0.787\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       370\n",
      "                sara       0.68      0.72      0.70       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.79      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.46      1365\n",
      "\n",
      "Training completed in 196.52712774276733 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8798, F1 Micro: 0.6855, F1 Macro: 0.6552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2544, Accuracy: 0.8992, F1 Micro: 0.7739, F1 Macro: 0.7682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9055, F1 Micro: 0.784, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.1798, Accuracy: 0.9047, F1 Micro: 0.7593, F1 Macro: 0.7471\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9052, F1 Micro: 0.7797, F1 Macro: 0.7767\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9023, F1 Micro: 0.78, F1 Macro: 0.7816\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9008, F1 Micro: 0.779, F1 Macro: 0.7766\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9033, F1 Micro: 0.783, F1 Macro: 0.7824\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9023, F1 Micro: 0.7748, F1 Macro: 0.7688\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.9027, F1 Micro: 0.7793, F1 Macro: 0.7776\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9055, F1 Micro: 0.784, F1 Macro: 0.7752\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.60      0.64       248\n",
      "         radikalisme       0.72      0.90      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 191.03012585639954 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3912, Accuracy: 0.8794, F1 Micro: 0.6917, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.252, Accuracy: 0.9009, F1 Micro: 0.7729, F1 Macro: 0.7683\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9014, F1 Micro: 0.7723, F1 Macro: 0.7636\n",
      "Epoch 4/10, Train Loss: 0.1717, Accuracy: 0.9013, F1 Micro: 0.7476, F1 Macro: 0.7378\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9027, F1 Micro: 0.7727, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9031, F1 Micro: 0.7771, F1 Macro: 0.7746\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9019, F1 Micro: 0.7744, F1 Macro: 0.7711\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.9031, F1 Micro: 0.7749, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.9041, F1 Micro: 0.7802, F1 Macro: 0.7754\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9016, F1 Micro: 0.7775, F1 Macro: 0.7739\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9041, F1 Micro: 0.7802, F1 Macro: 0.7754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 192.71141529083252 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9053, F1 Micro: 0.7839, F1 Macro: 0.7792\n",
      "Launching training on 2 GPUs.\n",
      "955\n",
      "BESRA Uncertainty Score Threshold 169.9423265988718\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 47.72584366798401 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3839, Accuracy: 0.8889, F1 Micro: 0.7291, F1 Macro: 0.726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.8992, F1 Micro: 0.7687, F1 Macro: 0.7629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2028, Accuracy: 0.9033, F1 Micro: 0.7841, F1 Macro: 0.78\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9086, F1 Micro: 0.7844, F1 Macro: 0.7789\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9055, F1 Micro: 0.7831, F1 Macro: 0.7772\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.9028, F1 Micro: 0.7763, F1 Macro: 0.7732\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9048, F1 Micro: 0.7786, F1 Macro: 0.7735\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.9069, F1 Micro: 0.7831, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9005, F1 Micro: 0.7726, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.8995, F1 Micro: 0.7825, F1 Macro: 0.7845\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9086, F1 Micro: 0.7844, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.73      0.60      0.66       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1365\n",
      "           macro avg       0.79      0.77      0.78      1365\n",
      "        weighted avg       0.79      0.78      0.78      1365\n",
      "         samples avg       0.46      0.44      0.44      1365\n",
      "\n",
      "Training completed in 197.94729042053223 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4017, Accuracy: 0.882, F1 Micro: 0.6972, F1 Macro: 0.6807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.8986, F1 Micro: 0.775, F1 Macro: 0.7747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2055, Accuracy: 0.9027, F1 Micro: 0.7807, F1 Macro: 0.7754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9062, F1 Micro: 0.782, F1 Macro: 0.7774\n",
      "Epoch 5/10, Train Loss: 0.1294, Accuracy: 0.903, F1 Micro: 0.778, F1 Macro: 0.7715\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.903, F1 Micro: 0.78, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.0691, Accuracy: 0.9033, F1 Micro: 0.7688, F1 Macro: 0.7622\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9019, F1 Micro: 0.7669, F1 Macro: 0.7633\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9, F1 Micro: 0.7726, F1 Macro: 0.7655\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.8997, F1 Micro: 0.7701, F1 Macro: 0.7661\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9062, F1 Micro: 0.782, F1 Macro: 0.7774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.65      0.66       248\n",
      "         radikalisme       0.75      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 197.73116850852966 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3846, Accuracy: 0.8863, F1 Micro: 0.7086, F1 Macro: 0.6948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2565, Accuracy: 0.9017, F1 Micro: 0.7746, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.9017, F1 Micro: 0.7775, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9066, F1 Micro: 0.7806, F1 Macro: 0.7748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9055, F1 Micro: 0.7869, F1 Macro: 0.7806\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9033, F1 Micro: 0.7732, F1 Macro: 0.7667\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.898, F1 Micro: 0.7752, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0516, Accuracy: 0.9031, F1 Micro: 0.7692, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.8992, F1 Micro: 0.7769, F1 Macro: 0.7712\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8988, F1 Micro: 0.7725, F1 Macro: 0.767\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9055, F1 Micro: 0.7869, F1 Macro: 0.7806\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.93      0.91       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.6712760925293 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9068, F1 Micro: 0.7844, F1 Macro: 0.779\n",
      "Launching training on 2 GPUs.\n",
      "777\n",
      "BESRA Uncertainty Score Threshold 176.69116614847357\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 39.44583797454834 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.885, F1 Micro: 0.7147, F1 Macro: 0.7106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2559, Accuracy: 0.9017, F1 Micro: 0.7654, F1 Macro: 0.7502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2001, Accuracy: 0.9038, F1 Micro: 0.775, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9069, F1 Micro: 0.785, F1 Macro: 0.7837\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9062, F1 Micro: 0.7845, F1 Macro: 0.7835\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.9042, F1 Micro: 0.7773, F1 Macro: 0.7771\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.8997, F1 Micro: 0.7819, F1 Macro: 0.7841\n",
      "Epoch 8/10, Train Loss: 0.0496, Accuracy: 0.9025, F1 Micro: 0.777, F1 Macro: 0.774\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.9019, F1 Micro: 0.7698, F1 Macro: 0.7677\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.9048, F1 Micro: 0.7799, F1 Macro: 0.7784\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9069, F1 Micro: 0.785, F1 Macro: 0.7837\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 203.60608196258545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3945, Accuracy: 0.8867, F1 Micro: 0.7246, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2574, Accuracy: 0.8998, F1 Micro: 0.7473, F1 Macro: 0.7284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9045, F1 Micro: 0.7858, F1 Macro: 0.7789\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9045, F1 Micro: 0.7828, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.903, F1 Micro: 0.7723, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9058, F1 Micro: 0.7816, F1 Macro: 0.7749\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9028, F1 Micro: 0.7741, F1 Macro: 0.775\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.9028, F1 Micro: 0.7712, F1 Macro: 0.7622\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9044, F1 Micro: 0.7808, F1 Macro: 0.7764\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9031, F1 Micro: 0.7716, F1 Macro: 0.7683\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9045, F1 Micro: 0.7858, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 201.65392661094666 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3804, Accuracy: 0.8878, F1 Micro: 0.7274, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.8978, F1 Micro: 0.7517, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9047, F1 Micro: 0.7838, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1697, Accuracy: 0.9061, F1 Micro: 0.7844, F1 Macro: 0.7778\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9013, F1 Micro: 0.7652, F1 Macro: 0.7501\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9052, F1 Micro: 0.7805, F1 Macro: 0.7759\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.9013, F1 Micro: 0.7715, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9009, F1 Micro: 0.7706, F1 Macro: 0.7618\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9042, F1 Micro: 0.7727, F1 Macro: 0.7638\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9025, F1 Micro: 0.7654, F1 Macro: 0.7591\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9061, F1 Micro: 0.7844, F1 Macro: 0.7778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.73      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 203.7764208316803 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9058, F1 Micro: 0.7851, F1 Macro: 0.7801\n",
      "Launching training on 2 GPUs.\n",
      "577\n",
      "BESRA Uncertainty Score Threshold 210.79898912530808\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.171286582946777 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3705, Accuracy: 0.8855, F1 Micro: 0.7022, F1 Macro: 0.6819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.8939, F1 Micro: 0.7751, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2003, Accuracy: 0.9036, F1 Micro: 0.7773, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.9086, F1 Micro: 0.778, F1 Macro: 0.7704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.8991, F1 Micro: 0.7826, F1 Macro: 0.7834\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9062, F1 Micro: 0.7771, F1 Macro: 0.7681\n",
      "Epoch 7/10, Train Loss: 0.0685, Accuracy: 0.9055, F1 Micro: 0.7812, F1 Macro: 0.7771\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9072, F1 Micro: 0.779, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0364, Accuracy: 0.9005, F1 Micro: 0.7766, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0293, Accuracy: 0.9039, F1 Micro: 0.7773, F1 Macro: 0.7722\n",
      "Model 1 - Iteration 5841: Accuracy: 0.8991, F1 Micro: 0.7826, F1 Macro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.91       370\n",
      "                sara       0.64      0.74      0.68       248\n",
      "         radikalisme       0.76      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.65      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 211.82588124275208 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3882, Accuracy: 0.8809, F1 Micro: 0.6809, F1 Macro: 0.659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.8902, F1 Micro: 0.7721, F1 Macro: 0.7746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.9023, F1 Micro: 0.7748, F1 Macro: 0.7642\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9064, F1 Micro: 0.7739, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.905, F1 Micro: 0.7799, F1 Macro: 0.7718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0944, Accuracy: 0.9022, F1 Micro: 0.7819, F1 Macro: 0.7787\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.9053, F1 Micro: 0.766, F1 Macro: 0.7592\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9039, F1 Micro: 0.7726, F1 Macro: 0.7651\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9028, F1 Micro: 0.7755, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9044, F1 Micro: 0.7828, F1 Macro: 0.7799\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9044, F1 Micro: 0.7828, F1 Macro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.77      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 212.5055365562439 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3734, Accuracy: 0.8784, F1 Micro: 0.6706, F1 Macro: 0.6453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2431, Accuracy: 0.8939, F1 Micro: 0.7761, F1 Macro: 0.7749\n",
      "Epoch 3/10, Train Loss: 0.1993, Accuracy: 0.9011, F1 Micro: 0.7759, F1 Macro: 0.7666\n",
      "Epoch 4/10, Train Loss: 0.1587, Accuracy: 0.9039, F1 Micro: 0.7587, F1 Macro: 0.7405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9033, F1 Micro: 0.7817, F1 Macro: 0.7764\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.898, F1 Micro: 0.769, F1 Macro: 0.7646\n",
      "Epoch 7/10, Train Loss: 0.0664, Accuracy: 0.9041, F1 Micro: 0.7761, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9041, F1 Micro: 0.7733, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7768\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9034, F1 Micro: 0.778, F1 Macro: 0.7717\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9033, F1 Micro: 0.7817, F1 Macro: 0.7764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.64      0.65       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 208.25231909751892 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9022, F1 Micro: 0.7824, F1 Macro: 0.7799\n",
      "Launching training on 2 GPUs.\n",
      "377\n",
      "BESRA Uncertainty Score Threshold 222.72535841974567\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.281638383865356 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.371, Accuracy: 0.8869, F1 Micro: 0.7542, F1 Macro: 0.7509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2463, Accuracy: 0.9008, F1 Micro: 0.7707, F1 Macro: 0.7625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.9027, F1 Micro: 0.779, F1 Macro: 0.7772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1588, Accuracy: 0.9055, F1 Micro: 0.7869, F1 Macro: 0.7845\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9069, F1 Micro: 0.7794, F1 Macro: 0.7751\n",
      "Epoch 6/10, Train Loss: 0.0882, Accuracy: 0.9008, F1 Micro: 0.7788, F1 Macro: 0.7777\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7743\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.9036, F1 Micro: 0.777, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.9011, F1 Micro: 0.7618, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9062, F1 Micro: 0.782, F1 Macro: 0.7774\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9055, F1 Micro: 0.7869, F1 Macro: 0.7845\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.77      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 215.4593677520752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3862, Accuracy: 0.8827, F1 Micro: 0.7507, F1 Macro: 0.7491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9005, F1 Micro: 0.7708, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9, F1 Micro: 0.7743, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9048, F1 Micro: 0.7856, F1 Macro: 0.783\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.8998, F1 Micro: 0.7578, F1 Macro: 0.7482\n",
      "Epoch 6/10, Train Loss: 0.0923, Accuracy: 0.9, F1 Micro: 0.7732, F1 Macro: 0.7712\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9033, F1 Micro: 0.7769, F1 Macro: 0.7719\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9016, F1 Micro: 0.7785, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9023, F1 Micro: 0.7703, F1 Macro: 0.7641\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9002, F1 Micro: 0.7707, F1 Macro: 0.7669\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9048, F1 Micro: 0.7856, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 215.52170777320862 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3717, Accuracy: 0.8848, F1 Micro: 0.7444, F1 Macro: 0.7389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.7656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2039, Accuracy: 0.8981, F1 Micro: 0.7738, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1621, Accuracy: 0.9045, F1 Micro: 0.7828, F1 Macro: 0.7765\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9009, F1 Micro: 0.7562, F1 Macro: 0.7459\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9016, F1 Micro: 0.7786, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9003, F1 Micro: 0.7596, F1 Macro: 0.751\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.9006, F1 Micro: 0.7689, F1 Macro: 0.7637\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.8994, F1 Micro: 0.7457, F1 Macro: 0.7362\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.903, F1 Micro: 0.7706, F1 Macro: 0.7631\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9045, F1 Micro: 0.7828, F1 Macro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.67      0.64      0.65       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 215.12582063674927 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9049, F1 Micro: 0.7851, F1 Macro: 0.7814\n",
      "Launching training on 2 GPUs.\n",
      "177\n",
      "BESRA Uncertainty Score Threshold 86.71106869585996\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 7.57703423500061 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3624, Accuracy: 0.8856, F1 Micro: 0.6911, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2317, Accuracy: 0.8991, F1 Micro: 0.7654, F1 Macro: 0.757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9059, F1 Micro: 0.767, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9067, F1 Micro: 0.7906, F1 Macro: 0.7877\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9005, F1 Micro: 0.7804, F1 Macro: 0.7782\n",
      "Epoch 6/10, Train Loss: 0.0852, Accuracy: 0.9011, F1 Micro: 0.7781, F1 Macro: 0.7763\n",
      "Epoch 7/10, Train Loss: 0.0659, Accuracy: 0.8989, F1 Micro: 0.7765, F1 Macro: 0.7723\n",
      "Epoch 8/10, Train Loss: 0.043, Accuracy: 0.9058, F1 Micro: 0.7734, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.035, Accuracy: 0.9058, F1 Micro: 0.7808, F1 Macro: 0.7782\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9, F1 Micro: 0.7848, F1 Macro: 0.7872\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9067, F1 Micro: 0.7906, F1 Macro: 0.7877\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 221.13651156425476 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3798, Accuracy: 0.8859, F1 Micro: 0.7054, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2368, Accuracy: 0.8986, F1 Micro: 0.7661, F1 Macro: 0.755\n",
      "Epoch 3/10, Train Loss: 0.1893, Accuracy: 0.8998, F1 Micro: 0.7558, F1 Macro: 0.7514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1509, Accuracy: 0.9053, F1 Micro: 0.789, F1 Macro: 0.7871\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9052, F1 Micro: 0.7827, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.083, Accuracy: 0.9003, F1 Micro: 0.7809, F1 Macro: 0.7767\n",
      "Epoch 7/10, Train Loss: 0.0659, Accuracy: 0.9052, F1 Micro: 0.7784, F1 Macro: 0.7709\n",
      "Epoch 8/10, Train Loss: 0.0463, Accuracy: 0.9011, F1 Micro: 0.757, F1 Macro: 0.748\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.9036, F1 Micro: 0.7785, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9033, F1 Micro: 0.771, F1 Macro: 0.7655\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9053, F1 Micro: 0.789, F1 Macro: 0.7871\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 219.20706272125244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3648, Accuracy: 0.8848, F1 Micro: 0.6902, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2335, Accuracy: 0.8963, F1 Micro: 0.7612, F1 Macro: 0.7519\n",
      "Epoch 3/10, Train Loss: 0.1887, Accuracy: 0.9005, F1 Micro: 0.7588, F1 Macro: 0.7514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9031, F1 Micro: 0.7825, F1 Macro: 0.7785\n",
      "Epoch 5/10, Train Loss: 0.1121, Accuracy: 0.8966, F1 Micro: 0.7771, F1 Macro: 0.7755\n",
      "Epoch 6/10, Train Loss: 0.0861, Accuracy: 0.8991, F1 Micro: 0.7798, F1 Macro: 0.7758\n",
      "Epoch 7/10, Train Loss: 0.0609, Accuracy: 0.8992, F1 Micro: 0.7758, F1 Macro: 0.775\n",
      "Epoch 8/10, Train Loss: 0.0449, Accuracy: 0.9034, F1 Micro: 0.7753, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9023, F1 Micro: 0.7761, F1 Macro: 0.7743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.902, F1 Micro: 0.7836, F1 Macro: 0.7833\n",
      "Model 3 - Iteration 6218: Accuracy: 0.902, F1 Micro: 0.7836, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.62      0.76      0.68       248\n",
      "         radikalisme       0.72      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 219.9605188369751 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9047, F1 Micro: 0.7877, F1 Macro: 0.786\n",
      "Total sampling time: 2479.48 seconds\n",
      "Total runtime: 13094.34163594246 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACCoAAAHqCAYAAADInz0eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3QUVR/G8e+mh5LQE4HQQi+CtNCLIh2liKBgAAErRaI06SqgqIAgiiK9ixRpAkrvIEUE6b2FDoH0ZPf9YyCYN0ESSDLJ5vmcsyc7d2dnfjcJcNl55l6LzWazISIiIiIiIiIiIiIiIiIiIpICHMwuQERERERERERERERERERERNIPBRVEREREREREREREREREREQkxSioICIiIiIiIiIiIiIiIiIiIilGQQURERERERERERERERERERFJMQoqiIiIiIiIiIiIiIiIiIiISIpRUEFERERERERERERERERERERSjIIKIiIiIiIiIiIiIiIiIiIikmIUVBAREREREREREREREREREZEUo6CCiIiIiIiIiIiIiIiIiIiIpBgFFUREREREREQkzenYsSMFChQwuwwREREREREReQIKKoiIJJPvvvsOi8WCn5+f2aWIiIiIiCTatGnTsFgs8T769esXs9+aNWvo3LkzpUuXxtHRMdHhgQfH7NKlS7yvDxgwIGaf69evP02XRERERMTOaQwrIpJ2OJldgIiIvZo9ezYFChRg165dnDhxgsKFC5tdkoiIiIhIon3yyScULFgwVlvp0qVjns+ZM4f58+dTvnx5cufO/UTncHNzY+HChXz33Xe4uLjEem3u3Lm4ubkRFhYWq33SpElYrdYnOp+IiIiI2LfUOoYVEZGHNKOCiEgyOH36NNu2bWP06NHkzJmT2bNnm11SvIKDg80uQURERERSuUaNGtG+fftYj3LlysW8PmLECIKCgti6dStly5Z9onM0bNiQoKAgfvvtt1jt27Zt4/Tp0zRp0iTOe5ydnXF1dX2i8/2b1WrVB8giIiIidia1jmGTmz7vFZG0REEFEZFkMHv2bLJmzUqTJk145ZVX4g0q3L59m169elGgQAFcXV3Jmzcv/v7+saYCCwsLY+jQoRQtWhQ3NzeeeeYZWrZsycmTJwHYsGEDFouFDRs2xDr2mTNnsFgsTJs2LaatY8eOZMqUiZMnT9K4cWMyZ85Mu3btANi8eTOtW7cmX758uLq64uPjQ69evQgNDY1T95EjR3j11VfJmTMn7u7uFCtWjAEDBgCwfv16LBYLixcvjvO+OXPmYLFY2L59e6K/nyIiIiKSeuXOnRtnZ+enOkaePHmoVasWc+bMidU+e/ZsypQpE+vutwc6duwYZ4peq9XKN998Q5kyZXBzcyNnzpw0bNiQP//8M2Yfi8VCt27dmD17NqVKlcLV1ZVVq1YBsG/fPho1aoSHhweZMmXihRdeYMeOHU/VNxERERFJfcwawybV57AAQ4cOxWKx8M8///D666+TNWtWatSoAUBUVBSffvopvr6+uLq6UqBAAT7++GPCw8Ofqs8iIklJSz+IiCSD2bNn07JlS1xcXHjttdf4/vvv2b17N5UqVQLg3r171KxZk8OHD/Pmm29Svnx5rl+/ztKlS7lw4QI5cuQgOjqapk2bsnbtWtq2bUvPnj25e/cuv//+OwcPHsTX1zfRdUVFRdGgQQNq1KjBV199RYYMGQBYsGABISEhvPvuu2TPnp1du3Yxfvx4Lly4wIIFC2Lef+DAAWrWrImzszNvvfUWBQoU4OTJkyxbtozhw4dTp04dfHx8mD17Ni1atIjzPfH19aVq1apP8Z0VERERkZR2586dOOvq5siRI8nP8/rrr9OzZ0/u3btHpkyZiIqKYsGCBQQEBCR4xoPOnTszbdo0GjVqRJcuXYiKimLz5s3s2LGDihUrxuy3bt06fv75Z7p160aOHDkoUKAAhw4dombNmnh4eNCnTx+cnZ354YcfqFOnDhs3bsTPzy/J+ywiIiIiySO1jmGT6nPYf2vdujVFihRhxIgR2Gw2ALp06cL06dN55ZVX+PDDD9m5cycjR47k8OHD8d5kJiJiBgUVRESS2J49ezhy5Ajjx48HoEaNGuTNm5fZs2fHBBW+/PJLDh48yKJFi2Jd0B84cGDMYHLGjBmsXbuW0aNH06tXr5h9+vXrF7NPYoWHh9O6dWtGjhwZq/2LL77A3d09Zvutt96icOHCfPzxx5w7d458+fIB0L17d2w2G3v37o1pA/j8888B4+609u3bM3r0aO7cuYOnpycA165dY82aNbESvyIiIiKSNtSrVy9O25OOR//LK6+8Qrdu3ViyZAnt27dnzZo1XL9+nddee42pU6c+9v3r169n2rRp9OjRg2+++Sam/cMPP4xT79GjR/n7778pWbJkTFuLFi2IjIxky5YtFCpUCAB/f3+KFStGnz592LhxYxL1VERERESSW2odwybV57D/VrZs2VizOvz1119Mnz6dLl26MGnSJADee+89cuXKxVdffcX69eupW7dukn0PRESelJZ+EBFJYrNnz8bLyytmsGexWGjTpg3z5s0jOjoagIULF1K2bNk4sw482P/BPjly5KB79+6P3OdJvPvuu3Ha/j04Dg4O5vr161SrVg2bzca+ffsAI2ywadMm3nzzzViD4/+vx9/fn/DwcH755ZeYtvnz5xMVFUX79u2fuG4RERERMceECRP4/fffYz2SQ9asWWnYsCFz584FjKXDqlWrRv78+RP0/oULF2KxWBgyZEic1/5//Fy7du1YIYXo6GjWrFlD8+bNY0IKAM888wyvv/46W7ZsISgo6Em6JSIiIiImSK1j2KT8HPaBd955J9b2ypUrAQgICIjV/uGHHwKwYsWKxHRRRCTZaEYFEZEkFB0dzbx586hbty6nT5+Oaffz8+Prr79m7dq11K9fn5MnT9KqVav/PNbJkycpVqwYTk5J91e1k5MTefPmjdN+7tw5Bg8ezNKlS7l161as1+7cuQPAqVOnAOJdW+3fihcvTqVKlZg9ezadO3cGjPBGlSpVKFy4cFJ0Q0RERERSUOXKlWMtm5CcXn/9dd544w3OnTvHkiVLGDVqVILfe/LkSXLnzk22bNkeu2/BggVjbV+7do2QkBCKFSsWZ98SJUpgtVo5f/48pUqVSnA9IiIiImKe1DqGTcrPYR/4/7Ht2bNncXBwiPNZrLe3N1myZOHs2bMJOq6ISHJTUEFEJAmtW7eOy5cvM2/ePObNmxfn9dmzZ1O/fv0kO9+jZlZ4MHPD/3N1dcXBwSHOvi+++CI3b96kb9++FC9enIwZM3Lx4kU6duyI1WpNdF3+/v707NmTCxcuEB4ezo4dO/j2228TfRwRERERSV9eeuklXF1d6dChA+Hh4bz66qvJcp5/38kmIiIiIvI0EjqGTY7PYeHRY9unmZVXRCQlKKggIpKEZs+eTa5cuZgwYUKc1xYtWsTixYuZOHEivr6+HDx48D+P5evry86dO4mMjMTZ2TnefbJmzQrA7du3Y7UnJhX7999/c+zYMaZPn46/v39M+/9Ph/ZgCtzH1Q3Qtm1bAgICmDt3LqGhoTg7O9OmTZsE1yQiIiIi6ZO7uzvNmzdn1qxZNGrUiBw5ciT4vb6+vqxevZqbN28maFaFf8uZMycZMmTg6NGjcV47cuQIDg4O+Pj4JOqYIiIiIpI+JHQMmxyfw8Ynf/78WK1Wjh8/TokSJWLar1y5wu3btxO8tJqISHJzePwuIiKSEKGhoSxatIimTZvyyiuvxHl069aNu3fvsnTpUlq1asVff/3F4sWL4xzHZrMB0KpVK65fvx7vTAQP9smfPz+Ojo5s2rQp1uvfffddgut2dHSMdcwHz7/55ptY++XMmZNatWoxZcoUzp07F289D+TIkYNGjRoxa9YsZs+eTcOGDRP1IbOIiIiIpF8fffQRQ4YMYdCgQYl6X6tWrbDZbAwbNizOa/8/Xv1/jo6O1K9fn19//ZUzZ87EtF+5coU5c+ZQo0YNPDw8ElWPiIiIiKQfCRnDJsfnsPFp3LgxAGPHjo3VPnr0aACaNGny2GOIiKQEzaggIpJEli5dyt27d3nppZfifb1KlSrkzJmT2bNnM2fOHH755Rdat27Nm2++SYUKFbh58yZLly5l4sSJlC1bFn9/f2bMmEFAQAC7du2iZs2aBAcH88cff/Dee+/x8ssv4+npSevWrRk/fjwWiwVfX1+WL1/O1atXE1x38eLF8fX15aOPPuLixYt4eHiwcOHCOGukAYwbN44aNWpQvnx53nrrLQoWLMiZM2dYsWIF+/fvj7Wvv78/r7zyCgCffvppwr+RIiIiIpKmHDhwgKVLlwJw4sQJ7ty5w2effQZA2bJladasWaKOV7ZsWcqWLZvoOurWrcsbb7zBuHHjOH78OA0bNsRqtbJ582bq1q1Lt27d/vP9n332Gb///js1atTgvffew8nJiR9++IHw8PD/XGdYRERERNIeM8awyfU5bHy1dOjQgR9//JHbt29Tu3Ztdu3axfTp02nevDl169ZNVN9ERJKLggoiIklk9uzZuLm58eKLL8b7uoODA02aNGH27NmEh4ezefNmhgwZwuLFi5k+fTq5cuXihRdeIG/evICRsF25ciXDhw9nzpw5LFy4kOzZs1OjRg3KlCkTc9zx48cTGRnJxIkTcXV15dVXX+XLL7+kdOnSCarb2dmZZcuW0aNHD0aOHImbmxstWrSgW7ducQbXZcuWZceOHQwaNIjvv/+esLAw8ufPH++6a82aNSNr1qxYrdZHhjdEREREJO3bu3dvnDvHHmx36NAh0R/yPo2pU6fy7LPPMnnyZHr37o2npycVK1akWrVqj31vqVKl2Lx5M/3792fkyJFYrVb8/PyYNWsWfn5+KVC9iIiIiKQUM8awyfU5bHx++uknChUqxLRp01i8eDHe3t7079+fIUOGJHm/RESelMWWkHliREREEikqKorcuXPTrFkzJk+ebHY5IiIiIiIiIiIiIiIikko4mF2AiIjYpyVLlnDt2jX8/f3NLkVERERERERERERERERSEc2oICIiSWrnzp0cOHCATz/9lBw5crB3716zSxIREREREREREREREZFURDMqiIhIkvr+++959913yZUrFzNmzDC7HBEREREREREREREREUllNKOCiIiIiIiIiIiIiIiIiIiIpBjNqCAiIiIiIiIiIiIiIiIiIiIpRkEFERERERERERERERERERERSTFOZheQUqxWK5cuXSJz5sxYLBazyxERERGRZGCz2bh79y65c+fGwcG+Mrkaz4qIiIjYP3sez4LGtCIiIiL2LjHj2XQTVLh06RI+Pj5mlyEiIiIiKeD8+fPkzZvX7DKSlMazIiIiIumHPY5nQWNaERERkfQiIePZdBNUyJw5M2B8Uzw8PEyuRkRERESSQ1BQED4+PjFjP3ui8ayIiIiI/bPn8SxoTCsiIiJi7xIznk03QYUHU4l5eHhoECwiIiJi5+xxGlmNZ0VERETSD3scz4LGtCIiIiLpRULGs/a30JmIiIiIiIiIiIiIiIiIiIikWgoqiIiIiIiIiIiIiIiIiIiISIpRUEFERERERERERERERERERERSjIIKIiIiIiIiIiIiIiIiIiIikmIUVBAREREREREREREREREREZEUo6CCiIiIiIiIiIiIiIiIiIiIpBgFFURERERERERERERERERERCTFKKggIiIiIiIiIiIiIiIiIiIiKUZBBREREREREREREREREREREUkxCiqIiIiIiIiIiIiIiIiIiIhIilFQQURERERERERERERERERERFKMggoiIiIiIiIiIiIiIiIiIiKSYhRUEBERERERERERERERERERkRSjoIKIiIiIiIiIiIiIiIiIiIikGAUVRERERB7DZoMdO+DyZbMrEREREbtjjYYrGyBwLdzcB8FnIfKuMQAREREREUnlbDYbm89u5kbIDbNLEZE0xsnsAkRERFKrvXth2DAoXx46dIACBcyuSMxgtcJHH8GYMeDiAm+/Df36Qe7cZlcmIiIiaV50BGx5BS4ui/uaxQlcs4HLvx6uj3numg2cPcGi+1JEREREJGX0/r03X2//GncndzqV60Svqr0onK2w2WWJSBpgsdnSR0Q/KCgIT09P7ty5g4eHh9nliIhIKrdyJbz6KgQHP2x7/nno1AlatoQMGcyrTVJOZCR06QIzZsRud3N7GFjw9janNomfPY/57LlvIiLpkjUKtraF8wvBwRUyF4aImxB+E6zhT3FgC7hk/VeAITv4tIBCncBB96uIpHb2Puaz9/6JiKQ3sw/Mpv3i9rHaLFhoXrw5H1X7iGo+1UyqTETMkpjxnoIKIiIi/+fHH+G99yA6GmrXBicnWLfu4ey7mTNDmzZGaKFqVbBYzK1XkkdoqPFzXrYMHB1hyhTIkweGDIGtW4193NyM35U+fcDLy9x6xWDPYz577puISLpjjYbt7eHsPHBwgVq/Qu6Gxms2G0SHPgwtRNxM+POo4Eef06MElPsC8jTVAFYkFbP3MZ+9909EJD3Zc2kPNabWICwqjI9rfEy9QvX4avtXrDy+MmafKnmr8GHVD2lRvAWODo4mVisiKUVBhXhoECwiIo9jtcLAgTBypLHdqRP88AM4O8PZs8Zd9dOmwalTD99TrBh07AhvvGFcxBb7cOcONGsGmzcbYYSffza2wbh28McfRmBh+3ajzd0dunWD3r0hZ07z6hb7HvPZc99ERNIVmxV2dILTM4zlHWougrzNkubY0eEQcSt2gOHOP3DkKwi/v2ZwrlpQ7kvIUTlpzikiScrex3z23j8RkfTiavBVKv5YkfNB52lSpAm/tv01Jojwz7V/GL19NDMPzCQiOgKAQlkL8YHfB3R6rhOZXDKZWbqIJDMFFeKhQbCIiPyX8HB4802YM8fYHjoUBg+Oe7OZ1WpcvJ46FRYsgJAQo93BAerXN8INL71kXNyWtCkwEBo2hL/+Ak9PY0aFmjXj7mezwerVRmBh1y6jLWNGI7Dw0UeQI0fK1i0Gex7z2XPfRETSDZsVdr0NJ38CiyPU+Bl8Wib/eSNuwz9fwNGxEB1mtOV7FcqOgMy+yX9+EUkwex/z2Xv/RETSg8joSOrNrMems5somr0ou7rswtPNM85+gfcCmbBrAt/9+R03Q28CkNUtK+9UfIfulbvzTOZnUrp0EUkBCirEQ4NgERF5lFu3oGVL2LDBWOZh0iRjloTHuXvXCCtMnQpbtjxsz5oVXn/dCC2UL6+ZddOSU6eMwMnJk8ZSDqtXQ9my//0emw1WrjQCC3v2GG2ZMkGPHvDhh5AtW/LXLQ/Z85jPnvsmIpIu2GzwZzc4/h1YHKDqbCjQNmVrCD4Pfw+GU9MBGzg4Q+F3ofQgcFPKUiRe1kjjz0oKsfcxn733T0QkPei2shsTdk8gs0tmdnXdRfEcxf9z/5DIEKbvn87oHaM5cfMEAM4OzrR7th0fVv2Q0rlKp0TZIpJCFFSIhwbBIiISn7NnoXFj+OcfyJwZFi6EF19M/HGOHzeWhZgxAy5ceNhepowRemjfHnLlSqqqJTkcOAANGhgzKhQsCL//Dr6JuMHQZoPly43Awr59RlvmzNCzJwQEGAEWSX72POaz576JiNg9mw32fghHxwAWqDINCvmbV8+tA7C/L1xeZWw7e0DJflCsJzhlMK8ukdTkzhE4+CmEXoQX1qdYAt3ex3z23j8REXs3ee9kuizrAsDStktpVizhS5hFW6NZdmwZX237iq3nt8a0N/BtwIdVP6ReoXpYdMeXSJqnoEI8NAgWEZH/t3cvNGliXJjOk8e4K/7ZZ5/umNHRsHatMcvC4sXGkhJgzNTQpIkxy0LjxuCccjfkSAJs3QpNm8Lt20a4ZPVqeOYJZ5+z2eDXX43lQ/76y2jz8IBeveCDDyBLlqSpWeJnz2M+e+6biIhds9ngr/7G0gsAfj+Bb2dza3og8A/Y1wdu3U9ZuueBZz+Fgv5wf41hkXQn6KgRUDg711iuBaDRfsj6mKnWkur0dj7ms/f+iYjYs+3nt1Nneh0ioiP4pM4nDKo96ImPtePCDr7e/jWLDi/Cev/f22e9nuXDqh/StnRbXBxdkqhqEUlpCirEQ4NgERH5t99+g9atITjYuDC9ciXkzZu057h1C+bNM0ILu3c/bPfygpkzn2zmBkl6K1YYvwuhoVC9OixbljSzH1itsGSJMcPCwYNGW5YsxuwKPXsa4QVJevY85rPnvomI2LUDQ+DgJ8bzihOg6Hvm1vP/bFY4Mxf++hhCzhltWcpAuVHwTAOtYybpR9BROPgZnJ3zMKCQ92UoPQSyPZdyZdj5mM/e+yciYq8u3b1ExR8rcvneZVqWaMmC1gtwsDg89XFP3TrFNzu+YfK+yQRHBgOQO3NuelTuwdsV3yaLW5anPodIema1WQkKD0rRP0sKKsRDg2AREXlg0iR4911j9oN69eCXX8DTM3nPeeiQsTTEzJlw5Qq4uRkXxOvVS97zyn+bNctYmiM62pjpYsECyJDEsx1brcaSIkOHGkuMgBGE+PBDI7CQKVPSni+9s+cxnz33TUTEbh0aAX8NMJ6XHwPFPzC1nP8UHQbHJhgXaiNvG21eL8BzoyBbeVNLE0lWQcfuz6Dwr4BCnpegzBBTfvftfcxn7/0TEbFH4VHh1J5Wm50Xd1IqZym2d95OZtfMSXqOW6G3+GHPD4zbOY7L9y4DkNE5I13Kd+GDKh9QIEuBJD2fSHoQGhmK/xJ/zt4+y4aOG8jgnDLL/CVmvPf0cScREZE0wmaDAQPgrbeMC9MdOhh30yd3SAGgVCn48ks4exZeegnCwqBZM2OZCDHHuHHwxhvG70L79sbsB0kdUgBwcDBmbDhwAObOheLFjdk2Bg6ERo2M30sRERGxQ4e/ehhSKPdF6g4pADi6QYkP4aWTUPxDcHCBK2thVQXY1h7unTG7QpGkFXQctvnDihJwZpYRUsjzEjT8E2r/qoCOiIgIYLPZeG/Fe+y8uJMsbln4te2vSR5SAMjqnpV+Nfpx5oMzTHt5GqVzlSY4Mphvdn6D7zhfXp73MosOLyI8KjzJzy1ij64GX6Xu9Lr88s8v7A/cz84LO80uKV4KKoiISLoQEWFclB4xwtgePNhYksElhZc7c3WFn3+GJk0ehhXWr0/ZGpJaZKQxS8XGjWnjorvNZvz8e/Y0tnv2hOnTwdk5ec/r6Aht2xrLQMyeDRkzwpYtRlhGRERE7MzRcbCvt/H82U+hZB9z60kM12xQ/itoehQKtDPazsyG5cVg70cQftPc+kSeVtBx2N4BVhSHMzPvBxSa/SugUMHsCkVERFKN73Z/x5T9U3CwODCv1Tx8s/km6/lcHF3oUK4DB945wOr2q3mx0ItYbVaWHl1Kq59bkXt0bt5f8T47L+wknUwYL5Joh68dxu8nP3Ze3ElWt6z84f8HdQvWNbuseCmoICIipli4EIoUgVat4M8/k/dct29Dw4bGxWEnJ5g8GYYNM2+5XVdXo/+NG0NoKDRtalzkT6u+/NKYpaJOHWPmiHHjjO95ahQdDe+9B59+amx/9hmMGWPMepBSHB3h9dfh/feN7aFD00bAQ0RERBLo+A+w534istRAKD3Q3HqeVKYCUG0WNNwDXs+DNQKOfA1LfY3ZIqIjzK5QJHHunoDtHY0ZFE7PMAIKuZtCg91Qe6kCCiIiIv9n45mNfLD6AwA+f+FzGhRukGLntlgs1Petz5o31nDovUP0rd6X3JlzczP0Jt/9+R1VJleh+ITiDN80nHN3zqVYXSKp3brT66g6uSpnbp/BN6sv2ztvp1b+WmaX9UgWWzqJHGn9MxGR1OHePeMO9ilTYre/+CJ8/DHUrp20AYJz54zp9f/5BzJlMgIC9esn3fGfRlgYtGgBq1YZd9f/9hvUrGl2VYkTHAz588ONG8aMBJGRRru7O7z2Grz7LlSsaG6ND4SHg7+/MaOFxQLffQfvvGNePdeuQYECEBICy5cbs2zI07PnMZ89901ExG6cnAo73zSel+htLPlgVjo2KdlscHk17O8Dt/822rL7Qc1fIENec2sTeZy7J+HQZ3B6JtiijbbcTaDMUMieSv6z8i/2Puaz9/6JiNiLc3fOUfHHilwLucZrpV9jdsvZWEwe10Zbo1l3eh0zDsxg0eFFhESGxLxWt0Bd/Mv606pEq2RZmkIkLZi6bypvLX+LKGsU1X2qs6TtEnJkyJHidSRmvKcZFUREJMXs3AnlyhkhBYsFPvzQuHDs6Ai//w5160L16sZF26SI0e3bB1WqGCGF3Llh8+bUE1IAcHODxYuNkEZwsBGo2LrV7KoS58cfjZCCry9cuQITJkDp0sZMEVOmQKVKxmPyZKOPZrl3z1hm4+efjUDFvHnmhhQAcuY0ZncAY4aP9BEdFRERsWOnZ8HOzsbzYj3tJ6QARj9yN4SG+8BvCrhkhRs7YVUFuLLB7OpE4nf3JOx401i25NQ0I6SQuwk02AV1lqfKkIKIiEhqEBIZQvN5zbkWco3nvJ/jp5d+Mj2kAODo4MiLvi8ys8VMAj8MZOrLU6lbwJjOfv2Z9XT6tRPeX3vzxuI3+P3k70Rbo02uWCRlWG1WBq4byJtL3yTKGkXb0m35w/8PU0IKifVEQYUJEyZQoEAB3Nzc8PPzY9euXY/cNzIykk8++QRfX1/c3NwoW7Ysq1atSvQx69Spg8ViifV4x+wrDCIikiBRUfDJJ0YI4eRJ8PGB9evhq69g+nQ4ccK4YOvqCtu3GxeUy5WDuXON9z6J336DWrXg8mXjwvmOHcYxUxs3N/j1V6hXz7iQ37Ch8T1IC8LCjGUfAPr1g6xZjZ/jgQOwZQu0bw8uLsbSHl26QJ480KOHERxJSTduwAsvGGGYjBlhxQp49dWUreFRPvrImH1i925jZg0RERF5SuE3YE+AMb37/v5w5Bs4+zNc3WSsSx95N3nSgWd/hh0dABsUeRfKj7GfkMK/OTiCbydo+CdkKQthV2FdPTgyRqlLe2aNhOBzcG2b8bt+eLTx52xLG/hrEIReMbvC2G4fgu0d7gcUpt4PKDSG+jvvBxQqmV2hiIhIqmWz2ei6rCv7AveRI0MOFrdZTAbnDGaXFUdm18x0LNeRdR3WcabnGT6r+xlFsxclJDKEWQdmUX9WffKPzU+/P/rxz7UU/jBSJAWFRYXRblE7hm8eDsCAmgOY3XI2bk5uJleWMIle+mH+/Pn4+/szceJE/Pz8GDt2LAsWLODo0aPkypUrzv59+/Zl1qxZTJo0ieLFi7N69WoCAgLYtm0bzz33XIKPWadOHYoWLconn3wSc+wMGTIkeIowTSsmImKO06eNC9bbthnbbdvC999Dlixx9w0MhDFjjNfv3jXafH2hb19j5gVX14Sd86efjLvlo6ONC9QLF4KnZ5J0J9mEhBgBjXXrIHNmWLPGmA0iNZs40VjaIW9eI4Di4hJ3n+vXYepU+OEHY58Hatc2fkYtW8b/vidhs8HVq3Do0MPHwYPw998QFATZssHKleDnlzTnSyoffgijRxt1bd9un9c0UpI9j/nsuW8iIknizmHY2Azunfzv/RwzgPsz4O4Nbve/uj8Dbv/+6g2uuYwL849zfglsecW4GOrbGSr/CJZ0MIFlVAjsehvOzDK287WBKpPBKaO5dUniREdA6CUIuQAh5+9/vQChFx62hQYC//HxoaMbFOoMJXtDxvwpVnocV7fAP1/ApeUP255pBGWGQI5U9p+A/2DvYz5775+ISFKw2WzYsOFgwpjy621f89HvH+FoceQP/z+oU6BOitfwpGw2G7su7mL6X9OZd3Aet8JuxbxW4ZkKdCjbgbal25IzY04TqxRJOtdDrvPyvJfZdn4bTg5O/Nj0Rzo918nsshI13kt0UMHPz49KlSrx7bffAmC1WvHx8aF79+7069cvzv65c+dmwIABvP/++zFtrVq1wt3dnVmzZiX4mHXq1KFcuXKMHTs2MeXG0CBYRCRl2Wwwaxa8/74ROsicGb77Dtq1e/yF2Fu3jCUExo417oYHY+mGjz6Crl0hU6ZHn3PwYPjsM2Pb3x8mTUq6C+HJLTgYmjaFDRvAw8OYAaByZbOril9kJBQtCmfOwDffGDMl/BerFf74wwihLFtmhEgAcuWCN9+Et96CggUTfv5r12IHEh48Hvy+/L8CBYyQQokSCT9HSgkMNPoeFmbMqtCggdkVpW32POaz576JiDy1S6tgaxuIDIKMBaBwV+Nu/9DLEBZofA0NhKi7CT+mxQFcc8YTYngQcvA2LuTu6GDccV7gDagyNWHhBnths8GxCbC3F9iiwLM01FwEHkXMrkys0RB+7f7vf+DDr6EXY4cSwhI4G4KDM7jnhQz/erjlgnO/GMuAAFicoMDrULIfeKbQwNtmhYvLjYDC9fvpeCzg0xJK9k2TsyfY+5jP3vsnIvK0Np/djP8Sf66HXKdUzlKUyVWGMl5lKJOrDKVzlU7Wi+y/n/ydhrMbYrVZGd9oPN0qd0u2cyW38KhwVhxfwYy/ZrDi+AqirMa0vU4OTjQp0gT/sv68XOxlHNPT2F3syrEbx2g8uzEnb53E09WTRW0W8XzB580uC0jGoEJERAQZMmTgl19+oXnz5jHtHTp04Pbt2/z6669x3pM9e3ZGjRpF586dY9rat2/Pli1bOHPmTIKPWadOHQ4dOoTNZsPb25tmzZoxaNAgMmRI2JQzGgSLiKSc27eNO+3nzTO2q1eHmTMTdyEajAv3kyYZS0RcvGi0ZcsGPXtCt27G8wciIozlBWbONLYHDYJhw9Le3enBwdC4MWzaZMwC8ccfUDEVLp06YwZ06GAEDU6fhgT+cwzAhQvGrBeTJsGlS0abxWIse/Huu0b/He//H+HmzfgDCVevxn9si8WYhaNUKWPJj1KljEeJEuDs/HR9Tk69ehnBnKpVYevWtPd7m5rY85jPnvsmIvLEbDY4Og72BRgXLHPWhJoLwe0RH+BGBd+/WPuvAMO/gwxh97+GXzWOl1D52kC1WeDglDT9SmuuboEtrY3vpbOn8b3I09TsqsxjjYbIO0bYxeIEFsd/PRyefLBns0Hk7djBg/i+hgVC+PWE/w47uP4rgOATO4yQIa8RUHDLGf9MITYbXN0Ah0ZA4B/3Gy3g0wJK9ofsyfSfmegIODsHDn8Jd+5P5+zgAgU7QImPwKNo8pw3Bdj7mM/e+yci8jSm759O12VdibRGPnIfr4xeMcGFB+GFUrlKPfXyDCdvnqTSpErcCrtFp3KdmPzSZCx28gHVteBrzDs4jxkHZvDnpT9j2lsUb8GC1gsUVpA0Z9PZTTSf15xbYbcokKUAK19fSYmcqecOvWQLKly6dIk8efKwbds2qlatGtPep08fNm7cyM6dO+O85/XXX+evv/5iyZIl+Pr6snbtWl5++WWio6MJDw9P8DF//PFH8ufPT+7cuTlw4AB9+/alcuXKLFq0KN5aw8PDCQ8Pj/VN8fHx0SBYRCSZbdwIb7wB588bF5uHDoV+/cDpKT6zDQ83AghffAEnThhtmTIZF7V79QJ3d2MJgfXrjXP+8AP8Kx+X5ty7B40awZYtxhIZa9dC+fJmV/VQdLRx8f/oUfj8c2NpjicRGQnLlxuzLPz++8N2Hx9jtoZDh4zZBh6lUKGHQYQHj+LFjd+HtObyZaM/YWHGsh8vvmhuPUuWGMuQFClihDyKF4c8edJGgMKeP/i0576JiDyR6Aj48304+ZOx7dsZKn4HjkkwnVasu9H/L8Tw73BD+A3I9wpU+s644zw9C71shBWubTW2Sw82ptxPD8tgPBAVDMcnwuGvjN+RR7E4/F94IQGP6DDjmNaIhNdjcTCWMHkwA4ibF2TI8zB8kNHH+OqaPWkGejd2w6GRcGHxwzbvF6HUx5CrdtKcI/IunJgER8cYM0IAOHtAkXehWE9jxpM0zt7HfPbePxGRJ2G1WRm4biAjt4wE4JWSrzCo1iCOXD/C31f+5u+rxuPUrVPxvt+CBd9svjHhhTJeRoChcLbCOCUgSHsv4h5VJ1fl4NWDVM5TmY0dN6aZ9e0T659r/zDjrxmM3TGW8Ohw3q/0PuMbjbebUIbYv5l/zaTz0s5EWiPxy+PH0teWkitjLrPLiiVVBRWuXbtG165dWbZsGRaLBV9fX+rVq8eUKVMIDQ19omMCrFu3jhdeeIETJ07g6+sb5/WhQ4cybNiwOO0aBIuIJI+ICBgyxAgT2GzGXe2zZ4NfEi4FGh0Nv/wCI0bAgQNGm6sreHnBuXNGeOGXX+xj6vy7d40ZBrZtg6xZjbDCc8+ZXZVhwQJ49VUjRHH2rLFMxdM6ccIImEydGnf5hvz54wYSSpSAjHa2/PEHHxjLaFSvDps3mxcK2L/fWHIk8v/C+5kzG4GF4sUfhhdKlDD+rKem2Srs+YNPe+6biEiihV2HLa/A1Y3GxdjnvoJiH6SNVJ09i46AfR/CMWNpT3I3NmZXcMlqbl3JLTLIWALjyGhjJoPk5pL1/jIkDwII3uDu9a/n97+65jBnKZI7/xhLMZyZDbb7a75lr2IEFvI0ebLwSthVY/aUYxOMWSXACCUU+wAKvw0unklVvensfcxn7/0TEUmskMgQ/Bf7s/DwQgAG1BzAJ3U/wSGefy/vRdzjn2v/xAov/H3lb66FXIv32K6OrpTMWTLW0hFlcpUhd+bcMRfmbTYbrRe0ZuHhhXhn8ubPrn+SxyNP8nU4lVj4z0JaL2iNDRuj6o2id/XeZpck8p9sNhvDNg5j2Ebj2vcrJV9hRvMZuDunvrv2UtXSDw+EhYVx48YNcufOTb9+/Vi+fDmHDh164mMGBweTKVMmVq1aRYN4rkhpRgURkZRz9Ci0awd79hjbnTsb09hnypQ857PZYOVKI7Cw7f4ypM88Y7SVK5c85zRDUJARutixw1jmYt06KFvW3JpsNiMw8ddfMHiwsbxGUgoLg2XLjKBG6dLGhfDMmZP2HKnVpUvGrArh4caSHy+8kPI1hIcbS40cPGiEjLy94cgRI0gSHR3/e5ycoHDh2OGFB4EGM3529vzBpz33TUQkUW4fgk0vwb1T4JQZqs+DPI3Nrkr+7fRM2PWWMQtApkJQcxFkNXkgmxwibsGRb+DoNw8vnmfyNS7I53/NuCBvi378w5qAfRxc7wcQvMDR1dRuJ9i9M8bsEid/Auv9z+g8S0Op/pDv1YQtlXLvlHGMU1ON3ycAj2JQojcUaJ92vheJYO9jPnvvn4hIYly+e5mX5r3En5f+xNnBmZ9e+gn/sv6JPs7V4KsPwwtX/ubgtYMcvHqQkMiQePfP6pY1JrwQEhnC1P1TcXZwZkPHDVTzqfa03Uozxu4YS6/VvQCY03IOr5V5zeSKROIXHhVOl2VdmHVgFgB9q/dlxAsj4g00pQbJFlQA8PPzo3LlyowfPx4Aq9VKvnz56NatG/369Xvs+yMjIylRogSvvvoqI0aMeOJjbt26lRo1avDXX3/x7LPPPva8GgSLiCQ9mw1+/NFYfiE01Ljzf9IkaNUq5c6/ebMxVf7bbxtLBtibO3egfn3YtQuyZzfCCgn4Zy/ZLF8OzZoZIZQzZ4yaJOn06AHjx0ONGrBpU8rfFNq3L4waBblyGWGFnPeX946IgJMn4fBh43HkyMOvwcGPPl7evEZgYdkycEuhGQPtecxnz30TEUmwiytha1uIumtcAK+9DDxLml2VxOfmPtjcEoLPgKM7VJ4EBduZXVXSCLsGR8YYM0dE3TXaPIpDqQGQv23CLsCnJ6GBcHQsHPvu4fcrUyEo0QcKdQDHeAaKN/cZszKcXwA2q9GWvTKU7Ad5X7brJUXsfcxn7/0TEUmo/YH7aTa3GReCLpDdPTuL2yymZv6aSXZ8q83K6VunY4UX/r7yN8duHCPaFvdulEnNJtGlfJckO39aEbA6gDE7xuDs4Mzq9qupW7Cu2SWJxHIz9CYt5rdg09lNOFoc+b7J93St0NXssv5TsgYV5s+fT4cOHfjhhx+oXLkyY8eO5eeff+bIkSN4eXnh7+9Pnjx5GDnSWEtn586dXLx4kXLlynHx4kWGDh3K6dOn2bt3L1myZEnQMU+ePMmcOXNo3Lgx2bNn58CBA/Tq1Yu8efOycePGJP+miIjI4127Bl26wNKlxvYLL8D06cY69pK0bt82wgq7d0OOHLB+vTHbQEqz2aBaNWOGh969jQvakrQuXjRmVYiIMJb7eP75lDv3li1Qq5bxc16yBF5++fHvsVqNmv8dXnjw/MoVY5+cOeHq1WQtPRZ7HvPZc99ERB7LZjMuDO/vbVy0zFUbavwCbjnMrkz+S/hN2PY6XF5tbBftAeW/AodUtG5UYoRehsNfw/HvIfr+HYpZykCpgeDTypxlFtKSiNvG0g1Hxz5cIsP9GSgeYCzf4JQJrqwzAgqBvz983zMNoWRf4899Oljexd7HfPbePxGRhFh6dCmvL3yd4MhgiucozvLXluObLe4S58khLCqMI9ePGOGFqwf55/o/VPepTr8aj78R2R5ZbVba/tKWBf8swNPVky1vbqF0LhM+eBWJx4mbJ2gypwnHbhwjs0tmfnn1F+r71je7rMdKzHgv0RHvNm3acO3aNQYPHkxgYCDlypVj1apVeHl5AXDu3DkcHB6mmsPCwhg4cCCnTp0iU6ZMNG7cmJkzZ8aEFBJyTBcXF/744w/Gjh1LcHAwPj4+tGrVioEDBya2fBERSQKrVkGnThAYCC4uxhIMvXqBg/3e1GKqLFmMWSPq1TOW13j+ediwAUqm8M2D69cbIQVXVwgISNlzpxd58kDXrjBhgrGsRkoFFe7dgw4djGtAHTsmLKQAxp95Hx/jUf//xsi3bhmBhRs3krxcERFJb6IjYPe7cGqKse3bFSp+C44u5tYlj+eaDWqvgL+HwqHP4Ng4uLUXavxsXKBOLGskBK6Dc/ON8INbLsjuZ9xpn93PmNUgOcICwefh8Cg4MenhEgbZKkDpQZCnmV3f3Z+kXLJA6QFQvJexHMThryDkPOzrDYdGQIZ8cPsvY1+LozE7RYne9rlsiIiIpEs2m42vt39Nn9/7YMPGi4Ve5OfWP5PFLUuK1eDm5EY573KU8y6XYudMzRwsDsxoMYPL9y6z5dwWGs9uzPbO28njobvxxBw2m43D1w+z4cwGBq8fzI3QG+TzzMeK11fYZYgm0TMqpFVK64qIPL2wMGNq+HHjjO2SJWHOHCirz41SxM2bRlhh3z7w8jKCAyVKpNz5X3jBWHri/ffh229T7rzpzYUL4OtrzKqwfj3UqZP853z3XZg4EfLlgwMHwNMz+c+ZXOx5zGfPfRMReaSwa7C5FVzbbFwMLj8GinZPF3dV250LS2H7GxAZZIQUavwCOROwBrI1Gq5uNMIJ5xdC+H+kIJ0yQ/aK98MLfpDD78kCEQ/cOw2HRsLpaUZIAiBHVSOg8ExD/R4+regIODMb/vkc7h4z2hzdwbeLMctCpgKmlmeWpBzzTZgwgS+//JLAwEDKli3L+PHjqVy5crz71qlTJ96Zaxs3bsyKFSsAuHfvHv369WPJkiXcuHGDggUL0qNHD955550E16QxrYikV5HRkby34j1+2vcTAO9UeIdxjcbh7JhGZ5qyMzdDb1J9SnWOXD/Cs17PsrnTZjxck/ffqYtBF8mZMScuCmCna1HWKP4K/ItNZzex+dxmNp/bzPWQ6zGvV8xdkaVtl/JM5qf4f00KS9alH9IqDYJFRJ7O33/D668b69YDdOtmTP3v7m5uXenNzZtGYGD/fvD2NmZWKFYs+c+7fbux7IOTE5w8aVzQluTz3nvw/fdGSGH9+uQ91+rV0LCh8Tyll5tIDvY85rPnvomIxOv2QdjYDILPgLMHVJ8PuRuaXZU8jaBjsLkl3DkEFieoMBaKvBf3gr/NCte2wtn5cP4XCLvy8DXXnMYyC/laQcQduLETbuyCm39CVHDcc2bI+zC4kL2yMROCc6bH13loBJyZBQ/WcM5V2wgoeD2vgEJSs0bDxWUQdhl8Wqf7JV2Sasw3f/58/P39mThxIn5+fowdO5YFCxZw9OhRcuXKFWf/mzdvEhEREbN948YNypYty08//UTHjh0BeOutt1i3bh0//fQTBQoUYM2aNbz33nssWrSIl156KUX7JyKSltwKvcUrC15h3el1OFgcGF1/ND38emDRmCJVOXP7DFUnVyXwXiD1CtVjxesrki1EMOvALDos6UDJnCXZ0GED2TNkT5bzSOoTHhXO7ku72Xx2M5vObWLrua3cjbgbax93J3eq5K1CvUL16OnXk4wuGU2q9skoqBAPDYJFRJ6M1WrMoNC3r3GHd65cMHUqNG5sdmXp140bxsXkAwfgmWeMsELRosl7zqZNYcUKePNNmDw5ec8lcO4cFC4MkZGwcSPUqpU857l1C0qXhkuXoEcP+Oab5DlPSrLnMZ89901EJI6Ly2HraxB1DzL5Qu1l4JmCU0lJ8om8Bzs7w7mfje2C/lBpIji6wfUdxswJ5xZA6KWH73HJBj4tId+r4FUXHOJZydQaBXf+eRhcuLHTCETYrLH3sziAZ+mHwYUcfuBR0lgy4vYhODTcqOHB+7zrQ+mBkKtm8nw/RP5PUo35/Pz8qFSpEt/enw7ParXi4+ND9+7d6dfv8euQjx07lsGDB3P58mUyZjQ+HC9dujRt2rRh0KBBMftVqFCBRo0a8dlnnyWoLo1pRSS9OX7jOE3nNuXYjWNkcsnEvFbzaFK0idllySPsvbyXWlNrERwZzBvPvsH05tOTPFDy+8nfaTynMVHWKACq5K3CH2/8keYuRkvC3Iu4x44LO9h0dhObzm5i58WdhEWFxdrH09WT6vmqUytfLWrlr0WF3BXS9EwbCirEQ4NgEZHEu3TJWK/+99+N7aZNjYvU8dx8ISns2jUjrHDwIOTODZs3Q6FCyXOu/fvhuefAwQGOHIEiRZLnPBLbO+/ADz8YP+e1a5PnHO3aGcu3FC1qLCmSIUPynCcl2fOYz577JiISw2Yz1q3f3xewGRelaywAV91hZFdsNjgy2vg526LBozhEhUDIuYf7OHtA3haQvw141wOHJ5gWOfIe3NxzP7xwP8AQciHufk4ZIXMxuLX3YVvupkZAIYdf4s8r8hSSYswXERFBhgwZ+OWXX2jevHlMe4cOHbh9+za//vrrY49RpkwZqlatyo8//hjT9tZbb7Fv3z6WLFlC7ty52bBhAy+99BIrVqygVgLT1RrTikh6svHMRlr+3JKboTfJ55mPZa8t41mvZ80uSx5j1YlVNJ3TlGhbNANqDuCz5xMWxkuI/YH7qTW1Fncj7tK4SGO2n9/OrbBbNPBtwNLXlqbpi9NpyZ2wO+wP3M/+wP0EhQeR0SUjGZwzxDwyOj/c/vdrGZ0z4u7sjoPF4ZHHvhV6iy3nthjBhHOb2HNpD9EPZmm7L1fGXNTMV5Na+Y1gQplcZXB0cEzubqeYxIz34omgi4iIwOLF0LWrcfe+uzt8/bVx4VQzkqUOOXM+nKb/0CHjbvjly5PnXCNGGF9ffVUhhZTUvz9MmQLr1sGWLVCjRtIef8ECI6Tg6AgzZ9pHSCGppcY1fUVE7Fp0OOx6G05PN7YLvwMVxz3ZBWpJ3SwWKPEhZCsPW9pA0BGj3SkT5HnJCCc80wAcXZ/uPM6ZwKu28Xgg5FLs4MKN3cbMHQ9CCj4todRAyPbc051bxETXr18nOjoaLy+vWO1eXl4cOXLkse/ftWsXBw8eZPL/Tac3fvx43nrrLfLmzYuTkxMODg5MmjTpP0MK4eHhhIeHx2wHBQUlsjciImnT1H1TeXv520RaI/HL48eStkvwzuRtdlmSAA0LN+THZj/SeWlnhm8eTj7PfLxV4a2nPu6Z22doNLsRdyPuUrdAXRa9uoh9gft4YcYLrD65mo5LOjKr5az/vAguiWOz2bh49yL7A/ez7/I+9l8xvp6+ffqpjuvm5BZvoOFO2B0OXj2IjdhzBOT3zE/N/DVjZkwomr2oln65T0EFEZEUFh4Oly9DYKDxNb5HYKCx5IK7u3Hx0N09eZ67usYNHty7B716wU8/GdvPPQezZ0MJzbSb6uTKBYsWQcmSxrIMyXEx+/Bh+OUX4/nHHyftseW/5c8PnTrBjz/CsGEPZzZJCpcvw7vvGs/794dHXHtP1+bPn09AQECsNX0bNGjwyDV9Fy1aFO+avq1bt45pCwgIYN26dcyaNSvWmr65c+dO8Jq+IiJ2K+wqbG4J17aCxRHKj4Wi7ysla++86kLDPXBiImQtD7kbg5N78p4zQ27I0AJ8Whjb1mgIOgy3/4asZcGzZPKeXyQNmDx5MmXKlIkT0h0/fjw7duxg6dKl5M+fn02bNvH++++TO3du6tWrF++xRo4cybBhw1KibBGRVMFqs9L/j/6M2jYKgDal2jD15am4OyfzGEeS1JvPvcm5O+cYtnEY7654l9yZc9O0aNMnPt7N0Js0mt2IwHuBlMlVhsVtFuPq5EqVvFVY+OpCms1txtyDc8nunp1xjcaZfhHbZrNxN+IuLo4uuDi6pInwRLQ1mqM3jsYKJewP3M/1kOvx7p/fMz/lvMuRK2MuQqNCCY4IJiQyhJDIEIIjHz4PiQwhOCKY0KjQmPeGRYURFhXGzdCb8R67WPZiMbMl1MxXk/xZ8idLn+2Bln4QEUkCNhvcvfvf4YMHj1u3zK72IYslbnjhzh24csV4rXdv+PRTcNGMU6na228bF7OrVzeWgEjKcWyHDjBjBrz8MixZknTHlYQ5c8aYxSIqygiiVK/+9Me02aBZMyPc8txzsGOHff0Zt+c1fTWeFRG7desAbHoJgs+Cs6ex1MMzL5pdlYiIKcxe+iE4OJjcuXPzySef0LNnz5j20NBQPD09Wbx4MU2aPFxbvUuXLly4cIFVq1bFe7z4ZlTw8fHRmFZE7FJwRDBvLH6DxUcWAzCo1iCG1hmaJi7ySlw2m43OSzszdf9UMjhnYEOHDVTKUynRxwmNDOXFmS+y9fxWfDx82N55O3k88sTaZ+7fc2m3qB02bAyrM4zBtQcnVTcS7fyd87T8uSV/Xvozps3ZwRlXJ1fcnNxwdXTF1ck11lc3J7c4bf/f7u7sTkbnjGRyyURGl/tfH7H9uOUVQiJD+PvK3+wL3GcEEwL38feVv2OFCR5wtDhSMmdJynmX4znv5yjnXY5y3uXI6p41Ud8Xq81KWFRYrEDD/4canByc8Mvjh1cmr8cf0I5p6QcRkSRisxlLHzwufHD5MoSEJPy4Li7g7Q3PPBP/w9sbnJwgNNR4hIQ8/nlC9wsJgejoh/0LCTEeN248rC9vXuPidN26Sfv9lOQxeLDx89q6FVauhH99ZvRUTp82ZtMAGDAgaY4piVOgAHTsaMxwMmwYrFnz9MecMsUIKbi4GL839hRSSCoRERHs2bOH/v37x7Q5ODhQr149tm/fnqBjTJ48mbZt28aEFACqVavG0qVLefPNN2PW9D127BhjxoxJ8j6IiKQZF36Fbe0gKhgyF4Hay8CjmNlViYikaS4uLlSoUIG1a9fGBBWsVitr166lW7du//neBQsWEB4eTvv27WO1R0ZGEhkZiYND7IsGjo6OWK3WRx7P1dUVV9enXMZFRCQNuBh0kZfmvcTey3txcXRhyktTaPdsO7PLkqdgsVj4oekPXLp7idUnV9NkThO2d96ObzbfBB8j2hpNu0Xt2Hp+K1ncsvBbu9/ihBQAXivzGjdCb9D9t+4M2TCEnBly8m6ld5OyOwmy9/Jems5pyuV7l2O1R1ojiYyI5F7EvRSrJaNzxngDDReCLnDsxjGstrjjj0wumSjrVTZWKKFUrlK4Obk9dT0OFoeYZR4k6SioICJy35YtMGtW7OUXAgMhMjLhx8iU6dHhg38/smY1dxbbyMhHhxkiI6FSJcic2bz6JHHy5IHu3eHLL43lGRo1AockCGqPGmWEWl580fidEHN8/DFMm2Ys/bB9O1St+uTHOn0aPvjAeP7ZZ1C6dFJUaH9Sy5q+Ws9XROyazQb/fAF/fQzYwOsFqPEzuGYzuzIREbsQEBBAhw4dqFixIpUrV2bs2LEEBwfTqVMnAPz9/cmTJw8jR46M9b7JkyfTvHlzsmfPHqvdw8OD2rVr07t3b9zd3cmfPz8bN25kxowZjB49OsX6JSKSGu29vJdmc5tx6e4lcmbIyeI2i6meLwmmxRTTOTs6s6D1AmpPq82+wH00mt2IbZ23kSNDjse+12az0XNVTxYfWYyLowtL2iyhVK5Sj9y/W+VuXAu+xiebPuH9le+TPUN2Xi31alJ25z+tOLaCNr+0ITgymFI5S/Fr21/JmTEn4VHhhEWFER4dTnhUeKyvYVFhcdoetX9oVCjBkcHci7hHcMT9r/FsPxAcGUxwZDBXg6/GW693Ju9YMyQ85/0cvtl8NYNJGqOggogIcOAAvPAC/Gt58ViyZ398+MDb2wgqpAXOzsZDsyzaj379jOUfDhyAefPg9def7ngXLxp33gMMHPj09cmTK1gQ/P2Nn8ewYfCIGVUfy2qFTp3g3j2oUQMCApK2Tnkoqdb01Xq+ImK3osNg51twZqaxXeQ9qDAWHJxNLUtExJ60adOGa9euMXjwYAIDAylXrhyrVq2KCeOeO3cuzuwIR48eZcuWLax5xFRu8+bNo3///rRr146bN2+SP39+hg8fzjvvvJPs/RERSa2WHFlCu0XtCIkMoWTOkix/bTkFsxY0uyxJQpldM7Pi9RVUmVyF4zeP89Lcl1jrvxZ3Z/f/fN+oraOYsHsCFizMbDGT2gVqP/ZcQ+sM5XrIdb778zvaL2pPFrcs1Petn1RdeaTvdn9H99+6Y7VZqVeoHr+0/gVPN0/jxRScGMlqsxIa+ehAw72Ie+TIkINy3uXwzuSdcoVJsrHYbDab2UWkBK3pKyKPEhZm3C1+8CDUqgWvvRY7gODlpanRJW0YPtwIFRQqBIcPP93vbUAAjBljXNDevDnpapQnc+oUFC1qzHCxYwf4+SX+GGPGGD/XjBmNQEuhQklfZ2pgT2v6aj1fEbFLoVdgU3O4sQMsjlBhHBR9z+yqRERSDXv/DNPe+yci6YfNZuPLbV/S749+2LDRwLcB81+Z//Dirtidw9cOU21KNW6H3aZF8RYsaL0ARwfHePedfWA27RcbyyiNaTCGD6p8kODzPFguYv6h+WR0zsha/7X45X2CDwMTwGqz0ntNb0bvMGZH6vxcZ75v8j3OjgqRy5NLzHhP81+ISLo3YIARUsiVCxYsgHfegZdfhsqVwcdHIQVJO3r2NII1p07B/804nyjXrsEPPxjPBwxImtrk6RQqBG+8YTx/khvs//kH+vc3no8ebb8hhaTy7zV9H3iwpm/Vx6y9kZRr+rq6uuLh4RHrISKSpt3aD6srGSEFl6xQd7VCCiIiIiKS5kRER9BlaRf6/tEXGzber/Q+y19frpCCnSuRswRL2y7FxdGFxUcW88GqD4jvXvA/Tv1Bp1+NpZY+rPphokIKAI4OjsxoMYMXC71IcGQwjec05vC1w0nRhVhCIkNovaB1TEhh+PPDmdRskkIKkqIUVBCRdG3tWuOiHRgXdnPlMrcekaeRKdPDZRo++QRCQp7sOGPHGu+tUAEaNEiy8uQpDRgAjo7w22+wa1fC3xcZaSwdER4OjRpB167JV6M9CQgIYNKkSUyfPp3Dhw/z7rvvxlnTt/+D9Me/JGRN3w0bNnD69GmmTZvGjBkzaNGiRYr0SUTEVOcXw5rqEHIePIpB/Z3g/YLZVYmIiIiIJMrN0Js0mNWAKfun4GBxYFzDcXzb+FucHLTSenpQM39NZrYwlrD7dve3fL3961iv7w/cT8v5LYm0RtK2dFtGvTjqic7j4ujCojaLqJynMjdDb1J/Vn3O3Tn31PU/cOXeFepOr8uiw4twcXRhTss5fFzzYywWS5KdQyQhFFQQkXTr1i3o0MF4/tZb0LSpufWIJIW33oICBSAwEMaNS/z7b9+Gb781ng8YABqbph6FC0O7dsbzTz5J+PtGjIA9eyBrVvjpJ/1ME6pNmzZ89dVXDB48mHLlyrF///44a/pevnw51nserOnbuXPneI85b948KlWqRLt27ShZsiSff/651vQVEftns8GhEbC5JUSHgPeLUH8HeBQxuzIRERERkUQ5duMYVX6qwoYzG8jskpnlry2nu193s8uSFPZqqVf5ur4RUOj9e2/mHZwHwNnbZ2k8uzF3I+5Sp0Adpr08DQfLk1+GzeSSiZWvr6REjhJcCLpA/Zn1uR5y/anrP3ztMFUmV2HXxV1kc8/GH2/8wWtlXnvq44o8CYstvnlJ7JDWPxOR//faazBvnnHxb98+4250EXswa5axTECWLMYyEFmzJvy9w4cbszKUKgUHDoCDIo2pyvHjULw4WK2wezdUrPjf+//5J1SpAtHRMHcutG2bMnWayZ7HfPbcNxGxU1GhsLMLnJ1jbBftDuVHg+42ExF5JHsf89l7/0TEfq0/vZ5WP7fiVtgt8nvmZ/nryymdq7TZZYlJbDYbvVb34pud3+Di6MK8VvP4eN3HHLl+hNK5SrO502ayuGVJknOdv3Oe6lOqcz7oPJVyV2Kt/1oyu2Z+omOtP72elj+35HbYbXyz+rKy3UqKZi+aJHWKPJCY8Z4uP4hIujRnjhFScHQ0LuoqpCD25LXXoHRpY3aEUYmYXSw4GMaMMZ7376+QQmpUpEjCZ1UIDTWWfIiOhldfTR8hBRERSUVCL8PaOkZIweIElSZCxXEKKYiIiIhImvPT3p+oP6s+t8JuUSVvFXZ22amQQjpnsVj4uv7XtCrRiojoCFr+3JIj14+Q1yMvv7X7LclCCgA+nj6seWMN2d2zs/vSblr+3JLwqPBEH2fGXzNoMKsBt8NuU82nGts7b1dIQUynSxAiku6cOwfvvWc8HzgQ/PzMrUckqTk6GtP9A3zzDfzf7PSP9MMPcOMG+PpCmzbJV588nYEDjRDJsmWwd++j9xswAA4fBm9v+O67lKtPRETSIZsNQi7A+UWwvx/8UReWFYEbu8AlKzy/Boq8bXaVIiIiIiKJEm2Npvea3nRd1pUoaxSvlX6N9R3W45XJy+zSJBVwdHBkZouZVPOpBoCnqye/tfuNvB55k/xcxXMU57d2v5HROSN/nPqDNxa/QbQ1OkHvtdlsDN0wlA5LOhBpjeTVUq+y1n8tOTPmTPI6RRJLQQURSVesVujYEe7cgcqVjQt5IvaoaVOoWtW4q/7TTx+/f1gYfPWV8bxfP3DSzY6pVtGixqwZAMOGxb/Pxo0wdqzxfPJkyJ49RUoTEZH0IuIOBK6FQyNgU3NYkgeW+MDmVvDPF3B1A0QFg2cpaLALvOqaXbGIiIiISKLci7hHq59b8dV24wOzYXWGMbvlbNyc3EyuTFITd2d3lr22jE/qfMLGjhuTdaaNSnkqsaTtElwcXVjwzwLeX/k+NpvtP98TER1BhyUdGLbR+BCxb/W+zG01V7/HkmpYbI/7LbYTWv9MRAC+/ho++ggyZID9+41p1EXs1aZNULu2ETo4csSYKeFRJk6Ed9+FvHnh5ElwcUm5OiXxjhyBkiWNG1j37oXnnnv4WlAQlC0LZ85A167w44+mlWkKex7z2XPfRCQVi46A238ZsyM8eAQdibufxRGylIHsle8//MCjBDg4pnzNIiJpmL2P+ey9fyKS9kVGRzL34FxGbB7B0RtHcXV0ZVrzabQtrTU1JXX45Z9feHXBq9iwMbDmQD59Pv671G6F3qLlzy3ZcGYDjhZHvm/yPV0rdE3haiU9Ssx4T/dLiki6ceAAfPyx8Xz0aIUUxP7VqgUNG8KqVTB4MMyeHf9+kZHwxRfG8969FVJIC4oXh7ZtYe5c+OQTWLz44WsBAUZIoWBBI5wlIiKSYDYr3D0RO5Rwax9YI+Lum7GgEUjI4Wd8zfocOGVI+ZpFRERERJLAvYh7TNoziTE7xnA+6DwAuTLmYkmbJVT1qWpydSIPvVLyFb5v8j3vrHiHzzZ/Rs6MOenh1yPWPqdvnabxnMYcuX6EzC6ZWdB6AQ0KNzCpYpFHU1BBRNKFsDBo3x4iIowp8d96y+yKRFLGiBFGUGHOHOjTx7jT/v/NmWNc2M6VC7p0SfES5QkNGgTz5sGSJfDXX8bPdvlyY6kHiwWmTYPMmc2uUkREUrXQK7FDCTd2QeTtuPu5Zodslf81W0IlcNN6piIiIiKS9l0Nvsq4neP4bvd33Aq7BYB3Jm8+8PuAtyu+TRa3LOYWKBKPtyu+zbWQawxaP4ieq3qS3T077Z5tB8DOCzt5ad5LXA2+Sl6PvKx4fQXPej1rcsUi8VNQQUTShYED4e+/IWdO+Okn4yKeSHrw3HPGnffz5sGAAcaF7H+LjoaRI43nAQHGsiiSNpQoAa++CvPnG7Mq/PDDw6BJQIAxo4aIiEiMyHtwa+/DQML1nRByLu5+jm6Qtfy/QgmVIVMhDaBFRERExK6cvHmSr7Z9xbS/phEWFQZA0exF6V2tN+2fbY+bk5vJFYr8twE1B3At+Brjdo2j468dyeaejdCoUNotakdYVBjPeT/H8teXkztzbrNLFXkki81ms5ldRErQ+mci6df69fDCC8Za7r/+Ci+9ZHZFIinr+HHjonZ0NGzaBDVrPnxtwQLjYneWLHD2LOifyLTl0CEoU8b4+616ddi6FUqWhD17wC2d/n/ansd89tw3EUli1ii4c+h+KGGn8fXOIWNph1gs4FkydighSxlwcDalbBERsf8xn733T0RSvz2X9jBq2yh++ecXrPfHx355/OhbvS8vFXsJRwdHkysUSTirzYr/Yn9m/z0bV0dXIqIjsGGjSZEmzHtlHplcMpldoqRDiRnvaUYFEbFrt29Dhw7GRbyuXRVSkPSpSBHjTvsffoD+/WHzZuOmSJsNhg839unRQyGFtKhUKWjdGn7+2QgpODnBjBnpN6QgIpKu2axwcjKcngE390B0aNx9MuSNHUrIVhGctU6QiIiIiNg3m83G76d+Z9TWUaw9vTamvXGRxvSp1oda+Wth0QxikgY5WByY+vJUbobe5LcTvwHwXsX3+KbRNzg56BKwpH76LRURu/b++3D+PBQuDKNHm12NiHkGDYLp042L2StXQpMmsGIF/PUXZMpkBBUkbRo0yAgqPHheoYK59YiIiAnunYGdneHKuodtzh6QrZIRSMjhZzzPoCk/RURERCT9iLJGseDQAkZtG8X+wP0AODk48Vrp1+hdrTdlvMqYW6BIEnB2dOaXV39h8PrBlMxZkk7lOil4I2mGggoiYrfmzYM5c8DREWbONC7GiqRXefIYYYRRo+Djj6FRo4ezKbz7LmTPbm598uRKl4Zx4+D0aWPGDBERSUdsNjg5CfZ+CFH3wNEdygyFPC+BR1GwOJhdoYiIiIhIiguJDGHKvil8vf1rztw+A0AG5wx0Ld+VgKoB5PPMZ26BIkksg3MGvqr/ldlliCSaggoiYpfOnzcuvgIMGABVqphbj0hq0LevsfzDgQPw9tuwYwe4ukJAgNmVydPq3t3sCkREJMUFn4OdXSDwd2M7Zw2oMhUyFza3LhERERERk9wIucGE3RMYv2s810OuA5AjQw56VO7Be5XeI3sG3akjIpKaKKggInbHaoWOHeH2bahUCQYONLsikdQhWzbo08cI7/z0k9HWpQt4e5tbl4iIiCSCzQanpsCeXhB115hFoewIKNZDMyiIiIiISLp09vZZRm8fzU/7fiIkMgSAglkK8lG1j+hYriMZnDOYXKGIiMRHQQURsTvffAPr1kGGDDBrFjg7m12RSOrRs6exTMCVK+DkZAQXREREJI0IuQA7u8LlVcZ2jmrGLAoeRc2tS0RERETEBAeuHGDU1lHMOziPaFs0AM95P0ff6n1pVbIVTg66BCYikprpb2kRsSsHDz5co/3rr6GoPrMViSVjRhg+3JhJ4e23IZ+W5BMREUn9bDY4NQ329oLIO+DgCmWHQ7EPwMHR7OpERERERFKMzWZj49mNfLH1C1adWBXTXq9QPfpW78sLBV/AYrGYWKGIiCSUggoiYjfCw6FdO+NrkybGRVgRiatzZ6hZEwoVMrsSEREReayQS7CrK1xaaWxn94Mq08CzuKlliYiIiIikpGhrNEuOLOGLrV+w+9JuABwsDrQu2Zo+1ftQ/pnyJlcoIiKJpaCCiNiNgQPhwAHIkQN++gkUnBV5NM02IiIiksrZbHBmFvzZAyJvg4MLPPspFP9QsyiIiIiISLoRFhXGjL9m8NW2rzh+8zgAbk5uvFnuTQKqBuCbzdfkCkVE5EkpqCAidmHDBmOpBzBCCt7eppYjIiIiIvLkQi/Drrfh4jJjO1slqDoNPEuaWpaIiIiISEq5HXab73d/zzc7v+FK8BUAsrplpVvlbnSr3I1cGXOZXKGIiDwtBRVEJM27fRv8/Y2bzjp3hpdfNrsiEREREZEnYLPBmTmwpztE3AIHZygzDEr0Bgf9911ERERE7N/FoIuM2TGGH/b8wL2IewD4ePgQUDWALuW7kMklk8kViohIUtEnHSKS5nXrBufPQ6FCMGaM2dWIiIiIiDyB0Cuw+x24sMTYzlYBqkyDLKXNrEpEREREJEVcCLrApxs/Zer+qURaIwEonas0far1oW3ptjg7OptcoYiIJDUFFUQkTZs/H2bPBgcHmDULMmc2uyIRERERkUSw2eDcz/Dn+xB+w5hFofRgKNnXeC4iIiIiYseuh1zn8y2f8+2ubwmPDgegdv7a9Kneh0aFG2GxWEyuUEREkouCCiKSZl24AO+8Yzz/+GOoWtXcekREREREEiXsKux+D84vNLazloMq0yHrs6aWJSIiIiKS3O6G32XMjjF8te0r7kbcBaBGvhqMeH4ENfPXNLk6ERFJCQoqiEiaZLVCx45w+zZUrAiDB5tdkYiIiIhIIpxbYIQUwq+DxQlKD4RSH2sWBRERERGxa2FRYXy/+3tGbBnB9ZDrAJTzLseI50fQsHBDzaAgIpKOKKggImnSuHGwdi24uxtLPjjr81wRERERSQvCrsOf3eDcfGM7y7NQdboxm4KIiIiIiJ2KskYxbf80hm0cxoWgCwAUzV6UT+t+yislX8HB4mByhSIiktIUVBCRNOfgQejXz3j+9ddQrJi59YiIiIiIJMj5xbD7HWPJB4ujMYNCqYHg6GJ2ZSIiIiIiycJqs/LLP78waP0gjt04BkBej7wMqT2EjuU64uSgy1QiIumV/gUQkTQlPBzatze+Nm4M77xjdkUiIiIiIo8RfgP+7A5n5xrbnqWMWRSyVTC3LhERERGRZGKz2Vh1YhUD1g1gX+A+AHJkyMHHNT7m3Urv4ubkZnKFIiJiNgUVRCRNGTwY/voLcuSAyZNBS5aJiIiISKp24VfY9TaEXQGLA5TsB6UHg6Or2ZWJiIiIiCSLree20n9tfzaf2wxAZpfMfFTtIz6o8gEerh4mVyciIqmFggoikmZs3Ahffmk8nzQJvL3NrUdERERE5JHCb8KennBmlrHtWRKqTIPslUwtS0REREQkuewP3M+AdQNYeXwlAK6OrnSr3I1+NfqRI0MOk6sTEZHURkEFEUkT7twBf3+w2eDNN6F5c7MrEhERERF5hIvLYddbEHrZmEWhRG8oMxQcNb2tiIiIiNif4zeOM3jDYOYdnAeAo8WRN597k8G1B5PXI6/J1YmISGrl8CRvmjBhAgUKFMDNzQ0/Pz927dr1yH0jIyP55JNP8PX1xc3NjbJly7Jq1apEHzMsLIz333+f7NmzkylTJlq1asWVK1eepHwRSYO6d4dz56BQIRg71uxqRERERETiEXEbtneEjc2MkIJHMXhxK5T7XCEFEREREbE7F4Iu8NaytygxoURMSKFt6bYcfv8wPzb7USEFERH5T4kOKsyfP5+AgACGDBnC3r17KVu2LA0aNODq1avx7j9w4EB++OEHxo8fzz///MM777xDixYt2LdvX6KO2atXL5YtW8aCBQvYuHEjly5domXLlk/QZRFJaxYsgJkzwcHB+Jo5s9kViYiIiIj8n0u/wYrScHo6YIESH0HDfZCjitmViYiIiIgkqesh1/lozUcUHleYSXsnEW2LpnGRxux7ex9zW82lSPYiZpcoIiJpgMVms9kS8wY/Pz8qVarEt99+C4DVasXHx4fu3bvTr1+/OPvnzp2bAQMG8P7778e0tWrVCnd3d2bNmpWgY965c4ecOXMyZ84cXnnlFQCOHDlCiRIl2L59O1WqPP6Dn6CgIDw9Pblz5w4eHh6J6bKImOjiRShTBm7dggED4LPPzK5IRERSM3se89lz30TStIg7sDcATk0xtjMXgSrTIGc1U8sSEZG0yd7HfPbePxF7dzf8LqO3j+br7V9zN+IuADXz1WTECyOoka+GydWJiEhqkJjxXqJmVIiIiGDPnj3Uq1fv4QEcHKhXrx7bt2+P9z3h4eG4ucWe4tLd3Z0tW7Yk+Jh79uwhMjIy1j7FixcnX758jzyviKR9Vit06mSEFCpUgCFDzK5IRERERORfLq+BlaXvhxQsUKwXNNqvkIKIiIiI2JWwqDDGbB9DoXGFGLpxKHcj7lLOuxwrX1/Jxo4bFVIQEZEn4pSYna9fv050dDReXl6x2r28vDhy5Ei872nQoAGjR4+mVq1a+Pr6snbtWhYtWkR0dHSCjxkYGIiLiwtZsmSJs09gYGC85w0PDyc8PDxmOygoKDFdFZFU4Ntv4fffwd0dZs0CZ2ezKxIRERERASKDYF9vOPGjsZ3JF6pMhVw1za1LRERERCQJRVmjmLZ/GsM2DuNC0AUAimYvyqd1P+WVkq/gYEn06uIiIiIxkv1fkW+++YYiRYpQvHhxXFxc6NatG506dcLBIXlPPXLkSDw9PWMePj4+yXo+EUlahw5B377G8y+/hOLFza1HRERERASbDc79AstLPAwpFO0Bjf9SSEFERERE7IbVZuXnQz9T6rtSdF3WlQtBF8jrkZefmv3EofcO8WqpVxVSEBGRp5aof0ly5MiBo6MjV65cidV+5coVvL29431Pzpw5WbJkCcHBwZw9e5YjR46QKVMmChUqlOBjent7ExERwe3btxN83v79+3Pnzp2Yx/nz5xPTVRExUUQEtG8PYWHQsCG8957ZFYmIiIhIunfvNGxoAltaQ+glyFQYXtgAFb8Bp4xmVyciIiIi8tRsNhu/Hf+Nij9WpM0vbTh24xg5MuRgdP3RHO9+nM7lO+PkkKiJukVERB4pUUEFFxcXKlSowNq1a2ParFYra9eupWrVqv/5Xjc3N/LkyUNUVBQLFy7k5ZdfTvAxK1SogLOzc6x9jh49yrlz5x55XldXVzw8PGI9RCRtGDwY9u+H7NlhyhSwWMyuSERERETSregIOPQ5rCgFl38DBxcoPRia/A1etc2uTkREREQkSWw9t5Xa02rTeE5j9gXuI7NLZobVGcapHqfoVbUXbk5uZpcoIiJ2JtHRt4CAADp06EDFihWpXLkyY8eOJTg4mE6dOgHg7+9Pnjx5GDlyJAA7d+7k4sWLlCtXjosXLzJ06FCsVit9+vRJ8DE9PT3p3LkzAQEBZMuWDQ8PD7p3707VqlWpUqVKUnwfRCSV2LQJRo0ynv/4IzzzjLn1iIiIiEg6dnUz7H4X7hwytr3qQqXvwaOYuXWJiIiIiCSR/YH7GbBuACuPrwTA1dGVbpW70a9GP3JkyGFydSIiYs8SHVRo06YN165dY/DgwQQGBlKuXDlWrVqFl5cXAOfOncPB4eFEDWFhYQwcOJBTp06RKVMmGjduzMyZM8mSJUuCjwkwZswYHBwcaNWqFeHh4TRo0IDvvvvuKbouIqnNnTvg728s/duxI7RsaXZFIiIiIpIuhd+AfX3g1BRj2zUnlB8NBdppui8RERERsQvHbxxn8IbBzDs4DwBHiyOdn+vMoNqDyOuR1+TqREQkPbDYbDab2UWkhKCgIDw9Pblz546WgRBJpTp0gBkzoEAB+Osv0B9VERFJLHse89lz30RSDZsNTk+HfR8ZYQUA365Q7nNwzWZubSIiki7Y+5jP3vsnkhbcDrvNkPVDmLB7AtG2aADalm7LJ3U+oUj2IiZXJyIiaV1ixnuJnlFBRCQ5/PKLEVJwcICZMxVSEBEREZEUducw7H4Hrm4ytrOUgUoTIWc1c+sSEREREUkCVpuVafun0e+PflwLuQZA4yKNGf78cMp5lzO3OBERSZcUVBAR0126BG+/bTzv1w9q1DC3HhERERFJR6JC4dBwODwKrJHgmAHKDIXiH4CDs9nViYiIiIg8td0Xd9Ptt27surgLgOI5ijO+0XjqFapncmUiIpKeKaggIqayWqFTJ7h5E8qXhyFDzK5IRERERNKNS6vgz/fh3iljO3dTqPQtZMxvbl0iIiIiIkngWvA1Pl77MZP3TcaGjcwumRlSewjd/brj4uhidnkiIpLOKaggIqaaMAHWrAE3N5g1C1w0PhYRERGR5BZ6GfZ8AOd+NrYz5IUK4yBvc7BYzKxMREREROSpRVmjmPjnRAatH8TtsNsAvPHsG3xR7wueyfyMucWJiIjcp6CCiJjmn3+gTx/j+ZdfQokS5tYjIiIiInbOGg3Hv4cDAyAyCCwOULQnPDsMnDObXZ2IiIiIyFPbfHYz3X7rxoErBwAo512Obxt9S/V81U2uTEREJDYFFUTEFBER0L49hIVBgwbw/vtmVyQiIiIidu3mXtj1Ntz809jOXhkqTYRsz5lbl4iIiIhIErh09xK9f+/NnL/nAJDVLSvDnx/OWxXewtHB0eTqRERE4lJQQURMMXQo7NsH2bLBlCmaYVdEREREkklkEBwYDMfGg80Kzp5QbiT4vgX6wFZERERE0riI6Ai+2fENn2z6hHsR97Bg4a0Kb/HZ85+RI0MOs8sTERF5JAUVRCTFRETA4cOweTN88YXR9uOPkDu3uXWJiIiIiB2y2eD8ItjTA0IvGW35X4Pyo8Hd29zaRERERESSwJqTa+jxWw+O3jgKQJW8Vfi20bdUyF3B5MpEREQeT0EFEUkWN2/CX3/B/v0Pv/7zD0RGPtynQwdo1cqsCkVERETEbt07DX92g0srje1MvlDpO3imvrl1iYiIiIgkgdO3ThOwJoAlR5YAkCtjLkbVG8UbZd/AweJgbnEiIiIJpKCCiDwVqxVOnXoYRngQTDh/Pv79PT2hXDmoWRP69UvBQkVERETE/lkj4fDXcPATiA4FB2co2Q9K9gcnd7OrExERERF5KqGRoYzaOorPt35OWFQYjhZHevj1YEjtIXi6eZpdnoiISKIoqCAiCRYSAgcPxp4l4cABuHcv/v0LFYKyZY1gwoOv+fKBxZJyNYuIiIhIOnF1C+x+B+4cMrZz1YFK34NncVPLEhERERF5WjabjV+P/kqv1b04c/sMAM8XfJ5xDcdRKlcpc4sTERF5QgoqiEgcNhsEBsZduuHYMWMGhf/n6gplyjwMI5QrB88+Cx4eKVu3iIiIiKRD4Tdgf184OdnYds0B5UdDgfZKyIqISLo2YcIEvvzySwIDAylbtizjx4+ncuXK8e5bp04dNm7cGKe9cePGrFixImb78OHD9O3bl40bNxIVFUXJkiVZuHAh+fLlS7Z+iKR3R68fpeeqnqw+uRqAvB55GV1/NK+UfAWLxrsiIpKGKaggks5FRcHRo3GXbrh6Nf79c+V6GEZ4EEwoWhSc9LeJiIiIiKQkmw1Oz4B9H0H4daPNtyuU+xxcs5lbm4iIiMnmz59PQEAAEydOxM/Pj7Fjx9KgQQOOHj1Krly54uy/aNEiIiIiYrZv3LhB2bJlad26dUzbyZMnqVGjBp07d2bYsGF4eHhw6NAh3NzcUqRPIunN3fC7fLbpM8bsGEOkNRIXRxd6V+tN/xr9yeiS0ezyREREnpouLYqkI3fuGEs1/HuWhIMHITw87r4ODlCsWNylG7y9U7ZmEREREZE47hyG3e/C1ft3fnqWhsoTIWd1c+sSERFJJUaPHk3Xrl3p1KkTABMnTmTFihVMmTKFfv36xdk/W7bYIb958+aRIUOGWEGFAQMG0LhxY0aNGhXT5uvrm0w9EEm/bDYbcw/Opffvvbl09xIATYo0YWzDsRTOVtjk6kRERJKOggoidshmg7Nn4y7dcPp0/PtnymQEEf69dEOpUpAhQ8rVLCIiIiLyWFGhcGg4HB4F1khwdIcyQ6F4L3BwNrs6ERGRVCEiIoI9e/bQv3//mDYHBwfq1avH9u3bE3SMyZMn07ZtWzJmNO7atlqtrFixgj59+tCgQQP27dtHwYIF6d+/P82bN3/kccLDwwn/1x0yQUFBT9YpkXTiwJUDdP+tO5vObgLAN6svYxuOpWnRpiZXJiIikvQUVBCxE1FRMGoUrF5tBBPu3Il/v3z54s6SULCgMYOCiIiIiEiqdWk1/Pke3DtlbOduChXHQ6YCppYlIiKS2ly/fp3o6Gi8vLxitXt5eXHkyJHHvn/Xrl0cPHiQyZMnx7RdvXqVe/fu8fnnn/PZZ5/xxRdfsGrVKlq2bMn69eupXbt2vMcaOXIkw4YNe7oOiaQDt0JvMWTDECbsnoDVZsXdyZ2BtQYSUDUANyctryIiIvZJQQUROxAVBf7+MHfuwzZnZ2NWhH+HEsqWhWxarldERERE0pLQy7CnF5ybb2y754GK4yBvC7BYzK1NRETEDk2ePJkyZcpQuXLlmDar1QrAyy+/TK9evQAoV64c27ZtY+LEiY8MKvTv35+AgICY7aCgIHx8fJKxepG0xWqzMnXfVPqt7cf1kOsAtC7Zmq/qf0U+z3wmVyciIpK8dA+1SBoXFQUdOhghBScnGDPGmFHh3j3Ytw+mTYMPPoC6dRVSEBERSYwJEyZQoEAB3Nzc8PPzY9euXY/ct06dOlgsljiPJk2axNrv8OHDvPTSS3h6epIxY0YqVarEuXPnkrsrImmTNRqOTYDlxY2QgsUBivWCpofBp6VCCiIiIo+QI0cOHB0duXLlSqz2K1eu4O3t/Z/vDQ4OZt68eXTu3DnOMZ2cnChZsmSs9hIlSvzneNbV1RUPD49YDxEx7Lq4iyo/VaHLsi5cD7lOyZwlWeu/lp9b/6yQgoiIpAsKKoikYdHR0LEjzJljhBQWLDBCCc8+Cy4uZlcnIiKSds2fP5+AgACGDBnC3r17KVu2LA0aNODq1avx7r9o0SIuX74c8zh48CCOjo60bt06Zp+TJ09So0YNihcvzoYNGzhw4ACDBg3CzU3TeIrEcXMvrKkKf3aDyCDIVgka/AkVRoNzZrOrExERSdVcXFyoUKECa9eujWmzWq2sXbuWqlWr/ud7FyxYQHh4OO3bt49zzEqVKnH06NFY7ceOHSN//vxJV7xIOnA1+CpdlnbB7yc/dl/aTWaXzIyuP5r9b+/n+YLPm12eiIhIitHSDyJp1IOQwuzZRkjh55+heXOzqxIREbEPo0ePpmvXrnTq1AmAiRMnsmLFCqZMmUK/fv3i7J/t/6YtmjdvHhkyZIgVVBgwYACNGzdm1KhRMW2+vr7J1AORNCryLhwYDMfGgc0Kzh5QdiQUfhscHM2uTkREJM0ICAigQ4cOVKxYkcqVKzN27FiCg4Njxrf+/v7kyZOHkSNHxnrf5MmTad68OdmzZ49zzN69e9OmTRtq1apF3bp1WbVqFcuWLWPDhg0p0SWRNC/KGsX3u79n0PpB3Am/A0CHsh34vN7neGf679lORERE7JGCCiJpUHQ0dOoEs2aBoyPMmwctWphdlYiIiH2IiIhgz5499O/fP6bNwcGBevXqsX379gQdY/LkybRt25aMGTMCxh1sK1asoE+fPjRo0IB9+/ZRsGBB+vfvT3MlDUUg9AqcmAjHv4Ow+zOX5G8L5UeD+zPm1iYiIpIGtWnThmvXrjF48GACAwMpV64cq1atwsvLC4Bz587h4BB7st2jR4+yZcsW1qxZE+8xW7RowcSJExk5ciQ9evSgWLFiLFy4kBo1aiR7f0TSuk1nN9FtZTf+vvo3AOWfKc+3jb6lqs9/z3IiIiJizxRUEEljoqPhzTdh5kwjpDB/PrRqZXZVIiIi9uP69etER0fHfIj7gJeXF0eOHHns+3ft2sXBgweZPHlyTNvVq1e5d+8en3/+OZ999hlffPEFq1atomXLlqxfv57atWvHOU54eDjh4eEx20FBQU/RK5FU6vbfcGQMnJkN1gijLZMvVJwAuRuYW5uIiEga161bN7p16xbva/HNglCsWDFsNtt/HvPNN9/kzTffTIryRNKFi0EX6f17b+YenAtANvdsjHh+BF3Kd8FRM4aJiEg6p6CCSBoSHQ2dO8OMGQ9nUlBIQUREJHWZPHkyZcqUoXLlyjFtVqsVgJdffplevXoBUK5cObZt28bEiRPjDSqMHDmSYcOGpUzRIinJZoVLvxkBhSsP184me2Uo1gvytQIHZ/PqExERERF5ShHREYzZPoZPN31KcGQwFiy8U/EdPq37KdkzxF1aRUREJD1SUEEkjbBaoWtXmD7dCCnMnQuvvGJ2VSIiIvYnR44cODo6cuXKlVjtV65cwdv7v9cNDQ4OZt68eXzyySdxjunk5ETJkiVjtZcoUYItW7bEe6z+/fsTEBAQsx0UFISPj09iuiKSukQFw+kZcPQbCDpqtFkcwKcVFPsAclQFi8XUEkVEREREntbqE6vpsaoHx24cA6CaTzXGNxpP+WfKm1yZiIhI6qKggkgaYLVCly4wdaoRUpg9G1q3NrsqERER++Ti4kKFChVYu3YtzZs3B4wZEdauXfvIqXMfWLBgAeHh4bRv3z7OMStVqsTRo0djtR87doz8+fPHeyxXV1dcXV2fvCMiqUXIRTj2LZz4ASJuGW3OHuDbFYp1h4zx/xkQEREREUkrTtw8wYpjK1hydAkbzmwAwCujF1+++CXtn22PRYFcERGROBRUEEnlHsykMHUqODgYIYU2bcyuSkRExL4FBATQoUMHKlasSOXKlRk7dizBwcF06tQJAH9/f/LkycPIkSNjvW/y5Mk0b96c7NnjTuXZu3dv2rRpQ61atahbty6rVq1i2bJl8a4PLGIXbvxpLO9w7mewRRltmQpBsZ5QqBM4Zza3PhERERGRJxQZHcmWc1tYfmw5K46v4OiNh6F0JwcnelTuwZA6Q/Bw9TCxShERkdRNQQWRVMxqhbffhilTFFIQERFJSW3atOHatWsMHjyYwMBAypUrx6pVq/Dy8gLg3LlzODg4xHrP0aNH2bJlC2vWrIn3mC1atGDixImMHDmSHj16UKxYMRYuXEiNGjWSvT8iKcYaDReWwNExcG3rw/ZctaBYL8jTDBwcTStPRERERORJXQ2+ym/Hf2PF8RWsPrmaoPCgmNecHJyoma8mTYs2pUXxFhTMWtDESkVERNIGi81ms5ldREoICgrC09OTO3fu4OGhFKOkflYrvPMOTJpkhBRmzoTXXze7KhERkdTNnsd89tw3sQORQXByMhwdB8FnjDaLE+RvC8V7QTatxysiIpIQ9j7ms/f+iX2x2WzsD9wfM2vCrou7sPHwckrODDlpXKQxTYs25cVCL+Lp5mlitSIiIqlDYsZ7mlFBJBWyWuHddx+GFGbMUEhBRERERFKhe6eNcMLJyRB112hzyQZF3oEi70OG3ObWJyIiIiKSCPci7rH21FqWH1vOyhMruXT3UqzXn/N+jqZFm9KkSBMq5amEg8XhEUcSERGRx1FQQSSVsVrhvffgxx+NkML06dCundlViYiIiIjcZ7MZyzocHWMs82CzGu0eJaD4B1CgPThlMLNCEREREZEEO3XrFCuOrWDF8RWsP7OeiOiImNcyOmekXqF6NCnShMZFGpPHI4+JlYqIiNgXBRVEUhGbDd5/H374ASwWI6TQvr3ZVYmIiIiIANZIOLcAjoyBm38+bPeubyzv8Ex90B1lIiIiIpLKRUZHsu38tpglHQ5fPxzr9YJZCsbMmlC7QG3cnNxMqlRERMS+Kaggkko8CClMnGiEFKZNU0hBRERERFKB8Jtw4gc49i2E3p/61sEVCr4BxT6ALKVMLU9ERERE5HGuh1znt+O/seL4CladWMWd8DsxrzlaHKmRr0ZMOKF4juJYLBYTqxUREUkfFFQQSQVsNujWDb7/3ggpTJ0K/v5mVyUiIiIi6VrQUTgyFk5Ph+hQo83NC4q8D0XeAbecppYnIiIiIvIoNpuNA1cOsOL4CpYfW86OCzuwYYt5Pbt7dhoXaUyTIk1oULgBWdyymFesiIhIOqWggojJbDbo3h2++84IKUyZAh06mF2ViIiIiKRLNhtcWWss73Bp5cP2LGWN5R3ytwVHV/PqExERERF5hJDIENaeWsuK4ytYcXwFF4IuxHq9rFfZmFkTKuepjKODo0mVioiICCioIGIqmw169oQJE4yQwuTJ0LGj2VWJiIiISLoTHQZn5sDRsXD77/uNFsjTDIp/ALnqGANWEREREZFU5MztM6w4ZgQT1p9ZT1hUWMxr7k7u1CtUjyZFmtC4SGN8PH1MrFRERET+n4IKIiax2eCDD2D8eOMz359+gk6dzK5KRERERNKV0Ctw/Hs48T2EXTXanDJCoU5QtAd4FDG3PhERERGRf4myRrH9/PaYJR0OXTsU6/X8nvlpUqQJTYs2pU6BOrg7u5tUqYiIiDyOggoiJrDZoFcvGDfO2J40Cd5809yaRERERCQduXXAmD3hzGywRhhtGXygaHco3AVcsppanoiIiIjIAzdCbrDqxCpWHF/BqhOruBV2K+Y1B4sD1X2qx4QTSuYsiUUzgYmIiKQJCiqIpDCbDQIC4JtvjO1Jk6BzZ3NrEhEREZF0wGaFSyvhyBi4su5he3Y/KN4LfFqCg7N59YmIiIiIADabjYNXD8bMmrD9wnasNmvM69ncs9GocCOaFGlCg8INyOaezcRqRURE5EkpqCCSgmw2+PBDGDvW2P7xR+jSxdSSRERERMTeRQXDqelw9Bu4e8xosziATyso1gtyVjW3PhERERGR+67cu0LTuU3589KfsdrL5CoTM2uCX14/nBx0aUNERCSt07/mIinEZoPevWHMGGP7hx+ga1dzaxIREREROxZyAY59Cyd+hIj70+M6e4BvVyjWHTLmN7c+EREREZF/CbwXyPPTn+fw9cO4ObnxfMHnaVqkKU2KNiGfZz6zyxMREZEkpqCCSAqw2aBPH/j6a2N74kR46y1zaxIRERERO3VzHxz+Es4tAFuU0ZbJF4r1hEIdwTmzqeWJiIiIiPy/y3cv8/yM5zly/Qh5PfKyvsN6CmcrbHZZIiIikowUVBBJZjYb9O0LX31lbH/3Hbz9trk1iYiIiIgdskbDoeFwcBg8WMM3V20o3gtyNwUHR3PrExERERGJx6W7l6g7vS7HbhzDx8OH9R3W45vN1+yyREREJJkpqCCSjGw26NcPvvzS2J4wAd5919yaRERERMQOhQbCtvZwZa2xna81lOwH2cqbW5eIiIiIyH+4GHSRutPrcvzmcfJ55mN9h/UUylrI7LJEREQkBTg8yZsmTJhAgQIFcHNzw8/Pj127dv3n/mPHjqVYsWK4u7vj4+NDr169CAsLi3n97t27fPDBB+TPnx93d3eqVavG7t27Yx2jY8eOWCyWWI+GDRs+SfkiKcJmg48/hlGjjO1vv4X33jO3JhERERGxQ4Fr4bdyRkjBKSNUnQE1flZIQURERERStQtBF6gzvQ7Hbx4nv2d+NnTYoJCCiIhIOpLoGRXmz59PQEAAEydOxM/Pj7Fjx9KgQQOOHj1Krly54uw/Z84c+vXrx5QpU6hWrRrHjh2LCR2MHj0agC5dunDw4EFmzpxJ7ty5mTVrFvXq1eOff/4hT548Mcdq2LAhU6dOjdl2dXV9kj6LJDubDQYMgM8/N7bHj4f33ze3JhERERGxM9ZoOPgJHPwUsEGWMlD9Z/AsbnZlIiIiIiL/6fyd89SdXpeTt05SIEsB1ndYT4EsBcwuS0RERFJQomdUGD16NF27dqVTp06ULFmSiRMnkiFDBqZMmRLv/tu2baN69eq8/vrrFChQgPr16/Paa6/FzMIQGhrKwoULGTVqFLVq1aJw4cIMHTqUwoUL8/3338c6lqurK97e3jGPrFmzPkGXRZKXzQYDB8LIkcb2uHHQrZu5NYmIiIiInQm5BOvqGUEFbODbFervVEhBRERERFK9c3fOUWd6HU7eOknBLAXZ0GGDQgoiIiLpUKKCChEREezZs4d69eo9PICDA/Xq1WP79u3xvqdatWrs2bMnJphw6tQpVq5cSePGjQGIiooiOjoaNze3WO9zd3dny5Ytsdo2bNhArly5KFasGO+++y43btxITPkiyc5mg0GDYMQIY/ubb6B7d3NrEhERERE7c3mNsdTD1Q3glAmqzQG/H8HJ3ezKRERERET+09nbZ6kzrQ6nbp2iUNZCbOy4kfxZ8ptdloiIiJggUUs/XL9+nejoaLy8vGK1e3l5ceTIkXjf8/rrr3P9+nVq1KiBzWYjKiqKd955h48//hiAzJkzU7VqVT799FNKlCiBl5cXc+fOZfv27RQuXDjmOA0bNqRly5YULFiQkydP8vHHH9OoUSO2b9+Oo6NjnPOGh4cTHh4esx0UFJSYrookms0GQ4bA8OHG9tix0KOHqSWJiIiIiD2xRsHfQ+HQCIylHspCjZ/Bo6jZlYmIiIiIPNaZ22eoO70uZ26fwTerLxs6biCvR16zyxIRERGTJHrph8TasGEDI0aM4LvvvmPv3r0sWrSIFStW8Omnn8bsM3PmTGw2G3ny5MHV1ZVx48bx2muv4eDwsLy2bdvy0ksvUaZMGZo3b87y5cvZvXs3GzZsiPe8I0eOxNPTM+bh4+OT3F2VdG7oUHjwaz16NPTsaWo5IiIiImJPQi7C2ufh0HDABoXfgQY7FFIQERERkTTh9K3T1J5WmzO3z1AkWxE2dtyokIKIiEg6l6igQo4cOXB0dOTKlSux2q9cuYK3t3e87xk0aBBvvPEGXbp0oUyZMrRo0YIRI0YwcuRIrFYrAL6+vmzcuJF79+5x/vx5du3aRWRkJIUKFXpkLYUKFSJHjhycOHEi3tf79+/PnTt3Yh7nz59PTFdFEmXoUPjkE+P5119Dr16mliMiIiIi9uTSKmOph2ubwSkzVJ8Plb8HR7fHvlVERERExGynbp2i9rTanLtzjqLZi7K+w3ryeOQxuywRERExWaKCCi4uLlSoUIG1a9fGtFmtVtauXUvVqlXjfU9ISEismRGAmKUabDZbrPaMGTPyzDPPcOvWLVavXs3LL7/8yFouXLjAjRs3eOaZZ+J93dXVFQ8Pj1gPkeQwbJjxAPjqKwgIMLceEREREbET1kjY3w82NILw65C1PDTaC/lfNbsyEREREZEEOXnzJLWn1eZ80HmKZS/Ghg4bFFIQERERAJwS+4aAgAA6dOhAxYoVqVy5MmPHjiU4OJhOnToB4O/vT548eRg5ciQAzZo1Y/To0Tz33HP4+flx4sQJBg0aRLNmzWICC6tXr8Zms1GsWDFOnDhB7969KV68eMwx7927x7Bhw2jVqhXe3t6cPHmSPn36ULhwYRo0aJBU3wuRRPv0U2M2BYAvv4QPPzS1HBERERGxF8HnYWtbuL7N2C7aDZ77Chxdza1LRERERCSBTtw8QZ1pdbh49yLFcxRnfYf1eGeKf2ZmERERSX8SHVRo06YN165dY/DgwQQGBlKuXDlWrVqFl5cXAOfOnYs1g8LAgQOxWCwMHDiQixcvkjNnTpo1a8bw4cNj9rlz5w79+/fnwoULZMuWjVatWjF8+HCcnZ0BYwaGAwcOMH36dG7fvk3u3LmpX78+n376Ka6u+qBOzPHZZzB4sPF81Cj46CNz6xERERERO3FxOWzvABE3wdkD/KZAvlZmVyUiIiIikmDHbxynzvQ6XLp7iZI5S7LOfx1embzMLktERERSEYvt/9dfsFNBQUF4enpy584dLQMhT234cBg40Hj+xRfQp4+59YiIiIjBnsd89tw3uc8aCfv7w5Gvje1sFaHGfMhUyNy6REREJMXY+5jP3vsnhqPXj1J3el0u37tMqZylWOu/ViEFERGRdCIx471Ez6ggkt6NGPEwpDBypEIKIiIiIpIEgs/CljZwY6exXawnlPtCSz2IiIiISJpy5PoR6k6vS+C9QErnKs1a/7XkypjL7LJEREQkFVJQQSQRPv8cBgwwno8YAf36mVuPiIiIiNiBC7/C9o4QeRucs0CVqeDT3NyaREREREQS6fC1w9SdXpcrwVcok6sMa/3XkjNjTrPLEhERkVRKQQWRBPriC+jf33g+fPjD5yIiIiIiTyQ6Avb3haNjje3sflB9HmQqYGZVIiIiIiKJ9s+1f6g7vS5Xg69S1qssf/j/QY4MOcwuS0RERFIxBRVEEmDUqIezJ3z2GXz8sbn1iIiIiEgad++0sdTDzd3GdvEPoewIcHQxty4RERERkUQ6ePUgz09/nmsh1yjnXY4/3viD7Bmym12WiIiIpHIKKog8xpdfQt++xvNPPnm49IOIiIiIyBM5vwh2vAmRd8AlK1SZDnmbmV2ViIiIiEii/X3lb16Y8QLXQq7xnPdz/OH/B9ncs5ldloiIiKQBCiqI/Ievv4Y+fYznw4bBoEHm1iMiIiIiaVh0OOz7CI59a2znqGos9ZAxn7l1iYiIiIg8gQNXDvDCjBe4HnKdCs9UYM0baxRSEBERkQRTUEHkEUaPho8+Mp4PHQqDB5tajoiIiIikZXdPwtY2cHOPsV2iD5T9DBycza1LREREROQJ7A/cT70Z9bgReoOKuSuypv0asrpnNbssERERSUMUVBCJx5gx8OGHxvMhQ4yHiIiIiMgTOfc/9u48Lspy///4e9gXFXNhUxS3tMW0NE1tsURROS7VKW3TyC3T6mSn0nKpPGrb158t5pJYdsq0zrEyIVxI+x7TpLQy+7qg4pICaiokJiBcvz8m5jiyyCByM/B6Ph7zmJt77vu63/c0M3zCz9zXJ9Km4VJeluRbX7rhfalRX6tTAQAAAOXyQ9oPivpnlI7/cVydGnXSyvtXqq5fXatjAQAAN0OjAnCeWbOkcePsy5Mm0aQAAACAcso/I20ZJ6XMsf/c8Eap20dSQGNrcwEAAADltCVti6Lej9KJMyfUuVFnrbx/pYL8gqyOBQAA3BCNCsA5Vq6UnnjCvjxxovTCC5LNZm0mAAAAuKGsFOmbu6UTP9p/vnKCdM2Lkgf/CwYAAAD3tPnwZkX9M0onz5xUl8ZdlHh/our41rE6FgAAcFP8lQz40++/SyNH2pdHj5ZefJEmBQAAAJTDvo+k5JHS2VOSbwOpywdSeLTVqQAAAIBy++7Qd+r1QS+dPHNSXSO66sv7vqRJAQAAXBQaFYA/PfecdOCAFBkpvfIKTQoAAABw0dk/pC1/k3bPt/8cfLPUdbEU0MjSWAAAAMDFSD6UrF7/7KXMnEx1i+imL+/7UrV9a1sdCwAAuDkaFQBJGzZIb71lX543T6pVy9o8AAAAcDNZO6X1d0snt0qySVc9J7WdwlQPAAAAcGvf/vqtoj+IVlZOlm5qcpPi742nSQEAAFQI/mqGGi8nRxo+XDJGevBBqVcvqxMBAADAraR+IH33sHQ2W/ILlrp+KIVGWZ0KAAAAuCgbD25U9AfR+j33d93c9GbF3xuvWj58wwsAAFQMGhVQ402bJm3fLoWESP/zP1anAQAAgNs4e1r6/lFp70L7zyG32psU/MOszQUAAABcpG8OfKPeH/bWqdxT6h7ZXSvuWaFAn0CrYwEAgGqERgXUaFu3SjNm2JffekuqV8/aPAAAAHATmf9nn+oh8xdJNvs0D1dNlDw8rU4GAAAAXJT1B9arz4d9dCr3lG5rdpu+uOcLBXgHWB0LAABUMzQqoMY6e1YaNsx+P3CgdOedVicCAACAW9j7nvTdGCn/tOQXKnVbbL+aAgAAAODm/rP/P+rzYR9l52WrR7MeWn7PcpoUAADAJUGjAmqs11+Xvv9eCgqSZs+WbDarEwEAAKBKO5stffeIlPq+/efQKKnLB5J/iLW5AAAAgArw9b6vFbM4Rtl52erZvKc+H/y5/L39rY4FAACqKRoVUCPt2SNNmmRffu01KTzc2jwAAACo4k5us0/1kLVdsnlIbV+QrpzAVA8AAACoFtbtW6eYxTE6nXdavVr00meDPqNJAQAAXFI0KqDGMUYaOVL64w/p1lvt0z8AAAAAxTJG2rtQ+v5RKf8PyT9c6rpYCrnF6mQAAABAhfgq9Sv9ZfFf9MfZP9S7ZW99OuhT+Xn5WR0LAABUczQqoMZZuFD66ivJ31965x2mfAAAAEAJ8k5J3z0s7fvQ/nNYtNTln5JfQ2tzAQAAABVkzd416vdRP505e0Z9W/XVv+/+N00KAACgUtCogBrl8GHpySfty1OnSi1aWJsHAAAAVdSJrdL6u6Tfd0k2T+maf0hXPm2f9gEAAACoBlbvWa3+S/rrzNkzimkVo3/f/W/5evlaHQsAANQQ/JUNNYYx0pgxUmamdP310uOPW50IAAAAVY4x0u750spO9iaFgMZSj3XSVeNpUgAAAG5l9uzZioyMlJ+fnzp37qzk5OQSt+3evbtsNluRW0xMTLHbP/zww7LZbJo1a9YlSo9LbeXulY4rKfS7vB9NCgAAoNLxlzbUGP/+t/TZZ5KXl7Rggf0eAAAAcMjLkjbcKyWPkgpypPC+Uu8fpOAbrU4GAADgkqVLl2rcuHGaMmWKtmzZonbt2ik6OlpHjhwpdvtly5YpLS3Ncdu2bZs8PT111113Fdn2008/1bfffqvw8PBLfRq4RBJ3J2rAkgHKyc/RgNYD9K+7/0WTAgAAqHQ0KqBGOH5cGjvWvjxhgnTNNdbmAQAAQBVz/Afpyw7S/iX2qR7avyLd8oXk18DqZAAAAC6bOXOmRowYodjYWF155ZWaO3euAgICtHDhwmK3r1evnkJDQx231atXKyAgoEijwqFDh/Too4/qww8/lLe3d2WcCipYQkqCo0nh9ja36+O7PpaPp4/VsQAAQA1EowJqhCeflDIypCuukJ57zuo0AAAAqFL2fyytukE6tVsKiJCi/iNd+RRTPQAAALeUm5urzZs3KyoqyrHOw8NDUVFR2rhxY5nGiIuL0+DBgxUYGOhYV1BQoAceeEBPPfWUrrrqqgrPjUtvxa4Vun3p7crNz9WdV9yppX9dSpMCAACwDBe/R7W3apX03nuSzWaf8sGXq5gBAACgkDHSlr9JBblSo37SDe9JvvWsTgUAAFBux44dU35+vkJCQpzWh4SEaMeOHRfcPzk5Wdu2bVNcXJzT+pdfflleXl567LHHypwlJydHOTk5jp+zsrLKvC8q1hc7v9CdH9+pvII8/fXKv2rxHYvl7clVMQAAgHX4ihCqtVOnpFGj7Mtjx0pdu1qbBwAAAFXMya3SH2mSZ4B048c0KQAAgBovLi5Obdu2VadOnRzrNm/erNdff13vvfeebDZbmceaMWOGgoKCHLeIiIhLERkX8PmOzx1NCndfdTdNCgAAoEqgUQHV2sSJ0r59UpMm0vTpVqcBAABAlZO20n4f0l3y9LM0CgAAQEVo0KCBPD09lZGR4bQ+IyNDoaGhpe6bnZ2tJUuWaNiwYU7r//Of/+jIkSNq0qSJvLy85OXlpf379+vJJ59UZGRkieNNmDBBmZmZjtvBgwfLfV4on0+3f6q/fvJX5RXkafDVg/XhHR/SpAAAAKoEGhVQbX37rfTGG/bl+fOlWrWszQMAAIAqqLBRISza2hwAAAAVxMfHRx06dFBSUpJjXUFBgZKSktSlS5dS9/3kk0+Uk5Oj+++/32n9Aw88oK1bt+rHH3903MLDw/XUU09p5cqVJY7n6+urOnXqON1QeZZtX6a7/3W3zhac1b1t79U/b/+nvDyYDRoAAFQNVCWolnJypGHD7FMODxkiRfN3ZwAAAJwv75R0dL19mUYFAABQjYwbN05Dhw5Vx44d1alTJ82aNUvZ2dmKjY2VJA0ZMkSNGjXSjBkznPaLi4vTwIEDVb9+faf19evXL7LO29tboaGhat269aU9GZTLv/7vXxr8r8HKN/m6/5r79d6A9+Tp4Wl1LAAAAAeuqIBqacYM6f/+T2rYUJo50+o0AADAHc2ePVuRkZHy8/NT586dlZycXOK23bt3l81mK3KLiYkpdvuHH35YNptNs2bNukTpUSZH1kkFuVJgpFT7cqvTAAAAVJhBgwbptdde0+TJk9W+fXv9+OOPSkxMVEhIiCTpwIEDSktLc9pn586dWr9+fZFpH+B+Pv7lY0eTwgPXPECTAgAAqJK4ogKqnW3bpOnT7ctvvSWd1+wNAABwQUuXLtW4ceM0d+5cde7cWbNmzVJ0dLR27typ4ODgItsvW7ZMubm5jp9/++03tWvXTnfddVeRbT/99FN9++23Cg8Pv6TngDI4d9oHm83aLAAAABVs7NixGjt2bLGPrVu3rsi61q1byxhT5vH37dtXzmS4lJZsW6L7l92vfJOvoe2GKq5/HE0KAACgSuKKCqhW8vPtUz7k5Un9+0vF/NsAAADABc2cOVMjRoxQbGysrrzySs2dO1cBAQFauHBhsdvXq1dPoaGhjtvq1asVEBBQpFHh0KFDevTRR/Xhhx/K29u7Mk4FpTm3UQEAAABwc4t/Xqz7lt2nfJOv2PaxNCkAAIAqjUYFVCtvvCElJ0t16khvv80X4wAAgOtyc3O1efNmRUVFOdZ5eHgoKipKGzduLNMYcXFxGjx4sAIDAx3rCgoK9MADD+ipp57SVVddVeG54aJTqdLvKZLNUwq5zeo0AAAAwEX5cOuHeuDTB1RgCjTs2mFa0H8BTQoAAKBKY+oHVBt790oTJ9qXX31VatTI2jwAAMA9HTt2TPn5+Y75ewuFhIRox44dF9w/OTlZ27ZtU1xcnNP6l19+WV5eXnrsscfKlCMnJ0c5OTmOn7Oyssq0H8qo8GoKDbpIPkHWZgEAAAAuwr//798a8tkQFZgCjbhuhOb+Za48bHxHEQAAVG1UK6gWjJFGjZJOn5a6d5eGD7c6EQAAqKni4uLUtm1bderUybFu8+bNev311/Xee+/JVsZLPs2YMUNBQUGOW0RExKWKXDOlJdrvw3pbmwMAAAC4SJPWTqJJAQAAuB0qFlQL770nrVkj+flJ77wjefDKBgAA5dSgQQN5enoqIyPDaX1GRoZCQ0NL3Tc7O1tLlizRsGHDnNb/5z//0ZEjR9SkSRN5eXnJy8tL+/fv15NPPqnIyMhix5owYYIyMzMdt4MHD17UeeEcBXlS+lf25bBoa7MAAAAAFyH1RKq2H9suT5unXun5Ck0KAADAbVC1wO2lpUnjxtmXX3xRatnS2jwAAMC9+fj4qEOHDkpKSnKsKygoUFJSkrp06VLqvp988olycnJ0//33O61/4IEHtHXrVv3444+OW3h4uJ566imtXLmy2LF8fX1Vp04dpxsqyLGN0tnfJd8GUr3rrE4DAAAAlFtCSoIkqVuTbqrrV9faMAAAAC7wsjoAcLEefVQ6eVLq0EF64gmr0wAAgOpg3LhxGjp0qDp27KhOnTpp1qxZys7OVmxsrCRpyJAhatSokWbMmOG0X1xcnAYOHKj69es7ra9fv36Rdd7e3goNDVXr1q0v7cmgqLQ/m0NCe0p84wwAAABuLD4lXpIU0yrG4iQAAACuoVEBbm3ZMunf/5a8vKS4OPs9AADAxRo0aJCOHj2qyZMnKz09Xe3bt1diYqJCQkIkSQcOHJDHeXNN7dy5U+vXr9eqVausiAxXHE6034f1tjYHAAAAcBFO553W2n1rJdGoAAAA3A//rAu3deKENGaMffmZZ6R27azNAwAAqpexY8dq7NixxT62bt26Iutat24tY0yZx9+3b185k+GinDkindhiXw7rZW0WAAAA4CJ8lfqVzpw9o6ZBTXVlwyutjgMAAOASrnMKt/X3v0vp6VLr1tLEiVanAQAAgFtIW22/r9tO8g+1NgsAAABwERJSEiRJfVv1lc1mszgNAACAa8rVqDB79mxFRkbKz89PnTt3VnJycqnbz5o1S61bt5a/v78iIiL0xBNP6MyZM47Hf//9d/3tb39T06ZN5e/vr65du+q7775zGsMYo8mTJyssLEz+/v6KiopSSkpKeeKjGkhKkhYulGw2+5QPfn5WJwIAAIBbSFtpvw+LtjYHAAAAcBGMMYpPiZfEtA8AAMA9udyosHTpUo0bN05TpkzRli1b1K5dO0VHR+vIkSPFbr948WKNHz9eU6ZM0fbt2xUXF6elS5fq2WefdWwzfPhwrV69Wv/85z/1888/q1evXoqKitKhQ4cc27zyyit64403NHfuXG3atEmBgYGKjo52anhAzZCdLY0YYV9+5BGpWzdr8wAAAMBNmAIpfZV9mUYFAAAAuLFfjv6iA5kH5Oflp1ub3Wp1HAAAAJe53Kgwc+ZMjRgxQrGxsbryyis1d+5cBQQEaOHChcVuv2HDBnXr1k333nuvIiMj1atXL91zzz2OqzD88ccf+ve//61XXnlFN998s1q2bKnnn39eLVu21Jw5cyTZu0NnzZqliRMnasCAAbrmmmv0/vvv6/Dhw/rss8/Kf/ZwS5MnS6mpUkSENGOG1WkAAADgNk78JJ3JkLwCpYZ0uwIAAMB9xe+yX03htma3KcA7wOI0AAAArnOpUSE3N1ebN29WVFTUfwfw8FBUVJQ2btxY7D5du3bV5s2bHY0Je/fuVUJCgvr27StJOnv2rPLz8+V33rX7/f39tX79eklSamqq0tPTnY4bFBSkzp07l3hcVE+bNkmzZtmX582Tate2NA4AAADcSeG0D8G3Sp6+1mYBAAAALgLTPgAAAHfn5crGx44dU35+vkJCQpzWh4SEaMeOHcXuc++99+rYsWO68cYbZYzR2bNn9fDDDzumfqhdu7a6dOmiqVOn6oorrlBISIg++ugjbdy4US1btpQkpaenO45z/nELHztfTk6OcnJyHD9nZWW5cqqognJzpeHDpYIC6f77pT59rE4EAAAAt1LYqMC0DwAAAHBjJ/44oQ0HN0iS+rbqa3EaAACA8nF56gdXrVu3TtOnT9fbb7+tLVu2aNmyZYqPj9fUqVMd2/zzn/+UMUaNGjWSr6+v3njjDd1zzz3y8Ch/vBkzZigoKMhxi4iIqIjTgYVeeknatk1q0ED6f//P6jQAAABwK3mnpGPf2JdpVAAAAIAbW7VnlfJNvq5seKUi60ZaHQcAAKBcXOoEaNCggTw9PZWRkeG0PiMjQ6GhocXuM2nSJD3wwAMaPny42rZtq9tvv13Tp0/XjBkzVFBQIElq0aKFvv76a506dUoHDx5UcnKy8vLy1Lx5c0lyjO3KcSdMmKDMzEzH7eDBg66cKqqYX36R/vEP+/Kbb9qbFQAAAIAyy1grFeRJgc2k2i2tTgMAAACUG9M+AACA6sClRgUfHx916NBBSUlJjnUFBQVKSkpSly5dit3n9OnTRa6M4OnpKUkyxjitDwwMVFhYmE6cOKGVK1dqwIABkqRmzZopNDTU6bhZWVnatGlTicf19fVVnTp1nG5wT/n59ikf8vKkv/xFGjTI6kQAAABwO2mJ9vvw3pLNZm0WAAAAoJzyC/L15e4vJdGoAAAA3JuXqzuMGzdOQ4cOVceOHdWpUyfNmjVL2dnZio2NlSQNGTJEjRo10owZMyRJ/fr108yZM3Xttdeqc+fO2r17tyZNmqR+/fo5GhZWrlwpY4xat26t3bt366mnnlKbNm0cY9psNv3tb3/TP/7xD7Vq1UrNmjXTpEmTFB4eroEDB1bQU4Gq6q23pG+/lWrXlubM4e/KAAAAKIe0lfZ7pn0AAACAG/vu8Hc6dvqYgnyD1DWiq9VxAAAAys3lRoVBgwbp6NGjmjx5stLT09W+fXslJiYqJCREknTgwAGnKyhMnDhRNptNEydO1KFDh9SwYUP169dP06ZNc2yTmZmpCRMm6Ndff1W9evV05513atq0afL29nZs8/TTTys7O1sjR47UyZMndeONNyoxMVF+fn4Xc/6o4vbtk5591r78yitS48aWxgEAAIA7+n2PdGqPZPOSQm61Og0AAABQbgkpCZKkXi16ydvT+wJbAwAAVF02c/78C9VUVlaWgoKClJmZyTQQbsIYKTpaWr1auvlmae1aycOlyUoAAEBNU51rvup8bpfcrrel78dIwTdLUV9bnQYAAKBE1b3mq+7nVxk6zO+gLWlb9N6A9zS0/VCr4wAAADhxpd7jn31RZb3/vr1JwddXeucdmhQAAABQTmmJ9vuw3tbmAAAAAC5C2u9p2pK2RTbZ1KdVH6vjAAAAXBT+6RdVUkaG9MQT9uUXXpAuv9zaPAAAAHBT+blSxlr7cli0tVkAAACAi1A47cP1ja5XcGCwxWkAAAAuDo0KqJIefVQ6cUK69lrpySetTgMAAAC3dWyDdPaU5NtQuqy91WkAAACAcotPiZckxbSKsTgJAADAxaNRAVXOZ59Jn3wieXpKcXGSl5fViQAAAOC20lba78N6STb+9wcAAADuKTc/V6v3rpYk9W3V1+I0AAAAF4+/1KFKOXlSeuQR+/LTT9uvqAAAAACUm6NRgWkfAAAA4L7+s/8/OpV7SiGBIbou7Dqr4wAAAFw0GhVQpTz1lJSWJl1+uTR5stVpAAAA4Nb+yJBO/GBfDu1lbRYAAADgIhRO+9C3VV95cKUwAABQDVDRoMr46itpwQL78oIFkp+ftXkAAADg5tJX2e8vu1byD7E2CwAAAHARChsVYlrFWJwEAACgYtCogCrh9GlpxAj78ujR0k03WZsHAAAA1QDTPgAAAKAa2H18t3b9tkteHl7q2aKn1XEAAAAqBI0KqBKmTJH27pUaN5ZeesnqNAAAAHB7pkBK+/OKCjQqAAAAwI0lpCRIkm5qcpPq+NaxOA0AAEDFoFEBlvvuO2nmTPvy3LlSHWptAAAAXKwTP0g5RyWvWlKDrlanAQAAAMqNaR8AAEB1RKMCLJWbKw0bJhUUSPfeK8VQawMAAKAiFE77EHKb5OljbRYAAACgnE7lntK6feskSTGX88dTAABQfdCoAEu98or0889S/frSrFlWpwEAAEC1UdiowLQPAAAAcGNJe5OUm5+r5pc1V+v6ra2OAwAAUGFoVIBltm+Xpk61L7/xhtSwobV5AAAAUE3kZUlHN9iXaVQAAACAGzt32gebzWZxGgAAgIpDowIskZ9vn/IhN1fq21e65x6rEwEAAKDayFgrmbNSrRZS7RZWpwEAAADKxRijhJQESVLfVn0tTgMAAFCxaFSAJd5+W9q4UapVS5o7V6IZGAAAABXmcKL9Pqy3tTkAAACAi7A1Y6sO/X5IAd4B6h7Z3eo4AAAAFYpGBVS6/fulCRPsyy+/LEVEWJsHAAAA1YgxUtpK+zLTPgAAAMCNFU770KNZD/l5+VmcBgAAoGLRqIBKZYw0apSUnS3deKP08MNWJwIAAEC18vtuKTtV8vCWQm61Og0AAABQboWNCjGtYixOAgAAUPFoVECl+uADaeVKyddXWrBA8uAVCAAAgIpUeDWFBt0k71rWZgEAAADK6bfTv+nbX7+VJPVt1dfiNAAAABWPfyZGpTlyRPrb3+zLU6ZIrVtbGgcAAADVUWGjQnhva3MAAAAAFyFxd6IKTIHaBrdVRBBz5wIAgOqHRgVUmscek44fl9q3l/7+d6vTAAAAoNrJz5EyvrIvh0VbmwUAAAC4CAm7EyQx7QMAAKi+aFRApVi+XFq6VPL0lOLiJG9vqxMBAACg2jn6jZR/WvILkepeY3UaAAAAoFzyC/KVuDtRkhRzOY0KAACgeqJRAZdcZqY0erR9+cknpeuuszYPAAAAqqnCaR9Ce0k2/lcHAAAA7unbX7/V8T+O6zK/y3RD4xusjgMAAHBJ8Nc7XHJPPy0dPiy1bCk9/7zVaQAAAFBtFTYqMO0DAAAA3Fh8SrwkqXfL3vLy8LI4DQAAwKVBowIuqXXrpPnz7csLFkj+/pbGAQAAQHX1R5p08idJNimsl9VpAAAAgHIrbFSIacW0DwAAoPqiUQGXzB9/SCNG2JdHjZJuucXaPAAAAKjG0lbZ7+tdJ/k1tDYLAAAAUE6/Zv2qrRlbZZNN0S25UhgAAKi+aFTAJfP889Lu3VKjRtLLL1udBgAAANUa0z4AAACgGkhISZAk3dD4BjUIaGBxGgAAgEuHRgVcEps3S6+9Zl+eM0cKCrI2DwAAAKoxUyClr7Yv06gAAAAAN8a0DwAAoKagUQEVLi9PGjZMKiiQBg+W+vWzOhEAAACqteNbpJxjkldtqUEXq9MAAAAA5XLm7Bmt2btGkhRzOY0KAACgeqNRARXu1Veln36S6tWTXn/d6jQAAACo9tIS7fehPSQPb2uzAAAAAOX09b6vdTrvtMJrh6tdSDur4wAAAFxSNCqgQu3YIb34on359del4GBr8wAAAKAGSFtpv2faBwAAALixhJQESVLfln1ls9ksTgMAAHBp0aiAClNQIA0fLuXkSH36SPfdZ3UiAAAAVHu5mdKxjfZlGhUAAADgpowxik+Jl8S0DwAAoGagUQEVZs4c6ZtvpFq17Ms0/QIAAOCSy/hKMvlS7VZSrWZWpwEAAADKZddvu7TnxB75ePooqnmU1XEAAAAuORoVUCEOHJDGj7cvz5ghNW1qbR4AAADUEI5pH3pbmwMAAAC4CIVXU7il6S2q5VPL4jQAAACXHo0KuGjGSA8/LJ06JXXrJj3yiNWJAAAAUCMYI6Ul2peZ9gEAAABuzDHtQyumfQAAADUDjQq4aIsXS19+Kfn4SAsWSB68qgAAAFAZft8lZe+XPHykkO5WpwEAAKhSZs+ercjISPn5+alz585KTk4ucdvu3bvLZrMVucXE2P/RPC8vT88884zatm2rwMBAhYeHa8iQITp8+HBlnU61lpWTpf/s/48kqW+rvhanAQAAqBz8kzIuSl6e9NRT9uXJk6U2bazNAwAAgBqkcNqHhjdKXoHWZgEAAKhCli5dqnHjxmnKlCnasmWL2rVrp+joaB05cqTY7ZctW6a0tDTHbdu2bfL09NRdd90lSTp9+rS2bNmiSZMmacuWLVq2bJl27typ/v37V+ZpVVtr9q5RXkGeWtVrpVb1W1kdBwAAoFJ4WR0A7m35ciktTQoJ+W/DAgAAAFApChsVmPYBAADAycyZMzVixAjFxsZKkubOnav4+HgtXLhQ48ePL7J9vXr1nH5esmSJAgICHI0KQUFBWr16tdM2b731ljp16qQDBw6oSZMml+hMaob4XUz7AAAAah6uqICLMn++/f6hh+xTPwAAAACVIv+MlLHWvhzW29osAAAAVUhubq42b96sqKgoxzoPDw9FRUVp48aNZRojLi5OgwcPVmBgyVetyszMlM1mU926dUvcJicnR1lZWU43OCswBUrYnSBJirmcRgUAAFBz0KiAcktNlVatsi8PH25tFgAAANQwR9dL+X9I/mFS3bZWpwEAAKgyjh07pvz8fIWEhDitDwkJUXp6+gX3T05O1rZt2zS8lD/4nTlzRs8884zuuece1alTp8TtZsyYoaCgIMctIiKi7CdSQ/yQ9oPST6Wrlk8t3dTkJqvjAAAAVBoaFVBu77xjv+/VS2re3NosAAAAqGEKp30I7SXZbNZmAQAAqEbi4uLUtm1bderUqdjH8/LydPfdd8sYozlz5pQ61oQJE5SZmem4HTx48FJEdmsJKfarKUQ1j5Kvl6/FaQAAACqPl9UB4J7y8qSFC+3LI0damwUAAAA1UGGjQli0tTkAAACqmAYNGsjT01MZGRlO6zMyMhQaGlrqvtnZ2VqyZIlefPHFYh8vbFLYv3+/vvrqq1KvpiBJvr6+8vXlH99LE58SL0mKacW0DwAAoGbhigool+XLpYwMKSRE6t/f6jQAAACoUU4flk7+LMkmhfa0Og0AAECV4uPjow4dOigpKcmxrqCgQElJSerSpUup+37yySfKycnR/fffX+SxwiaFlJQUrVmzRvXr16/w7DXN0eyjSj6ULEnq26qvxWkAAAAqF1dUQLnMn2+/f+ghydvb2iwAAACoYQqvplCvo+TXwNosAAAAVdC4ceM0dOhQdezYUZ06ddKsWbOUnZ2t2NhYSdKQIUPUqFEjzZgxw2m/uLg4DRw4sEgTQl5env76179qy5YtWrFihfLz85Weni5Jqlevnnx8fCrnxKqZL3d/KSOja0OvVXjtcKvjAAAAVCoaFeCy1FRp1Sr78vDh1mYBAABADcS0DwAAAKUaNGiQjh49qsmTJys9PV3t27dXYmKiQkJCJEkHDhyQh4fzxXZ37typ9evXa1XhH/7OcejQIS1fvlyS1L59e6fH1q5dq+7du1+S86jumPYBAADUZOWa+mH27NmKjIyUn5+fOnfurOTk5FK3nzVrllq3bi1/f39FREToiSee0JkzZxyP5+fna9KkSWrWrJn8/f3VokULTZ06VcYYxzYPPvigbDab0613797liY+L9M479vtevaTmza3NAgAAgBqmIF9KX21fplEBAACgRGPHjtX+/fuVk5OjTZs2qXPnzo7H1q1bp/fee89p+9atW8sYo549i06tFRkZKWNMsTeaFMrnbMFZrdxtb8Bl2gcAAFATudyosHTpUo0bN05TpkzRli1b1K5dO0VHR+vIkSPFbr948WKNHz9eU6ZM0fbt2xUXF6elS5fq2WefdWzz8ssva86cOXrrrbe0fft2vfzyy3rllVf05ptvOo3Vu3dvpaWlOW4fffSRq/FxkfLypIUL7csjR1qbBQAA4FJypTm3e/fuRZpqbTabYmLs34zKy8vTM888o7Zt2yowMFDh4eEaMmSIDh8+XFmnU30c3yzlHpe860gNOl94ewAAAKAK2nBwgzJzMtUgoIE6NepkdRwAAIBK53KjwsyZMzVixAjFxsbqyiuv1Ny5cxUQEKCFhf96fZ4NGzaoW7duuvfeexUZGalevXrpnnvucfpD74YNGzRgwADFxMQoMjJSf/3rX9WrV68ifwz29fVVaGio43bZZZe5Gh8X6YsvpIwMKSRE6t/f6jQAAACXhqvNucuWLXNqqN22bZs8PT111113SZJOnz6tLVu2aNKkSdqyZYuWLVumnTt3qj8FlevSEu33oVGSh7e1WQAAAIByit9ln/ahd8ve8vTwtDgNAABA5XOpUSE3N1ebN29WVFTUfwfw8FBUVJQ2btxY7D5du3bV5s2bHU0He/fuVUJCgvr27eu0TVJSknbt2iVJ+umnn7R+/Xr16dPHaax169YpODhYrVu31ujRo/Xbb7+VmDUnJ0dZWVlON1y8efPs9w89JHnzd2EAAFBNudqcW69ePaeG2tWrVysgIMDRqBAUFKTVq1fr7rvvVuvWrXXDDTforbfe0ubNm3XgwIHKPDX3l2a/PC7TPgAAAMCdxafYGxViWsVYnAQAAMAaXq5sfOzYMeXn5yskJMRpfUhIiHbs2FHsPvfee6+OHTumG2+8UcYYnT17Vg8//LDT1A/jx49XVlaW2rRpI09PT+Xn52vatGm67777HNv07t1bd9xxh5o1a6Y9e/bo2WefVZ8+fbRx40Z5ehbtOJ0xY4ZeeOEFV04PF5CaKq1aZV8ePtzaLAAAAJdKYXPuhAkTHOsu1Jx7vri4OA0ePFiBgYElbpOZmSmbzaa6detebOSaI/ek9Nsm+zKNCgAAAHBT+0/u1y9Hf5GHzUPRLahrAQBAzeTy1A+uWrdunaZPn663337bcZnb+Ph4TZ061bHNxx9/rA8//FCLFy/Wli1btGjRIr322mtatGiRY5vBgwerf//+atu2rQYOHKgVK1bou+++07p164o97oQJE5SZmem4HTx48FKfarX3zjv2+549pebNrc0CAABwqZTWnJuenn7B/ZOTk7Vt2zYNL6Wz88yZM3rmmWd0zz33qE6dOsVuwxXCipGeJJl8qU5rKbCp1WkAAACAcim8mkLXiK66zJ/pjQEAQM3k0hUVGjRoIE9PT2VkZDitz8jIUGhoaLH7TJo0SQ888IDjD7Vt27ZVdna2Ro4cqeeee04eHh566qmnNH78eA0ePNixzf79+zVjxgwNHTq02HGbN2+uBg0aaPfu3erRo0eRx319feXr6+vK6aEUeXlS4ZWOR42yNgsAAEBVFhcXp7Zt26pTp07FPp6Xl6e7775bxhjNmTOnxHG4QlgxHNM+9LY2BwAAAHARElISJDHtAwAAqNlcuqKCj4+POnTooKSkJMe6goICJSUlqUuXLsXuc/r0aXl4OB+mcKoGY0yp2xQUFJSY5ddff9Vvv/2msLAwV04B5fTFF1JGhhQSIvXvb3UaAACAS6c8zbmFsrOztWTJEg0bNqzYxwubFPbv36/Vq1eXeDUFiSuEFWGMlJZoX2baBwAAALipP/L+0FepX0miUQEAANRsLk/9MG7cOL3zzjtatGiRtm/frtGjRys7O1uxsbGSpCFDhjjN59uvXz/NmTNHS5YsUWpqqlavXq1JkyapX79+joaFfv36adq0aYqPj9e+ffv06aefaubMmbr99tslSadOndJTTz2lb7/9Vvv27VNSUpIGDBigli1bKjqaP1JWhnnz7PcPPSR5e1ubBQAA4FIqT3NuoU8++UQ5OTm6//77izxW2KSQkpKiNWvWqH79+qWO5evrqzp16jjdarSsHdLpg5KHrxR8i9VpAAAAgHJZu2+t/jj7hyLqROjq4KutjgMAAGAZl6Z+kKRBgwbp6NGjmjx5stLT09W+fXslJiY65vA9cOCA09URJk6cKJvNpokTJ+rQoUNq2LChozGh0JtvvqlJkybpkUce0ZEjRxQeHq5Ro0Zp8uTJkuxXV9i6dasWLVqkkydPKjw8XL169dLUqVOZ3qESpKZKq1bZl0uZahkAAKDaGDdunIYOHaqOHTuqU6dOmjVrVpHm3EaNGmnGjBlO+8XFxWngwIFFmhDy8vL017/+VVu2bNGKFSuUn5+v9PR0SVK9evXk4+NTOSfmzgqnfQi+SfIKsDYLAAAAUE7xu+Il2a+mYLPZLE4DAABgHZcbFSRp7NixGjt2bLGPrVu3zvkAXl6aMmWKpkyZUuJ4tWvX1qxZszRr1qxiH/f399fKlSvLExUVYMEC+33PnlLz5tZmAQAAqAyuNudK0s6dO7V+/XqtKuzwPMehQ4e0fPlySVL79u2dHlu7dq26d+9+Sc6jWilsVGDaBwAAALgpY4ziU+yNCn1b9bU4DQAAgLXK1aiAmiMvT1q40L48apS1WQAAACqTK825ktS6dWsZY4rdPjIyssTHUAZn/5COfG1fDuttbRYAAACgnLYf2679mfvl6+mr25rdZnUcAAAAS3lceBPUZF98IaWnSyEhUv/+VqcBAABAjXT0P1L+H5J/IynoKqvTAAAAAOVSOO3Drc1uVaBPoMVpAAAArEWjAko1b579PjZW8va2NgsAAABqKMe0D70k5vEFAACAmyqc9iGmVYzFSQAAAKxHowJKlJoqFU6xPGKEtVkAAABQgzkaFaKtzQEAAACU08kzJ7X+wHpJUt9WfS1OAwAAYD0aFVCiBQvs9z17Ss2bW5sFAAAANdTpX6XMXyTZpNAoq9MAAAAA5bJqzyrlm3y1adBGzS/jj60AAAA0KqBYeXnSwoX25VGjrM0CAACAGqzwagr1O0m+9a3NAgAAAJRTQkqCJKZ9AAAAKESjAor1xRdSeroUEiL17291GgAAANRYTPsAAAAAN1dgCvTl7i8l0agAAABQiEYFFGv+fPt9bKzk7W1tFgAAANRQBflS+hr7Mo0KAAAAcFPfH/5eR7KPqI5vHd3Y5Ear4wAAAFQJNCqgiNRUadUq+/KIEdZmAQAAQA12/Dsp94TkHWSf+gEAAABwQ/G74iVJvVr0krcn3woDAACQaFRAMRYskIyRevaUmje3Og0AAABqrMJpH0J7Sh5e1mYBAAAAyik+xd6o0LdlX4uTAAAAVB00KsBJXp60cKF9eeRIa7MAAACghjucaL9n2gcAAAC4qfRT6dqctlmS1KdVH4vTAAAAVB00KsDJF19I6elSSIg0YIDVaQAAAFBj5Z6Qjifbl2lUAAAAgJv6MuVLSVLH8I4KrRVqcRoAAICqg0YFOJk/334fGyt5M10aAAAArJK+RjIFUp0rpMAIq9MAAAAA5VI47UNMqxiLkwAAAFQtNCrAITVVWrXKvjxihLVZAAAAUMOlrbTfczUFAAAAuKnc/Fyt2mP/gyuNCgAAAM5oVIDDggWSMVLPnlLz5lanAQAAQI1ljHQ40b4c1tvaLAAAAEA5rT+wXr/n/q7gwGB1CO9gdRwAAIAqhUYFSJLy8qSFC+3LI0damwUAAAA1XOb/SX8ckjz9pOCbrU4DAAAAlEtCSoIkqU/LPvKw8ad4AACAc1EdQZL0xRdSeroUEiINGGB1GgAAANRohdM+NLxZ8vK3NgsAAABQTvEp8ZKY9gEAAKA4NCpAkjR/vv0+Nlby9rY2CwAAAGq4wkaFsGhrcwAAAADltPfEXu04tkNeHl7q1aKX1XEAAACqHBoVoNRUadUq+/Lw4dZmAQAAQA139g/p6P/al8N7W5sFAAAAKKf4XfarKdzY5EYF+QVZnAYAAKDqoVEBWrBAMkbq2VNq0cLqNAAAAKjRjnwt5Z+RAhpLda6wOg0AAABQLoXTPvRt2dfiJAAAAFUTjQo1XF6etHChfXnkSGuzAAAAAE7TPths1mYBAAAAyiE7N1vr9q2TJMVcHmNtGAAAgCqKRoUa7osvpPR0KSREGjDA6jQAAACo8c5tVAAAAADc0FepXyknP0eRdSN1RQOuEgYAAFAcGhVquPnz7fexsZK3t7VZAAAAUMNlH5Sytks2Dyk0yuo0AAAAQLkUTvsQ0ypGNq4SBgAAUCwaFWqw1FRp1Sr78vDh1mYBAAAAHFdTqN9Z8rnM2iwAAABAORhjnBoVAAAAUDwaFWqwBQskY6SePaUWLaxOAwAAgBovLdF+z7QPAAAAcFM/H/lZv2b9Kn8vf3WP7G51HAAAgCqLRoUaKi9PWrjQvjxypLVZAAAAABWcldLX2JdpVAAAAICbSkhJkCTd1uw2+Xv7W5wGAACg6qJRoYZasUJKT5dCQqQBA6xOAwAAgBrvt2QpL9M+5UO9661OAwAAAJQL0z4AAACUDY0KNdS8efb72FjJ29vaLAAAAIDSVtrvQ3tKHp7WZgEAAADK4fgfx7Xh4AZJUszlNCoAAACUhkaFGig1VVq1yr48fLi1WQAAAABJUlqi/Z5pHwAAAOCmVu5eqQJToKuDr1aToCZWxwEAAKjSaFSogRYskIyRevaUWrSwOg0AAABqvJzfpN++sy+H9bI2CwAAAFBOhdM+9G3Z1+IkAAAAVR+NCjVMXp60cKF9eeRIa7MAAAAAkqT0NZKMFHSVFNDY6jQAAACAy/IL8pW4236VMKZ9AAAAuDAaFWqYFSuk9HQpOFjq39/qNAAAAICktJX2e6Z9AAAAgJtKPpSs3/74TXX96qprRFer4wAAAFR5NCrUMPPm2e8fekjy8bE2CwAAACBjzmlU6G1tFgAAAKCcCqd9iG4RLS8PL4vTAAAAVH00KtQgqanSqlX25eHDrc0CAAAASJIyt0l/HJY8/aXgm6xOAwAAAJRLYaNCTCumfQAAACgLGhVqkLg4+xfWevaUWrSwOg0AAACg/15NIfgWydPP2iwAAABAORzKOqQf03+UTTb1bslVwgAAAMqCRoUaIi/P3qggSSNHWpsFAAAAcHBM+xBtbQ4AAACgnBJSEiRJnRp1UsPAhhanAQAAcA80KtQQK1ZI6elScLDUv7/VaQAAAABJZ09LR/5jXw7jm2cAAABwTwm77Y0KTPsAAABQdjQq1BDz5tnvH3pI8vGxNgsAAAAgSTrytVSQIwU0keq0tjoNAAAA4LKcszlavWe1JCnmchoVAAAAyopGhRogNVVatcq+PHy4tVkAAAAAh8OJ9vuwaMlmszYLAAAAUA7/u/9/lZ2XrbBaYbo29Fqr4wAAALgNGhVqgLg4yRgpKkpq0cLqNAAAAMCf0lfa78Oirc0BAAAAlFN8SrwkqU/LPrLRfAsAAFBmNCpUc3l59kYFSRo1ytosAAAAgEP2filrp2TzlEJ7WJ0GAAAAKJfCRgWmfQAAAHANjQrV3IoVUnq6FBws9e9vdRoAAADgT2l/Xk2hwQ2ST11LowAAAADlkfJbinYf3y1vD2/1bN7T6jgAAABuhUaFam7+fPv9Qw9JPj7WZgEAAAAcDifa70OZ9gEAAADuqfBqCjc3vVm1fWtbnAYAAMC9lKtRYfbs2YqMjJSfn586d+6s5OTkUrefNWuWWrduLX9/f0VEROiJJ57QmTNnHI/n5+dr0qRJatasmfz9/dWiRQtNnTpVxhjHNsYYTZ48WWFhYfL391dUVJRSUlLKE7/G2LdPWvnnF9WGD7c0CgAAAPBfBXlSRpJ9OYxGBQAAALgnx7QPrZj2AQAAwFUuNyosXbpU48aN05QpU7Rlyxa1a9dO0dHROnLkSLHbL168WOPHj9eUKVO0fft2xcXFaenSpXr22Wcd27z88suaM2eO3nrrLW3fvl0vv/yyXnnlFb355puObV555RW98cYbmjt3rjZt2qTAwEBFR0c7NTzA2YIFkjFSVJTUooXVaQAAAIA/Hdsk5WVJPvWkeh2sTgMAAFAtufJls+7du8tmsxW5xcT89x/g+SKZs99zftfX+76WJPVt1dfiNAAAAO7H5UaFmTNnasSIEYqNjdWVV16puXPnKiAgQAsXLix2+w0bNqhbt2669957FRkZqV69eumee+5xKow3bNigAQMGKCYmRpGRkfrrX/+qXr16ObYxxmjWrFmaOHGiBgwYoGuuuUbvv/++Dh8+rM8++6x8Z17N5eVJcXH25VGjrM0CAAAAOEn787JfoT0lD09rswAAAFRDrn7ZbNmyZUpLS3Pctm3bJk9PT911112ObfgimbM1e9coryBPLS5rocvrX251HAAAALfjUqNCbm6uNm/erKioqP8O4OGhqKgobdy4sdh9unbtqs2bNzuaDvbu3auEhAT17dvXaZukpCTt2rVLkvTTTz9p/fr16tOnjyQpNTVV6enpTscNCgpS586dSzxuTk6OsrKynG41yYoVUnq6FBws9e9vdRoAAADgHIWNCuG9rc0BAABQTbn6ZbN69eopNDTUcVu9erUCAgIcjQp8kayohJQESfZpH2w2m8VpAAAA3I+XKxsfO3ZM+fn5CgkJcVofEhKiHTt2FLvPvffeq2PHjunGG2+UMUZnz57Vww8/7DT1w/jx45WVlaU2bdrI09NT+fn5mjZtmu677z5JUnp6uuM45x+38LHzzZgxQy+88IIrp1etzJ9vv4+NlXx8rM0CAAAAOJw5Jh3/3r4c2svaLAAAANVQ4ZfNJkyY4Fh3oS+bnS8uLk6DBw9WYGCgpAt/kWzw4MEVexJVnDFGCbv/bFS4POYCWwMAAKA4Lk/94Kp169Zp+vTpevvtt7VlyxYtW7ZM8fHxmjp1qmObjz/+WB9++KEWL16sLVu2aNGiRXrttde0aNGich93woQJyszMdNwOHjxYEafjFvbtk1b++SW1ESMsjQIAAAA4S18tyUh120oB4VanAQAAqHZK+7JZSV/6OldycrK2bdum4cOHO9aV54tkUvW96u2P6T/q8O+HFeAdoFua3mJ1HAAAALfk0hUVGjRoIE9PT2VkZDitz8jIUGhoaLH7TJo0SQ888ICjsG3btq2ys7M1cuRIPffcc/Lw8NBTTz2l8ePHOzpv27Ztq/3792vGjBkaOnSoY+yMjAyFhYU5Hbd9+/bFHtfX11e+vr6unF61sWCBZIwUFSW1aGF1GgAAAOAchdM+hEVbmwMAAADFiouLU9u2bdWpU6eLHqu6XvU2PiVekhTVPEq+XjXzb9AAAAAXy6UrKvj4+KhDhw5KSkpyrCsoKFBSUpK6dOlS7D6nT5+Wh4fzYTw9PSXZL5FV2jYFBQWSpGbNmik0NNTpuFlZWdq0aVOJx62p8vKkwqnmRo2yNgsAAADgxJhzGhV6W5sFAACgmirPl80KZWdna8mSJRo2bJjT+nO/SObKmNX1qreFjQoxrZj2AQAAoLxcnvph3Lhxeuedd7Ro0SJt375do0ePVnZ2tmJjYyVJQ4YMcZr/rF+/fpozZ46WLFmi1NRUrV69WpMmTVK/fv0cDQv9+vXTtGnTFB8fr3379unTTz/VzJkzdfvtt0uSbDab/va3v+kf//iHli9frp9//llDhgxReHi4Bg4cWAFPQ/WxYoWUliYFB0v9+1udBgAAADjHya3SmXTJM0BqeKPVaQAAAKql8nzZrNAnn3yinJwc3X///U7ry/tFMl9fX9WpU8fp5u6OnT6mTb9ukiT1bdXX4jQAAADuy6WpHyRp0KBBOnr0qCZPnqz09HS1b99eiYmJjvnJDhw44HR1hIkTJ8pms2nixIk6dOiQGjZs6GhMKPTmm29q0qRJeuSRR3TkyBGFh4dr1KhRmjx5smObp59+2jFlxMmTJ3XjjTcqMTFRfn5+F3P+1c78+fb72FjJx8faLAAAAICTwqsphHSXPLlELgAAwKUybtw4DR06VB07dlSnTp00a9asIl82a9SokWbMmOG0X1xcnAYOHKj69es7rT/3i2StWrVSs2bNNGnSpBr5RbLE3YkyMmoX0k6N6zS2Og4AAIDbspnC+RequaysLAUFBSkzM7NadO4WZ98+qXlz+xV1d++WWrSwOhEAAEDlqs41X7U4t6QeUsZXUofXpdaPWZ0GAACgyqnImu+tt97Sq6++6viy2RtvvKHOnTtLkrp3767IyEi99957ju137typNm3aaNWqVerZs2eR8YwxmjJliubPn+/4Itnbb7+tyy+/3JLzs8o9/75HS7Yt0bM3PqtpPaZdeAcAAIAaxJV6j0aFamTiRGnaNCkqSlq92uo0AAAAla8613xuf25ns6V/1ZMKcqW/7JDqtLY6EQAAQJXj9jXfBbj7+Z0tOKuGrzbUyTMntT52vbo16WZ1JAAAgCrFlXrPo9RH4Tby8qSFC+3LI0damwUAAAAoImOdvUkhMFKqXfZv3QEAAABVxcaDG3XyzEnV86+nGxrfYHUcAAAAt0ajQjWxYoWUliYFB0sDBlidBgAAADhPWqL9PixastmszQIAAACUQ0JKgiSpd8ve8vTwtDgNAACAe6NRoZqYP99+Hxsr+fhYmwUAAAAoIm2l/T4s2tocAAAAQDnFp8RLkmJaxVicBAAAwP3RqFAN7Nsnrfzz774jRlgaBQAAACjqVKr0e4pk85RCbrM6DQAAAOCyA5kH9PORn+Vh81B0C5pvAQAALhaNCtXAggWSMVJUlNSihdVpAAAAqofZs2crMjJSfn5+6ty5s5KTk0vctnv37rLZbEVuMTH//aaVMUaTJ09WWFiY/P39FRUVpZSUlMo4FesVXk2hQVfJJ8jaLAAAAEA5FE77cEPjG1Q/oL7FaQAAANwfjQpuLi9PWrjQvjxypLVZAAAAqoulS5dq3LhxmjJlirZs2aJ27dopOjpaR44cKXb7ZcuWKS0tzXHbtm2bPD09dddddzm2eeWVV/TGG29o7ty52rRpkwIDAxUdHa0zZ85U1mlZh2kfAAAA4OaY9gEAAKBi0ajg5laskNLSpOBgacAAq9MAAABUDzNnztSIESMUGxurK6+8UnPnzlVAQIAWFnaInqdevXoKDQ113FavXq2AgABHo4IxRrNmzdLEiRM1YMAAXXPNNXr//fd1+PBhffbZZ5V4ZhYoyJPSk+zLNCoAAADADZ05e0ZJe+01LY0KAAAAFYNGBTc3f779PjZW8vGxNgsAAEB1kJubq82bNysqKsqxzsPDQ1FRUdq4cWOZxoiLi9PgwYMVGBgoSUpNTVV6errTmEFBQercuXOJY+bk5CgrK8vp5paObZTO/i75NpDqXWd1GgAAAMBl6/at0x9n/1DjOo11Tcg1VscBAACoFmhUcGP79kkr/7yK7ogRlkYBAACoNo4dO6b8/HyFhIQ4rQ8JCVF6evoF909OTta2bds0fPhwx7rC/VwZc8aMGQoKCnLcIiIiXD2VqqFw2ofQnpKN//0AAACA+4nfZZ/2oW/LvrLZbBanAQAAqB74S6EbW7BAMkaKipJatLA6DQAAACT71RTatm2rTp06XdQ4EyZMUGZmpuN28ODBCkpYyQobFcJ6W5sDAAAAKAdjjOJT/mxUaNXX4jQAAADVB40KbiovTyqcInnkSGuzAAAAVCcNGjSQp6enMjIynNZnZGQoNDS01H2zs7O1ZMkSDRs2zGl94X6ujOnr66s6deo43dzOmSPS8c325bBe1mYBAAAAymHHsR1KPZkqH08f9Wjew+o4AAAA1QaNCm4qPl5KS5OCg6UBA6xOAwAAUH34+PioQ4cOSkpKcqwrKChQUlKSunTpUuq+n3zyiXJycnT//fc7rW/WrJlCQ0OdxszKytKmTZsuOKZbS1ttv6/bTvIvvckDAAAAqIoSUhIkSd0ju6uWTy2L0wAAAFQfXlYHQPnMm2e/j42VfHyszQIAAFDdjBs3TkOHDlXHjh3VqVMnzZo1S9nZ2YqNjZUkDRkyRI0aNdKMGTOc9ouLi9PAgQNVv359p/U2m01/+9vf9I9//EOtWrVSs2bNNGnSJIWHh2vgwIGVdVqVzzHtQ7S1OQAAAIByKpz2IaZVjMVJAAAAqhcaFdzQvn3Syj//5jt8uKVRAAAAqqVBgwbp6NGjmjx5stLT09W+fXslJiYqJCREknTgwAF5eDhfnGznzp1av369Vq1aVeyYTz/9tLKzszVy5EidPHlSN954oxITE+Xn53fJz8cSpkBK//O5CO9tbRYAAACgHDLPZOo/B/4jSerbqq/FaQAAAKoXGhXc0IIFkjFSVJTUsqXVaQAAAKqnsWPHauzYscU+tm7duiLrWrduLWNMiePZbDa9+OKLevHFFysqYtV2cqt0JkPyCpQadLM6DQAAAOCy1XtX62zBWV1e/3K1rMcfYgEAACqSx4U3QVWSlyctXGhfHjnS2iwAAABAiQ4n2u+Db5U8masMAAAA7odpHwAAAC4dGhXcTHy8lJYmBQdLAwZYnQYAAAAoQdqfc5WFRVubAwAAACiHAlOghJQESTQqAAAAXAo0KriZefPs97Gxkg9fTAMAAEBVlHdKOvaNfZlGBQAAALihLWlbdCT7iGr71NZNTW+yOg4AAEC1Q6OCG9m3T1r55xfThg+3NAoAAABQsoy1UkGeVKu5VJu5fAEAAOB+4nfZp33o2aKnfJjKDAAAoMLRqOBG4uIkY6SoKKklf+8FAABAVZWWaL8Pi5ZsNmuzAAAAAOUQn2JvVOjbsq/FSQAAAKonGhXcRF6evVFBkkaOtDYLAAAAUKq0Py8DxrQPAAAAcEMZpzL03eHvJEl9W9GoAAAAcCnQqOAm4uOltDQpOFgaMMDqNAAAAEAJft8jndoj2bykkFutTgMAAAC47MvdX0qSrgu7TmG1wyxOAwAAUD3RqOAm5s2z38fGSj5MiQYAAICqqvBqCg27St51rM0CAAAAlENCSoIkKaZVjMVJAAAAqi8aFdzAvn3Syj//3jt8uKVRAAAAgNI5pn3obW0OAAAAoBzy8vO0co+9pqVRAQAA4NKhUcENxMVJxkhRUVLLllanAQAAAEqQnytlfGVfDou2NgsAAABQDt8c/EZZOVlqENBAHcM7Wh0HAACg2qJRoYrLy7M3KkjSyJHWZgEAAABKdWyDdPaU5NtQuqy91WkAAAAAl8Xvipck9WnZR54enhanAQAAqL5oVKji4uOltDQpOFgaMMDqNAAAAEApHNM+9JJs/K8GAAAA3E98ir1RgWkfAAAALi3+eljFzZ9vv4+NlXx8rM0CAAAAlMrRqNDb2hwAAABAOaSeSNX2Y9vlafNUdEumMgMAALiUaFSowvbtkxIT7cvDh1saBQAAACjdHxnSiR/sy2G9rM0CAAAAlENCSoIkqVuTbqrrV9faMAAAANUcjQpVWFycZIzUo4fUsqXVaQAAAIBSpK+y3192reQXbG0WAAAAoBwKp33o27KvxUkAAACqPxoVqqi8PHujgiSNGmVtFgAAAOCCHNM+cIlcAAAAuJ/Teae1dt9aSVLM5TEWpwEAAKj+aFSoouLjpbQ0KThYGjDA6jQAAABAKUyBlPbnFRVoVAAAAIAb+ir1K505e0ZNgproqoZXWR0HAACg2qNRoYqaP99+Hxsr+fhYmwUAAAAo1YkfpZyjklctqUFXq9MAAAAALktISZAkxbSKkc1mszgNAABA9UejQhW0b5+UmGhfHj7c0igAAADAhaX9WbyG3CZ50mULAAAA92KMUXxKvCR7owIAAAAuPRoVqqC4OMkYqUcPqWVLq9MAAAAAF5C20n7PtA8AAABwQ78c/UUHMg/Iz8tPtza71eo4AAAANQKNClXM2bP2RgVJGjXK2iwAAADABeVlSUc32JdpVAAAAIAbit9lv5rCrZG3KsA7wOI0AAAANQONClXMihVSWprUsKE0YIDVaQAAAIALyFgrmbNSrZZS7RZWpwEAAABcxrQPAAAAlY9GhSpm/nz7fWys5MP0vgAAAKjqmPYBAAAAbuzEHye04aD9CmExl9OoAAAAUFloVKhC9u2TEhPtyyNGWBoFAAAAuDBjpMN/FrA0KgAAAMANrdqzSvkmX1c2vFKRdSOtjgMAAFBj0KhQhcTF2f/W26OH1LKl1WkAAACAC/h9t5SdKnl4SyG3Wp0GAAAAcBnTPgAAAFiDRoUq4uxZe6OCJI0aZW0WAAAAoEwKp31o0E3yrmVtFgAAAMBF+QX5+nL3l5Kkvq36WpwGAACgZqFRoYpYsUJKS5MaNpQGDLA6DQAAAFAGhY0K4b2tzQEAAACUw3eHv9Ox08cU5BukbhHdrI4DAABQo5SrUWH27NmKjIyUn5+fOnfurOTk5FK3nzVrllq3bi1/f39FREToiSee0JkzZxyPR0ZGymazFbmNGTPGsU337t2LPP7www+XJ36VNH++/T42VvLxsTYLAAAAcEH5OdKRtfblsGhrswAAAADlkJCSIEnq1aKXvD29LU4DAABQs3i5usPSpUs1btw4zZ07V507d9asWbMUHR2tnTt3Kjg4uMj2ixcv1vjx47Vw4UJ17dpVu3bt0oMPPiibzaaZM2dKkr777jvl5+c79tm2bZt69uypu+66y2msESNG6MUXX3T8HBAQ4Gr8Kmn/fikx0b48YoS1WQAAAIAyOfqNdDZb8guR6l5jdRoAAADAZfEp8ZKkmFYxFicBAACoeVxuVJg5c6ZGjBih2NhYSdLcuXMVHx+vhQsXavz48UW237Bhg7p166Z7771Xkv3qCffcc482bdrk2KZhw4ZO+7z00ktq0aKFbrnlFqf1AQEBCg0NdTVylbdggWSM1KOH1LKl1WkAAACAMiic9iG0l2RjRjkAAAC4l7Tf07QlbYskqU+rPhanAQAAqHlc+otibm6uNm/erKioqP8O4OGhqKgobdy4sdh9unbtqs2bNzumh9i7d68SEhLUt2/fEo/xwQcf6KGHHpLNZnN67MMPP1SDBg109dVXa8KECTp9+nSJWXNycpSVleV0q4rOnpXi4uzLI0damwUAAAAos8JGhfDe1uYAAAAAyqFw2ofrw69XcGDRKwUDAADg0nLpigrHjh1Tfn6+QkJCnNaHhIRox44dxe5z77336tixY7rxxhtljNHZs2f18MMP69lnny12+88++0wnT57Ugw8+WGScpk2bKjw8XFu3btUzzzyjnTt3atmyZcWOM2PGDL3wwguunJ4lVqyQ0tKkhg2lgQOtTgMAAACUwR9p0smfJNmk0J5WpwEAAABcxrQPAAAA1nJ56gdXrVu3TtOnT9fbb7+tzp07a/fu3Xr88cc1depUTZo0qcj2cXFx6tOnj8LDw53WjzzncgNt27ZVWFiYevTooT179qhFixZFxpkwYYLGjRvn+DkrK0sREREVeGYVY/58+31srOTjY20WAAAAoEzSVtnv610n+TUsfVsAAACgisnNz9XqvaslSTGX06gAAABgBZcaFRo0aCBPT09lZGQ4rc/IyFBoaGix+0yaNEkPPPCAhg8fLsneZJCdna2RI0fqueeek4fHf2ef2L9/v9asWVPiVRLO1blzZ0nS7t27i21U8PX1la+vb5nPzQr790uJifblESOszQIAAACUWeG0D2HR1uYAAAAAyuE/+/+jU7mnFBIYouvCrrM6DgAAQI3kceFN/svHx0cdOnRQUlKSY11BQYGSkpLUpUuXYvc5ffq0UzOCJHl6ekqSjDFO6999910FBwcrJubCXaw//vijJCksLMyVU6hSFiyQjJF69JBatrQ6DQAAAFAGpkBKt3/7jEYFAAAAuKPCaR/6tuorD5tLfyIHAABABXF56odx48Zp6NCh6tixozp16qRZs2YpOztbsbGxkqQhQ4aoUaNGmjFjhiSpX79+mjlzpq699lrH1A+TJk1Sv379HA0Lkr3h4d1339XQoUPl5eUca8+ePVq8eLH69u2r+vXra+vWrXriiSd0880365prrrmY87fM2bNSXJx9+ZxZLQAAAICq7fgWKeeY5FVbalB8szIAAABQlZ3bqAAAAABruNyoMGjQIB09elSTJ09Wenq62rdvr8TERIWEhEiSDhw44HQFhYkTJ8pms2nixIk6dOiQGjZsqH79+mnatGlO465Zs0YHDhzQQw89VOSYPj4+WrNmjaMpIiIiQnfeeacmTpzoavwqIz5eSkuTGjaUBg60Og0AAABQRoXTPoT2kDy8rc0CAAAAuGj38d3a9dsueXl4qWfznlbHAQAAqLFcblSQpLFjx2rs2LHFPrZu3TrnA3h5acqUKZoyZUqpY/bq1avIVBCFIiIi9PXXX5cnapU1b579PjZW8vGxNgsAAABQZmmJ9numfQAAAIAbSkhJkCTd1OQmBfkFWZwGAACg5mICLgvs3y8l/vn33REjrM0CAAAAlFlupnRso32ZRgUAAAC4ocJpH2JaxVicBAAAoGajUcECCxZIxkg9ekgtW1qdBgAAACijjK8kky/VbiXVamZ1GgAAAMAlp3JPad2+dZKkmMtpVAAAALASjQqV7OxZKS7OvjxypLVZAAAAAJekrbTfh/W2NgcAAAAuaPbs2YqMjJSfn586d+6s5OTkUrc/efKkxowZo7CwMPn6+uryyy9XQkKC4/H8/HxNmjRJzZo1k7+/v1q0aKGpU6eWOJ1vVZS0N0m5+blqVreZWtdvbXUcAACAGs3L6gA1TXy8lJYmNWwoDRxodRoAAACgjIyR0v6cv4xpHwAAAKq0pUuXaty4cZo7d646d+6sWbNmKTo6Wjt37lRwcHCR7XNzc9WzZ08FBwfrX//6lxo1aqT9+/erbt26jm1efvllzZkzR4sWLdJVV12l77//XrGxsQoKCtJjjz1WiWdXfudO+2Cz2SxOAwAAULPRqFDJ5s2z38fGSj4+1mYBAAAAyuz3XVL2fsnDRwrpbnUaAAAAlGLmzJkaMWKEYmNjJUlz585VfHy8Fi5cqPHjxxfZfuHChTp+/Lg2bNggb29vSVJkZKTTNhs2bNCAAQMUExPjePyjjz664JUaqgpjjBJS7FeIYNoHAAAA6zH1QyXav19K/PNLaCNGWJsFAAAAcEnhtA8Nb5S8Aq3NAgAAgBLl5uZq8+bNioqKcqzz8PBQVFSUNm7cWOw+y5cvV5cuXTRmzBiFhITo6quv1vTp05Wfn+/YpmvXrkpKStKuXbskST/99JPWr1+vPn36lJglJydHWVlZTjerbM3YqkO/H1KAd4C6R3a3LAcAAADsuKJCJYqLs18xt0cPqWVLq9MAAAAALihsVAjrbW0OAAAAlOrYsWPKz89XSEiI0/qQkBDt2LGj2H327t2rr776Svfdd58SEhK0e/duPfLII8rLy9OUKVMkSePHj1dWVpbatGkjT09P5efna9q0abrvvvtKzDJjxgy98MILFXdyF6Fw2ocezXrIz8vP4jQAAADgigqV5OxZe6OCJI0caW0WAAAAwCX5Z6SMdfblsGhLowAAAKDiFRQUKDg4WPPnz1eHDh00aNAgPffcc5o7d65jm48//lgffvihFi9erC1btmjRokV67bXXtGjRohLHnTBhgjIzMx23gwcPVsbpFKuwUaFvq76WZQAAAMB/cUWFShIfLx0+LDVsKA0caHUaAAAAwAVH10v5pyX/MKluW6vTAAAAoBQNGjSQp6enMjIynNZnZGQoNDS02H3CwsLk7e0tT09Px7orrrhC6enpys3NlY+Pj5566imNHz9egwcPliS1bdtW+/fv14wZMzR06NBix/X19ZWvr28FnVn5/Xb6N33767eSaFQAAACoKriiQiWZN89+Hxsr+fhYmwUAAABwSeG0D6G9JJvN2iwAAAAolY+Pjzp06KCkpCTHuoKCAiUlJalLly7F7tOtWzft3r1bBQUFjnW7du1SWFiYfP78Y+bp06fl4eH852RPT0+nfaqqxN2JKjAFahvcVk2CmlgdBwAAAKJRoVLs3y8lJtqXR4ywNgsAAADgssJGBaZ9AAAAcAvjxo3TO++8o0WLFmn79u0aPXq0srOzFRsbK0kaMmSIJkyY4Nh+9OjROn78uB5//HHt2rVL8fHxmj59usaMGePYpl+/fpo2bZri4+O1b98+ffrpp5o5c6Zuv/32Sj8/VyXsTpAkxbSKsTgJAAAACjH1QyWIi5OMkXr0kFq2tDoNAAAA4ILTh6WTP0uySaE9rU4DAACAMhg0aJCOHj2qyZMnKz09Xe3bt1diYqJCQkIkSQcOHHC6OkJERIRWrlypJ554Qtdcc40aNWqkxx9/XM8884xjmzfffFOTJk3SI488oiNHjig8PFyjRo3S5MmTK/38XJFfkK/E3fZvkcVcTqMCAABAVWEzxhirQ1SGrKwsBQUFKTMzU3Xq1Km04549KzVtKh0+LC1dKt19d6UdGgAAoMaxquarDJad2973pG9jpXrXS72TK++4AAAANVB1rmcla87vmwPf6MZ3b9RlfpfpyFNH5OXBd/cAAAAuFVfqPaZ+uMTi4+1NCg0bSgMHWp0GAAAAcNHhP+cwY9oHAAAAuKH4lHhJUnTLaJoUAAAAqhAaFS6x+fPt97Gxko+PtVkAAAAAlxTkS+mr7cs0KgAAAMANFTYqxLRi2gcAAICqhEaFS2j/funLL+3LI0ZYmwUAAABw2fHNUu5xyTtIanCD1WkAAAAAl/ya9au2ZmyVTTb1btnb6jgAAAA4B40Kl1BcnGSM1KOH1LKl1WkAAAAAF6WttN+H9pC4TC4AAADcTEJKgiTphsY3qEFAA4vTAAAA4Fw0Klwi+fnSwoX25ZEjrc0CAAAAlEtaov2eaR8AAADghgqnfejbqq/FSQAAAHA+vhZ1iXh6SmvWSO+/Lw0caHUaAAAAoByun2NvVgjnD7sAAABwP6/1fE3dm3anUQEAAKAK4ooKl1CbNtL06ZKPj9VJAAAA4KrZs2crMjJSfn5+6ty5s5KTk0vd/uTJkxozZozCwsLk6+uryy+/XAkJCY7H8/PzNWnSJDVr1kz+/v5q0aKFpk6dKmPMpT6V8rvsGunKp6WAxlYnAQAAAFzWqn4rPdHlCbVu0NrqKAAAADgPV1QAAAAAzrN06VKNGzdOc+fOVefOnTVr1ixFR0dr586dCg4OLrJ9bm6uevbsqeDgYP3rX/9So0aNtH//ftWtW9exzcsvv6w5c+Zo0aJFuuqqq/T9998rNjZWQUFBeuyxxyrx7AAAAAAAAADAWjQqAAAAAOeZOXOmRowYodjYWEnS3LlzFR8fr4ULF2r8+PFFtl+4cKGOHz+uDRs2yNvbW5IUGRnptM2GDRs0YMAAxcTEOB7/6KOPLnilBgAAAAAAAACobpj6AQAAADhHbm6uNm/erKioKMc6Dw8PRUVFaePGjcXus3z5cnXp0kVjxoxRSEiIrr76ak2fPl35+fmObbp27aqkpCTt2rVLkvTTTz9p/fr16tOnT7Fj5uTkKCsry+kGAAAAAAAAANUBV1QAAAAAznHs2DHl5+crJCTEaX1ISIh27NhR7D579+7VV199pfvuu08JCQnavXu3HnnkEeXl5WnKlCmSpPHjxysrK0tt2rSRp6en8vPzNW3aNN13333Fjjljxgy98MILFXtyAAAAAAAAAFAFcEUFAAAA4CIVFBQoODhY8+fPV4cOHTRo0CA999xzmjt3rmObjz/+WB9++KEWL16sLVu2aNGiRXrttde0aNGiYsecMGGCMjMzHbeDBw9W1ukAAAAAAAAAwCXFFRUAAACAczRo0ECenp7KyMhwWp+RkaHQ0NBi9wkLC5O3t7c8PT0d66644gqlp6crNzdXPj4+euqppzR+/HgNHjxYktS2bVvt379fM2bM0NChQ4uM6evrK19f3wo8MwAAAAAAAACoGriiAgAAAHAOHx8fdejQQUlJSY51BQUFSkpKUpcuXYrdp1u3btq9e7cKCgoc63bt2qWwsDD5+PhIkk6fPi0PD+fy29PT02kfAAAAAAAAAKgJaFQAAAAAzjNu3Di98847WrRokbZv367Ro0crOztbsbGxkqQhQ4ZowoQJju1Hjx6t48eP6/HHH9euXbsUHx+v6dOna8yYMY5t+vXrp2nTpik+Pl779u3Tp59+qpkzZ+r222+v9PMDAAAAAAAAACsx9QMAAABwnkGDBuno0aOaPHmy0tPT1b59eyUmJiokJESSdODAAaerI0RERGjlypV64okndM0116hRo0Z6/PHH9cwzzzi2efPNNzVp0iQ98sgjOnLkiMLDwzVq1ChNnjy50s8PAAAAAAAAAKxkM8YYq0NUhqysLAUFBSkzM1N16tSxOg4AAAAugepc81XncwMAAIBdda/5qvv5AQAA1HSu1HtM/QAAAAAAAAAAAAAAACoNjQoAAAAAAAAAAAAAAKDS0KgAAAAAAAAAAAAAAAAqDY0KAAAAAAAAAAAAAACg0tCoAAAAAAAAAAAAAAAAKo2X1QEqizFGkpSVlWVxEgAAAFwqhbVeYe1XnVDPAgAAVH/VuZ6VqGkBAACqO1fq2RrTqPD7779LkiIiIixOAgAAgEvt999/V1BQkNUxKhT1LAAAQM1RHetZiZoWAACgpihLPWsz1bU99zwFBQU6fPiwateuLZvNVinHzMrKUkREhA4ePKg6depUyjGtUN3O053Px12yV9WcVSmXlVkq89gVcaxLnbeix68q41WVHO6UrarmqsrZrPgsM8bo999/V3h4uDw8qtcsZ9Szl051O093Ph93yV5Vc1alXNSz1oxTWWNXhdqjKmRwt2xVNVdVzkY9W/Equ6atSr8bL6Xqdp7ufD7ukr2q5qxKuahnrRmnssauCrVHVcjgbtmqaq6qnK2q17M15ooKHh4eaty4sSXHrlOnjuW/VCtDdTtPdz4fd8leVXNWpVxWZqnMY1fEsS513ooev6qMV1VyXOqxKnK8qpqroseqyPEq+7OsOn7zTKKerQzV7Tzd+XzcJXtVzVmVclHPWjNOZY1dFWqPqpChMsaqyPGqaq6KHqsix6OerThW1bRV6XfjpVTdztOdz8ddslfVnFUpF/WsNeNU1thVofaoChkqY6yKHK+q5qrosSpyvKpaz1a/tlwAAAAAAAAAAAAAAFBl0agAAAAAAAAAAAAAAAAqDY0Kl5Cvr6+mTJkiX19fq6NcUtXtPN35fNwle1XNWZVyWZmlMo9dEce61HkrevyqMl5VyXGpx6rI8apqrooeqyLHq0qfqyifmvLfsLqdpzufj7tkr6o5q1Iu6llrxqmssatC7VEVMlTGWBU5XlXNVdFjVeR4VelzFeVTU/4bVrfzdOfzcZfsVTVnVcpFPWvNOJU1dlWoPapChsoYqyLHq6q5KnqsihyvKn2uFsdmjDFWhwAAAAAAAAAAAAAAADUDV1QAAAAAAAAAAAAAAACVhkYFAAAAAAAAAAAAAABQaWhUAAAAAAAAAAAAAAAAlYZGhXJ6/vnnZbPZnG5t2rQpdZ9PPvlEbdq0kZ+fn9q2bauEhIRKSlt2//u//6t+/fopPDxcNptNn332meOxvLw8PfPMM2rbtq0CAwMVHh6uIUOG6PDhw6WOWZ7nqqKUdj6SlJGRoQcffFDh4eEKCAhQ7969lZKSUuqY77zzjm666SZddtlluuyyyxQVFaXk5OQKzz5jxgxdf/31ql27toKDgzVw4EDt3LnTaZvu3bsXeW4ffvjhUsd9/vnn1aZNGwUGBjryb9q0qdw558yZo2uuuUZ16tRRnTp11KVLF3355ZeOx8+cOaMxY8aofv36qlWrlu68805lZGSUOuapU6c0duxYNW7cWP7+/rryyis1d+7cCs1Vnufu/O0Lb6+++mqZc7300kuy2Wz629/+5ljn6nNU3vdicccuZIxRnz59in2flOfY5x9r3759JT5/n3zyiWO/4j4virsFBgaW+fVkjNHkyZNVq1atUj+LRo0apRYtWsjf318NGzbUgAEDtGPHjlLHnjJlSpExmzdv7njc1ddZaef/6quvKj09XQ888IBCQ0MVGBio6667Tv/+97916NAh3X///apfv778/f3Vtm1bff/995Ls74W2bdvK19dXHh4e8vDw0LXXXlvqZ13heIGBgY59rrrqKiUnJ5fr9Vc43mWXXSYvLy95eXnJ19fXkfPBBx8scr69e/cudbxevXrJx8fHsf1rr73meLws79XIyMgyvdb8/PzK9Forabz77rtPx48f16OPPqrWrVvL399fTZo00WOPPabMzEyXxvL29tb111+vLl26uPS6Kmm8MWPGlPm9KUn5+fmaNGmSmjVrVuI+r7zyiiZPnqywsDD5+/srKirqgr9XJWn27NmKjIyUn5+fOnfufEl+r6Io6lnqWepZO+pZ6lnqWepZ6lnqWepZ91Uda1rqWepZV1HPUs+6Sz0bFhYmLy+vCq1pi8sbGBjo+ByhnnUej3qWerYkltWzBuUyZcoUc9VVV5m0tDTH7ejRoyVu/8033xhPT0/zyiuvmP/7v/8zEydONN7e3ubnn3+uxNQXlpCQYJ577jmzbNkyI8l8+umnjsdOnjxpoqKizNKlS82OHTvMxo0bTadOnUyHDh1KHdPV56oilXY+BQUF5oYbbjA33XSTSU5ONjt27DAjR440TZo0MadOnSpxzHvvvdfMnj3b/PDDD2b79u3mwQcfNEFBQebXX3+t0OzR0dHm3XffNdu2bTM//vij6du3b5Fst9xyixkxYoTTc5uZmVnquB9++KFZvXq12bNnj9m2bZsZNmyYqVOnjjly5Ei5ci5fvtzEx8ebXbt2mZ07d5pnn33WeHt7m23bthljjHn44YdNRESESUpKMt9//7254YYbTNeuXUsdc8SIEaZFixZm7dq1JjU11cybN894enqazz//vMJylee5O3fbtLQ0s3DhQmOz2cyePXvKlCk5OdlERkaaa665xjz++OOO9a4+R+V5L5Z07EIzZ840ffr0KfI+Kc+xizvW2bNnizx/L7zwgqlVq5b5/fffHfue/3nx008/mW3btjl+7t69u5Fk/vnPf5b59fTSSy+ZoKAgM2jQINOiRQvTq1cvExERYVJTU50+i+bNm2e+/vprk5qaajZv3mz69etnIiIizNmzZ0scu0ePHsbDw8O8++67JikpyfTq1cs0adLE/PHHH8YY119nU6ZMMa1btzY//fST4/b66687Xmc9e/Y0119/vdm0aZPZs2ePmTp1qrHZbCYsLMw8+OCDZtOmTWbv3r1m5cqVZvfu3cYY+3vhwQcfNLVr1zazZ882w4cPNzabzTRu3NiR81zHjx83TZs2Nbfccovx8vIyL7/8spk/f74ZNGiQqVu3rklJSXHp9Vc43j333GNCQ0PNnXfeaV5//XWzdu1aR86hQ4ea3r17Oz1Px48fL3W8qKgo8+CDD5o5c+YYSebtt992bFOW9+qRI0ectvnkk0+MJPPvf//bpKWlmb/85S9Gkvmf//mfMr3Wjhw5Yp577jlTu3Zt8+6775p58+YZSSY0NNR8//335o477jDLly83u3fvNklJSaZVq1bmzjvvLHGstLQ0s3HjRlO3bl1z1113GUnmgw8+MJ9//rnp2rWrS6+rI0eOmDfeeMP8/e9/N6+99pqRZCSZtWvXlvm9aYwx06ZNM/Xr1zcrVqwwycnJ5p133jGBgYFm6tSpjuf46aefNkFBQeazzz4zP/30k+nfv79p1qxZsa+1QkuWLDE+Pj5m4cKF5pdffjEjRowwdevWNRkZGSXug4pBPUs9Sz1rRz1LPUs9Sz1LPUs9Sz3rvqpjTUs9Sz3rKupZ6ll3qWc/++wz8/DDD5vatWs76tnzP49crWmnTJliQkJCHDVMUlKSiY6Odvz+pp6lnqWerdr1LI0K5TRlyhTTrl27Mm9/9913m5iYGKd1nTt3NqNGjargZBXnQr8QjbH/wpNk9u/fX+I2rj5Xl8r557Nz504jyVEYGWNMfn6+adiwoXnnnXfKPO7Zs2dN7dq1zaJFiyoybhFHjhwxkszXX3/tWHfLLbcUW9S4IjMz00gya9asuciE/3XZZZeZBQsWmJMnTxpvb2/zySefOB7bvn27kWQ2btxY4v5XXXWVefHFF53WXXfddea5556rkFzGVMxzN2DAAHPbbbeVadvff//dtGrVyqxevdrp2OV9js5X2nuxpGMX+uGHH0yjRo1MWlpamd73pR37Qsc6V/v27c1DDz3ktK60z4uTJ08am81mrr76ase6Cz1XBQUFJjQ01Lz66quOsU+ePGl8fX3NRx99VOp5/fTTT0aSo6AsbuzAwEATFhbmlPHcsV19nRV3/ue+zgIDA83777/v9Lifn59p2bJliWOe+xwUqlu3rvHy8ir2OXjmmWfMjTfeaDp16mTGjBnjWJ+fn2/Cw8PNjBkziuxT2uuvcLzC++IMHTrUDBgwoMRzKG68c13odVuW9+rjjz9uWrRoYQoKCszJkyeNh4eHCQkJMQUFBcYY115rheM1a9bM+Pj4FPs8f/zxx8bHx8fk5eWVmGnQoEHm/vvvd8pmzMV9fqWmphpJJiIiwjHe+Yp7bxpjTExMTJH1d9xxh7nvvvvMgAEDzK233lrktVaW95srrzVULOpZO+pZ6tniUM8WRT1bFPVsUdSzF0Y9Sz2LilXda1rq2bKhni2KerYo6tmiKrueLRz/6quvLlM9a8yFa9rJkycbLy+vEn9/U89Sz1LPVu16lqkfLkJKSorCw8PVvHlz3XfffTpw4ECJ227cuFFRUVFO66Kjo7Vx48ZLHfOSyszMlM1mU926dUvdzpXnqrLk5ORIkvz8/BzrPDw85Ovrq/Xr15d5nNOnTysvL0/16tWr8IznKrwEzfnH+fDDD9WgQQNdffXVmjBhgk6fPl3mMXNzczV//nwFBQWpXbt2F50xPz9fS5YsUXZ2trp06aLNmzcrLy/P6bXfpk0bNWnSpNTXfteuXbV8+XIdOnRIxhitXbtWu3btUq9evSokV6GLee4yMjIUHx+vYcOGlWn7MWPGKCYmpsjnQHmfo/OV9l4s6diS/fV77733avbs2QoNDS3z8Uo6dmnHOtfmzZv1448/Fvv8lfR5sWbNGhlj9Nhjjzm2vdBzlZqaqvT0dEeelJQUXXHFFbLZbHr++edL/CzKzs7Wu+++q2bNmikiIqLEsbOzs3XixAlH3kceeUTt2rVzyuPq6+zc87/zzju1YsUKx/PUtWtXLV26VMePH1dBQYGWLFminJwc3XjjjbrrrrsUHBysa6+9Vu+8806xz0Hhe+H06dNq3759sc/b8uXLde211yo5OVn//Oc/HeN5eHgoKiqq2H1Ke/0tX75cHTt21Ntvv63NmzfrsssuU+3atYvkXLdunYKDg9W6dWuNHj1av/32W7HPT+F4555vacryXs3NzdUHH3yghx56SDabTd9++60KCgo0YsQI2Ww2Sa691grHGz58uG644YYSn7M6derIy8ur2PEKCgoUHx+v5s2b6+2331ZaWppuuOEGx6X/yvv5lZubK0kaMGCA49zOVdp7s2vXrkpKStKuXbskST/99JPWr1+vrl27Kj4+Xv3793d6v0lSUFCQOnfuXOLzlpubq82bNzvtU9prDRWPepZ6VqKePRf1bMmoZ51Rz5aMepZ6VqKepZ6tXDW9pqWepZ49F/VsyahnnVlVz0rS3r17ZYzRqFGjSv08KktNe/LkSZ09e1Yvv/yyI29mZqbT72/qWepZ6tkqXM9e8laIaiohIcF8/PHH5qeffjKJiYmmS5cupkmTJiYrK6vY7b29vc3ixYud1s2ePdsEBwdXRtxy0QU6oP744w9z3XXXmXvvvbfUcVx9ri6V888nNzfXNGnSxNx1113m+PHjJicnx7z00ktGkunVq1eZxx09erRp3rx5qZdNuVj5+fkmJibGdOvWzWn9vHnzTGJiotm6dav54IMPTKNGjcztt99+wfG++OILExgYaGw2mwkPDzfJyckXlW/r1q0mMDDQeHp6mqCgIBMfH2+MsV/GzMfHp8j2119/vXn66adLHO/MmTNmyJAhRpLx8vIyPj4+5eqILimXMeV/7gq9/PLL5rLLLivTf/ePPvrIXH311U6XTy3stivvc3Su0t6LpR3bGGNGjhxphg0b5vj5Qu/70o59oWOda/To0eaKK64osr60z4vBgwcbSUWe89Keq2+++cZIMocPH3Ya+6abbjL169cv8lk0e/ZsExgYaCSZ1q1bl9ipe+7Y8+bNc8obEBDgeC25+jo7//ybNGliPDw8HJf+O3HihOnVq5fjvVGnTh3j7e1tfH19zYQJE8yWLVvMvHnzjJ+fn3nvvfeccvr7+zu9F+666y5z9913F8ng6+trfH19jSTHJbIKx3vqqadMp06dnLa/0O+CwvE8PT2Nt7e36d27t/H19TUPPvigY9yPPvrIfP7552br1q3m008/NVdccYW5/vrri72kW+F4556vJPPoo48We/yyvFeXLl1qPD09zaFDh4wxxjz66KNGkuPnQmV9rZ07XnHP89GjR02TJk3Ms88+W2Kmwg56Hx8f4+HhYVauXGlmzJhhbDabefLJJ8v9+fXmm28aSWblypXFPl7Se9MY+++iZ555xthsNuPl5WVsNpuZPn264zn+6quvHM/BuUp6rRljzKFDh4wks2HDBqf1xb3WUPGoZ6lnC1HPUs9eCPVsUdSzxaOepZ4tRD1LPVtZqntNSz1bNtSz1LMXQj1blBX17Lnj9+zZ09x8883Ffh65UtMWXkZ/zZo1TnkHDhxo7r77bupZQz1LPVu161kaFSrIiRMnTJ06dRyXLTqfuxXBxpT+CzE3N9f069fPXHvttRecN+p8F3quLpXizuf777837dq1M5KMp6eniY6ONn369DG9e/cu05gzZswwl112mfnpp58uQeL/evjhh03Tpk3NwYMHS90uKSmp1MsgFTp16pRJSUkxGzduNA899JCJjIy8qLlmcnJyTEpKivn+++/N+PHjTYMGDcwvv/xS7iLv1VdfNZdffrlZvny5+emnn8ybb75patWqZVavXl0huYpT1ueuUOvWrc3YsWMvuN2BAwdMcHCw02ukIgvh0t6LFzr2559/blq2bOk0z5ErhfC5x/7ll19KPda5Tp8+bYKCgsxrr712wWOc+3kRFhZmPDw8imzjSiFc6K677jIDBw4s8ll08uRJs2vXLvP111+bfv36meuuu67EAqq4sU+cOGG8vLxMx44di93H1ddZy5YtjY+PjyPj2LFjTadOncyaNWvMjz/+aJ5//nkjqcjlyB599FFzww03OOX85ptvnN4L0dHRxRYn3t7epkOHDk7FSeF45xcnZfld4O3tbbp06eK4P3e8c3Oea8+ePSVe8vDccQpJMpdffnmxxy/Le7VXr17mL3/5i+Pntm3bXtRr7dzxzi8CMzMzTadOnUzv3r1Nbm5uiZkKC8TQ0FCnbP369TODBw922taV19VNN91kJJkffvihyGMXem9+9NFHpnHjxuajjz4yW7duNe+//76pV6+eCQ0NNWPHji31/VZVC2E4o54tO+pZ11HPUs+WhHqWepZ6lnqWehYVqbrVtNSzF0Y9a0c9WzLq2ceL7FdV6tm777672M+ji6lpC8fr2LFjsb+/qWepZ6lniz9PGhWqgY4dO5rx48cX+1hERIT5f//v/zmtmzx5srnmmmsqIVn5lPQLMTc31wwcONBcc8015tixY+Uau7Tn6lIp7Rf8yZMnHR1xnTp1Mo888sgFx3v11VdNUFCQ+e677yoyZhFjxowxjRs3Nnv37r3gtqdOnTKSTGJiokvHaNmypZk+fXp5IxbRo0cPM3LkSMeH84kTJ5web9KkiZk5c2ax+54+fdp4e3ubFStWOK0fNmyYiY6OrpBcxXHlufvf//1fI8n8+OOPF9z2008/dfyPVuFNkrHZbMbT09OsWbPG5eeo0IXeixc69tixYx3L5z7u4eFhbrnlFpeOfaFjndt5+f777xtvb2/He+5COnbsaO677z4jyeXnqrCgOv+X/s0332wee+yxUj+LcnJyTEBAQJE/YFxo7Fq1apkOHToUu095XmdXXnmlGT9+vNm9e7eRnOdtNMY+B1qbNm2c1r399tsmPDy8xJw9evQwYWFh5rHHHity3CZNmpjY2Fjj6enp+MwsHG/IkCGmf//+xpiy/y5o0qSJGTZsmOP+3PHOzXm+Bg0amLlz55Y43rkkmXr16hXZtizv1X379hkPDw/z2WefOX622Wzlfq3Fx8c7jVf4WjPGmKysLNOlSxfTo0ePC3b75+TkGE9PT2Oz2RxjGWPM008/bbp27eq0bVlfV4XnWlIhfKH3ZuPGjc1bb73ltG7YsGGO5/hC77fSzvP838/nvtZQuahny456tuyoZ+2oZ4uinr3wc0U9Sz1LPVv0XKlncSHVqaalni0d9WzJqGf/i3q2atezheNXZE3bsWNHExERUezvb+pZ6lnq2eLP06p61kOoEKdOndKePXsUFhZW7ONdunRRUlKS07rVq1c7zcfkDvLy8nT33XcrJSVFa9asUf369V0e40LPlRWCgoLUsGFDpaSk6Pvvv9eAAQNK3f6VV17R1KlTlZiYqI4dO16STMYYjR07Vp9++qm++uorNWvW7IL7/Pjjj5Lk8nNbUFDgmBOuIhSO16FDB3l7ezu99nfu3KkDBw6U+NrPy8tTXl6ePDycP548PT1VUFBQIbmK48pzFxcXpw4dOpRp3rgePXro559/1o8//ui4dezYUffdd59j2dXnSCrbe/FCx37uuee0detWp8cl6f/9v/+nd99916VjX+hYnp6eTs9f//791bBhwws+f4WfFykpKWrfvr3Lz1WzZs0UGhrqtE9WVpY2bdqka6+9ttTPImNv5ivxNVPc2IcPH9apU6d09dVXF7uPq6+z9u3bKy0tTWFhYY45rs5/b9StW1cnTpxwWrdr1y41bdq0xJy5ubnKyMgo9nnr1q2bUlJS1KFDB8c+heMlJSWpS5cuLv0u6Natm3bu3Om4P3e8c3Oe69dff9Vvv/1W7PN07jjnKu71VJb36rvvvqvg4GDFxMQ4fm7YsGG5X2uzZs1yjFf4WuvSpYuysrLUq1cv+fj4aPny5U7zbxbHx8dHYWFh8vX1dWSTVOxzVtbX1bvvvlvqf6sLvTdPnz5d5PX3ww8/yNfXV+3atSv1/VbS8+bj4+P0WpPsn9WFrzVULurZsqOeLRvqWepZ6lnqWepZ6lnqWVS2mlDTUs/aUc+WbTzqWerZqlzPdunS5YKfR67WtKdOndLu3bt1+PDhYjNRz1LPUs8WPU9L69lL3gpRTT355JNm3bp1JjU11XzzzTcmKirKNGjQwNHl8sADDzh1gH3zzTfGy8vLvPbaa2b79u1mypQpxtvb2/z8889WnUKxfv/9d/PDDz+YH374wUgyM2fOND/88IPZv3+/yc3NNf379zeNGzc2P/74o0lLS3PccnJyHGPcdttt5s0333T8fKHnyqrzMcaYjz/+2Kxdu9bs2bPHfPbZZ6Zp06bmjjvucBrj/P+WL730kvHx8TH/+te/nJ6Dcy/PVBFGjx5tgoKCzLp165yOc/r0aWOMMbt37zYvvvii+f77701qaqr5/PPPTfPmzc3NN9/sNE7r1q3NsmXLjDH2rq4JEyaYjRs3mn379pnvv//exMbGGl9f3yJdgGU1fvx48/XXX5vU1FSzdetWM378eGOz2cyqVauMMfbLojVp0sR89dVX5vvvvzddunQpclmgczMaY78k1VVXXWXWrl1r9u7da959913j5+dn3n777QrJVZ7nrlBmZqYJCAgwc+bMcfWpcjq/cy+55epzVNb3YlmOfT4V09le3mMXd6yUlBRjs9nMl19+WezxL7vsMjN16lSnz4v69esbf39/M2fOnHK9nl566SVTt25dM3DgQLNw4ULTs2dPExYWZm677TbHZ9GePXvM9OnTzffff2/2799vvvnmG9OvXz9Tr149p8vunT/2TTfdZGrVqmXmz59v3n//fdOwYUPj4eFhDhw4UK7XWeHn5datW42vr69p06aNI2Nubq5p2bKluemmm8ymTZvM7t27HXOweXp6mmnTppmUlBRz5ZVXGh8fH/PBBx8YY+zvhVGjRpk6deqY119/3Tz00EOOS1ad2zVa+NmdnJxsvLy8zKBBg4yPj48ZNWqU8ff3N7feequpW7euOXjwoEu/CwrHGz16tPH09DR333238ff3N4888ogJCAgwCxYsMH//+9/Nxo0bTWpqqlmzZo257rrrTKtWrcyZM2dKHG/y5Mnm888/N9OnTzeSzH333ef0+X6h9+ptt91mXn/9ddOkSRPzzDPPGGPsc3wV/lye19r06dONzWYzd9xxh9m6dasZMGCAadasmcnIyDCdO3c2bdu2Nbt373Z6zs7tZj93vPz8fNOgQQPj4eFh5s+fb1JSUsybb75pPDw8zLBhw1z+/Dp69KgJDQ01f/3rX40ks2TJEvPDDz+YtLQ0Y8yF35utW7c2t956q2nUqJFZsWKFSU1NNR988IGRnOcNLXy/Fc5pV/gcFPdaK7RkyRLj6+tr3nvvPfN///d/ZuTIkaZu3bomPT292CyoONSz1LPUs3bUs66jnqWeLSkv9Sz1LPUs9Wxlq441LfUs9ayrqGddRz1rTT37+eefmyFDhphu3bqZxo0bm6+++srp86g8Ne2TTz5pRo4caWrXrm1eeuklc8MNNxgfHx/TpEkT88svv1DPUs9Sz1bxepZGhXIaNGiQCQsLMz4+PqZRo0Zm0KBBTnOP3HLLLWbo0KFO+3z88cfm8ssvNz4+Puaqq64y8fHxlZz6wtauXeu4fM+5t6FDh5rU1NRiH5Nk1q5d6xijadOmZsqUKY6fL/RcWXU+xhjz+uuvm8aNGxtvb2/TpEkTM3HixGJ/mZ/737Jp06bFjnnuOVeEkp7rd9991xhjn9/q5ptvNvXq1TO+vr6mZcuW5qmnnioyD9G5+/zxxx/m9ttvN+Hh4cbHx8eEhYWZ/v37m+Tk5HLnfOihh0zTpk2Nj4+PadiwoenRo4ejCC485iOPPGIuu+wyExAQYG6//XbHB29xGY0xJi0tzTz44IMmPDzc+Pn5mdatW5v/+Z//MQUFBRWSqzzPXaF58+YZf39/c/LkyTJnOd/5BaKrz1FZ34tlOfb5iiuEy3vs4o41YcIEExERYfLz80s8ft26dZ0+L/7xj384nvPyvJ4KCgrMpEmTjK+vr+NyZyEhIU6fRYcOHTJ9+vQxwcHBxtvb2zRu3Njce++9ZseOHaWOPWjQIFOrVi3HcxAcHOyYq688r7PCz0svLy8jydxxxx1On5e7du0yd9xxhwkODjYBAQHmmmuuMe+//7754osvzNVXX218fX2Nl5eX05xZDz30kGnSpInx8PAwNpvNeHh4mGuvvdbs3LnTKce5n92F43l5eRkvLy/j6elpOnXqZL799tty/S4oHM/b29uRsU2bNmb+/Pnm9OnTplevXqZhw4bG29vbNG3a1IwYMaJIEXT+eM2aNSv18/1C79WmTZua+++/30hyPBcrV650/Fye11piYqKRZOrXr298fX1Njx49zM6dO0v8XSTJpKamFjteYZZp06aZli1bGj8/P9OuXTvzzjvvlOvz68knnyz1d1dZ3ptvv/22efzxx02TJk2Mn5+fadCggfHy8nL6w1bh+y0kJMTpOSjpv2WhN9980zRp0sT4+Pg4Xmu49KhnqWepZ+2oZ11HPUs9W9KY1LPUs9Sz1LOVrTrWtNSz1LOuop51HfWsNfVsSEiI8fDwMD4+Psbb27vI51F5atrCzzdPT0/j4eFhPDw8TJcuXczOnTupZ6lnqWfdoJ61GWOMAAAAAAAAAAAAAAAAKoHHhTcBAAAAAAAAAAAAAACoGDQqAAAAAAAAAAAAAACASkOjAgAAAAAAAAAAAAAAqDQ0KgAAAAAAAAAAAAAAgEpDowIAAAAAAAAAAAAAAKg0NCoAAAAAAAAAAAAAAIBKQ6MCAAAAAAAAAAAAAACoNDQqAAAAAAAAAAAAAACASkOjAgDUcM8//7xCQkJks9n02WeflWmfdevWyWaz6eTJk5c0W1USGRmpWbNmWR0DAAAA56GeLRvqWQAAgKqJerZsqGeB6odGBQBVzoMPPiibzSabzSYfHx+1bNlSL774os6ePWt1tAtypZisCrZv364XXnhB8+bNU1pamvr06XPJjtW9e3f97W9/u2TjAwAAVBXUs5WHehYAAKDiUc9WHupZADWZl9UBAKA4vXv31rvvvqucnBwlJCRozJgx8vb21oQJE1weKz8/XzabTR4e9Gadb8+ePZKkAQMGyGazWZwGAACg+qCerRzUswAAAJcG9WzloJ4FUJPxWwFAleTr66vQ0FA1bdpUo0ePVlRUlJYvXy5JysnJ0d///nc1atRIgYGB6ty5s9atW+fY97333lPdunW1fPlyXXnllfL19dWBAweUk5OjZ555RhEREfL19VXLli0VFxfn2G/btm3q06ePatWqpZCQED3wwAM6duyY4/Hu3bvrscce09NPP6169eopNDRUzz//vOPxyMhISdLtt98um83m+HnPnj0aMGCAQkJCVKtWLV1//fVas2aN0/mmpaUpJiZG/v7+atasmRYvXlzkUlYnT57U8OHD1bBhQ9WpU0e33Xabfvrpp1Kfx59//lm33Xab/P39Vb9+fY0cOVKnTp2SZL+kWL9+/SRJHh4epRbCCQkJuvzyy+Xv769bb71V+/btc3r8t99+0z333KNGjRopICBAbdu21UcffeR4/MEHH9TXX3+t119/3dGNvW/fPuXn52vYsGFq1qyZ/P391bp1a73++uulnlPhf99zffbZZ075/397dx9TZf3/cfwVoAmIRU1NJo5NwZtGBs4xKCSDKdWYgjelJGRx05TMkkKpjGqzmVnZnelah25M07yphUZo4BQKDgx0JgMiBCKUJbZ1CO84n98fzDNP3Ijfr1+U/Z6Pv7g+n+v6XO/rOgxeZ3vvuo4cOaLp06fLy8tLw4YN05QpU1RaWuqYP3z4sMLDw+Xu7i5fX18tW7ZMbW1tjvmWlhbFxMQ4Po8tW7b0WhMAAMC/kWfJsz0hzwIAgIGAPEue7Ql5FsC1QqMCgAHB3d1d58+flySlpaXpp59+0rZt23T06FHNmzdP0dHRqqmpcez/zz//aO3atfr444/1yy+/aMSIEUpISNDWrVv17rvvqrKyUps2bdLQoUMldYbM+++/X0FBQSotLdX333+vU6dOaf78+U51fPrpp/L09FRxcbHeeOMNvfrqq8rLy5MkWa1WSZLFYlFzc7Nj22az6cEHH9SBAwdUXl6u6OhoxcTEqKGhwbFuQkKC/vjjDxUUFGjnzp3avHmzWlpanM49b948tbS0aN++fSorK1NwcLAiIyPV2tra7T1ra2vTzJkz5e3tLavVqh07dmj//v1KS0uTJKWnp8tisUjqDOLNzc3drtPY2Ki4uDjFxMSooqJCSUlJWrlypdM+Z8+e1ZQpU5STk6Njx44pJSVFixYtUklJiSRpw4YNCg0NVXJysuNcvr6+stvtGj16tHbs2KHjx49r9erVyszM1Pbt27utpa/i4+M1evRoWa1WlZWVaeXKlRo0aJCkzi8m0dHRmjNnjo4ePaqvvvpKhw8fdtwXqTO4NzY2Kj8/X19//bU+/PDDLp8HAADA1SDPkmevBnkWAADcaMiz5NmrQZ4F0CcGAG4wiYmJZtasWcYYY+x2u8nLyzM333yzSU9PN/X19cbV1dU0NTU5HRMZGWlWrVpljDHGYrEYSaaiosIxX1VVZSSZvLy8bs/52muvmRkzZjiNNTY2GkmmqqrKGGNMRESEuffee532mTp1qsnIyHBsSzK7d+++4jXeeeed5r333jPGGFNZWWkkGavV6pivqakxkszbb79tjDHm0KFDZtiwYebs2bNO64wdO9Zs2rSp23Ns3rzZeHt7G5vN5hjLyckxLi4u5uTJk8YYY3bv3m2u9K9g1apVZtKkSU5jGRkZRpI5c+ZMj8c99NBDZsWKFY7tiIgI8/TTT/d6LmOMWbp0qZkzZ06P8xaLxdxyyy1OY/++Di8vL5Odnd3t8U888YRJSUlxGjt06JBxcXEx7e3tjt+VkpISx/ylz+jS5wEAANAb8ix5ljwLAAAGMvIseZY8C6A/uP3POyEA4D/w3XffaejQobpw4YLsdrsWLlyorKwsFRQUqKOjQwEBAU77nzt3Trfffrtje/Dgwbrrrrsc2xUVFXJ1dVVERES35zty5Ijy8/MdHbyXq62tdZzv8jUladSoUVfs5LTZbMrKylJOTo6am5t18eJFtbe3Ozp2q6qq5ObmpuDgYMcx48aNk7e3t1N9NpvN6Rolqb293fEes3+rrKzU5MmT5enp6Ri75557ZLfbVVVVpZEjR/Za9+XrhISEOI2FhoY6bXd0dGjNmjXavn27mpqadP78eZ07d04eHh5XXP+DDz7QJ598ooaGBrW3t+v8+fO6++67+1RbT5599lklJSXp888/V1RUlObNm6exY8dK6ryXR48edXpcmDFGdrtddXV1qq6ulpubm6ZMmeKYnzBhQpfHmQEAAPSGPEue/W+QZwEAwPVGniXP/jfIswD6gkYFADek6dOna+PGjRo8eLB8fHzk5tb558pms8nV1VVlZWVydXV1OubyEOvu7u70Tix3d/dez2ez2RQTE6O1a9d2mRs1apTj50uPp7rkpptukt1u73Xt9PR05eXl6c0339S4cePk7u6uuXPnOh6V1hc2m02jRo1yetfbJTdCQFu3bp02bNigd955R4GBgfL09NTy5cuveI3btm1Tenq61q9fr9DQUHl5eWndunUqLi7u8RgXFxcZY5zGLly44LSdlZWlhQsXKicnR/v27dPLL7+sbdu2KTY2VjabTampqVq2bFmXtceMGaPq6uqruHIAAIDu9ZEJMAAABbVJREFUkWe71kee7USeBQAAAwF5tmt95NlO5FkA1wqNCgBuSJ6enho3blyX8aCgIHV0dKilpUXh4eF9Xi8wMFB2u10HDx5UVFRUl/ng4GDt3LlTfn5+jtD9nxg0aJA6OjqcxgoLC/XYY48pNjZWUmeoPXHihGN+/PjxunjxosrLyx1dor/++qvOnDnjVN/Jkyfl5uYmPz+/PtUyceJEZWdnq62tzdG1W1hYKBcXF40fP77P1zRx4kR9++23TmM///xzl2ucNWuWHn30UUmS3W5XdXW1Jk2a5Nhn8ODB3d6bsLAwLVmyxDHWUwfyJcOHD9fff//tdF0VFRVd9gsICFBAQICeeeYZLViwQBaLRbGxsQoODtbx48e7/f2SOrtzL168qLKyMk2dOlVSZ1f1X3/91WtdAAAAlyPPkmd7Qp4FAAADAXmWPNsT8iyAa8XlehcAAFcjICBA8fHxSkhI0K5du1RXV6eSkhK9/vrrysnJ6fE4Pz8/JSYm6vHHH9eePXtUV1engoICbd++XZK0dOlStba2asGCBbJaraqtrVVubq4WL17cJbz1xs/PTwcOHNDJkycdQdbf31+7du1SRUWFjhw5ooULFzp1+U6YMEFRUVFKSUlRSUmJysvLlZKS4tR1HBUVpdDQUM2ePVs//PCDTpw4oaKiIr3wwgsqLS3ttpb4+HgNGTJEiYmJOnbsmPLz8/XUU09p0aJFfX6smCQ9+eSTqqmp0XPPPaeqqip9+eWXys7OdtrH399feXl5KioqUmVlpVJTU3Xq1Kku96a4uFgnTpzQn3/+KbvdLn9/f5WWlio3N1fV1dV66aWXZLVae60nJCREHh4eyszMVG1tbZd62tvblZaWpoKCAtXX16uwsFBWq1UTJ06UJGVkZKioqEhpaWmqqKhQTU2NvvnmG6WlpUnq/GISHR2t1NRUFRcXq6ysTElJSVfs+gYAAOgL8ix5ljwLAAAGMvIseZY8C+BaoVEBwIBjsViUkJCgFStWaPz48Zo9e7asVqvGjBnT63EbN27U3LlztWTJEk2YMEHJyclqa2uTJPn4+KiwsFAdHR2aMWOGAgMDtXz5ct16661ycen7n8r169crLy9Pvr6+CgoKkiS99dZb8vb2VlhYmGJiYjRz5kyn951J0meffaaRI0dq2rRpio2NVXJysry8vDRkyBBJnY8w27t3r6ZNm6bFixcrICBAjzzyiOrr63sMtR4eHsrNzVVra6umTp2quXPnKjIyUu+//36fr0fqfNzWzp07tWfPHk2ePFkfffSR1qxZ47TPiy++qODgYM2cOVP33Xef7rjjDs2ePdtpn/T0dLm6umrSpEkaPny4GhoalJqaqri4OD388MMKCQnR6dOnnbp3u3Pbbbfpiy++0N69exUYGKitW7cqKyvLMe/q6qrTp08rISFBAQEBmj9/vh544AG98sorkjrfY3fw4EFVV1crPDxcQUFBWr16tXx8fBxrWCwW+fj4KCIiQnFxcUpJSdGIESOu6r4BAAD0hDxLniXPAgCAgYw8S54lzwK4Fm4y/36RDADguvv999/l6+ur/fv3KzIy8nqXAwAAAFwV8iwAAAAGMvIsAPzv0agAADeAH3/8UTabTYGBgWpubtbzzz+vpqYmVVdXa9CgQde7PAAAAKBX5FkAAAAMZORZAOh/bte7AACAdOHCBWVmZuq3336Tl5eXwsLCtGXLFkIwAAAABgTyLAAAAAYy8iwA9D+eqAAAAAAAAAAAAAAAAPqNy/UuAAAAAAAAAAAAAAAA/P9BowIAAAAAAAAAAAAAAOg3NCoAAAAAAAAAAAAAAIB+Q6MCAAAAAAAAAAAAAADoNzQqAAAAAAAAAAAAAACAfkOjAgAAAAAAAAAAAAAA6Dc0KgAAAAAAAAAAAAAAgH5DowIAAAAAAAAAAAAAAOg3NCoAAAAAAAAAAAAAAIB+83+4dVUVjty/iAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dee49",
   "metadata": {
    "papermill": {
     "duration": 0.145272,
     "end_time": "2025-06-07T21:56:31.461249",
     "exception": false,
     "start_time": "2025-06-07T21:56:31.315977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19e41c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T21:56:31.739057Z",
     "iopub.status.busy": "2025-06-07T21:56:31.738701Z",
     "iopub.status.idle": "2025-06-08T01:35:15.605566Z",
     "shell.execute_reply": "2025-06-08T01:35:15.604557Z"
    },
    "papermill": {
     "duration": 13124.007173,
     "end_time": "2025-06-08T01:35:15.607166",
     "exception": false,
     "start_time": "2025-06-07T21:56:31.599993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5641, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4377, Accuracy: 0.7956, F1 Micro: 0.0802, F1 Macro: 0.0667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4131, Accuracy: 0.8189, F1 Micro: 0.2669, F1 Macro: 0.1815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3655, Accuracy: 0.8245, F1 Micro: 0.349, F1 Macro: 0.2293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3001, Accuracy: 0.8288, F1 Micro: 0.3583, F1 Macro: 0.2455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2612, Accuracy: 0.832, F1 Micro: 0.4248, F1 Macro: 0.3435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2389, Accuracy: 0.8489, F1 Micro: 0.5335, F1 Macro: 0.4777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2128, Accuracy: 0.8502, F1 Micro: 0.5663, F1 Macro: 0.5281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1624, Accuracy: 0.8534, F1 Micro: 0.6108, F1 Macro: 0.5766\n",
      "Epoch 10/10, Train Loss: 0.1418, Accuracy: 0.8506, F1 Micro: 0.5858, F1 Macro: 0.5351\n",
      "Model 1 - Iteration 388: Accuracy: 0.8534, F1 Micro: 0.6108, F1 Macro: 0.5766\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.78      0.84       370\n",
      "                sara       0.56      0.34      0.42       248\n",
      "         radikalisme       0.62      0.37      0.46       243\n",
      "pencemaran_nama_baik       0.64      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.70      0.54      0.61      1365\n",
      "           macro avg       0.68      0.51      0.58      1365\n",
      "        weighted avg       0.69      0.54      0.60      1365\n",
      "         samples avg       0.33      0.32      0.32      1365\n",
      "\n",
      "Training completed in 57.741183042526245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5756, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4529, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4383, Accuracy: 0.8164, F1 Micro: 0.2463, F1 Macro: 0.1702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3845, Accuracy: 0.825, F1 Micro: 0.3204, F1 Macro: 0.2136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3182, Accuracy: 0.8286, F1 Micro: 0.367, F1 Macro: 0.2456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2785, Accuracy: 0.8316, F1 Micro: 0.4254, F1 Macro: 0.3292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2513, Accuracy: 0.8447, F1 Micro: 0.5146, F1 Macro: 0.4523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2208, Accuracy: 0.8473, F1 Micro: 0.5296, F1 Macro: 0.4806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1736, Accuracy: 0.8533, F1 Micro: 0.6019, F1 Macro: 0.5694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1531, Accuracy: 0.8559, F1 Micro: 0.6097, F1 Macro: 0.5741\n",
      "Model 2 - Iteration 388: Accuracy: 0.8559, F1 Micro: 0.6097, F1 Macro: 0.5741\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.80      0.84       370\n",
      "                sara       0.60      0.35      0.44       248\n",
      "         radikalisme       0.66      0.34      0.45       243\n",
      "pencemaran_nama_baik       0.65      0.51      0.57       504\n",
      "\n",
      "           micro avg       0.72      0.53      0.61      1365\n",
      "           macro avg       0.70      0.50      0.57      1365\n",
      "        weighted avg       0.71      0.53      0.60      1365\n",
      "         samples avg       0.33      0.32      0.31      1365\n",
      "\n",
      "Training completed in 59.080729246139526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5409, Accuracy: 0.7866, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4369, Accuracy: 0.7987, F1 Micro: 0.1105, F1 Macro: 0.0891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4044, Accuracy: 0.8228, F1 Micro: 0.3068, F1 Macro: 0.2064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3623, Accuracy: 0.8263, F1 Micro: 0.3703, F1 Macro: 0.2476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3034, Accuracy: 0.835, F1 Micro: 0.4286, F1 Macro: 0.3134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2554, Accuracy: 0.8439, F1 Micro: 0.5134, F1 Macro: 0.4238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2287, Accuracy: 0.8491, F1 Micro: 0.5569, F1 Macro: 0.5164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1992, Accuracy: 0.8516, F1 Micro: 0.5697, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1582, Accuracy: 0.8542, F1 Micro: 0.614, F1 Macro: 0.5745\n",
      "Epoch 10/10, Train Loss: 0.1293, Accuracy: 0.8567, F1 Micro: 0.608, F1 Macro: 0.5771\n",
      "Model 3 - Iteration 388: Accuracy: 0.8542, F1 Micro: 0.614, F1 Macro: 0.5745\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.82      0.86       370\n",
      "                sara       0.57      0.29      0.38       248\n",
      "         radikalisme       0.64      0.39      0.48       243\n",
      "pencemaran_nama_baik       0.62      0.54      0.58       504\n",
      "\n",
      "           micro avg       0.71      0.54      0.61      1365\n",
      "           macro avg       0.68      0.51      0.57      1365\n",
      "        weighted avg       0.69      0.54      0.60      1365\n",
      "         samples avg       0.33      0.32      0.32      1365\n",
      "\n",
      "Training completed in 56.60361433029175 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8545, F1 Micro: 0.6115, F1 Macro: 0.5751\n",
      "Launching training on 2 GPUs.\n",
      "5830\n",
      "BESRA Uncertainty Score Threshold 97.21607475808929\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 281.9719009399414 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5383, Accuracy: 0.7873, F1 Micro: 0.0058, F1 Macro: 0.0053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4089, Accuracy: 0.8317, F1 Micro: 0.3891, F1 Macro: 0.2919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3482, Accuracy: 0.8597, F1 Micro: 0.5951, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2841, Accuracy: 0.8723, F1 Micro: 0.6837, F1 Macro: 0.6787\n",
      "Epoch 5/10, Train Loss: 0.2346, Accuracy: 0.8759, F1 Micro: 0.678, F1 Macro: 0.6574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1859, Accuracy: 0.8767, F1 Micro: 0.7183, F1 Macro: 0.7123\n",
      "Epoch 7/10, Train Loss: 0.1522, Accuracy: 0.8792, F1 Micro: 0.7136, F1 Macro: 0.7054\n",
      "Epoch 8/10, Train Loss: 0.1193, Accuracy: 0.8792, F1 Micro: 0.7176, F1 Macro: 0.706\n",
      "Epoch 9/10, Train Loss: 0.0851, Accuracy: 0.8809, F1 Micro: 0.7062, F1 Macro: 0.6991\n",
      "Epoch 10/10, Train Loss: 0.0793, Accuracy: 0.8797, F1 Micro: 0.6823, F1 Macro: 0.6709\n",
      "Model 1 - Iteration 971: Accuracy: 0.8767, F1 Micro: 0.7183, F1 Macro: 0.7123\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.81      0.86       370\n",
      "                sara       0.55      0.64      0.59       248\n",
      "         radikalisme       0.68      0.71      0.69       243\n",
      "pencemaran_nama_baik       0.66      0.75      0.70       504\n",
      "\n",
      "           micro avg       0.70      0.74      0.72      1365\n",
      "           macro avg       0.70      0.73      0.71      1365\n",
      "        weighted avg       0.72      0.74      0.72      1365\n",
      "         samples avg       0.40      0.41      0.40      1365\n",
      "\n",
      "Training completed in 67.81153512001038 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5482, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4295, Accuracy: 0.8313, F1 Micro: 0.3677, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3602, Accuracy: 0.8559, F1 Micro: 0.5778, F1 Macro: 0.5697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2889, Accuracy: 0.873, F1 Micro: 0.6795, F1 Macro: 0.6777\n",
      "Epoch 5/10, Train Loss: 0.2402, Accuracy: 0.8773, F1 Micro: 0.6755, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1874, Accuracy: 0.8828, F1 Micro: 0.7239, F1 Macro: 0.7236\n",
      "Epoch 7/10, Train Loss: 0.1475, Accuracy: 0.8848, F1 Micro: 0.7184, F1 Macro: 0.7124\n",
      "Epoch 8/10, Train Loss: 0.119, Accuracy: 0.8823, F1 Micro: 0.7085, F1 Macro: 0.6868\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.8861, F1 Micro: 0.7235, F1 Macro: 0.7099\n",
      "Epoch 10/10, Train Loss: 0.0771, Accuracy: 0.883, F1 Micro: 0.7223, F1 Macro: 0.7103\n",
      "Model 2 - Iteration 971: Accuracy: 0.8828, F1 Micro: 0.7239, F1 Macro: 0.7236\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.78      0.86       370\n",
      "                sara       0.58      0.66      0.62       248\n",
      "         radikalisme       0.71      0.76      0.73       243\n",
      "pencemaran_nama_baik       0.68      0.69      0.69       504\n",
      "\n",
      "           micro avg       0.73      0.72      0.72      1365\n",
      "           macro avg       0.73      0.72      0.72      1365\n",
      "        weighted avg       0.74      0.72      0.73      1365\n",
      "         samples avg       0.39      0.40      0.38      1365\n",
      "\n",
      "Training completed in 68.45999264717102 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5202, Accuracy: 0.7973, F1 Micro: 0.0974, F1 Macro: 0.0796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4022, Accuracy: 0.8367, F1 Micro: 0.4318, F1 Macro: 0.3415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3421, Accuracy: 0.863, F1 Micro: 0.6118, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2778, Accuracy: 0.8733, F1 Micro: 0.6932, F1 Macro: 0.6857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2342, Accuracy: 0.8814, F1 Micro: 0.696, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1719, Accuracy: 0.8825, F1 Micro: 0.7243, F1 Macro: 0.7183\n",
      "Epoch 7/10, Train Loss: 0.1386, Accuracy: 0.8842, F1 Micro: 0.7232, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.112, Accuracy: 0.8808, F1 Micro: 0.7314, F1 Macro: 0.7243\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.8809, F1 Micro: 0.7309, F1 Macro: 0.7259\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.883, F1 Micro: 0.7024, F1 Macro: 0.6839\n",
      "Model 3 - Iteration 971: Accuracy: 0.8808, F1 Micro: 0.7314, F1 Macro: 0.7243\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.88      0.87       370\n",
      "                sara       0.61      0.61      0.61       248\n",
      "         radikalisme       0.72      0.73      0.73       243\n",
      "pencemaran_nama_baik       0.64      0.76      0.70       504\n",
      "\n",
      "           micro avg       0.70      0.76      0.73      1365\n",
      "           macro avg       0.71      0.75      0.72      1365\n",
      "        weighted avg       0.71      0.76      0.73      1365\n",
      "         samples avg       0.42      0.43      0.41      1365\n",
      "\n",
      "Training completed in 70.99355506896973 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8801, F1 Micro: 0.7245, F1 Macro: 0.7201\n",
      "Launching training on 2 GPUs.\n",
      "5247\n",
      "BESRA Uncertainty Score Threshold 239.108276190879\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 254.03028798103333 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4899, Accuracy: 0.8234, F1 Micro: 0.3225, F1 Macro: 0.2064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3727, Accuracy: 0.8572, F1 Micro: 0.5572, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3022, Accuracy: 0.8809, F1 Micro: 0.7125, F1 Macro: 0.7062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2374, Accuracy: 0.8864, F1 Micro: 0.7432, F1 Macro: 0.7413\n",
      "Epoch 5/10, Train Loss: 0.1938, Accuracy: 0.8891, F1 Micro: 0.7343, F1 Macro: 0.7264\n",
      "Epoch 6/10, Train Loss: 0.1516, Accuracy: 0.8905, F1 Micro: 0.7199, F1 Macro: 0.7107\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.8869, F1 Micro: 0.7255, F1 Macro: 0.7121\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.8884, F1 Micro: 0.7379, F1 Macro: 0.7309\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.888, F1 Micro: 0.7373, F1 Macro: 0.7324\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.888, F1 Micro: 0.7384, F1 Macro: 0.735\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8864, F1 Micro: 0.7432, F1 Macro: 0.7413\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.83      0.87       370\n",
      "                sara       0.62      0.65      0.63       248\n",
      "         radikalisme       0.72      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.65      0.78      0.71       504\n",
      "\n",
      "           micro avg       0.72      0.77      0.74      1365\n",
      "           macro avg       0.73      0.76      0.74      1365\n",
      "        weighted avg       0.73      0.77      0.75      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 81.91562175750732 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5033, Accuracy: 0.8175, F1 Micro: 0.2542, F1 Macro: 0.1743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3782, Accuracy: 0.8616, F1 Micro: 0.5875, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3025, Accuracy: 0.8777, F1 Micro: 0.7092, F1 Macro: 0.7094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2305, Accuracy: 0.8888, F1 Micro: 0.7441, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1901, Accuracy: 0.8913, F1 Micro: 0.7445, F1 Macro: 0.7331\n",
      "Epoch 6/10, Train Loss: 0.1476, Accuracy: 0.8919, F1 Micro: 0.7278, F1 Macro: 0.7158\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.8855, F1 Micro: 0.7175, F1 Macro: 0.6961\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.8884, F1 Micro: 0.7202, F1 Macro: 0.7065\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.8898, F1 Micro: 0.7382, F1 Macro: 0.7309\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.8909, F1 Micro: 0.7374, F1 Macro: 0.7285\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8913, F1 Micro: 0.7445, F1 Macro: 0.7331\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.66      0.57      0.61       248\n",
      "         radikalisme       0.76      0.69      0.72       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.74      0.74      1365\n",
      "           macro avg       0.75      0.72      0.73      1365\n",
      "        weighted avg       0.75      0.74      0.74      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 82.7463960647583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4803, Accuracy: 0.8267, F1 Micro: 0.3434, F1 Macro: 0.2221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3683, Accuracy: 0.8586, F1 Micro: 0.5659, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2986, Accuracy: 0.8839, F1 Micro: 0.7218, F1 Macro: 0.7117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2301, Accuracy: 0.8908, F1 Micro: 0.7515, F1 Macro: 0.7469\n",
      "Epoch 5/10, Train Loss: 0.181, Accuracy: 0.892, F1 Micro: 0.747, F1 Macro: 0.7388\n",
      "Epoch 6/10, Train Loss: 0.1422, Accuracy: 0.8925, F1 Micro: 0.7315, F1 Macro: 0.7217\n",
      "Epoch 7/10, Train Loss: 0.1193, Accuracy: 0.8895, F1 Micro: 0.7347, F1 Macro: 0.7237\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.8909, F1 Micro: 0.7428, F1 Macro: 0.7373\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.8878, F1 Micro: 0.745, F1 Macro: 0.7433\n",
      "Epoch 10/10, Train Loss: 0.0583, Accuracy: 0.89, F1 Micro: 0.7231, F1 Macro: 0.7132\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8908, F1 Micro: 0.7515, F1 Macro: 0.7469\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.64      0.65      0.65       248\n",
      "         radikalisme       0.69      0.79      0.74       243\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.75      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 82.28935408592224 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8895, F1 Micro: 0.7464, F1 Macro: 0.7405\n",
      "Launching training on 2 GPUs.\n",
      "4722\n",
      "BESRA Uncertainty Score Threshold 137.8340685867488\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 228.49012279510498 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4682, Accuracy: 0.8388, F1 Micro: 0.4809, F1 Macro: 0.4074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.342, Accuracy: 0.8708, F1 Micro: 0.6573, F1 Macro: 0.6586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2856, Accuracy: 0.8855, F1 Micro: 0.7072, F1 Macro: 0.7029\n",
      "Epoch 4/10, Train Loss: 0.2228, Accuracy: 0.8867, F1 Micro: 0.6973, F1 Macro: 0.6795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1939, Accuracy: 0.8942, F1 Micro: 0.7308, F1 Macro: 0.7175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1403, Accuracy: 0.8858, F1 Micro: 0.7473, F1 Macro: 0.7433\n",
      "Epoch 7/10, Train Loss: 0.1163, Accuracy: 0.8903, F1 Micro: 0.7434, F1 Macro: 0.7308\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.8922, F1 Micro: 0.7408, F1 Macro: 0.7322\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.8873, F1 Micro: 0.7303, F1 Macro: 0.7207\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.8897, F1 Micro: 0.7393, F1 Macro: 0.7324\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8858, F1 Micro: 0.7473, F1 Macro: 0.7433\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.87      0.88       370\n",
      "                sara       0.58      0.68      0.63       248\n",
      "         radikalisme       0.73      0.76      0.74       243\n",
      "pencemaran_nama_baik       0.65      0.80      0.72       504\n",
      "\n",
      "           micro avg       0.71      0.79      0.75      1365\n",
      "           macro avg       0.71      0.78      0.74      1365\n",
      "        weighted avg       0.72      0.79      0.75      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 96.12149286270142 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4852, Accuracy: 0.8383, F1 Micro: 0.4329, F1 Macro: 0.3497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3451, Accuracy: 0.8709, F1 Micro: 0.6781, F1 Macro: 0.6779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2839, Accuracy: 0.8839, F1 Micro: 0.6858, F1 Macro: 0.675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2196, Accuracy: 0.888, F1 Micro: 0.6984, F1 Macro: 0.6846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1914, Accuracy: 0.8928, F1 Micro: 0.7252, F1 Macro: 0.7146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.8902, F1 Micro: 0.7446, F1 Macro: 0.7363\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.8913, F1 Micro: 0.7445, F1 Macro: 0.7329\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.8906, F1 Micro: 0.7251, F1 Macro: 0.7174\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.8939, F1 Micro: 0.7425, F1 Macro: 0.7341\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.8917, F1 Micro: 0.7444, F1 Macro: 0.7389\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8902, F1 Micro: 0.7446, F1 Macro: 0.7363\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.65      0.58      0.61       248\n",
      "         radikalisme       0.74      0.74      0.74       243\n",
      "pencemaran_nama_baik       0.67      0.76      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.75      0.74      1365\n",
      "           macro avg       0.74      0.73      0.74      1365\n",
      "        weighted avg       0.74      0.75      0.74      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 96.99509167671204 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4572, Accuracy: 0.8405, F1 Micro: 0.4988, F1 Macro: 0.425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3373, Accuracy: 0.8825, F1 Micro: 0.7152, F1 Macro: 0.7065\n",
      "Epoch 3/10, Train Loss: 0.2809, Accuracy: 0.8861, F1 Micro: 0.6971, F1 Macro: 0.6862\n",
      "Epoch 4/10, Train Loss: 0.2157, Accuracy: 0.8886, F1 Micro: 0.6988, F1 Macro: 0.684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1841, Accuracy: 0.8947, F1 Micro: 0.7346, F1 Macro: 0.7269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.8916, F1 Micro: 0.7496, F1 Macro: 0.7435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.8958, F1 Micro: 0.7527, F1 Macro: 0.7424\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.8919, F1 Micro: 0.7445, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.892, F1 Micro: 0.7535, F1 Macro: 0.7501\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.8923, F1 Micro: 0.7442, F1 Macro: 0.7389\n",
      "Model 3 - Iteration 1969: Accuracy: 0.892, F1 Micro: 0.7535, F1 Macro: 0.7501\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.61      0.67      0.64       248\n",
      "         radikalisme       0.72      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1365\n",
      "           macro avg       0.74      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 97.27889156341553 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8893, F1 Micro: 0.7485, F1 Macro: 0.7432\n",
      "Launching training on 2 GPUs.\n",
      "4249\n",
      "BESRA Uncertainty Score Threshold 250.62281027606008\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 206.44246625900269 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4629, Accuracy: 0.8391, F1 Micro: 0.4365, F1 Macro: 0.3572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3138, Accuracy: 0.8791, F1 Micro: 0.7263, F1 Macro: 0.7263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2616, Accuracy: 0.8888, F1 Micro: 0.7504, F1 Macro: 0.7508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2161, Accuracy: 0.8945, F1 Micro: 0.7604, F1 Macro: 0.7582\n",
      "Epoch 5/10, Train Loss: 0.1674, Accuracy: 0.8944, F1 Micro: 0.7451, F1 Macro: 0.735\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.8895, F1 Micro: 0.7392, F1 Macro: 0.7261\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.8928, F1 Micro: 0.7457, F1 Macro: 0.7356\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.8908, F1 Micro: 0.7548, F1 Macro: 0.7518\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.8916, F1 Micro: 0.7516, F1 Macro: 0.7475\n",
      "Epoch 10/10, Train Loss: 0.0409, Accuracy: 0.8941, F1 Micro: 0.7507, F1 Macro: 0.7478\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8945, F1 Micro: 0.7604, F1 Macro: 0.7582\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.84      0.88       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.74      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.76      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 107.42142271995544 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4787, Accuracy: 0.8486, F1 Micro: 0.5018, F1 Macro: 0.44\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3152, Accuracy: 0.8748, F1 Micro: 0.6771, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2603, Accuracy: 0.8883, F1 Micro: 0.76, F1 Macro: 0.7598\n",
      "Epoch 4/10, Train Loss: 0.2107, Accuracy: 0.8953, F1 Micro: 0.7576, F1 Macro: 0.7572\n",
      "Epoch 5/10, Train Loss: 0.1683, Accuracy: 0.898, F1 Micro: 0.7507, F1 Macro: 0.7436\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.8927, F1 Micro: 0.7514, F1 Macro: 0.7391\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.8964, F1 Micro: 0.754, F1 Macro: 0.7458\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.898, F1 Micro: 0.7577, F1 Macro: 0.7526\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.8938, F1 Micro: 0.7432, F1 Macro: 0.7331\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.8988, F1 Micro: 0.7549, F1 Macro: 0.7485\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8883, F1 Micro: 0.76, F1 Macro: 0.7598\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.60      0.74      0.66       248\n",
      "         radikalisme       0.70      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.64      0.84      0.73       504\n",
      "\n",
      "           micro avg       0.70      0.83      0.76      1365\n",
      "           macro avg       0.71      0.82      0.76      1365\n",
      "        weighted avg       0.71      0.83      0.76      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 105.10646510124207 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4559, Accuracy: 0.8406, F1 Micro: 0.4457, F1 Macro: 0.3594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3097, Accuracy: 0.8828, F1 Micro: 0.7368, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2564, Accuracy: 0.89, F1 Micro: 0.7587, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2116, Accuracy: 0.8959, F1 Micro: 0.759, F1 Macro: 0.7564\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.8938, F1 Micro: 0.7485, F1 Macro: 0.7397\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.8947, F1 Micro: 0.7586, F1 Macro: 0.7538\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.8923, F1 Micro: 0.7455, F1 Macro: 0.7386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0782, Accuracy: 0.8892, F1 Micro: 0.7596, F1 Macro: 0.7619\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.8908, F1 Micro: 0.7371, F1 Macro: 0.7304\n",
      "Epoch 10/10, Train Loss: 0.0383, Accuracy: 0.8922, F1 Micro: 0.738, F1 Macro: 0.7336\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8892, F1 Micro: 0.7596, F1 Macro: 0.7619\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       370\n",
      "                sara       0.58      0.80      0.67       248\n",
      "         radikalisme       0.73      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.80      0.72       504\n",
      "\n",
      "           micro avg       0.71      0.82      0.76      1365\n",
      "           macro avg       0.72      0.82      0.76      1365\n",
      "        weighted avg       0.72      0.82      0.76      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 109.12478351593018 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8907, F1 Micro: 0.76, F1 Macro: 0.76\n",
      "Launching training on 2 GPUs.\n",
      "3824\n",
      "BESRA Uncertainty Score Threshold 153.76512433837522\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 187.59325504302979 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.455, Accuracy: 0.845, F1 Micro: 0.4855, F1 Macro: 0.4156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3112, Accuracy: 0.8867, F1 Micro: 0.7326, F1 Macro: 0.7326\n",
      "Epoch 3/10, Train Loss: 0.2475, Accuracy: 0.8903, F1 Micro: 0.721, F1 Macro: 0.7097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2059, Accuracy: 0.8952, F1 Micro: 0.7482, F1 Macro: 0.7387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1609, Accuracy: 0.9008, F1 Micro: 0.7705, F1 Macro: 0.7652\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.8919, F1 Micro: 0.7155, F1 Macro: 0.6955\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.8989, F1 Micro: 0.7612, F1 Macro: 0.7507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9022, F1 Micro: 0.7759, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.8978, F1 Micro: 0.7634, F1 Macro: 0.7591\n",
      "Epoch 10/10, Train Loss: 0.0398, Accuracy: 0.8945, F1 Micro: 0.7492, F1 Macro: 0.7449\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9022, F1 Micro: 0.7759, F1 Macro: 0.7722\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.90      0.89       370\n",
      "                sara       0.69      0.68      0.69       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 120.91180968284607 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.469, Accuracy: 0.8512, F1 Micro: 0.524, F1 Macro: 0.4632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3042, Accuracy: 0.8898, F1 Micro: 0.7409, F1 Macro: 0.7428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.8978, F1 Micro: 0.7534, F1 Macro: 0.7491\n",
      "Epoch 4/10, Train Loss: 0.1971, Accuracy: 0.8923, F1 Micro: 0.7409, F1 Macro: 0.7313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1585, Accuracy: 0.8977, F1 Micro: 0.767, F1 Macro: 0.7655\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.8956, F1 Micro: 0.7291, F1 Macro: 0.7078\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.8986, F1 Micro: 0.759, F1 Macro: 0.7532\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.8964, F1 Micro: 0.7572, F1 Macro: 0.7518\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.8941, F1 Micro: 0.7654, F1 Macro: 0.7658\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.8964, F1 Micro: 0.7544, F1 Macro: 0.747\n",
      "Model 2 - Iteration 2777: Accuracy: 0.8977, F1 Micro: 0.767, F1 Macro: 0.7655\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 119.35439085960388 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4466, Accuracy: 0.8559, F1 Micro: 0.555, F1 Macro: 0.4939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3041, Accuracy: 0.882, F1 Micro: 0.7209, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.8975, F1 Micro: 0.7473, F1 Macro: 0.737\n",
      "Epoch 4/10, Train Loss: 0.1995, Accuracy: 0.8914, F1 Micro: 0.7372, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1548, Accuracy: 0.8969, F1 Micro: 0.7643, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.8969, F1 Micro: 0.7349, F1 Macro: 0.722\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.8956, F1 Micro: 0.7491, F1 Macro: 0.7408\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.8947, F1 Micro: 0.7572, F1 Macro: 0.7547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.8972, F1 Micro: 0.7683, F1 Macro: 0.7677\n",
      "Epoch 10/10, Train Loss: 0.0398, Accuracy: 0.8975, F1 Micro: 0.7592, F1 Macro: 0.7584\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8972, F1 Micro: 0.7683, F1 Macro: 0.7677\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       370\n",
      "                sara       0.66      0.73      0.69       248\n",
      "         radikalisme       0.73      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 120.0605959892273 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.899, F1 Micro: 0.7704, F1 Macro: 0.7685\n",
      "Launching training on 2 GPUs.\n",
      "3441\n",
      "BESRA Uncertainty Score Threshold 269.8709769850938\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 168.87679171562195 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4319, Accuracy: 0.8605, F1 Micro: 0.579, F1 Macro: 0.5423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2998, Accuracy: 0.8773, F1 Micro: 0.6519, F1 Macro: 0.6417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2425, Accuracy: 0.8936, F1 Micro: 0.7526, F1 Macro: 0.7399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2, Accuracy: 0.8984, F1 Micro: 0.7652, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.154, Accuracy: 0.8973, F1 Micro: 0.7681, F1 Macro: 0.763\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.8927, F1 Micro: 0.7334, F1 Macro: 0.7175\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.8956, F1 Micro: 0.7654, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.897, F1 Micro: 0.7599, F1 Macro: 0.7568\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.8977, F1 Micro: 0.7651, F1 Macro: 0.7623\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.8983, F1 Micro: 0.7616, F1 Macro: 0.7577\n",
      "Model 1 - Iteration 3122: Accuracy: 0.8973, F1 Micro: 0.7681, F1 Macro: 0.763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.86      0.89       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 130.19664001464844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.45, Accuracy: 0.8581, F1 Micro: 0.5693, F1 Macro: 0.5461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3005, Accuracy: 0.8833, F1 Micro: 0.6696, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.8928, F1 Micro: 0.7472, F1 Macro: 0.7236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.203, Accuracy: 0.9006, F1 Micro: 0.7627, F1 Macro: 0.7516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.8994, F1 Micro: 0.7744, F1 Macro: 0.7688\n",
      "Epoch 6/10, Train Loss: 0.1092, Accuracy: 0.8978, F1 Micro: 0.7567, F1 Macro: 0.7493\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.8964, F1 Micro: 0.7655, F1 Macro: 0.7619\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9033, F1 Micro: 0.7712, F1 Macro: 0.7674\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9014, F1 Micro: 0.772, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.9006, F1 Micro: 0.7704, F1 Macro: 0.766\n",
      "Model 2 - Iteration 3122: Accuracy: 0.8994, F1 Micro: 0.7744, F1 Macro: 0.7688\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.89       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.72      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.74      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 129.35339641571045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.424, Accuracy: 0.8669, F1 Micro: 0.6223, F1 Macro: 0.6006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.8892, F1 Micro: 0.6932, F1 Macro: 0.6843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2349, Accuracy: 0.8967, F1 Micro: 0.762, F1 Macro: 0.7538\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.8967, F1 Micro: 0.7505, F1 Macro: 0.7396\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.8958, F1 Micro: 0.7563, F1 Macro: 0.7445\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.8947, F1 Micro: 0.7457, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.895, F1 Micro: 0.7728, F1 Macro: 0.7733\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.8975, F1 Micro: 0.7625, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.8984, F1 Micro: 0.7619, F1 Macro: 0.7591\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.8984, F1 Micro: 0.754, F1 Macro: 0.7452\n",
      "Model 3 - Iteration 3122: Accuracy: 0.895, F1 Micro: 0.7728, F1 Macro: 0.7733\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.89      0.88       370\n",
      "                sara       0.65      0.79      0.72       248\n",
      "         radikalisme       0.72      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.66      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.84      0.77      1365\n",
      "           macro avg       0.72      0.83      0.77      1365\n",
      "        weighted avg       0.73      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 128.6552128791809 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8972, F1 Micro: 0.7718, F1 Macro: 0.7684\n",
      "Launching training on 2 GPUs.\n",
      "3096\n",
      "BESRA Uncertainty Score Threshold 209.54125461748214\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 152.7152464389801 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4305, Accuracy: 0.8633, F1 Micro: 0.621, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2818, Accuracy: 0.8916, F1 Micro: 0.7412, F1 Macro: 0.7307\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.8942, F1 Micro: 0.7229, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1872, Accuracy: 0.8967, F1 Micro: 0.7499, F1 Macro: 0.7398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9005, F1 Micro: 0.7509, F1 Macro: 0.7358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.8988, F1 Micro: 0.7633, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0878, Accuracy: 0.9005, F1 Micro: 0.7649, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.8964, F1 Micro: 0.7665, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.8983, F1 Micro: 0.7737, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.8992, F1 Micro: 0.7607, F1 Macro: 0.759\n",
      "Model 1 - Iteration 3432: Accuracy: 0.8983, F1 Micro: 0.7737, F1 Macro: 0.7729\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 145.10076642036438 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4416, Accuracy: 0.8631, F1 Micro: 0.6086, F1 Macro: 0.6\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8956, F1 Micro: 0.752, F1 Macro: 0.7404\n",
      "Epoch 3/10, Train Loss: 0.2251, Accuracy: 0.8953, F1 Micro: 0.7281, F1 Macro: 0.7228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.902, F1 Micro: 0.764, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1596, Accuracy: 0.9023, F1 Micro: 0.773, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9042, F1 Micro: 0.7745, F1 Macro: 0.7658\n",
      "Epoch 7/10, Train Loss: 0.0899, Accuracy: 0.8989, F1 Micro: 0.7713, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.8978, F1 Micro: 0.7702, F1 Macro: 0.7654\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9023, F1 Micro: 0.7706, F1 Macro: 0.7663\n",
      "Epoch 10/10, Train Loss: 0.0393, Accuracy: 0.902, F1 Micro: 0.7642, F1 Macro: 0.7596\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9042, F1 Micro: 0.7745, F1 Macro: 0.7658\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       370\n",
      "                sara       0.70      0.62      0.66       248\n",
      "         radikalisme       0.76      0.76      0.76       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 139.62301969528198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4236, Accuracy: 0.867, F1 Micro: 0.6292, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.8952, F1 Micro: 0.7452, F1 Macro: 0.7368\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.8969, F1 Micro: 0.7385, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.8989, F1 Micro: 0.7578, F1 Macro: 0.7474\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.8998, F1 Micro: 0.7425, F1 Macro: 0.7297\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.8998, F1 Micro: 0.7499, F1 Macro: 0.7374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.8978, F1 Micro: 0.7581, F1 Macro: 0.754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.8991, F1 Micro: 0.7606, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.897, F1 Micro: 0.7671, F1 Macro: 0.7624\n",
      "Epoch 10/10, Train Loss: 0.0395, Accuracy: 0.9, F1 Micro: 0.7555, F1 Macro: 0.753\n",
      "Model 3 - Iteration 3432: Accuracy: 0.897, F1 Micro: 0.7671, F1 Macro: 0.7624\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.90      0.88       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.77      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 141.06001353263855 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8998, F1 Micro: 0.7718, F1 Macro: 0.767\n",
      "Launching training on 2 GPUs.\n",
      "2786\n",
      "BESRA Uncertainty Score Threshold 306.07571672872336\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 136.14025163650513 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4211, Accuracy: 0.8661, F1 Micro: 0.6451, F1 Macro: 0.6335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2867, Accuracy: 0.892, F1 Micro: 0.7252, F1 Macro: 0.7234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2399, Accuracy: 0.8994, F1 Micro: 0.7662, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1869, Accuracy: 0.8931, F1 Micro: 0.7725, F1 Macro: 0.774\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.8958, F1 Micro: 0.7593, F1 Macro: 0.7503\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.8998, F1 Micro: 0.7697, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9008, F1 Micro: 0.7771, F1 Macro: 0.776\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.8961, F1 Micro: 0.7639, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.8989, F1 Micro: 0.7552, F1 Macro: 0.746\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.8997, F1 Micro: 0.7664, F1 Macro: 0.7641\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9008, F1 Micro: 0.7771, F1 Macro: 0.776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.67      0.68      0.67       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 147.56368851661682 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4307, Accuracy: 0.8664, F1 Micro: 0.6379, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2844, Accuracy: 0.8963, F1 Micro: 0.7412, F1 Macro: 0.7382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2365, Accuracy: 0.8998, F1 Micro: 0.7713, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.8964, F1 Micro: 0.7738, F1 Macro: 0.7744\n",
      "Epoch 5/10, Train Loss: 0.1506, Accuracy: 0.8992, F1 Micro: 0.7689, F1 Macro: 0.7625\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.9, F1 Micro: 0.773, F1 Macro: 0.7686\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9061, F1 Micro: 0.7698, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9017, F1 Micro: 0.775, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9045, F1 Micro: 0.7745, F1 Macro: 0.7688\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9041, F1 Micro: 0.7728, F1 Macro: 0.7681\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9017, F1 Micro: 0.775, F1 Macro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 146.93823385238647 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4142, Accuracy: 0.872, F1 Micro: 0.6625, F1 Macro: 0.6571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2833, Accuracy: 0.8939, F1 Micro: 0.7294, F1 Macro: 0.7259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2344, Accuracy: 0.8984, F1 Micro: 0.76, F1 Macro: 0.7547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.8927, F1 Micro: 0.7738, F1 Macro: 0.7744\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.8978, F1 Micro: 0.7712, F1 Macro: 0.7691\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9009, F1 Micro: 0.7629, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9025, F1 Micro: 0.7665, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.8989, F1 Micro: 0.7731, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.902, F1 Micro: 0.7633, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0418, Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.7756\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.7756\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 146.7442364692688 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.9024, F1 Micro: 0.7764, F1 Macro: 0.7745\n",
      "Launching training on 2 GPUs.\n",
      "2507\n",
      "BESRA Uncertainty Score Threshold 235.05679785614228\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 123.50360560417175 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4219, Accuracy: 0.8752, F1 Micro: 0.6975, F1 Macro: 0.7009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.8928, F1 Micro: 0.7596, F1 Macro: 0.7586\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.8955, F1 Micro: 0.7408, F1 Macro: 0.7376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9008, F1 Micro: 0.7715, F1 Macro: 0.7678\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9013, F1 Micro: 0.7661, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.1183, Accuracy: 0.8984, F1 Micro: 0.7674, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.8997, F1 Micro: 0.7727, F1 Macro: 0.7687\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9025, F1 Micro: 0.7658, F1 Macro: 0.7583\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9002, F1 Micro: 0.7665, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0393, Accuracy: 0.9019, F1 Micro: 0.7679, F1 Macro: 0.7657\n",
      "Model 1 - Iteration 3886: Accuracy: 0.8997, F1 Micro: 0.7727, F1 Macro: 0.7687\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 151.6865417957306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4287, Accuracy: 0.8737, F1 Micro: 0.6888, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2789, Accuracy: 0.8939, F1 Micro: 0.7677, F1 Macro: 0.7667\n",
      "Epoch 3/10, Train Loss: 0.2356, Accuracy: 0.8981, F1 Micro: 0.7488, F1 Macro: 0.7455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9044, F1 Micro: 0.7779, F1 Macro: 0.773\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.902, F1 Micro: 0.7727, F1 Macro: 0.7631\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9006, F1 Micro: 0.7719, F1 Macro: 0.7704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0807, Accuracy: 0.8989, F1 Micro: 0.7791, F1 Macro: 0.779\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9028, F1 Micro: 0.7667, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9044, F1 Micro: 0.7733, F1 Macro: 0.7718\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.8992, F1 Micro: 0.7554, F1 Macro: 0.7483\n",
      "Model 2 - Iteration 3886: Accuracy: 0.8989, F1 Micro: 0.7791, F1 Macro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.90      0.89       370\n",
      "                sara       0.67      0.76      0.71       248\n",
      "         radikalisme       0.70      0.86      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 151.09246277809143 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4094, Accuracy: 0.8769, F1 Micro: 0.7068, F1 Macro: 0.7063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.8931, F1 Micro: 0.7595, F1 Macro: 0.7574\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.8984, F1 Micro: 0.7473, F1 Macro: 0.7436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1827, Accuracy: 0.8989, F1 Micro: 0.7779, F1 Macro: 0.7775\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9031, F1 Micro: 0.7717, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1119, Accuracy: 0.9036, F1 Micro: 0.778, F1 Macro: 0.7796\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.8964, F1 Micro: 0.7699, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9009, F1 Micro: 0.7624, F1 Macro: 0.7556\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9013, F1 Micro: 0.7644, F1 Macro: 0.7637\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.905, F1 Micro: 0.7706, F1 Macro: 0.7658\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9036, F1 Micro: 0.778, F1 Macro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.85      0.90       370\n",
      "                sara       0.68      0.70      0.69       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 150.75606727600098 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.9007, F1 Micro: 0.7766, F1 Macro: 0.7758\n",
      "Launching training on 2 GPUs.\n",
      "2332\n",
      "BESRA Uncertainty Score Threshold 277.94511183979154\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 114.3490719795227 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4154, Accuracy: 0.8714, F1 Micro: 0.6505, F1 Macro: 0.6147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2797, Accuracy: 0.8913, F1 Micro: 0.7159, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2283, Accuracy: 0.8988, F1 Micro: 0.7564, F1 Macro: 0.7492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1807, Accuracy: 0.9027, F1 Micro: 0.7597, F1 Macro: 0.746\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9025, F1 Micro: 0.7549, F1 Macro: 0.7442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.902, F1 Micro: 0.7649, F1 Macro: 0.7552\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9028, F1 Micro: 0.7563, F1 Macro: 0.7435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.8992, F1 Micro: 0.772, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.902, F1 Micro: 0.7739, F1 Macro: 0.769\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9006, F1 Micro: 0.7535, F1 Macro: 0.7441\n",
      "Model 1 - Iteration 4120: Accuracy: 0.902, F1 Micro: 0.7739, F1 Macro: 0.769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.70      0.65      0.67       248\n",
      "         radikalisme       0.76      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 163.80697679519653 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4228, Accuracy: 0.8745, F1 Micro: 0.6573, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2792, Accuracy: 0.8945, F1 Micro: 0.7196, F1 Macro: 0.6946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.23, Accuracy: 0.9011, F1 Micro: 0.7653, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.9061, F1 Micro: 0.7686, F1 Macro: 0.7521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9084, F1 Micro: 0.7755, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9066, F1 Micro: 0.78, F1 Macro: 0.777\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9058, F1 Micro: 0.7577, F1 Macro: 0.7444\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9036, F1 Micro: 0.7737, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9009, F1 Micro: 0.7815, F1 Macro: 0.783\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9075, F1 Micro: 0.7788, F1 Macro: 0.7749\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9009, F1 Micro: 0.7815, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.68      0.75      0.71       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 163.24460768699646 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4052, Accuracy: 0.8781, F1 Micro: 0.6734, F1 Macro: 0.6471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2769, Accuracy: 0.8916, F1 Micro: 0.7072, F1 Macro: 0.6894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9019, F1 Micro: 0.7705, F1 Macro: 0.7649\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9041, F1 Micro: 0.7611, F1 Macro: 0.7502\n",
      "Epoch 5/10, Train Loss: 0.1514, Accuracy: 0.8988, F1 Micro: 0.7307, F1 Macro: 0.7173\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9022, F1 Micro: 0.7678, F1 Macro: 0.7665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9041, F1 Micro: 0.7734, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9055, F1 Micro: 0.7796, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.9041, F1 Micro: 0.7757, F1 Macro: 0.773\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9055, F1 Micro: 0.7796, F1 Macro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.78      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 161.355233669281 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.9028, F1 Micro: 0.7783, F1 Macro: 0.7759\n",
      "Launching training on 2 GPUs.\n",
      "2098\n",
      "BESRA Uncertainty Score Threshold 157.24822077946243\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 103.39355707168579 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4036, Accuracy: 0.8806, F1 Micro: 0.7218, F1 Macro: 0.7197\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.8908, F1 Micro: 0.7113, F1 Macro: 0.6988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2332, Accuracy: 0.8981, F1 Micro: 0.7599, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9005, F1 Micro: 0.7798, F1 Macro: 0.7772\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9006, F1 Micro: 0.7589, F1 Macro: 0.7561\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.8969, F1 Micro: 0.7727, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9056, F1 Micro: 0.7788, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9022, F1 Micro: 0.7611, F1 Macro: 0.7592\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9041, F1 Micro: 0.7695, F1 Macro: 0.7642\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.9027, F1 Micro: 0.7769, F1 Macro: 0.7742\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9005, F1 Micro: 0.7798, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.90       370\n",
      "                sara       0.62      0.76      0.68       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 162.3754060268402 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4099, Accuracy: 0.8778, F1 Micro: 0.7197, F1 Macro: 0.7212\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.8911, F1 Micro: 0.7075, F1 Macro: 0.6936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2295, Accuracy: 0.9017, F1 Micro: 0.7717, F1 Macro: 0.7659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.8992, F1 Micro: 0.775, F1 Macro: 0.7711\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9013, F1 Micro: 0.764, F1 Macro: 0.7612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.8992, F1 Micro: 0.7766, F1 Macro: 0.7755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.9061, F1 Micro: 0.779, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9045, F1 Micro: 0.7829, F1 Macro: 0.7801\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9045, F1 Micro: 0.7724, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.9041, F1 Micro: 0.7827, F1 Macro: 0.7823\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9045, F1 Micro: 0.7829, F1 Macro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.67      0.68      0.68       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 167.16987776756287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3948, Accuracy: 0.8823, F1 Micro: 0.7296, F1 Macro: 0.7254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.8964, F1 Micro: 0.7405, F1 Macro: 0.7373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.9014, F1 Micro: 0.7723, F1 Macro: 0.7668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.9008, F1 Micro: 0.7804, F1 Macro: 0.7773\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.903, F1 Micro: 0.7668, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9009, F1 Micro: 0.7824, F1 Macro: 0.7833\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9033, F1 Micro: 0.7743, F1 Macro: 0.7725\n",
      "Epoch 8/10, Train Loss: 0.0586, Accuracy: 0.9017, F1 Micro: 0.7809, F1 Macro: 0.779\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7732\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.9014, F1 Micro: 0.7771, F1 Macro: 0.7774\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9009, F1 Micro: 0.7824, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.65      0.78      0.71       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 165.56385684013367 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.902, F1 Micro: 0.7817, F1 Macro: 0.7802\n",
      "Launching training on 2 GPUs.\n",
      "1888\n",
      "BESRA Uncertainty Score Threshold 288.51247354729685\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 93.64360404014587 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4046, Accuracy: 0.8792, F1 Micro: 0.722, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.8959, F1 Micro: 0.7668, F1 Macro: 0.7635\n",
      "Epoch 3/10, Train Loss: 0.2202, Accuracy: 0.9013, F1 Micro: 0.759, F1 Macro: 0.7519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9031, F1 Micro: 0.7678, F1 Macro: 0.7538\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.8964, F1 Micro: 0.7638, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9019, F1 Micro: 0.7696, F1 Macro: 0.7648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9025, F1 Micro: 0.7817, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.903, F1 Micro: 0.7723, F1 Macro: 0.7651\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.9034, F1 Micro: 0.7764, F1 Macro: 0.7716\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9048, F1 Micro: 0.7765, F1 Macro: 0.7735\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9025, F1 Micro: 0.7817, F1 Macro: 0.7787\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       370\n",
      "                sara       0.63      0.79      0.70       248\n",
      "         radikalisme       0.74      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 171.52422833442688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4146, Accuracy: 0.8789, F1 Micro: 0.7177, F1 Macro: 0.7132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.8973, F1 Micro: 0.7743, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9045, F1 Micro: 0.7776, F1 Macro: 0.7701\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9055, F1 Micro: 0.774, F1 Macro: 0.7587\n",
      "Epoch 5/10, Train Loss: 0.1396, Accuracy: 0.9017, F1 Micro: 0.7705, F1 Macro: 0.7593\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9055, F1 Micro: 0.7704, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.8994, F1 Micro: 0.7808, F1 Macro: 0.7789\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9042, F1 Micro: 0.7686, F1 Macro: 0.7615\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9039, F1 Micro: 0.7743, F1 Macro: 0.769\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9019, F1 Micro: 0.7762, F1 Macro: 0.7731\n",
      "Model 2 - Iteration 4530: Accuracy: 0.8994, F1 Micro: 0.7808, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       370\n",
      "                sara       0.61      0.79      0.69       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 169.64604544639587 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4003, Accuracy: 0.8789, F1 Micro: 0.7197, F1 Macro: 0.713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.8939, F1 Micro: 0.7685, F1 Macro: 0.7655\n",
      "Epoch 3/10, Train Loss: 0.2202, Accuracy: 0.9014, F1 Micro: 0.7602, F1 Macro: 0.7495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1413, Accuracy: 0.8986, F1 Micro: 0.7757, F1 Macro: 0.7665\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.9027, F1 Micro: 0.7567, F1 Macro: 0.7485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9056, F1 Micro: 0.776, F1 Macro: 0.7717\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9031, F1 Micro: 0.7734, F1 Macro: 0.7703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9017, F1 Micro: 0.7806, F1 Macro: 0.7803\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.902, F1 Micro: 0.7665, F1 Macro: 0.761\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9017, F1 Micro: 0.7806, F1 Macro: 0.7803\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.67      0.74      0.70       248\n",
      "         radikalisme       0.74      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 173.0694100856781 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.9012, F1 Micro: 0.781, F1 Macro: 0.7793\n",
      "Launching training on 2 GPUs.\n",
      "1688\n",
      "BESRA Uncertainty Score Threshold 168.62170555624874\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 83.37850904464722 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4002, Accuracy: 0.88, F1 Micro: 0.6948, F1 Macro: 0.6862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2689, Accuracy: 0.8938, F1 Micro: 0.7145, F1 Macro: 0.6848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.8981, F1 Micro: 0.7712, F1 Macro: 0.7685\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9013, F1 Micro: 0.7611, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9027, F1 Micro: 0.7753, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9016, F1 Micro: 0.768, F1 Macro: 0.7615\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.902, F1 Micro: 0.7722, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9067, F1 Micro: 0.7781, F1 Macro: 0.7671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9053, F1 Micro: 0.7903, F1 Macro: 0.7909\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9048, F1 Micro: 0.7823, F1 Macro: 0.7791\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9053, F1 Micro: 0.7903, F1 Macro: 0.7909\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       370\n",
      "                sara       0.66      0.78      0.71       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 177.304345369339 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.883, F1 Micro: 0.7138, F1 Macro: 0.7099\n",
      "Epoch 2/10, Train Loss: 0.2677, Accuracy: 0.8914, F1 Micro: 0.7044, F1 Macro: 0.6736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.8988, F1 Micro: 0.7733, F1 Macro: 0.771\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9016, F1 Micro: 0.7518, F1 Macro: 0.7429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.9034, F1 Micro: 0.7832, F1 Macro: 0.7789\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9009, F1 Micro: 0.7701, F1 Macro: 0.7654\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9055, F1 Micro: 0.7762, F1 Macro: 0.7708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.9059, F1 Micro: 0.7867, F1 Macro: 0.7844\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.9, F1 Micro: 0.7778, F1 Macro: 0.7797\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9045, F1 Micro: 0.7766, F1 Macro: 0.7708\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9059, F1 Micro: 0.7867, F1 Macro: 0.7844\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.70      0.71      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 174.58248925209045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3915, Accuracy: 0.8855, F1 Micro: 0.7182, F1 Macro: 0.7072\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.8927, F1 Micro: 0.707, F1 Macro: 0.6841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.8989, F1 Micro: 0.7719, F1 Macro: 0.7708\n",
      "Epoch 4/10, Train Loss: 0.1805, Accuracy: 0.9033, F1 Micro: 0.7587, F1 Macro: 0.7544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1337, Accuracy: 0.8997, F1 Micro: 0.7751, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9042, F1 Micro: 0.7767, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9058, F1 Micro: 0.7689, F1 Macro: 0.7591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.902, F1 Micro: 0.7795, F1 Macro: 0.7765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.9083, F1 Micro: 0.7898, F1 Macro: 0.7892\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9045, F1 Micro: 0.774, F1 Macro: 0.7693\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9083, F1 Micro: 0.7898, F1 Macro: 0.7892\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.71      0.72      0.71       248\n",
      "         radikalisme       0.78      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 177.43024826049805 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.9065, F1 Micro: 0.7889, F1 Macro: 0.7881\n",
      "Launching training on 2 GPUs.\n",
      "1555\n",
      "BESRA Uncertainty Score Threshold 221.99726265159757\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 77.08748960494995 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3954, Accuracy: 0.8845, F1 Micro: 0.726, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2682, Accuracy: 0.8969, F1 Micro: 0.7426, F1 Macro: 0.7277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.8941, F1 Micro: 0.7659, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1768, Accuracy: 0.8981, F1 Micro: 0.7746, F1 Macro: 0.7714\n",
      "Epoch 5/10, Train Loss: 0.1352, Accuracy: 0.8998, F1 Micro: 0.7734, F1 Macro: 0.7732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9044, F1 Micro: 0.7768, F1 Macro: 0.7724\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9025, F1 Micro: 0.7719, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.903, F1 Micro: 0.7822, F1 Macro: 0.7807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9061, F1 Micro: 0.7863, F1 Macro: 0.7837\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.903, F1 Micro: 0.7781, F1 Macro: 0.7734\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9061, F1 Micro: 0.7863, F1 Macro: 0.7837\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 184.45210647583008 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4033, Accuracy: 0.8858, F1 Micro: 0.7148, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.8994, F1 Micro: 0.7465, F1 Macro: 0.7365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.8967, F1 Micro: 0.7714, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.8984, F1 Micro: 0.7771, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.903, F1 Micro: 0.7789, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9077, F1 Micro: 0.7882, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9047, F1 Micro: 0.7828, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9008, F1 Micro: 0.7718, F1 Macro: 0.7699\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9016, F1 Micro: 0.7811, F1 Macro: 0.7806\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9052, F1 Micro: 0.7825, F1 Macro: 0.7805\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9077, F1 Micro: 0.7882, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.72      0.61      0.66       248\n",
      "         radikalisme       0.75      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 183.01471781730652 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8889, F1 Micro: 0.7264, F1 Macro: 0.7199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2655, Accuracy: 0.8966, F1 Micro: 0.7392, F1 Macro: 0.7184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.8947, F1 Micro: 0.7689, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.8997, F1 Micro: 0.7807, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.7708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9014, F1 Micro: 0.7835, F1 Macro: 0.7836\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9006, F1 Micro: 0.7779, F1 Macro: 0.7758\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9042, F1 Micro: 0.7791, F1 Macro: 0.7786\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.8981, F1 Micro: 0.7761, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9, F1 Micro: 0.7742, F1 Macro: 0.7723\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9014, F1 Micro: 0.7835, F1 Macro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.66      0.76      0.70       248\n",
      "         radikalisme       0.68      0.91      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 181.9941337108612 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.9051, F1 Micro: 0.786, F1 Macro: 0.7831\n",
      "Launching training on 2 GPUs.\n",
      "1355\n",
      "BESRA Uncertainty Score Threshold 144.03429597780678\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 67.13509821891785 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3943, Accuracy: 0.8725, F1 Micro: 0.6288, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.8983, F1 Micro: 0.7606, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9009, F1 Micro: 0.7679, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1776, Accuracy: 0.9039, F1 Micro: 0.7782, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.902, F1 Micro: 0.7727, F1 Macro: 0.7656\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.8997, F1 Micro: 0.7671, F1 Macro: 0.7627\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.902, F1 Micro: 0.7776, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.907, F1 Micro: 0.7792, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9061, F1 Micro: 0.7799, F1 Macro: 0.774\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9061, F1 Micro: 0.7731, F1 Macro: 0.7664\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9061, F1 Micro: 0.7799, F1 Macro: 0.774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.70      0.65      0.67       248\n",
      "         radikalisme       0.76      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 191.75768780708313 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8708, F1 Micro: 0.6101, F1 Macro: 0.5856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.8995, F1 Micro: 0.7605, F1 Macro: 0.7534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9061, F1 Micro: 0.7896, F1 Macro: 0.7887\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9067, F1 Micro: 0.7806, F1 Macro: 0.7808\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9058, F1 Micro: 0.7794, F1 Macro: 0.7741\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.902, F1 Micro: 0.7795, F1 Macro: 0.7775\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9034, F1 Micro: 0.7832, F1 Macro: 0.7826\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9094, F1 Micro: 0.7713, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9069, F1 Micro: 0.7831, F1 Macro: 0.7768\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9038, F1 Micro: 0.7643, F1 Macro: 0.7599\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9061, F1 Micro: 0.7896, F1 Macro: 0.7887\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.85      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 185.81816911697388 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3869, Accuracy: 0.8741, F1 Micro: 0.634, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2614, Accuracy: 0.897, F1 Micro: 0.7536, F1 Macro: 0.747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.8989, F1 Micro: 0.7705, F1 Macro: 0.7693\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9009, F1 Micro: 0.7701, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.9061, F1 Micro: 0.7836, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9033, F1 Micro: 0.772, F1 Macro: 0.7707\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9067, F1 Micro: 0.7824, F1 Macro: 0.7819\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9072, F1 Micro: 0.7769, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9073, F1 Micro: 0.7849, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9047, F1 Micro: 0.7858, F1 Macro: 0.7856\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9047, F1 Micro: 0.7858, F1 Macro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.66      0.75      0.70       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 192.36193013191223 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9056, F1 Micro: 0.7851, F1 Macro: 0.7828\n",
      "Launching training on 2 GPUs.\n",
      "1155\n",
      "BESRA Uncertainty Score Threshold 236.6025138447602\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 58.29068684577942 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3913, Accuracy: 0.8869, F1 Micro: 0.7209, F1 Macro: 0.7177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.8975, F1 Micro: 0.7511, F1 Macro: 0.7473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2149, Accuracy: 0.9008, F1 Micro: 0.7705, F1 Macro: 0.768\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.9036, F1 Micro: 0.7637, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1347, Accuracy: 0.9048, F1 Micro: 0.7812, F1 Macro: 0.7788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9059, F1 Micro: 0.7897, F1 Macro: 0.7897\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9022, F1 Micro: 0.7743, F1 Macro: 0.7709\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9022, F1 Micro: 0.7846, F1 Macro: 0.786\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.9052, F1 Micro: 0.7848, F1 Macro: 0.7835\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9005, F1 Micro: 0.7732, F1 Macro: 0.7718\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9059, F1 Micro: 0.7897, F1 Macro: 0.7897\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.62      0.81      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 195.7582950592041 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3983, Accuracy: 0.8866, F1 Micro: 0.7175, F1 Macro: 0.7128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9006, F1 Micro: 0.7556, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2142, Accuracy: 0.9006, F1 Micro: 0.7793, F1 Macro: 0.7765\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9066, F1 Micro: 0.7702, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9081, F1 Micro: 0.7915, F1 Macro: 0.7921\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9067, F1 Micro: 0.7844, F1 Macro: 0.7828\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.9013, F1 Micro: 0.7717, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9027, F1 Micro: 0.7827, F1 Macro: 0.7842\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9016, F1 Micro: 0.7816, F1 Macro: 0.7821\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.8983, F1 Micro: 0.7723, F1 Macro: 0.7696\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9081, F1 Micro: 0.7915, F1 Macro: 0.7921\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.76      0.88      0.82       243\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.82      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.2548336982727 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.8894, F1 Micro: 0.7292, F1 Macro: 0.7184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.8986, F1 Micro: 0.7531, F1 Macro: 0.7454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9008, F1 Micro: 0.7743, F1 Macro: 0.7722\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9003, F1 Micro: 0.748, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9067, F1 Micro: 0.7827, F1 Macro: 0.7812\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9052, F1 Micro: 0.7789, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9009, F1 Micro: 0.7783, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.9036, F1 Micro: 0.7858, F1 Macro: 0.7891\n",
      "Epoch 9/10, Train Loss: 0.043, Accuracy: 0.9047, F1 Micro: 0.782, F1 Macro: 0.7791\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9041, F1 Micro: 0.7796, F1 Macro: 0.7782\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9036, F1 Micro: 0.7858, F1 Macro: 0.7891\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.63      0.84      0.72       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 195.09091567993164 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9059, F1 Micro: 0.789, F1 Macro: 0.7903\n",
      "Launching training on 2 GPUs.\n",
      "955\n",
      "BESRA Uncertainty Score Threshold 187.24194624711288\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 47.49664545059204 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3771, Accuracy: 0.8902, F1 Micro: 0.7428, F1 Macro: 0.7341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2524, Accuracy: 0.8966, F1 Micro: 0.7608, F1 Macro: 0.7565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9044, F1 Micro: 0.7821, F1 Macro: 0.7774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.9019, F1 Micro: 0.7825, F1 Macro: 0.7781\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.8995, F1 Micro: 0.7716, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.9023, F1 Micro: 0.7797, F1 Macro: 0.7783\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9042, F1 Micro: 0.7811, F1 Macro: 0.7782\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.8986, F1 Micro: 0.7763, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9044, F1 Micro: 0.7722, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.9019, F1 Micro: 0.7837, F1 Macro: 0.7801\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9019, F1 Micro: 0.7837, F1 Macro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.94      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 200.77687811851501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3784, Accuracy: 0.8931, F1 Micro: 0.7529, F1 Macro: 0.7422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.898, F1 Micro: 0.7685, F1 Macro: 0.7646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9056, F1 Micro: 0.7894, F1 Macro: 0.7861\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.9053, F1 Micro: 0.7877, F1 Macro: 0.7841\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.9025, F1 Micro: 0.7747, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.9072, F1 Micro: 0.7856, F1 Macro: 0.7835\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9017, F1 Micro: 0.7631, F1 Macro: 0.7548\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.9038, F1 Micro: 0.773, F1 Macro: 0.7678\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.9053, F1 Micro: 0.7817, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.031, Accuracy: 0.903, F1 Micro: 0.782, F1 Macro: 0.7745\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9056, F1 Micro: 0.7894, F1 Macro: 0.7861\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 196.7117795944214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3701, Accuracy: 0.8923, F1 Micro: 0.7484, F1 Macro: 0.734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2468, Accuracy: 0.8967, F1 Micro: 0.7663, F1 Macro: 0.7648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9039, F1 Micro: 0.7838, F1 Macro: 0.7768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9045, F1 Micro: 0.7867, F1 Macro: 0.7848\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9003, F1 Micro: 0.768, F1 Macro: 0.7616\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9039, F1 Micro: 0.7779, F1 Macro: 0.7767\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.902, F1 Micro: 0.7739, F1 Macro: 0.7698\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9033, F1 Micro: 0.7763, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9053, F1 Micro: 0.7783, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9005, F1 Micro: 0.7693, F1 Macro: 0.7652\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9045, F1 Micro: 0.7867, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 199.05639505386353 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.904, F1 Micro: 0.7866, F1 Macro: 0.7837\n",
      "Launching training on 2 GPUs.\n",
      "777\n",
      "BESRA Uncertainty Score Threshold 201.50513151485623\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 39.32701635360718 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3755, Accuracy: 0.8884, F1 Micro: 0.7375, F1 Macro: 0.7351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.8983, F1 Micro: 0.7557, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9041, F1 Micro: 0.7766, F1 Macro: 0.772\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9003, F1 Micro: 0.7702, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9009, F1 Micro: 0.7777, F1 Macro: 0.775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.9042, F1 Micro: 0.786, F1 Macro: 0.782\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.9059, F1 Micro: 0.7801, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.902, F1 Micro: 0.76, F1 Macro: 0.7521\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9022, F1 Micro: 0.768, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9036, F1 Micro: 0.7799, F1 Macro: 0.7784\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9042, F1 Micro: 0.786, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 206.46173739433289 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3784, Accuracy: 0.8891, F1 Micro: 0.7331, F1 Macro: 0.7302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2512, Accuracy: 0.9009, F1 Micro: 0.7578, F1 Macro: 0.755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.9075, F1 Micro: 0.7874, F1 Macro: 0.7812\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9025, F1 Micro: 0.7792, F1 Macro: 0.7748\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.903, F1 Micro: 0.7805, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9028, F1 Micro: 0.7871, F1 Macro: 0.7866\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9084, F1 Micro: 0.7842, F1 Macro: 0.776\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9044, F1 Micro: 0.7718, F1 Macro: 0.7658\n",
      "Epoch 9/10, Train Loss: 0.0448, Accuracy: 0.9047, F1 Micro: 0.7857, F1 Macro: 0.7826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9062, F1 Micro: 0.7878, F1 Macro: 0.7858\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9062, F1 Micro: 0.7878, F1 Macro: 0.7858\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.74      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 206.12779784202576 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3701, Accuracy: 0.8919, F1 Micro: 0.7458, F1 Macro: 0.7399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.251, Accuracy: 0.8998, F1 Micro: 0.7597, F1 Macro: 0.7564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.9048, F1 Micro: 0.7816, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.905, F1 Micro: 0.7844, F1 Macro: 0.7827\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9052, F1 Micro: 0.7836, F1 Macro: 0.7827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9033, F1 Micro: 0.7895, F1 Macro: 0.7908\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9058, F1 Micro: 0.7683, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9061, F1 Micro: 0.774, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9036, F1 Micro: 0.7764, F1 Macro: 0.7756\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9069, F1 Micro: 0.7812, F1 Macro: 0.78\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9033, F1 Micro: 0.7895, F1 Macro: 0.7908\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.79      0.71       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.75      0.85      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.48      0.48      0.47      1365\n",
      "\n",
      "Training completed in 207.52879309654236 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9046, F1 Micro: 0.7878, F1 Macro: 0.7862\n",
      "Launching training on 2 GPUs.\n",
      "577\n",
      "BESRA Uncertainty Score Threshold 206.3917251867377\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 29.793487548828125 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3621, Accuracy: 0.8936, F1 Micro: 0.7437, F1 Macro: 0.7323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.8989, F1 Micro: 0.7622, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2036, Accuracy: 0.9034, F1 Micro: 0.7736, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1667, Accuracy: 0.8992, F1 Micro: 0.7792, F1 Macro: 0.7803\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9033, F1 Micro: 0.7742, F1 Macro: 0.7681\n",
      "Epoch 6/10, Train Loss: 0.0915, Accuracy: 0.8995, F1 Micro: 0.7683, F1 Macro: 0.7649\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9055, F1 Micro: 0.7767, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9023, F1 Micro: 0.7775, F1 Macro: 0.7743\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.8928, F1 Micro: 0.7761, F1 Macro: 0.7786\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.9034, F1 Micro: 0.774, F1 Macro: 0.7727\n",
      "Model 1 - Iteration 5841: Accuracy: 0.8992, F1 Micro: 0.7792, F1 Macro: 0.7803\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.65      0.86      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 211.23759388923645 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3697, Accuracy: 0.8923, F1 Micro: 0.7455, F1 Macro: 0.7347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2454, Accuracy: 0.9041, F1 Micro: 0.7723, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.9048, F1 Micro: 0.7802, F1 Macro: 0.7768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9056, F1 Micro: 0.791, F1 Macro: 0.7899\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9047, F1 Micro: 0.7791, F1 Macro: 0.7707\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9041, F1 Micro: 0.7711, F1 Macro: 0.7631\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9058, F1 Micro: 0.7843, F1 Macro: 0.7807\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9045, F1 Micro: 0.7811, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9011, F1 Micro: 0.7803, F1 Macro: 0.7766\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.8994, F1 Micro: 0.7685, F1 Macro: 0.7658\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9056, F1 Micro: 0.791, F1 Macro: 0.7899\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.68      0.71      0.69       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.47      1365\n",
      "\n",
      "Training completed in 211.30039834976196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3567, Accuracy: 0.8928, F1 Micro: 0.7541, F1 Macro: 0.7483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9044, F1 Micro: 0.7763, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9014, F1 Micro: 0.7856, F1 Macro: 0.7881\n",
      "Epoch 5/10, Train Loss: 0.1266, Accuracy: 0.9053, F1 Micro: 0.7732, F1 Macro: 0.7669\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9044, F1 Micro: 0.7792, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9056, F1 Micro: 0.7823, F1 Macro: 0.7835\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.9045, F1 Micro: 0.7785, F1 Macro: 0.7742\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9013, F1 Micro: 0.781, F1 Macro: 0.7822\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9052, F1 Micro: 0.7734, F1 Macro: 0.7675\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9014, F1 Micro: 0.7856, F1 Macro: 0.7881\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.75      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.65      0.86      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 209.59991431236267 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9021, F1 Micro: 0.7853, F1 Macro: 0.7861\n",
      "Launching training on 2 GPUs.\n",
      "377\n",
      "BESRA Uncertainty Score Threshold 165.70446493599283\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.874772548675537 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3718, Accuracy: 0.8861, F1 Micro: 0.7104, F1 Macro: 0.6939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9013, F1 Micro: 0.7613, F1 Macro: 0.7459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.9062, F1 Micro: 0.7687, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.8963, F1 Micro: 0.7814, F1 Macro: 0.7815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1249, Accuracy: 0.8991, F1 Micro: 0.7816, F1 Macro: 0.78\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.903, F1 Micro: 0.7826, F1 Macro: 0.782\n",
      "Epoch 7/10, Train Loss: 0.0661, Accuracy: 0.9053, F1 Micro: 0.782, F1 Macro: 0.7795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0446, Accuracy: 0.9072, F1 Micro: 0.7863, F1 Macro: 0.7862\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9062, F1 Micro: 0.7832, F1 Macro: 0.7777\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9044, F1 Micro: 0.7816, F1 Macro: 0.7788\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9072, F1 Micro: 0.7863, F1 Macro: 0.7862\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.75      0.71       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 220.39342856407166 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3772, Accuracy: 0.8891, F1 Micro: 0.7282, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9039, F1 Micro: 0.7685, F1 Macro: 0.7535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1983, Accuracy: 0.9094, F1 Micro: 0.7755, F1 Macro: 0.7665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.8983, F1 Micro: 0.7807, F1 Macro: 0.7792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.907, F1 Micro: 0.7901, F1 Macro: 0.7874\n",
      "Epoch 6/10, Train Loss: 0.086, Accuracy: 0.9058, F1 Micro: 0.7772, F1 Macro: 0.772\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9042, F1 Micro: 0.777, F1 Macro: 0.771\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.9041, F1 Micro: 0.7753, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.7744\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.9034, F1 Micro: 0.7818, F1 Macro: 0.7798\n",
      "Model 2 - Iteration 6041: Accuracy: 0.907, F1 Micro: 0.7901, F1 Macro: 0.7874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 217.27910923957825 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3673, Accuracy: 0.8895, F1 Micro: 0.7271, F1 Macro: 0.7198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2429, Accuracy: 0.9013, F1 Micro: 0.7675, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.9078, F1 Micro: 0.7763, F1 Macro: 0.7689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1538, Accuracy: 0.8981, F1 Micro: 0.7805, F1 Macro: 0.7801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9033, F1 Micro: 0.7817, F1 Macro: 0.7806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.9011, F1 Micro: 0.7872, F1 Macro: 0.7894\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9033, F1 Micro: 0.7835, F1 Macro: 0.7835\n",
      "Epoch 8/10, Train Loss: 0.0472, Accuracy: 0.903, F1 Micro: 0.7819, F1 Macro: 0.7832\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9069, F1 Micro: 0.7844, F1 Macro: 0.782\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9039, F1 Micro: 0.7776, F1 Macro: 0.7775\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9011, F1 Micro: 0.7872, F1 Macro: 0.7894\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.63      0.79      0.70       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.66      0.86      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.86      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.86      0.79      1365\n",
      "         samples avg       0.48      0.49      0.47      1365\n",
      "\n",
      "Training completed in 217.92960119247437 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9051, F1 Micro: 0.7879, F1 Macro: 0.7877\n",
      "Launching training on 2 GPUs.\n",
      "177\n",
      "BESRA Uncertainty Score Threshold 98.02311200407401\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 7.3044798374176025 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3547, Accuracy: 0.8909, F1 Micro: 0.7295, F1 Macro: 0.7253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.234, Accuracy: 0.8991, F1 Micro: 0.7757, F1 Macro: 0.7718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1912, Accuracy: 0.9061, F1 Micro: 0.7804, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1613, Accuracy: 0.9059, F1 Micro: 0.7913, F1 Macro: 0.7886\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9044, F1 Micro: 0.7611, F1 Macro: 0.7531\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9058, F1 Micro: 0.7689, F1 Macro: 0.7638\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.902, F1 Micro: 0.7812, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0496, Accuracy: 0.9019, F1 Micro: 0.7684, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9023, F1 Micro: 0.7829, F1 Macro: 0.7837\n",
      "Epoch 10/10, Train Loss: 0.0274, Accuracy: 0.9042, F1 Micro: 0.7874, F1 Macro: 0.7868\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9059, F1 Micro: 0.7913, F1 Macro: 0.7886\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 222.26829361915588 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.361, Accuracy: 0.8947, F1 Micro: 0.7478, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2334, Accuracy: 0.9025, F1 Micro: 0.7789, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1901, Accuracy: 0.9067, F1 Micro: 0.7853, F1 Macro: 0.7802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1598, Accuracy: 0.908, F1 Micro: 0.7903, F1 Macro: 0.7853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9108, F1 Micro: 0.7909, F1 Macro: 0.787\n",
      "Epoch 6/10, Train Loss: 0.083, Accuracy: 0.9086, F1 Micro: 0.7824, F1 Macro: 0.7735\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.905, F1 Micro: 0.78, F1 Macro: 0.7743\n",
      "Epoch 8/10, Train Loss: 0.0476, Accuracy: 0.9022, F1 Micro: 0.762, F1 Macro: 0.7515\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9019, F1 Micro: 0.7842, F1 Macro: 0.7828\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.9059, F1 Micro: 0.7819, F1 Macro: 0.7757\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9108, F1 Micro: 0.7909, F1 Macro: 0.787\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.77      0.72      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1365\n",
      "           macro avg       0.78      0.79      0.79      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 223.9882423877716 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3492, Accuracy: 0.893, F1 Micro: 0.7366, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2309, Accuracy: 0.8992, F1 Micro: 0.7726, F1 Macro: 0.7668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9086, F1 Micro: 0.7872, F1 Macro: 0.781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.9073, F1 Micro: 0.7892, F1 Macro: 0.7882\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9052, F1 Micro: 0.7764, F1 Macro: 0.7741\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9069, F1 Micro: 0.7672, F1 Macro: 0.7617\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9044, F1 Micro: 0.7775, F1 Macro: 0.7744\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9042, F1 Micro: 0.775, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0393, Accuracy: 0.9067, F1 Micro: 0.7849, F1 Macro: 0.7845\n",
      "Epoch 10/10, Train Loss: 0.026, Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7852\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9073, F1 Micro: 0.7892, F1 Macro: 0.7882\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.68      0.75      0.71       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 223.36113691329956 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.908, F1 Micro: 0.7905, F1 Macro: 0.7879\n",
      "Total sampling time: 2480.84 seconds\n",
      "Total runtime: 13123.164631128311 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6739372,
     "sourceId": 11761127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26251.193861,
   "end_time": "2025-06-08T01:35:19.343766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-07T18:17:48.149905",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02b4c44a5dcf42588f41d48d245d3abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1e2b637b209b47458dd6a33999f26825",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a71a6f002c814a9996c966755b157546",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "067a55bb590449c1ab23f990e0cb8065": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1553511ebf9e4cbcbdfbfbd6e463af0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e8fcde3b7254a01b510c91d133d49da",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3291d1670d804e829299ea64703a203e",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1.53k/1.53kâ€‡[00:00&lt;00:00,â€‡170kB/s]"
      }
     },
     "179ac274a19e4558ab7d744f09d5fd87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19c02ce4e48f425da938b7afb4b41ee8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e2b637b209b47458dd6a33999f26825": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f603e9835714630897a112878ee2ea5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40703ed51de24f19aef0ab92af5b5a38",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d7adf3b6d73947e1bebdb52763ceb7e5",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "21331843034a46fd8bf34d52a57de1d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_49ac2068eb4343b8986f67e2765f2a39",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_386a067b09b54270864981c1217dbdf1",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "2451d873886d49f987cf05f8d1fce6a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25c486ed17e44ed79cba2358546cc6b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2790ff99758a4323971a6dd3777bdfde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2f98f6a4afb14aa797faac36439197eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30423fb62b084590ad4c6b9ef728ee93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2f98f6a4afb14aa797faac36439197eb",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b9661affe5c7428496669d12ad1904b8",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡112/112â€‡[00:00&lt;00:00,â€‡10.7kB/s]"
      }
     },
     "3291d1670d804e829299ea64703a203e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "386a067b09b54270864981c1217dbdf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "40703ed51de24f19aef0ab92af5b5a38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4888e75e83a84d75b7eaf25718fcc0b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_19c02ce4e48f425da938b7afb4b41ee8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_dd008e5787cb471181eb00114afc73f2",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:â€‡100%"
      }
     },
     "49ac2068eb4343b8986f67e2765f2a39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4bd0a03470c44b0cb80b73181ba68d46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b2932c81a410444d83505e1f85e239d8",
        "IPY_MODEL_1f603e9835714630897a112878ee2ea5",
        "IPY_MODEL_bb138cafc9044013b81e20af03116ba5"
       ],
       "layout": "IPY_MODEL_2451d873886d49f987cf05f8d1fce6a5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4fa1310306cf4f819bf8a67b0d1d9920": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eef130615fc54d43aefb45191a0bd86a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_067a55bb590449c1ab23f990e0cb8065",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:â€‡100%"
      }
     },
     "50b329d34bad4f7fb5083afca084274b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7dd06ed2a78d4a3a80c2d5e5bf673bf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4fa1310306cf4f819bf8a67b0d1d9920",
        "IPY_MODEL_21331843034a46fd8bf34d52a57de1d1",
        "IPY_MODEL_1553511ebf9e4cbcbdfbfbd6e463af0d"
       ],
       "layout": "IPY_MODEL_977e371313c64634989e55c9257fbd0c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7e8fcde3b7254a01b510c91d133d49da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80be937f35bf40ab90b03d65a9688ab9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c47dc9d24d154b8e83e141d381a4e317",
        "IPY_MODEL_b0534c6322fa49fea30ff69021b67e97",
        "IPY_MODEL_30423fb62b084590ad4c6b9ef728ee93"
       ],
       "layout": "IPY_MODEL_92e46977d5f24d62bc3c6f64efd3ce86",
       "tabbable": null,
       "tooltip": null
      }
     },
     "87ac39ba1d6b48688fc40a5415c00475": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "92e46977d5f24d62bc3c6f64efd3ce86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "977e371313c64634989e55c9257fbd0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a079244dde53477a8de681e4c323f143": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a71a6f002c814a9996c966755b157546": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "af49625722ce449d9c13d3ece0a713d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4888e75e83a84d75b7eaf25718fcc0b1",
        "IPY_MODEL_02b4c44a5dcf42588f41d48d245d3abc",
        "IPY_MODEL_d278a95dddb24cedad8b6728d4a42c69"
       ],
       "layout": "IPY_MODEL_c374a30d212d435f925fdc7a187631bd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b0534c6322fa49fea30ff69021b67e97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5d3644d67e149fd8a90b5f53e53bd99",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_25c486ed17e44ed79cba2358546cc6b3",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "b2932c81a410444d83505e1f85e239d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_179ac274a19e4558ab7d744f09d5fd87",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c117a9c6abca4d268b9639b8b35259a8",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:â€‡100%"
      }
     },
     "b9661affe5c7428496669d12ad1904b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb138cafc9044013b81e20af03116ba5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50b329d34bad4f7fb5083afca084274b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c6dd8ee65a854b7581b3c3c8d09be6c9",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡229k/229kâ€‡[00:00&lt;00:00,â€‡5.77MB/s]"
      }
     },
     "c117a9c6abca4d268b9639b8b35259a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c374a30d212d435f925fdc7a187631bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c47dc9d24d154b8e83e141d381a4e317": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea42c37f9b8942079a9754509ecae5e2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_87ac39ba1d6b48688fc40a5415c00475",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:â€‡100%"
      }
     },
     "c5d3644d67e149fd8a90b5f53e53bd99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6dd8ee65a854b7581b3c3c8d09be6c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d278a95dddb24cedad8b6728d4a42c69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a079244dde53477a8de681e4c323f143",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_2790ff99758a4323971a6dd3777bdfde",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2.00/2.00â€‡[00:00&lt;00:00,â€‡181B/s]"
      }
     },
     "d7adf3b6d73947e1bebdb52763ceb7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd008e5787cb471181eb00114afc73f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ea42c37f9b8942079a9754509ecae5e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eef130615fc54d43aefb45191a0bd86a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
