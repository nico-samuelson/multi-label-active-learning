{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647d105f",
   "metadata": {
    "papermill": {
     "duration": 0.006058,
     "end_time": "2025-03-13T06:41:41.243627",
     "exception": false,
     "start_time": "2025-03-13T06:41:41.237569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53055e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:41:41.255408Z",
     "iopub.status.busy": "2025-03-13T06:41:41.255108Z",
     "iopub.status.idle": "2025-03-13T06:42:06.494576Z",
     "shell.execute_reply": "2025-03-13T06:42:06.493647Z"
    },
    "papermill": {
     "duration": 25.247596,
     "end_time": "2025-03-13T06:42:06.496779",
     "exception": false,
     "start_time": "2025-03-13T06:41:41.249183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999991e9",
   "metadata": {
    "papermill": {
     "duration": 0.005407,
     "end_time": "2025-03-13T06:42:06.508283",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.502876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496156f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.520211Z",
     "iopub.status.busy": "2025-03-13T06:42:06.519738Z",
     "iopub.status.idle": "2025-03-13T06:42:06.523290Z",
     "shell.execute_reply": "2025-03-13T06:42:06.522523Z"
    },
    "papermill": {
     "duration": 0.010859,
     "end_time": "2025-03-13T06:42:06.524603",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.513744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c8db6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.536198Z",
     "iopub.status.busy": "2025-03-13T06:42:06.535977Z",
     "iopub.status.idle": "2025-03-13T06:42:06.539590Z",
     "shell.execute_reply": "2025-03-13T06:42:06.538960Z"
    },
    "papermill": {
     "duration": 0.010701,
     "end_time": "2025-03-13T06:42:06.540763",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.530062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c25859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.552434Z",
     "iopub.status.busy": "2025-03-13T06:42:06.552223Z",
     "iopub.status.idle": "2025-03-13T06:42:06.561551Z",
     "shell.execute_reply": "2025-03-13T06:42:06.560993Z"
    },
    "papermill": {
     "duration": 0.01649,
     "end_time": "2025-03-13T06:42:06.562800",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.546310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51d2f0",
   "metadata": {
    "papermill": {
     "duration": 0.005173,
     "end_time": "2025-03-13T06:42:06.573749",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.568576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f12b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.585257Z",
     "iopub.status.busy": "2025-03-13T06:42:06.585050Z",
     "iopub.status.idle": "2025-03-13T06:42:06.639145Z",
     "shell.execute_reply": "2025-03-13T06:42:06.637555Z"
    },
    "papermill": {
     "duration": 0.061755,
     "end_time": "2025-03-13T06:42:06.640966",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.579211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-besra'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcc4bd",
   "metadata": {
    "papermill": {
     "duration": 0.005247,
     "end_time": "2025-03-13T06:42:06.651510",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.646263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba63c0a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.663291Z",
     "iopub.status.busy": "2025-03-13T06:42:06.663032Z",
     "iopub.status.idle": "2025-03-13T06:42:06.842804Z",
     "shell.execute_reply": "2025-03-13T06:42:06.842000Z"
    },
    "papermill": {
     "duration": 0.187175,
     "end_time": "2025-03-13T06:42:06.844100",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.656925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-2/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-2/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97596298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.856067Z",
     "iopub.status.busy": "2025-03-13T06:42:06.855850Z",
     "iopub.status.idle": "2025-03-13T06:42:06.882207Z",
     "shell.execute_reply": "2025-03-13T06:42:06.881323Z"
    },
    "papermill": {
     "duration": 0.033492,
     "end_time": "2025-03-13T06:42:06.883436",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.849944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e61e00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.895646Z",
     "iopub.status.busy": "2025-03-13T06:42:06.895396Z",
     "iopub.status.idle": "2025-03-13T06:42:06.912625Z",
     "shell.execute_reply": "2025-03-13T06:42:06.911868Z"
    },
    "papermill": {
     "duration": 0.024684,
     "end_time": "2025-03-13T06:42:06.913894",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.889210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6218,) (6218, 4)\n",
      "(1555,) (1555, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "train_labels = train_data.columns[2:6]\n",
    "val_labels = val_data.columns[2:6]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['processed_text'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['processed_text'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432d296",
   "metadata": {
    "papermill": {
     "duration": 0.00546,
     "end_time": "2025-03-13T06:42:06.925263",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.919803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02bc91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:06.937519Z",
     "iopub.status.busy": "2025-03-13T06:42:06.937293Z",
     "iopub.status.idle": "2025-03-13T06:42:08.476073Z",
     "shell.execute_reply": "2025-03-13T06:42:08.475393Z"
    },
    "papermill": {
     "duration": 1.546403,
     "end_time": "2025-03-13T06:42:08.477514",
     "exception": false,
     "start_time": "2025-03-13T06:42:06.931111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2162d30133946488650a54fcef38f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abec03f47924e31965613df9585d7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38585e4d0964c3498be73400860a327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838c38e718974221aade135289f12c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948a85b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.491606Z",
     "iopub.status.busy": "2025-03-13T06:42:08.491316Z",
     "iopub.status.idle": "2025-03-13T06:42:08.495380Z",
     "shell.execute_reply": "2025-03-13T06:42:08.494777Z"
    },
    "papermill": {
     "duration": 0.012137,
     "end_time": "2025-03-13T06:42:08.496588",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.484451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf24d9",
   "metadata": {
    "papermill": {
     "duration": 0.005761,
     "end_time": "2025-03-13T06:42:08.509485",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.503724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35f4cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.522212Z",
     "iopub.status.busy": "2025-03-13T06:42:08.521856Z",
     "iopub.status.idle": "2025-03-13T06:42:08.525891Z",
     "shell.execute_reply": "2025-03-13T06:42:08.525255Z"
    },
    "papermill": {
     "duration": 0.011698,
     "end_time": "2025-03-13T06:42:08.527090",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.515392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0ded80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.539967Z",
     "iopub.status.busy": "2025-03-13T06:42:08.539761Z",
     "iopub.status.idle": "2025-03-13T06:42:08.544217Z",
     "shell.execute_reply": "2025-03-13T06:42:08.543616Z"
    },
    "papermill": {
     "duration": 0.01221,
     "end_time": "2025-03-13T06:42:08.545351",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.533141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    )  \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33be33ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.558396Z",
     "iopub.status.busy": "2025-03-13T06:42:08.558192Z",
     "iopub.status.idle": "2025-03-13T06:42:08.570418Z",
     "shell.execute_reply": "2025-03-13T06:42:08.569646Z"
    },
    "papermill": {
     "duration": 0.020107,
     "end_time": "2025-03-13T06:42:08.571645",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.551538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(train_labels),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230795be",
   "metadata": {
    "papermill": {
     "duration": 0.005785,
     "end_time": "2025-03-13T06:42:08.583511",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.577726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8046b8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.596422Z",
     "iopub.status.busy": "2025-03-13T06:42:08.596180Z",
     "iopub.status.idle": "2025-03-13T06:42:08.601621Z",
     "shell.execute_reply": "2025-03-13T06:42:08.600808Z"
    },
    "papermill": {
     "duration": 0.013159,
     "end_time": "2025-03-13T06:42:08.602783",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.589624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0d3aa",
   "metadata": {
    "papermill": {
     "duration": 0.005761,
     "end_time": "2025-03-13T06:42:08.614518",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.608757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c698db2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.627424Z",
     "iopub.status.busy": "2025-03-13T06:42:08.627194Z",
     "iopub.status.idle": "2025-03-13T06:42:08.646773Z",
     "shell.execute_reply": "2025-03-13T06:42:08.645975Z"
    },
    "papermill": {
     "duration": 0.0275,
     "end_time": "2025-03-13T06:42:08.647995",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.620495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (âˆ†Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = NetifierDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "    \n",
    "    accelerator.wait_for_everyone()    \n",
    "    if accelerator.is_local_main_process:\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'pornografi': [y_train[i][0] for i in temp],\n",
    "                'sara': [y_train[i][1] for i in temp],\n",
    "                'radikalisme': [y_train[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'pornografi': [y_train[i][0] for i in temp],\n",
    "                    'sara': [y_train[i][1] for i in temp],\n",
    "                    'radikalisme': [y_train[i][2] for i in temp],\n",
    "                    'pencemaran_nama_baik': [y_train[i][3] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ce52b",
   "metadata": {
    "papermill": {
     "duration": 0.005771,
     "end_time": "2025-03-13T06:42:08.659891",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.654120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6728c1b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.672497Z",
     "iopub.status.busy": "2025-03-13T06:42:08.672277Z",
     "iopub.status.idle": "2025-03-13T06:42:08.681747Z",
     "shell.execute_reply": "2025-03-13T06:42:08.681155Z"
    },
    "papermill": {
     "duration": 0.017147,
     "end_time": "2025-03-13T06:42:08.682978",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.665831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae4aa44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.695841Z",
     "iopub.status.busy": "2025-03-13T06:42:08.695637Z",
     "iopub.status.idle": "2025-03-13T06:42:08.698680Z",
     "shell.execute_reply": "2025-03-13T06:42:08.698106Z"
    },
    "papermill": {
     "duration": 0.010859,
     "end_time": "2025-03-13T06:42:08.699959",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.689100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43532ba5",
   "metadata": {
    "papermill": {
     "duration": 0.005732,
     "end_time": "2025-03-13T06:42:08.711623",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.705891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98796daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T06:42:08.724079Z",
     "iopub.status.busy": "2025-03-13T06:42:08.723874Z",
     "iopub.status.idle": "2025-03-13T10:00:43.960845Z",
     "shell.execute_reply": "2025-03-13T10:00:43.959849Z"
    },
    "papermill": {
     "duration": 11915.244869,
     "end_time": "2025-03-13T10:00:43.962419",
     "exception": false,
     "start_time": "2025-03-13T06:42:08.717550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4486, Accuracy: 0.7886, F1 Micro: 0.0174, F1 Macro: 0.0157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3911, Accuracy: 0.8078, F1 Micro: 0.18, F1 Macro: 0.1337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.381, Accuracy: 0.8263, F1 Micro: 0.3293, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3204, Accuracy: 0.8355, F1 Micro: 0.416, F1 Macro: 0.3338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.281, Accuracy: 0.8527, F1 Micro: 0.5464, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2621, Accuracy: 0.8564, F1 Micro: 0.5767, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2083, Accuracy: 0.8581, F1 Micro: 0.5979, F1 Macro: 0.5834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1662, Accuracy: 0.8673, F1 Micro: 0.6473, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.149, Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "Model 1 - Iteration 388: Accuracy: 0.867, F1 Micro: 0.6581, F1 Macro: 0.6506\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.84      0.86       370\n",
      "                sara       0.63      0.47      0.54       248\n",
      "         radikalisme       0.67      0.62      0.64       243\n",
      "pencemaran_nama_baik       0.66      0.48      0.56       504\n",
      "\n",
      "           micro avg       0.73      0.60      0.66      1365\n",
      "           macro avg       0.71      0.60      0.65      1365\n",
      "        weighted avg       0.72      0.60      0.65      1365\n",
      "         samples avg       0.34      0.33      0.33      1365\n",
      "\n",
      "Training completed in 57.58709406852722 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4335, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3767, Accuracy: 0.8175, F1 Micro: 0.2561, F1 Macro: 0.1748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3582, Accuracy: 0.8288, F1 Micro: 0.3613, F1 Macro: 0.2669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2972, Accuracy: 0.8455, F1 Micro: 0.5013, F1 Macro: 0.467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2541, Accuracy: 0.855, F1 Micro: 0.5739, F1 Macro: 0.5572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2294, Accuracy: 0.8559, F1 Micro: 0.582, F1 Macro: 0.5669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1796, Accuracy: 0.8684, F1 Micro: 0.6509, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1463, Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "Epoch 10/10, Train Loss: 0.132, Accuracy: 0.8741, F1 Micro: 0.6753, F1 Macro: 0.6652\n",
      "Model 2 - Iteration 388: Accuracy: 0.8675, F1 Micro: 0.6822, F1 Macro: 0.6778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.57      0.52      0.54       248\n",
      "         radikalisme       0.65      0.79      0.71       243\n",
      "pencemaran_nama_baik       0.66      0.55      0.60       504\n",
      "\n",
      "           micro avg       0.70      0.67      0.68      1365\n",
      "           macro avg       0.69      0.68      0.68      1365\n",
      "        weighted avg       0.70      0.67      0.68      1365\n",
      "         samples avg       0.36      0.37      0.36      1365\n",
      "\n",
      "Training completed in 57.10183382034302 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5779, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.456, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4117, Accuracy: 0.7941, F1 Micro: 0.0666, F1 Macro: 0.0564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3928, Accuracy: 0.8244, F1 Micro: 0.3237, F1 Macro: 0.2387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3221, Accuracy: 0.8383, F1 Micro: 0.4396, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2818, Accuracy: 0.8533, F1 Micro: 0.5802, F1 Macro: 0.5753\n",
      "Epoch 7/10, Train Loss: 0.2525, Accuracy: 0.8531, F1 Micro: 0.5528, F1 Macro: 0.53\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1925, Accuracy: 0.8631, F1 Micro: 0.6244, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1636, Accuracy: 0.8678, F1 Micro: 0.6672, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1389, Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "Model 3 - Iteration 388: Accuracy: 0.8745, F1 Micro: 0.6812, F1 Macro: 0.675\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.84      0.86       370\n",
      "                sara       0.64      0.52      0.57       248\n",
      "         radikalisme       0.69      0.65      0.67       243\n",
      "pencemaran_nama_baik       0.70      0.52      0.59       504\n",
      "\n",
      "           micro avg       0.74      0.63      0.68      1365\n",
      "           macro avg       0.73      0.63      0.67      1365\n",
      "        weighted avg       0.74      0.63      0.68      1365\n",
      "         samples avg       0.35      0.35      0.34      1365\n",
      "\n",
      "Training completed in 58.66919231414795 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8697, F1 Micro: 0.6738, F1 Macro: 0.6678\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 123.3062379360199 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.8125, F1 Micro: 0.2167, F1 Macro: 0.1636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4094, Accuracy: 0.8358, F1 Micro: 0.4018, F1 Macro: 0.3083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3171, Accuracy: 0.8622, F1 Micro: 0.5935, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2572, Accuracy: 0.8727, F1 Micro: 0.6504, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2238, Accuracy: 0.8788, F1 Micro: 0.6739, F1 Macro: 0.6619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1796, Accuracy: 0.8864, F1 Micro: 0.7109, F1 Macro: 0.7032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1386, Accuracy: 0.8869, F1 Micro: 0.7344, F1 Macro: 0.7304\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.8877, F1 Micro: 0.731, F1 Macro: 0.7205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0997, Accuracy: 0.8908, F1 Micro: 0.7412, F1 Macro: 0.7288\n",
      "Epoch 10/10, Train Loss: 0.0851, Accuracy: 0.8841, F1 Micro: 0.6851, F1 Macro: 0.6596\n",
      "Model 1 - Iteration 971: Accuracy: 0.8908, F1 Micro: 0.7412, F1 Macro: 0.7288\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       370\n",
      "                sara       0.64      0.52      0.57       248\n",
      "         radikalisme       0.71      0.78      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.73      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.73      0.74      1365\n",
      "           macro avg       0.74      0.72      0.73      1365\n",
      "        weighted avg       0.75      0.73      0.74      1365\n",
      "         samples avg       0.42      0.41      0.41      1365\n",
      "\n",
      "Training completed in 73.2979085445404 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.513, Accuracy: 0.8188, F1 Micro: 0.2649, F1 Macro: 0.1931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3905, Accuracy: 0.8384, F1 Micro: 0.4236, F1 Macro: 0.3361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3043, Accuracy: 0.8631, F1 Micro: 0.5982, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2465, Accuracy: 0.8722, F1 Micro: 0.6498, F1 Macro: 0.6243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2128, Accuracy: 0.8806, F1 Micro: 0.6801, F1 Macro: 0.6695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.167, Accuracy: 0.8872, F1 Micro: 0.7146, F1 Macro: 0.7082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1335, Accuracy: 0.8859, F1 Micro: 0.7274, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.8881, F1 Micro: 0.7356, F1 Macro: 0.7245\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.888, F1 Micro: 0.7211, F1 Macro: 0.7018\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.8861, F1 Micro: 0.7076, F1 Macro: 0.6787\n",
      "Model 2 - Iteration 971: Accuracy: 0.8881, F1 Micro: 0.7356, F1 Macro: 0.7245\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.68      0.51      0.58       248\n",
      "         radikalisme       0.70      0.78      0.74       243\n",
      "pencemaran_nama_baik       0.68      0.72      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.73      0.74      1365\n",
      "           macro avg       0.74      0.72      0.72      1365\n",
      "        weighted avg       0.74      0.73      0.73      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 73.78225708007812 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5416, Accuracy: 0.8066, F1 Micro: 0.1747, F1 Macro: 0.1351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4215, Accuracy: 0.8345, F1 Micro: 0.4034, F1 Macro: 0.3142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3211, Accuracy: 0.8578, F1 Micro: 0.5791, F1 Macro: 0.562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2493, Accuracy: 0.8703, F1 Micro: 0.6471, F1 Macro: 0.6226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2232, Accuracy: 0.878, F1 Micro: 0.6777, F1 Macro: 0.6641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1756, Accuracy: 0.885, F1 Micro: 0.7208, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1301, Accuracy: 0.8852, F1 Micro: 0.7317, F1 Macro: 0.7242\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.8848, F1 Micro: 0.7237, F1 Macro: 0.7102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0935, Accuracy: 0.8883, F1 Micro: 0.739, F1 Macro: 0.7299\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.8842, F1 Micro: 0.7047, F1 Macro: 0.6889\n",
      "Model 3 - Iteration 971: Accuracy: 0.8883, F1 Micro: 0.739, F1 Macro: 0.7299\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.88      0.87       370\n",
      "                sara       0.64      0.55      0.59       248\n",
      "         radikalisme       0.69      0.82      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.69      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.74      0.74      1365\n",
      "           macro avg       0.73      0.74      0.73      1365\n",
      "        weighted avg       0.74      0.74      0.74      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 73.62653255462646 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8794, F1 Micro: 0.7062, F1 Macro: 0.6978\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 110.51404452323914 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5091, Accuracy: 0.8309, F1 Micro: 0.3908, F1 Macro: 0.2868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3576, Accuracy: 0.8677, F1 Micro: 0.6586, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.278, Accuracy: 0.8781, F1 Micro: 0.6811, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2241, Accuracy: 0.8844, F1 Micro: 0.7107, F1 Macro: 0.6959\n",
      "Epoch 5/10, Train Loss: 0.1791, Accuracy: 0.8855, F1 Micro: 0.7036, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.89, F1 Micro: 0.7246, F1 Macro: 0.7138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.8895, F1 Micro: 0.725, F1 Macro: 0.7135\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.8898, F1 Micro: 0.7192, F1 Macro: 0.7083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.8902, F1 Micro: 0.7384, F1 Macro: 0.7293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.8933, F1 Micro: 0.7549, F1 Macro: 0.7522\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8933, F1 Micro: 0.7549, F1 Macro: 0.7522\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.66      0.65      0.65       248\n",
      "         radikalisme       0.69      0.85      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.71      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.77      0.75      1365\n",
      "           macro avg       0.74      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.76      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 87.92854714393616 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4871, Accuracy: 0.8339, F1 Micro: 0.4051, F1 Macro: 0.2985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3456, Accuracy: 0.8666, F1 Micro: 0.6656, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2681, Accuracy: 0.8814, F1 Micro: 0.7077, F1 Macro: 0.6955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.213, Accuracy: 0.8867, F1 Micro: 0.7162, F1 Macro: 0.7023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1694, Accuracy: 0.89, F1 Micro: 0.722, F1 Macro: 0.6963\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.8898, F1 Micro: 0.7161, F1 Macro: 0.7046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.8881, F1 Micro: 0.7308, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.8906, F1 Micro: 0.7495, F1 Macro: 0.743\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.8913, F1 Micro: 0.742, F1 Macro: 0.7357\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.8927, F1 Micro: 0.7409, F1 Macro: 0.734\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8906, F1 Micro: 0.7495, F1 Macro: 0.743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       370\n",
      "                sara       0.67      0.59      0.63       248\n",
      "         radikalisme       0.72      0.79      0.75       243\n",
      "pencemaran_nama_baik       0.67      0.76      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1365\n",
      "           macro avg       0.73      0.75      0.74      1365\n",
      "        weighted avg       0.74      0.77      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 85.88097262382507 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5167, Accuracy: 0.8209, F1 Micro: 0.3322, F1 Macro: 0.2403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3616, Accuracy: 0.8639, F1 Micro: 0.6292, F1 Macro: 0.6188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2783, Accuracy: 0.8783, F1 Micro: 0.6996, F1 Macro: 0.69\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.225, Accuracy: 0.8831, F1 Micro: 0.7043, F1 Macro: 0.6887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.8867, F1 Micro: 0.7136, F1 Macro: 0.6896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.8891, F1 Micro: 0.7319, F1 Macro: 0.7239\n",
      "Epoch 7/10, Train Loss: 0.1026, Accuracy: 0.8844, F1 Micro: 0.6886, F1 Macro: 0.6715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.8897, F1 Micro: 0.7346, F1 Macro: 0.7254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.8913, F1 Micro: 0.735, F1 Macro: 0.7249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.8875, F1 Micro: 0.7484, F1 Macro: 0.7476\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8875, F1 Micro: 0.7484, F1 Macro: 0.7476\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       370\n",
      "                sara       0.64      0.66      0.65       248\n",
      "         radikalisme       0.68      0.88      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.73      0.69       504\n",
      "\n",
      "           micro avg       0.72      0.78      0.75      1365\n",
      "           macro avg       0.72      0.79      0.75      1365\n",
      "        weighted avg       0.72      0.78      0.75      1365\n",
      "         samples avg       0.43      0.44      0.42      1365\n",
      "\n",
      "Training completed in 88.334068775177 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8831, F1 Micro: 0.7211, F1 Macro: 0.7144\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 99.604336977005 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4804, Accuracy: 0.8439, F1 Micro: 0.4661, F1 Macro: 0.389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3293, Accuracy: 0.8777, F1 Micro: 0.6779, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2687, Accuracy: 0.8881, F1 Micro: 0.7161, F1 Macro: 0.7006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2276, Accuracy: 0.8898, F1 Micro: 0.7529, F1 Macro: 0.7464\n",
      "Epoch 5/10, Train Loss: 0.1785, Accuracy: 0.8944, F1 Micro: 0.743, F1 Macro: 0.7376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1442, Accuracy: 0.8908, F1 Micro: 0.7543, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.8942, F1 Micro: 0.7546, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.8931, F1 Micro: 0.7562, F1 Macro: 0.7523\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.8934, F1 Micro: 0.7373, F1 Macro: 0.7294\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.892, F1 Micro: 0.7535, F1 Macro: 0.7468\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8931, F1 Micro: 0.7562, F1 Macro: 0.7523\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.63      0.67      0.65       248\n",
      "         radikalisme       0.70      0.81      0.75       243\n",
      "pencemaran_nama_baik       0.69      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.78      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 99.41589426994324 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4587, Accuracy: 0.8512, F1 Micro: 0.5206, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3203, Accuracy: 0.8769, F1 Micro: 0.6658, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2633, Accuracy: 0.8877, F1 Micro: 0.7181, F1 Macro: 0.7001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2216, Accuracy: 0.8816, F1 Micro: 0.7444, F1 Macro: 0.7384\n",
      "Epoch 5/10, Train Loss: 0.1763, Accuracy: 0.8947, F1 Micro: 0.7369, F1 Macro: 0.724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.8933, F1 Micro: 0.7508, F1 Macro: 0.7448\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.8969, F1 Micro: 0.7508, F1 Macro: 0.7417\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.8945, F1 Micro: 0.7431, F1 Macro: 0.7359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.8956, F1 Micro: 0.7573, F1 Macro: 0.7528\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.8944, F1 Micro: 0.7522, F1 Macro: 0.7432\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8956, F1 Micro: 0.7573, F1 Macro: 0.7528\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.65      0.65      0.65       248\n",
      "         radikalisme       0.68      0.84      0.75       243\n",
      "pencemaran_nama_baik       0.72      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.76      0.76      1365\n",
      "           macro avg       0.74      0.77      0.75      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 98.37487626075745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4881, Accuracy: 0.8352, F1 Micro: 0.4123, F1 Macro: 0.3352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3348, Accuracy: 0.8717, F1 Micro: 0.6637, F1 Macro: 0.6548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.8833, F1 Micro: 0.6984, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.223, Accuracy: 0.8827, F1 Micro: 0.7467, F1 Macro: 0.7427\n",
      "Epoch 5/10, Train Loss: 0.1799, Accuracy: 0.8923, F1 Micro: 0.7297, F1 Macro: 0.7123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.8897, F1 Micro: 0.7509, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.8934, F1 Micro: 0.7524, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.078, Accuracy: 0.8925, F1 Micro: 0.7532, F1 Macro: 0.7502\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.8939, F1 Micro: 0.7387, F1 Macro: 0.7341\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.8916, F1 Micro: 0.7373, F1 Macro: 0.7313Model 3 - Iteration 1969: Accuracy: 0.8925, F1 Micro: 0.7532, F1 Macro: 0.7502\n",
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.61      0.71      0.66       248\n",
      "         radikalisme       0.73      0.76      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.77      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.76      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 98.74247765541077 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8857, F1 Micro: 0.7297, F1 Macro: 0.7237\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 89.76929903030396 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4819, Accuracy: 0.8427, F1 Micro: 0.4714, F1 Macro: 0.3625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3207, Accuracy: 0.8794, F1 Micro: 0.7104, F1 Macro: 0.6994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2517, Accuracy: 0.8925, F1 Micro: 0.7427, F1 Macro: 0.736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2066, Accuracy: 0.8947, F1 Micro: 0.746, F1 Macro: 0.7391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1569, Accuracy: 0.8913, F1 Micro: 0.7502, F1 Macro: 0.7415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1161, Accuracy: 0.8947, F1 Micro: 0.7648, F1 Macro: 0.7603\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.8938, F1 Micro: 0.7409, F1 Macro: 0.7324\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.8928, F1 Micro: 0.7448, F1 Macro: 0.737\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.8936, F1 Micro: 0.7366, F1 Macro: 0.7247\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.8959, F1 Micro: 0.7533, F1 Macro: 0.7448\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8947, F1 Micro: 0.7648, F1 Macro: 0.7603\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.78      0.74      0.76       243\n",
      "pencemaran_nama_baik       0.66      0.83      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.80      0.76      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 112.08278203010559 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.467, Accuracy: 0.8508, F1 Micro: 0.5339, F1 Macro: 0.4446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.314, Accuracy: 0.8822, F1 Micro: 0.7199, F1 Macro: 0.7106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2456, Accuracy: 0.8942, F1 Micro: 0.7446, F1 Macro: 0.7394\n",
      "Epoch 4/10, Train Loss: 0.2036, Accuracy: 0.8933, F1 Micro: 0.7416, F1 Macro: 0.7349\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.8891, F1 Micro: 0.7368, F1 Macro: 0.7248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1126, Accuracy: 0.892, F1 Micro: 0.7603, F1 Macro: 0.7533\n",
      "Epoch 7/10, Train Loss: 0.0837, Accuracy: 0.8938, F1 Micro: 0.7571, F1 Macro: 0.7504\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.8969, F1 Micro: 0.7559, F1 Macro: 0.7459\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8958, F1 Micro: 0.7534, F1 Macro: 0.7424\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.8944, F1 Micro: 0.7563, F1 Macro: 0.7495\n",
      "Model 2 - Iteration 2394: Accuracy: 0.892, F1 Micro: 0.7603, F1 Macro: 0.7533\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.92      0.88       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.75      0.73      0.74       243\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.80      0.76      1365\n",
      "           macro avg       0.73      0.78      0.75      1365\n",
      "        weighted avg       0.73      0.80      0.76      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 109.2192952632904 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4952, Accuracy: 0.8342, F1 Micro: 0.4186, F1 Macro: 0.3123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3228, Accuracy: 0.8764, F1 Micro: 0.708, F1 Macro: 0.7052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2553, Accuracy: 0.8928, F1 Micro: 0.7407, F1 Macro: 0.7333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2084, Accuracy: 0.8903, F1 Micro: 0.7471, F1 Macro: 0.7447\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.8944, F1 Micro: 0.7436, F1 Macro: 0.7329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.8952, F1 Micro: 0.7598, F1 Macro: 0.753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0943, Accuracy: 0.8902, F1 Micro: 0.7623, F1 Macro: 0.7625\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.8948, F1 Micro: 0.759, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.8989, F1 Micro: 0.7634, F1 Macro: 0.757\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.8969, F1 Micro: 0.7603, F1 Macro: 0.753\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8989, F1 Micro: 0.7634, F1 Macro: 0.757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.67      0.63      0.65       248\n",
      "         radikalisme       0.74      0.77      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.76      0.76      0.76      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 114.26971673965454 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8876, F1 Micro: 0.7364, F1 Macro: 0.7303\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 81.1163957118988 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4529, Accuracy: 0.8669, F1 Micro: 0.6257, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.305, Accuracy: 0.8845, F1 Micro: 0.7345, F1 Macro: 0.733\n",
      "Epoch 3/10, Train Loss: 0.2451, Accuracy: 0.8908, F1 Micro: 0.724, F1 Macro: 0.7128\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.8914, F1 Micro: 0.7141, F1 Macro: 0.6978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.8963, F1 Micro: 0.7657, F1 Macro: 0.7605\n",
      "Epoch 6/10, Train Loss: 0.1156, Accuracy: 0.8969, F1 Micro: 0.7566, F1 Macro: 0.7471\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.8975, F1 Micro: 0.7601, F1 Macro: 0.7544\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.8967, F1 Micro: 0.765, F1 Macro: 0.759\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.8945, F1 Micro: 0.7595, F1 Macro: 0.7552\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.8991, F1 Micro: 0.757, F1 Macro: 0.7529\n",
      "Model 1 - Iteration 2777: Accuracy: 0.8963, F1 Micro: 0.7657, F1 Macro: 0.7605\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.91      0.89       370\n",
      "                sara       0.67      0.64      0.65       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1365\n",
      "           macro avg       0.73      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.76      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 118.83433246612549 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4341, Accuracy: 0.8727, F1 Micro: 0.6739, F1 Macro: 0.6497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2933, Accuracy: 0.8881, F1 Micro: 0.7377, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.8963, F1 Micro: 0.74, F1 Macro: 0.7378\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.8955, F1 Micro: 0.7282, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.8952, F1 Micro: 0.7561, F1 Macro: 0.7482\n",
      "Epoch 6/10, Train Loss: 0.1105, Accuracy: 0.8975, F1 Micro: 0.7549, F1 Macro: 0.7478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.8973, F1 Micro: 0.7678, F1 Macro: 0.764\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.8933, F1 Micro: 0.7544, F1 Macro: 0.7453\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.8959, F1 Micro: 0.7666, F1 Macro: 0.76\n",
      "Epoch 10/10, Train Loss: 0.0376, Accuracy: 0.8969, F1 Micro: 0.7485, F1 Macro: 0.7413\n",
      "Model 2 - Iteration 2777: Accuracy: 0.8973, F1 Micro: 0.7678, F1 Macro: 0.764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.89       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.76      0.76      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 122.1164915561676 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4628, Accuracy: 0.8634, F1 Micro: 0.6216, F1 Macro: 0.5882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3061, Accuracy: 0.8873, F1 Micro: 0.7344, F1 Macro: 0.7317\n",
      "Epoch 3/10, Train Loss: 0.2444, Accuracy: 0.8903, F1 Micro: 0.726, F1 Macro: 0.7242\n",
      "Epoch 4/10, Train Loss: 0.1966, Accuracy: 0.8939, F1 Micro: 0.7227, F1 Macro: 0.7095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1569, Accuracy: 0.8959, F1 Micro: 0.7594, F1 Macro: 0.7467\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.8989, F1 Micro: 0.7533, F1 Macro: 0.7423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.8939, F1 Micro: 0.7607, F1 Macro: 0.7547\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.8947, F1 Micro: 0.7547, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.8994, F1 Micro: 0.7617, F1 Macro: 0.7558\n",
      "Epoch 10/10, Train Loss: 0.0383, Accuracy: 0.8963, F1 Micro: 0.7548, F1 Macro: 0.7446\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8994, F1 Micro: 0.7617, F1 Macro: 0.7558\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.61      0.64       248\n",
      "         radikalisme       0.71      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.73      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1365\n",
      "           macro avg       0.76      0.75      0.76      1365\n",
      "        weighted avg       0.77      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 121.31765484809875 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8893, F1 Micro: 0.7411, F1 Macro: 0.7353\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 73.41370701789856 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4473, Accuracy: 0.8708, F1 Micro: 0.6553, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2947, Accuracy: 0.8842, F1 Micro: 0.7351, F1 Macro: 0.7272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2302, Accuracy: 0.8947, F1 Micro: 0.746, F1 Macro: 0.7328\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.8961, F1 Micro: 0.7377, F1 Macro: 0.7252\n",
      "Epoch 5/10, Train Loss: 0.1512, Accuracy: 0.8948, F1 Micro: 0.7218, F1 Macro: 0.7064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9005, F1 Micro: 0.7541, F1 Macro: 0.7424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.8975, F1 Micro: 0.7585, F1 Macro: 0.7508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.8991, F1 Micro: 0.7609, F1 Macro: 0.7538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.8983, F1 Micro: 0.7616, F1 Macro: 0.755\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.8995, F1 Micro: 0.7602, F1 Macro: 0.7555\n",
      "Model 1 - Iteration 3122: Accuracy: 0.8983, F1 Micro: 0.7616, F1 Macro: 0.755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.68      0.61      0.64       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1365\n",
      "           macro avg       0.75      0.76      0.75      1365\n",
      "        weighted avg       0.76      0.76      0.76      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 135.63938570022583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4322, Accuracy: 0.8703, F1 Micro: 0.6462, F1 Macro: 0.6186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2887, Accuracy: 0.8916, F1 Micro: 0.7521, F1 Macro: 0.7443\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.8955, F1 Micro: 0.7369, F1 Macro: 0.7194\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.8961, F1 Micro: 0.7249, F1 Macro: 0.7075\n",
      "Epoch 5/10, Train Loss: 0.1433, Accuracy: 0.8989, F1 Micro: 0.7382, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9017, F1 Micro: 0.7584, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9005, F1 Micro: 0.7718, F1 Macro: 0.7665\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9034, F1 Micro: 0.7691, F1 Macro: 0.7625\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.898, F1 Micro: 0.7584, F1 Macro: 0.7496\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9019, F1 Micro: 0.7665, F1 Macro: 0.7615\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9005, F1 Micro: 0.7718, F1 Macro: 0.7665\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       370\n",
      "                sara       0.65      0.63      0.64       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 129.9207079410553 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4575, Accuracy: 0.8623, F1 Micro: 0.6048, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2956, Accuracy: 0.8878, F1 Micro: 0.738, F1 Macro: 0.7274\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.8948, F1 Micro: 0.7337, F1 Macro: 0.7212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.8981, F1 Micro: 0.7443, F1 Macro: 0.7368\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.8978, F1 Micro: 0.7378, F1 Macro: 0.7258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9025, F1 Micro: 0.7578, F1 Macro: 0.74\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.8984, F1 Micro: 0.7633, F1 Macro: 0.7586\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.8972, F1 Micro: 0.7469, F1 Macro: 0.7353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8983, F1 Micro: 0.7654, F1 Macro: 0.7611\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9002, F1 Micro: 0.7577, F1 Macro: 0.7516\n",
      "Model 3 - Iteration 3122: Accuracy: 0.8983, F1 Micro: 0.7654, F1 Macro: 0.7611\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.66      0.64      0.65       248\n",
      "         radikalisme       0.73      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.73      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.75      0.77      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 133.8638994693756 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8907, F1 Micro: 0.7447, F1 Macro: 0.739\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 66.489737033844 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4422, Accuracy: 0.8717, F1 Micro: 0.6814, F1 Macro: 0.6696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2843, Accuracy: 0.8889, F1 Micro: 0.7308, F1 Macro: 0.7232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2367, Accuracy: 0.8956, F1 Micro: 0.7604, F1 Macro: 0.752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1935, Accuracy: 0.8991, F1 Micro: 0.7671, F1 Macro: 0.7613\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9009, F1 Micro: 0.755, F1 Macro: 0.7472\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.8994, F1 Micro: 0.7593, F1 Macro: 0.7514\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.8997, F1 Micro: 0.7641, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.8981, F1 Micro: 0.7699, F1 Macro: 0.7686\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.8988, F1 Micro: 0.7691, F1 Macro: 0.7629\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.8995, F1 Micro: 0.7668, F1 Macro: 0.7598\n",
      "Model 1 - Iteration 3432: Accuracy: 0.8981, F1 Micro: 0.7699, F1 Macro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.63      0.70      0.67       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 140.01513004302979 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4215, Accuracy: 0.8709, F1 Micro: 0.6904, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2774, Accuracy: 0.8894, F1 Micro: 0.736, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2307, Accuracy: 0.8959, F1 Micro: 0.7621, F1 Macro: 0.7523\n",
      "Epoch 4/10, Train Loss: 0.187, Accuracy: 0.9009, F1 Micro: 0.7556, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9025, F1 Micro: 0.7754, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.102, Accuracy: 0.8992, F1 Micro: 0.7701, F1 Macro: 0.765\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.8975, F1 Micro: 0.77, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.8975, F1 Micro: 0.766, F1 Macro: 0.7624\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9017, F1 Micro: 0.7735, F1 Macro: 0.7693\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.8984, F1 Micro: 0.7719, F1 Macro: 0.7672\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9025, F1 Micro: 0.7754, F1 Macro: 0.7726\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.89       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 138.92695116996765 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4438, Accuracy: 0.8712, F1 Micro: 0.6814, F1 Macro: 0.6713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2843, Accuracy: 0.8927, F1 Micro: 0.747, F1 Macro: 0.7417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.8992, F1 Micro: 0.7747, F1 Macro: 0.7703\n",
      "Epoch 4/10, Train Loss: 0.187, Accuracy: 0.9003, F1 Micro: 0.7646, F1 Macro: 0.7593\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9008, F1 Micro: 0.7639, F1 Macro: 0.7565\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.8963, F1 Micro: 0.7537, F1 Macro: 0.7447\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.8998, F1 Micro: 0.77, F1 Macro: 0.7672\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.8988, F1 Micro: 0.76, F1 Macro: 0.7532\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9003, F1 Micro: 0.767, F1 Macro: 0.7599\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.8975, F1 Micro: 0.7616, F1 Macro: 0.7554\n",
      "Model 3 - Iteration 3432: Accuracy: 0.8992, F1 Micro: 0.7747, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.70      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 137.68032479286194 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8918, F1 Micro: 0.7483, F1 Macro: 0.7429\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 60.40176177024841 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4337, Accuracy: 0.8734, F1 Micro: 0.6826, F1 Macro: 0.6786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.8952, F1 Micro: 0.7457, F1 Macro: 0.7306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2343, Accuracy: 0.8992, F1 Micro: 0.7545, F1 Macro: 0.7367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1832, Accuracy: 0.8991, F1 Micro: 0.7553, F1 Macro: 0.747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1505, Accuracy: 0.9002, F1 Micro: 0.7566, F1 Macro: 0.7506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.8995, F1 Micro: 0.7729, F1 Macro: 0.7722\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.8959, F1 Micro: 0.753, F1 Macro: 0.7427\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.8958, F1 Micro: 0.7696, F1 Macro: 0.7667\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.8997, F1 Micro: 0.7631, F1 Macro: 0.7581\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9006, F1 Micro: 0.7687, F1 Macro: 0.7608\n",
      "Model 1 - Iteration 3711: Accuracy: 0.8995, F1 Micro: 0.7729, F1 Macro: 0.7722\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.69      0.67       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.78      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 151.51663970947266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4153, Accuracy: 0.8794, F1 Micro: 0.7119, F1 Macro: 0.7104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2704, Accuracy: 0.8975, F1 Micro: 0.7515, F1 Macro: 0.742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.8997, F1 Micro: 0.7613, F1 Macro: 0.7449\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9019, F1 Micro: 0.761, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9016, F1 Micro: 0.7651, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9028, F1 Micro: 0.7732, F1 Macro: 0.7724\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.8986, F1 Micro: 0.7601, F1 Macro: 0.7539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9003, F1 Micro: 0.7755, F1 Macro: 0.7714\n",
      "Epoch 9/10, Train Loss: 0.0458, Accuracy: 0.9027, F1 Micro: 0.7645, F1 Macro: 0.7548\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9008, F1 Micro: 0.77, F1 Macro: 0.7616\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9003, F1 Micro: 0.7755, F1 Macro: 0.7714\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.65      0.70      0.68       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 152.63249158859253 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4358, Accuracy: 0.8737, F1 Micro: 0.6846, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.8928, F1 Micro: 0.7386, F1 Macro: 0.7225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.8978, F1 Micro: 0.7504, F1 Macro: 0.7324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.8992, F1 Micro: 0.753, F1 Macro: 0.743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.149, Accuracy: 0.9025, F1 Micro: 0.7594, F1 Macro: 0.7548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9003, F1 Micro: 0.7675, F1 Macro: 0.7638\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.8989, F1 Micro: 0.7622, F1 Macro: 0.7527\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.8997, F1 Micro: 0.7633, F1 Macro: 0.7534\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9, F1 Micro: 0.7623, F1 Macro: 0.7517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.8991, F1 Micro: 0.7693, F1 Macro: 0.7618\n",
      "Model 3 - Iteration 3711: Accuracy: 0.8991, F1 Micro: 0.7693, F1 Macro: 0.7618\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.69      0.59      0.63       248\n",
      "         radikalisme       0.75      0.79      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 151.06589794158936 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8927, F1 Micro: 0.751, F1 Macro: 0.7457\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 54.20586967468262 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4279, Accuracy: 0.872, F1 Micro: 0.6447, F1 Macro: 0.635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2816, Accuracy: 0.8892, F1 Micro: 0.7602, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2277, Accuracy: 0.8966, F1 Micro: 0.7619, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1877, Accuracy: 0.8992, F1 Micro: 0.771, F1 Macro: 0.7668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.8977, F1 Micro: 0.7753, F1 Macro: 0.7736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1078, Accuracy: 0.9052, F1 Micro: 0.7763, F1 Macro: 0.7705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9038, F1 Micro: 0.7763, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.8983, F1 Micro: 0.777, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.8978, F1 Micro: 0.7774, F1 Macro: 0.7769\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.8994, F1 Micro: 0.7622, F1 Macro: 0.7546\n",
      "Model 1 - Iteration 3886: Accuracy: 0.8978, F1 Micro: 0.7774, F1 Macro: 0.7769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.64      0.77      0.69       248\n",
      "         radikalisme       0.70      0.86      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 160.12366223335266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.8781, F1 Micro: 0.6777, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.8938, F1 Micro: 0.7671, F1 Macro: 0.7649\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.8988, F1 Micro: 0.765, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9002, F1 Micro: 0.7765, F1 Macro: 0.7714\n",
      "Epoch 5/10, Train Loss: 0.1427, Accuracy: 0.8986, F1 Micro: 0.7764, F1 Macro: 0.7743\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9033, F1 Micro: 0.7761, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.8991, F1 Micro: 0.776, F1 Macro: 0.7737\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.8995, F1 Micro: 0.7743, F1 Macro: 0.7697\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.8983, F1 Micro: 0.7756, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9042, F1 Micro: 0.7778, F1 Macro: 0.7734\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9042, F1 Micro: 0.7778, F1 Macro: 0.7734\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.67      0.68      0.67       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 152.58902645111084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.8736, F1 Micro: 0.6591, F1 Macro: 0.6524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.8881, F1 Micro: 0.7612, F1 Macro: 0.7625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2304, Accuracy: 0.8975, F1 Micro: 0.7729, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9016, F1 Micro: 0.7764, F1 Macro: 0.7712\n",
      "Epoch 5/10, Train Loss: 0.1486, Accuracy: 0.8972, F1 Micro: 0.7709, F1 Macro: 0.767\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.8992, F1 Micro: 0.7718, F1 Macro: 0.7686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9002, F1 Micro: 0.78, F1 Macro: 0.7796\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9, F1 Micro: 0.7727, F1 Macro: 0.7687\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.8984, F1 Micro: 0.7757, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8975, F1 Micro: 0.7561, F1 Macro: 0.748\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9002, F1 Micro: 0.78, F1 Macro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.77      0.70       248\n",
      "         radikalisme       0.72      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 154.6230866909027 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8935, F1 Micro: 0.7537, F1 Macro: 0.7488\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 50.96171808242798 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4165, Accuracy: 0.8747, F1 Micro: 0.6716, F1 Macro: 0.6528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.8975, F1 Micro: 0.7615, F1 Macro: 0.7503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2188, Accuracy: 0.8984, F1 Micro: 0.7766, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9031, F1 Micro: 0.751, F1 Macro: 0.7447\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.905, F1 Micro: 0.7634, F1 Macro: 0.7533\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9072, F1 Micro: 0.7748, F1 Macro: 0.7691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9027, F1 Micro: 0.7812, F1 Macro: 0.7759\n",
      "Epoch 8/10, Train Loss: 0.0548, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.766\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9058, F1 Micro: 0.7784, F1 Macro: 0.7727\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9031, F1 Micro: 0.7768, F1 Macro: 0.7733\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9027, F1 Micro: 0.7812, F1 Macro: 0.7759\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.56358408927917 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4025, Accuracy: 0.8758, F1 Micro: 0.6678, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.8973, F1 Micro: 0.7638, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9009, F1 Micro: 0.7764, F1 Macro: 0.7719\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.908, F1 Micro: 0.7748, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9069, F1 Micro: 0.7797, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.9075, F1 Micro: 0.7843, F1 Macro: 0.7759\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9025, F1 Micro: 0.7815, F1 Macro: 0.7777\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9078, F1 Micro: 0.7724, F1 Macro: 0.7621\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9038, F1 Micro: 0.7811, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9044, F1 Micro: 0.7771, F1 Macro: 0.7712\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9075, F1 Micro: 0.7843, F1 Macro: 0.7759\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.69      0.62      0.65       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 162.51909065246582 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4218, Accuracy: 0.8747, F1 Micro: 0.681, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2684, Accuracy: 0.8963, F1 Micro: 0.7617, F1 Macro: 0.7495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.8913, F1 Micro: 0.7655, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9028, F1 Micro: 0.7667, F1 Macro: 0.7631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9041, F1 Micro: 0.7815, F1 Macro: 0.7799\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.8991, F1 Micro: 0.7795, F1 Macro: 0.7772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9033, F1 Micro: 0.7835, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9041, F1 Micro: 0.7734, F1 Macro: 0.7667\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9025, F1 Micro: 0.777, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.9039, F1 Micro: 0.7751, F1 Macro: 0.7646\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9033, F1 Micro: 0.7835, F1 Macro: 0.7797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 164.31512689590454 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8945, F1 Micro: 0.7564, F1 Macro: 0.7514\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 46.16045784950256 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8766, F1 Micro: 0.6877, F1 Macro: 0.6819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.276, Accuracy: 0.8988, F1 Micro: 0.7551, F1 Macro: 0.748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.905, F1 Micro: 0.7781, F1 Macro: 0.7737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9027, F1 Micro: 0.7833, F1 Macro: 0.7781\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9038, F1 Micro: 0.7811, F1 Macro: 0.7802\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9044, F1 Micro: 0.7824, F1 Macro: 0.7782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9014, F1 Micro: 0.7847, F1 Macro: 0.7849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9078, F1 Micro: 0.7885, F1 Macro: 0.7868\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.907, F1 Micro: 0.7808, F1 Macro: 0.7766\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9006, F1 Micro: 0.7757, F1 Macro: 0.7723\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9078, F1 Micro: 0.7885, F1 Macro: 0.7868\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.68      0.71      0.69       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 170.482008934021 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4007, Accuracy: 0.8834, F1 Micro: 0.707, F1 Macro: 0.7033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2679, Accuracy: 0.8983, F1 Micro: 0.7526, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9039, F1 Micro: 0.7689, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.9, F1 Micro: 0.7829, F1 Macro: 0.7803\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9008, F1 Micro: 0.7829, F1 Macro: 0.7817\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9011, F1 Micro: 0.7773, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0555, Accuracy: 0.9047, F1 Micro: 0.7741, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.9042, F1 Micro: 0.7765, F1 Macro: 0.773\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9038, F1 Micro: 0.7797, F1 Macro: 0.7737\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9008, F1 Micro: 0.7829, F1 Macro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 169.7001883983612 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.872, F1 Micro: 0.6779, F1 Macro: 0.6755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2772, Accuracy: 0.897, F1 Micro: 0.7462, F1 Macro: 0.7397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.9019, F1 Micro: 0.7767, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.8988, F1 Micro: 0.779, F1 Macro: 0.7769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9048, F1 Micro: 0.7826, F1 Macro: 0.7794\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9013, F1 Micro: 0.7687, F1 Macro: 0.7605\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9016, F1 Micro: 0.7789, F1 Macro: 0.7765\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.905, F1 Micro: 0.7816, F1 Macro: 0.7786\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9052, F1 Micro: 0.7787, F1 Macro: 0.7788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9028, F1 Micro: 0.7828, F1 Macro: 0.783\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9028, F1 Micro: 0.7828, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 169.44642448425293 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8953, F1 Micro: 0.7588, F1 Macro: 0.7541\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 41.8418231010437 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4035, Accuracy: 0.8794, F1 Micro: 0.6902, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2722, Accuracy: 0.8975, F1 Micro: 0.7642, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.902, F1 Micro: 0.7745, F1 Macro: 0.767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.905, F1 Micro: 0.7779, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.903, F1 Micro: 0.7829, F1 Macro: 0.7846\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9044, F1 Micro: 0.7668, F1 Macro: 0.7549\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9034, F1 Micro: 0.7785, F1 Macro: 0.7767\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9022, F1 Micro: 0.7825, F1 Macro: 0.7828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9033, F1 Micro: 0.7839, F1 Macro: 0.7821\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.8984, F1 Micro: 0.7801, F1 Macro: 0.7804\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9033, F1 Micro: 0.7839, F1 Macro: 0.7821\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 176.18945240974426 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3923, Accuracy: 0.8811, F1 Micro: 0.7052, F1 Macro: 0.692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.8958, F1 Micro: 0.762, F1 Macro: 0.7562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2157, Accuracy: 0.9044, F1 Micro: 0.7738, F1 Macro: 0.7692\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.9056, F1 Micro: 0.7702, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9038, F1 Micro: 0.775, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9022, F1 Micro: 0.7623, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9038, F1 Micro: 0.7744, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9036, F1 Micro: 0.7794, F1 Macro: 0.7757\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.8992, F1 Micro: 0.7731, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9036, F1 Micro: 0.7695, F1 Macro: 0.7617\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9036, F1 Micro: 0.7794, F1 Macro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.76      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 174.61764931678772 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4061, Accuracy: 0.8809, F1 Micro: 0.7103, F1 Macro: 0.7018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2727, Accuracy: 0.8958, F1 Micro: 0.7581, F1 Macro: 0.7503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2167, Accuracy: 0.9013, F1 Micro: 0.769, F1 Macro: 0.7606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9044, F1 Micro: 0.7692, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9038, F1 Micro: 0.7773, F1 Macro: 0.7731\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9033, F1 Micro: 0.7656, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9013, F1 Micro: 0.7775, F1 Macro: 0.7741\n",
      "Epoch 8/10, Train Loss: 0.0533, Accuracy: 0.9034, F1 Micro: 0.7766, F1 Macro: 0.7712\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.9005, F1 Micro: 0.775, F1 Macro: 0.7718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9028, F1 Micro: 0.7794, F1 Macro: 0.7745\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9028, F1 Micro: 0.7794, F1 Macro: 0.7745\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.67      0.66       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 178.52930688858032 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8959, F1 Micro: 0.7605, F1 Macro: 0.7559\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 36.881264448165894 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4067, Accuracy: 0.8786, F1 Micro: 0.6698, F1 Macro: 0.6613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.8978, F1 Micro: 0.7637, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.9061, F1 Micro: 0.7753, F1 Macro: 0.7687\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9053, F1 Micro: 0.7699, F1 Macro: 0.7645\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9059, F1 Micro: 0.7723, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.9033, F1 Micro: 0.7793, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.9009, F1 Micro: 0.7812, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9022, F1 Micro: 0.7675, F1 Macro: 0.7576\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9039, F1 Micro: 0.7807, F1 Macro: 0.7773\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9002, F1 Micro: 0.7779, F1 Macro: 0.7747\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9009, F1 Micro: 0.7812, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.61      0.75      0.67       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 178.51283717155457 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3938, Accuracy: 0.8792, F1 Micro: 0.662, F1 Macro: 0.6445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2601, Accuracy: 0.8986, F1 Micro: 0.7678, F1 Macro: 0.7628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.9072, F1 Micro: 0.7832, F1 Macro: 0.7788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9069, F1 Micro: 0.7834, F1 Macro: 0.7765\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9073, F1 Micro: 0.7746, F1 Macro: 0.7619\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9019, F1 Micro: 0.7816, F1 Macro: 0.7813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9028, F1 Micro: 0.7842, F1 Macro: 0.7809\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9016, F1 Micro: 0.7758, F1 Macro: 0.77\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.905, F1 Micro: 0.7832, F1 Macro: 0.7771\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9059, F1 Micro: 0.7825, F1 Macro: 0.7786\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9028, F1 Micro: 0.7842, F1 Macro: 0.7809\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 178.72651839256287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4206, Accuracy: 0.8712, F1 Micro: 0.6464, F1 Macro: 0.6406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.8972, F1 Micro: 0.7653, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2196, Accuracy: 0.9059, F1 Micro: 0.7747, F1 Macro: 0.7679\n",
      "Epoch 4/10, Train Loss: 0.1684, Accuracy: 0.9042, F1 Micro: 0.7727, F1 Macro: 0.7666\n",
      "Epoch 5/10, Train Loss: 0.1364, Accuracy: 0.9077, F1 Micro: 0.7721, F1 Macro: 0.7565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9006, F1 Micro: 0.7787, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9005, F1 Micro: 0.7806, F1 Macro: 0.78\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0548, Accuracy: 0.9077, F1 Micro: 0.7838, F1 Macro: 0.7788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9041, F1 Micro: 0.7847, F1 Macro: 0.7819\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9047, F1 Micro: 0.7832, F1 Macro: 0.781\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9041, F1 Micro: 0.7847, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.64      0.69      0.67       248\n",
      "         radikalisme       0.76      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 181.62162685394287 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8964, F1 Micro: 0.7621, F1 Macro: 0.7577\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 33.90602493286133 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8841, F1 Micro: 0.7099, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.8944, F1 Micro: 0.7305, F1 Macro: 0.7052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.9019, F1 Micro: 0.7449, F1 Macro: 0.733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9042, F1 Micro: 0.7827, F1 Macro: 0.7755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1336, Accuracy: 0.9056, F1 Micro: 0.789, F1 Macro: 0.7871\n",
      "Epoch 6/10, Train Loss: 0.1057, Accuracy: 0.9039, F1 Micro: 0.7758, F1 Macro: 0.7691\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9062, F1 Micro: 0.7815, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9031, F1 Micro: 0.7786, F1 Macro: 0.7786\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.9019, F1 Micro: 0.7852, F1 Macro: 0.7855\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9053, F1 Micro: 0.7834, F1 Macro: 0.7789\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9056, F1 Micro: 0.789, F1 Macro: 0.7871\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.69      0.71      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 183.6299889087677 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8897, F1 Micro: 0.7283, F1 Macro: 0.7196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.8972, F1 Micro: 0.7508, F1 Macro: 0.7313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.902, F1 Micro: 0.7513, F1 Macro: 0.7403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1706, Accuracy: 0.9042, F1 Micro: 0.781, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9036, F1 Micro: 0.7817, F1 Macro: 0.7798\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9053, F1 Micro: 0.7774, F1 Macro: 0.7702\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9048, F1 Micro: 0.7794, F1 Macro: 0.7748\n",
      "Epoch 8/10, Train Loss: 0.0546, Accuracy: 0.902, F1 Micro: 0.7704, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0411, Accuracy: 0.9034, F1 Micro: 0.7859, F1 Macro: 0.7858\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9025, F1 Micro: 0.7804, F1 Macro: 0.7758\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9034, F1 Micro: 0.7859, F1 Macro: 0.7858\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.68      0.71      0.69       248\n",
      "         radikalisme       0.75      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 186.42041492462158 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4079, Accuracy: 0.8805, F1 Micro: 0.6975, F1 Macro: 0.6925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.8964, F1 Micro: 0.7512, F1 Macro: 0.7372\n",
      "Epoch 3/10, Train Loss: 0.2223, Accuracy: 0.9008, F1 Micro: 0.7479, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9011, F1 Micro: 0.777, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9034, F1 Micro: 0.7798, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9044, F1 Micro: 0.7822, F1 Macro: 0.7815\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.8995, F1 Micro: 0.7719, F1 Macro: 0.7668\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.8998, F1 Micro: 0.7755, F1 Macro: 0.7744\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9047, F1 Micro: 0.7818, F1 Macro: 0.7799\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9041, F1 Micro: 0.7771, F1 Macro: 0.7706\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9044, F1 Micro: 0.7822, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 183.57333612442017 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8969, F1 Micro: 0.7637, F1 Macro: 0.7595\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 28.303521871566772 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4004, Accuracy: 0.888, F1 Micro: 0.7281, F1 Macro: 0.7205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2565, Accuracy: 0.8986, F1 Micro: 0.7419, F1 Macro: 0.7293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2103, Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9038, F1 Micro: 0.7792, F1 Macro: 0.7759\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9019, F1 Micro: 0.7579, F1 Macro: 0.7522\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9034, F1 Micro: 0.7636, F1 Macro: 0.7592\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9013, F1 Micro: 0.7541, F1 Macro: 0.7425\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.9027, F1 Micro: 0.7687, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9027, F1 Micro: 0.7807, F1 Macro: 0.7789\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.903, F1 Micro: 0.7721, F1 Macro: 0.7691\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9027, F1 Micro: 0.7807, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.76      0.86      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 192.84049558639526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3913, Accuracy: 0.8898, F1 Micro: 0.7302, F1 Macro: 0.7267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2494, Accuracy: 0.8997, F1 Micro: 0.7458, F1 Macro: 0.7403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2053, Accuracy: 0.9028, F1 Micro: 0.7713, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9067, F1 Micro: 0.7793, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9083, F1 Micro: 0.7815, F1 Macro: 0.7783\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9056, F1 Micro: 0.7631, F1 Macro: 0.7554\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.9048, F1 Micro: 0.7715, F1 Macro: 0.7602\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9075, F1 Micro: 0.7735, F1 Macro: 0.7712\n",
      "Epoch 9/10, Train Loss: 0.0382, Accuracy: 0.9061, F1 Micro: 0.7788, F1 Macro: 0.7739\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9056, F1 Micro: 0.7781, F1 Macro: 0.774\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9083, F1 Micro: 0.7815, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.69      0.66      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.76      0.68      0.72       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.79      0.77      0.78      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 191.96921300888062 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4083, Accuracy: 0.8856, F1 Micro: 0.7088, F1 Macro: 0.7044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.8964, F1 Micro: 0.7295, F1 Macro: 0.7176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2119, Accuracy: 0.9034, F1 Micro: 0.7753, F1 Macro: 0.7709\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9008, F1 Micro: 0.7703, F1 Macro: 0.7648\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9002, F1 Micro: 0.7604, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9028, F1 Micro: 0.7727, F1 Macro: 0.7656\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9006, F1 Micro: 0.766, F1 Macro: 0.7583\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9041, F1 Micro: 0.7731, F1 Macro: 0.7691\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9058, F1 Micro: 0.7725, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9058, F1 Micro: 0.7758, F1 Macro: 0.7703\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9058, F1 Micro: 0.7758, F1 Macro: 0.7703\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.65      0.66       248\n",
      "         radikalisme       0.77      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.76      0.69      0.72       504\n",
      "\n",
      "           micro avg       0.79      0.76      0.78      1365\n",
      "           macro avg       0.78      0.76      0.77      1365\n",
      "        weighted avg       0.79      0.76      0.77      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 189.88565373420715 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8975, F1 Micro: 0.7647, F1 Macro: 0.7605\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.875982522964478 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4041, Accuracy: 0.8867, F1 Micro: 0.7312, F1 Macro: 0.7268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2561, Accuracy: 0.9013, F1 Micro: 0.774, F1 Macro: 0.7678\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.9048, F1 Micro: 0.771, F1 Macro: 0.7577\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9048, F1 Micro: 0.7796, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9077, F1 Micro: 0.7797, F1 Macro: 0.7742\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9022, F1 Micro: 0.7735, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0566, Accuracy: 0.9047, F1 Micro: 0.7736, F1 Macro: 0.7731\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9027, F1 Micro: 0.7793, F1 Macro: 0.7792\n",
      "Epoch 10/10, Train Loss: 0.0363, Accuracy: 0.9016, F1 Micro: 0.7771, F1 Macro: 0.7765\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9077, F1 Micro: 0.7797, F1 Macro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.67      0.64      0.66       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.76      0.70      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 195.59558939933777 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3879, Accuracy: 0.8884, F1 Micro: 0.7306, F1 Macro: 0.7259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2505, Accuracy: 0.9016, F1 Micro: 0.7702, F1 Macro: 0.764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9031, F1 Micro: 0.7726, F1 Macro: 0.7611\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9025, F1 Micro: 0.7629, F1 Macro: 0.7548\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.9042, F1 Micro: 0.7707, F1 Macro: 0.7625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9044, F1 Micro: 0.774, F1 Macro: 0.7716\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9025, F1 Micro: 0.7677, F1 Macro: 0.7606\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9031, F1 Micro: 0.7709, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.9008, F1 Micro: 0.7771, F1 Macro: 0.7746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9017, F1 Micro: 0.7777, F1 Macro: 0.7749\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9017, F1 Micro: 0.7777, F1 Macro: 0.7749\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.64      0.70      0.67       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.17068696022034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4082, Accuracy: 0.8808, F1 Micro: 0.7102, F1 Macro: 0.7067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.9003, F1 Micro: 0.7726, F1 Macro: 0.7678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9042, F1 Micro: 0.7727, F1 Macro: 0.7622\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9002, F1 Micro: 0.7551, F1 Macro: 0.7473\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.9013, F1 Micro: 0.7697, F1 Macro: 0.7638\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9025, F1 Micro: 0.762, F1 Macro: 0.7556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9034, F1 Micro: 0.7804, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9002, F1 Micro: 0.7768, F1 Macro: 0.7776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9033, F1 Micro: 0.7821, F1 Macro: 0.7792\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9002, F1 Micro: 0.7696, F1 Macro: 0.7666\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9033, F1 Micro: 0.7821, F1 Macro: 0.7792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.68      0.66       248\n",
      "         radikalisme       0.76      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 196.9167742729187 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8979, F1 Micro: 0.7655, F1 Macro: 0.7614\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 20.66141700744629 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8856, F1 Micro: 0.7426, F1 Macro: 0.738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2626, Accuracy: 0.8939, F1 Micro: 0.7567, F1 Macro: 0.7548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9041, F1 Micro: 0.7651, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9044, F1 Micro: 0.7779, F1 Macro: 0.7716\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9041, F1 Micro: 0.7588, F1 Macro: 0.75\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9047, F1 Micro: 0.7801, F1 Macro: 0.7774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9069, F1 Micro: 0.7904, F1 Macro: 0.7879\n",
      "Epoch 8/10, Train Loss: 0.0547, Accuracy: 0.902, F1 Micro: 0.7813, F1 Macro: 0.7794\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9055, F1 Micro: 0.7704, F1 Macro: 0.7625\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.9045, F1 Micro: 0.7753, F1 Macro: 0.7708\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9069, F1 Micro: 0.7904, F1 Macro: 0.7879\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.78      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 205.54104614257812 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3735, Accuracy: 0.8888, F1 Micro: 0.7407, F1 Macro: 0.7321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2555, Accuracy: 0.8948, F1 Micro: 0.7649, F1 Macro: 0.7657\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.9027, F1 Micro: 0.7509, F1 Macro: 0.739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.905, F1 Micro: 0.7839, F1 Macro: 0.7816\n",
      "Epoch 5/10, Train Loss: 0.1316, Accuracy: 0.9028, F1 Micro: 0.7593, F1 Macro: 0.7514\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.8992, F1 Micro: 0.7778, F1 Macro: 0.777\n",
      "Epoch 7/10, Train Loss: 0.0691, Accuracy: 0.903, F1 Micro: 0.7777, F1 Macro: 0.7728\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7699\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9017, F1 Micro: 0.7735, F1 Macro: 0.7687\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9058, F1 Micro: 0.7763, F1 Macro: 0.7703\n",
      "Model 2 - Iteration 5441: Accuracy: 0.905, F1 Micro: 0.7839, F1 Macro: 0.7816\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.76      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.45989966392517 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3941, Accuracy: 0.8842, F1 Micro: 0.7426, F1 Macro: 0.7389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.8958, F1 Micro: 0.767, F1 Macro: 0.7666\n",
      "Epoch 3/10, Train Loss: 0.2103, Accuracy: 0.9041, F1 Micro: 0.7567, F1 Macro: 0.7494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9089, F1 Micro: 0.7898, F1 Macro: 0.7857\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9023, F1 Micro: 0.7548, F1 Macro: 0.7467\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.8997, F1 Micro: 0.7731, F1 Macro: 0.7714\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9031, F1 Micro: 0.7871, F1 Macro: 0.7854\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9023, F1 Micro: 0.7828, F1 Macro: 0.7834\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.902, F1 Micro: 0.7707, F1 Macro: 0.7675\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.9038, F1 Micro: 0.7775, F1 Macro: 0.7742\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9089, F1 Micro: 0.7898, F1 Macro: 0.7857\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.80      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.79      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 199.80110478401184 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.8984, F1 Micro: 0.7668, F1 Macro: 0.7627\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.26056408882141 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.397, Accuracy: 0.8859, F1 Micro: 0.6963, F1 Macro: 0.6867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2578, Accuracy: 0.8995, F1 Micro: 0.7427, F1 Macro: 0.7359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9006, F1 Micro: 0.7629, F1 Macro: 0.7498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.91, F1 Micro: 0.7904, F1 Macro: 0.7858\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.9041, F1 Micro: 0.7699, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9027, F1 Micro: 0.7773, F1 Macro: 0.7757\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.907, F1 Micro: 0.7723, F1 Macro: 0.7681\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9042, F1 Micro: 0.7797, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9031, F1 Micro: 0.7834, F1 Macro: 0.7837\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9044, F1 Micro: 0.7813, F1 Macro: 0.7772\n",
      "Model 1 - Iteration 5641: Accuracy: 0.91, F1 Micro: 0.7904, F1 Macro: 0.7858\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.71      0.62      0.67       248\n",
      "         radikalisme       0.78      0.86      0.82       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1365\n",
      "           macro avg       0.79      0.79      0.79      1365\n",
      "        weighted avg       0.79      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 206.95604395866394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3851, Accuracy: 0.8841, F1 Micro: 0.6848, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2534, Accuracy: 0.9048, F1 Micro: 0.7684, F1 Macro: 0.7656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.907, F1 Micro: 0.775, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9062, F1 Micro: 0.7832, F1 Macro: 0.7775\n",
      "Epoch 5/10, Train Loss: 0.1276, Accuracy: 0.9045, F1 Micro: 0.7636, F1 Macro: 0.7526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9025, F1 Micro: 0.7867, F1 Macro: 0.7861\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9075, F1 Micro: 0.7866, F1 Macro: 0.7832\n",
      "Epoch 8/10, Train Loss: 0.0531, Accuracy: 0.907, F1 Micro: 0.786, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9075, F1 Micro: 0.7804, F1 Macro: 0.774\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8994, F1 Micro: 0.7705, F1 Macro: 0.7662\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9025, F1 Micro: 0.7867, F1 Macro: 0.7861\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 209.75261402130127 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8869, F1 Micro: 0.7042, F1 Macro: 0.6944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.9011, F1 Micro: 0.7497, F1 Macro: 0.7412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.905, F1 Micro: 0.7753, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9058, F1 Micro: 0.7816, F1 Macro: 0.7742\n",
      "Epoch 5/10, Train Loss: 0.1276, Accuracy: 0.9, F1 Micro: 0.7669, F1 Macro: 0.763\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.8986, F1 Micro: 0.7789, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9058, F1 Micro: 0.7839, F1 Macro: 0.781\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.9006, F1 Micro: 0.7707, F1 Macro: 0.7667\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.902, F1 Micro: 0.7804, F1 Macro: 0.7796\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9003, F1 Micro: 0.7823, F1 Macro: 0.7819\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9058, F1 Micro: 0.7839, F1 Macro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 209.52366018295288 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.8988, F1 Micro: 0.7679, F1 Macro: 0.7639\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.324679613113403 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8878, F1 Micro: 0.71, F1 Macro: 0.6938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2477, Accuracy: 0.8953, F1 Micro: 0.767, F1 Macro: 0.7688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.907, F1 Micro: 0.7804, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9048, F1 Micro: 0.7914, F1 Macro: 0.7896\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.9064, F1 Micro: 0.7764, F1 Macro: 0.7661\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9044, F1 Micro: 0.7711, F1 Macro: 0.7628\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9028, F1 Micro: 0.7761, F1 Macro: 0.7719\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.9023, F1 Micro: 0.7799, F1 Macro: 0.7775\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9053, F1 Micro: 0.7735, F1 Macro: 0.7695\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.9031, F1 Micro: 0.7669, F1 Macro: 0.7621\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9048, F1 Micro: 0.7914, F1 Macro: 0.7896\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.74      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.84      0.76       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 212.57831621170044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3728, Accuracy: 0.8875, F1 Micro: 0.7012, F1 Macro: 0.6895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2462, Accuracy: 0.8995, F1 Micro: 0.774, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9045, F1 Micro: 0.7811, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1582, Accuracy: 0.9073, F1 Micro: 0.7883, F1 Macro: 0.7839\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.905, F1 Micro: 0.7844, F1 Macro: 0.777\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.9041, F1 Micro: 0.7787, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.0668, Accuracy: 0.9048, F1 Micro: 0.7791, F1 Macro: 0.7713\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.9058, F1 Micro: 0.7833, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9055, F1 Micro: 0.7767, F1 Macro: 0.7713\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9014, F1 Micro: 0.7613, F1 Macro: 0.7531\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9073, F1 Micro: 0.7883, F1 Macro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.76      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 212.98786759376526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3915, Accuracy: 0.8866, F1 Micro: 0.7155, F1 Macro: 0.6989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2458, Accuracy: 0.8983, F1 Micro: 0.7747, F1 Macro: 0.7732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2072, Accuracy: 0.9062, F1 Micro: 0.7818, F1 Macro: 0.7733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9055, F1 Micro: 0.7876, F1 Macro: 0.786\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9033, F1 Micro: 0.7801, F1 Macro: 0.774\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9042, F1 Micro: 0.7772, F1 Macro: 0.7718\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.9019, F1 Micro: 0.7705, F1 Macro: 0.7661\n",
      "Epoch 8/10, Train Loss: 0.0499, Accuracy: 0.9011, F1 Micro: 0.7775, F1 Macro: 0.7744\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9014, F1 Micro: 0.77, F1 Macro: 0.7622\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9041, F1 Micro: 0.7751, F1 Macro: 0.7687\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9055, F1 Micro: 0.7876, F1 Macro: 0.786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 212.6708643436432 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.8991, F1 Micro: 0.7689, F1 Macro: 0.765\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.17925477027893 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8859, F1 Micro: 0.7451, F1 Macro: 0.7421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2414, Accuracy: 0.9005, F1 Micro: 0.7727, F1 Macro: 0.7715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9011, F1 Micro: 0.7851, F1 Macro: 0.7842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1584, Accuracy: 0.9078, F1 Micro: 0.7925, F1 Macro: 0.7891\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.8997, F1 Micro: 0.7791, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9045, F1 Micro: 0.7669, F1 Macro: 0.7596\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9041, F1 Micro: 0.766, F1 Macro: 0.7579\n",
      "Epoch 8/10, Train Loss: 0.0495, Accuracy: 0.9003, F1 Micro: 0.7774, F1 Macro: 0.7756\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9039, F1 Micro: 0.7847, F1 Macro: 0.7825\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9039, F1 Micro: 0.7804, F1 Macro: 0.7774\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9078, F1 Micro: 0.7925, F1 Macro: 0.7891\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.84      0.76       504\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 218.2044005393982 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3632, Accuracy: 0.89, F1 Micro: 0.7491, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2365, Accuracy: 0.8991, F1 Micro: 0.7698, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9022, F1 Micro: 0.7859, F1 Macro: 0.7864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1576, Accuracy: 0.9094, F1 Micro: 0.79, F1 Macro: 0.7813\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9041, F1 Micro: 0.7806, F1 Macro: 0.7732\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.9078, F1 Micro: 0.7765, F1 Macro: 0.7683\n",
      "Epoch 7/10, Train Loss: 0.0587, Accuracy: 0.9091, F1 Micro: 0.7822, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0482, Accuracy: 0.903, F1 Micro: 0.7781, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9039, F1 Micro: 0.7774, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0307, Accuracy: 0.8995, F1 Micro: 0.7735, F1 Macro: 0.768\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9094, F1 Micro: 0.79, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.70      0.61      0.66       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 218.28524374961853 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.8859, F1 Micro: 0.749, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.8997, F1 Micro: 0.7747, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1958, Accuracy: 0.8958, F1 Micro: 0.7759, F1 Macro: 0.7769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9044, F1 Micro: 0.7875, F1 Macro: 0.7854\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9019, F1 Micro: 0.7743, F1 Macro: 0.7688\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9058, F1 Micro: 0.7747, F1 Macro: 0.7662\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9077, F1 Micro: 0.7735, F1 Macro: 0.7631\n",
      "Epoch 8/10, Train Loss: 0.0499, Accuracy: 0.9055, F1 Micro: 0.7828, F1 Macro: 0.7776\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9033, F1 Micro: 0.781, F1 Macro: 0.7789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9073, F1 Micro: 0.7875, F1 Macro: 0.7873\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9073, F1 Micro: 0.7875, F1 Macro: 0.7873\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       370\n",
      "                sara       0.66      0.73      0.69       248\n",
      "         radikalisme       0.75      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 220.34808731079102 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.8996, F1 Micro: 0.7699, F1 Macro: 0.766\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.174498081207275 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3583, Accuracy: 0.8886, F1 Micro: 0.7476, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2312, Accuracy: 0.9006, F1 Micro: 0.7706, F1 Macro: 0.7649\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.898, F1 Micro: 0.7444, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1566, Accuracy: 0.9062, F1 Micro: 0.7846, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9038, F1 Micro: 0.7734, F1 Macro: 0.7657\n",
      "Epoch 6/10, Train Loss: 0.0847, Accuracy: 0.9078, F1 Micro: 0.781, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0638, Accuracy: 0.8964, F1 Micro: 0.7732, F1 Macro: 0.7738\n",
      "Epoch 8/10, Train Loss: 0.0496, Accuracy: 0.9044, F1 Micro: 0.7802, F1 Macro: 0.7759\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9017, F1 Micro: 0.7737, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9064, F1 Micro: 0.7799, F1 Macro: 0.7741\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9062, F1 Micro: 0.7846, F1 Macro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 222.9882881641388 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3476, Accuracy: 0.8911, F1 Micro: 0.7508, F1 Macro: 0.7504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9023, F1 Micro: 0.7738, F1 Macro: 0.7688\n",
      "Epoch 3/10, Train Loss: 0.1891, Accuracy: 0.8992, F1 Micro: 0.7407, F1 Macro: 0.7271\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9059, F1 Micro: 0.7718, F1 Macro: 0.7638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.9036, F1 Micro: 0.7791, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0796, Accuracy: 0.9081, F1 Micro: 0.7824, F1 Macro: 0.7783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0599, Accuracy: 0.9028, F1 Micro: 0.7827, F1 Macro: 0.7811\n",
      "Epoch 8/10, Train Loss: 0.0451, Accuracy: 0.9038, F1 Micro: 0.777, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.0332, Accuracy: 0.9061, F1 Micro: 0.7794, F1 Macro: 0.7761\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9002, F1 Micro: 0.774, F1 Macro: 0.7699\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9028, F1 Micro: 0.7827, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.73      0.69       248\n",
      "         radikalisme       0.78      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 227.14463710784912 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3625, Accuracy: 0.8881, F1 Micro: 0.7428, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.23, Accuracy: 0.9011, F1 Micro: 0.7697, F1 Macro: 0.7675\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9019, F1 Micro: 0.7545, F1 Macro: 0.743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1551, Accuracy: 0.9048, F1 Micro: 0.7785, F1 Macro: 0.7718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9066, F1 Micro: 0.7795, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.0809, Accuracy: 0.9039, F1 Micro: 0.7716, F1 Macro: 0.7628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0643, Accuracy: 0.9011, F1 Micro: 0.781, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9039, F1 Micro: 0.7829, F1 Macro: 0.7804\n",
      "Epoch 9/10, Train Loss: 0.0375, Accuracy: 0.8986, F1 Micro: 0.779, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0293, Accuracy: 0.9034, F1 Micro: 0.7818, F1 Macro: 0.7777\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9039, F1 Micro: 0.7829, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.93       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.77      0.78      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 227.5340554714203 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.8998, F1 Micro: 0.7705, F1 Macro: 0.7666\n",
      "Total sampling time: 1083.35 seconds\n",
      "Total runtime: 11914.385688304901 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac56ac2",
   "metadata": {
    "papermill": {
     "duration": 0.184059,
     "end_time": "2025-03-13T10:00:44.280002",
     "exception": false,
     "start_time": "2025-03-13T10:00:44.095943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25178ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6045, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3839, Accuracy: 0.8116, F1 Micro: 0.2087, F1 Macro: 0.1506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3762, Accuracy: 0.8338, F1 Micro: 0.4095, F1 Macro: 0.342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3143, Accuracy: 0.8447, F1 Micro: 0.4918, F1 Macro: 0.4438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2621, Accuracy: 0.8547, F1 Micro: 0.565, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2351, Accuracy: 0.8594, F1 Micro: 0.6053, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1834, Accuracy: 0.8698, F1 Micro: 0.6445, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1462, Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "Epoch 10/10, Train Loss: 0.1326, Accuracy: 0.877, F1 Micro: 0.6883, F1 Macro: 0.6815\n",
      "Model 1 - Iteration 388: Accuracy: 0.872, F1 Micro: 0.6961, F1 Macro: 0.692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.81      0.85       370\n",
      "                sara       0.58      0.56      0.57       248\n",
      "         radikalisme       0.66      0.74      0.70       243\n",
      "pencemaran_nama_baik       0.66      0.63      0.64       504\n",
      "\n",
      "           micro avg       0.71      0.69      0.70      1365\n",
      "           macro avg       0.70      0.69      0.69      1365\n",
      "        weighted avg       0.71      0.69      0.70      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 57.45415472984314 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5934, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.439, Accuracy: 0.7909, F1 Micro: 0.0388, F1 Macro: 0.034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3734, Accuracy: 0.8158, F1 Micro: 0.2398, F1 Macro: 0.1687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3544, Accuracy: 0.8344, F1 Micro: 0.4137, F1 Macro: 0.3332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2904, Accuracy: 0.8481, F1 Micro: 0.5272, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2499, Accuracy: 0.8531, F1 Micro: 0.5595, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2281, Accuracy: 0.8608, F1 Micro: 0.607, F1 Macro: 0.5951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1774, Accuracy: 0.8648, F1 Micro: 0.6211, F1 Macro: 0.6062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1432, Accuracy: 0.8706, F1 Micro: 0.662, F1 Macro: 0.6503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1304, Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "Model 2 - Iteration 388: Accuracy: 0.8678, F1 Micro: 0.6746, F1 Macro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.85      0.87       370\n",
      "                sara       0.57      0.51      0.54       248\n",
      "         radikalisme       0.66      0.68      0.67       243\n",
      "pencemaran_nama_baik       0.66      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.67      1365\n",
      "           macro avg       0.70      0.64      0.67      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.37      0.36      0.35      1365\n",
      "\n",
      "Training completed in 58.63295555114746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6052, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4376, Accuracy: 0.7931, F1 Micro: 0.0583, F1 Macro: 0.0499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3737, Accuracy: 0.8203, F1 Micro: 0.274, F1 Macro: 0.1872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3629, Accuracy: 0.8316, F1 Micro: 0.3896, F1 Macro: 0.2908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3015, Accuracy: 0.842, F1 Micro: 0.4648, F1 Macro: 0.4003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2601, Accuracy: 0.8594, F1 Micro: 0.5917, F1 Macro: 0.565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2378, Accuracy: 0.8631, F1 Micro: 0.6158, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1836, Accuracy: 0.8686, F1 Micro: 0.6453, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1511, Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "Epoch 10/10, Train Loss: 0.13, Accuracy: 0.8755, F1 Micro: 0.6876, F1 Macro: 0.6794\n",
      "Model 3 - Iteration 388: Accuracy: 0.8733, F1 Micro: 0.6957, F1 Macro: 0.6882\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.86      0.86       370\n",
      "                sara       0.60      0.54      0.57       248\n",
      "         radikalisme       0.67      0.71      0.69       243\n",
      "pencemaran_nama_baik       0.67      0.60      0.63       504\n",
      "\n",
      "           micro avg       0.71      0.68      0.70      1365\n",
      "           macro avg       0.70      0.68      0.69      1365\n",
      "        weighted avg       0.71      0.68      0.69      1365\n",
      "         samples avg       0.37      0.37      0.36      1365\n",
      "\n",
      "Training completed in 56.7948956489563 s\n",
      "Averaged - Iteration 388: Accuracy: 0.871, F1 Micro: 0.6888, F1 Macro: 0.6828\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 109.45823788642883 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.545, Accuracy: 0.8119, F1 Micro: 0.212, F1 Macro: 0.1606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3964, Accuracy: 0.8409, F1 Micro: 0.4437, F1 Macro: 0.372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3154, Accuracy: 0.8705, F1 Micro: 0.6444, F1 Macro: 0.6281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2403, Accuracy: 0.8795, F1 Micro: 0.7029, F1 Macro: 0.6916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2079, Accuracy: 0.8861, F1 Micro: 0.7173, F1 Macro: 0.7144\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.8869, F1 Micro: 0.7159, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1276, Accuracy: 0.8877, F1 Micro: 0.7369, F1 Macro: 0.7348\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.8884, F1 Micro: 0.7003, F1 Macro: 0.6921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.8909, F1 Micro: 0.7409, F1 Macro: 0.7336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.8922, F1 Micro: 0.7454, F1 Macro: 0.738\n",
      "Model 1 - Iteration 971: Accuracy: 0.8922, F1 Micro: 0.7454, F1 Macro: 0.738\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       370\n",
      "                sara       0.67      0.54      0.60       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       504\n",
      "\n",
      "           micro avg       0.75      0.74      0.75      1365\n",
      "           macro avg       0.75      0.73      0.74      1365\n",
      "        weighted avg       0.75      0.74      0.74      1365\n",
      "         samples avg       0.43      0.42      0.41      1365\n",
      "\n",
      "Training completed in 72.46342468261719 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.548, Accuracy: 0.8136, F1 Micro: 0.2268, F1 Macro: 0.1701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3866, Accuracy: 0.8464, F1 Micro: 0.4878, F1 Macro: 0.4214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.304, Accuracy: 0.8683, F1 Micro: 0.6445, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2402, Accuracy: 0.8739, F1 Micro: 0.6851, F1 Macro: 0.6727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2118, Accuracy: 0.8819, F1 Micro: 0.7115, F1 Macro: 0.7065\n",
      "Epoch 6/10, Train Loss: 0.1515, Accuracy: 0.8838, F1 Micro: 0.705, F1 Macro: 0.7047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1412, Accuracy: 0.8856, F1 Micro: 0.7325, F1 Macro: 0.7305\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.8872, F1 Micro: 0.7151, F1 Macro: 0.7117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.8884, F1 Micro: 0.7488, F1 Macro: 0.7445\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.8875, F1 Micro: 0.7262, F1 Macro: 0.7192\n",
      "Model 2 - Iteration 971: Accuracy: 0.8884, F1 Micro: 0.7488, F1 Macro: 0.7445\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.86      0.87       370\n",
      "                sara       0.64      0.63      0.63       248\n",
      "         radikalisme       0.72      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.66      0.77      0.71       504\n",
      "\n",
      "           micro avg       0.72      0.78      0.75      1365\n",
      "           macro avg       0.72      0.77      0.74      1365\n",
      "        weighted avg       0.72      0.78      0.75      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 70.77575469017029 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.8188, F1 Micro: 0.2621, F1 Macro: 0.1911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.388, Accuracy: 0.8403, F1 Micro: 0.4347, F1 Macro: 0.3646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3131, Accuracy: 0.8711, F1 Micro: 0.6479, F1 Macro: 0.6344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2441, Accuracy: 0.8797, F1 Micro: 0.7013, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2107, Accuracy: 0.8869, F1 Micro: 0.7207, F1 Macro: 0.7125\n",
      "Epoch 6/10, Train Loss: 0.1594, Accuracy: 0.8859, F1 Micro: 0.6976, F1 Macro: 0.6921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1409, Accuracy: 0.8858, F1 Micro: 0.7394, F1 Macro: 0.7376\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.8866, F1 Micro: 0.7027, F1 Macro: 0.6959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.8898, F1 Micro: 0.7461, F1 Macro: 0.7371\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.8889, F1 Micro: 0.7195, F1 Macro: 0.7081\n",
      "Model 3 - Iteration 971: Accuracy: 0.8898, F1 Micro: 0.7461, F1 Macro: 0.7371\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.88      0.87       370\n",
      "                sara       0.67      0.57      0.62       248\n",
      "         radikalisme       0.73      0.77      0.75       243\n",
      "pencemaran_nama_baik       0.67      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.76      0.75      1365\n",
      "           macro avg       0.73      0.74      0.74      1365\n",
      "        weighted avg       0.73      0.76      0.74      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 70.6482310295105 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8806, F1 Micro: 0.7178, F1 Macro: 0.7113\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 99.11719107627869 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4991, Accuracy: 0.8331, F1 Micro: 0.3946, F1 Macro: 0.3022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3492, Accuracy: 0.8705, F1 Micro: 0.6471, F1 Macro: 0.5992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2751, Accuracy: 0.8808, F1 Micro: 0.691, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2112, Accuracy: 0.8898, F1 Micro: 0.7181, F1 Macro: 0.7039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.175, Accuracy: 0.8914, F1 Micro: 0.7531, F1 Macro: 0.7497\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.8945, F1 Micro: 0.7454, F1 Macro: 0.736\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.8922, F1 Micro: 0.7253, F1 Macro: 0.7171\n",
      "Epoch 8/10, Train Loss: 0.0824, Accuracy: 0.8934, F1 Micro: 0.7525, F1 Macro: 0.7459\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.8948, F1 Micro: 0.7409, F1 Macro: 0.7271\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.8958, F1 Micro: 0.7463, F1 Macro: 0.7337\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8914, F1 Micro: 0.7531, F1 Macro: 0.7497\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.70      0.80      0.75       243\n",
      "pencemaran_nama_baik       0.67      0.77      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.78      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.78      0.75      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 82.50606727600098 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5018, Accuracy: 0.8331, F1 Micro: 0.3883, F1 Macro: 0.2735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3419, Accuracy: 0.8706, F1 Micro: 0.6669, F1 Macro: 0.6454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.8784, F1 Micro: 0.7026, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2178, Accuracy: 0.8867, F1 Micro: 0.7094, F1 Macro: 0.6943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1873, Accuracy: 0.8911, F1 Micro: 0.7335, F1 Macro: 0.7278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1503, Accuracy: 0.8909, F1 Micro: 0.7509, F1 Macro: 0.7472\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.8925, F1 Micro: 0.7417, F1 Macro: 0.7357\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.8939, F1 Micro: 0.7488, F1 Macro: 0.7419\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.8914, F1 Micro: 0.7433, F1 Macro: 0.7361\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.8898, F1 Micro: 0.7417, F1 Macro: 0.7367\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8909, F1 Micro: 0.7509, F1 Macro: 0.7472\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.84      0.87       370\n",
      "                sara       0.63      0.66      0.64       248\n",
      "         radikalisme       0.70      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1365\n",
      "           macro avg       0.73      0.77      0.75      1365\n",
      "        weighted avg       0.74      0.77      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 83.9923312664032 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4986, Accuracy: 0.8381, F1 Micro: 0.443, F1 Macro: 0.3462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3432, Accuracy: 0.8756, F1 Micro: 0.6566, F1 Macro: 0.6056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2719, Accuracy: 0.8825, F1 Micro: 0.6946, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2135, Accuracy: 0.8895, F1 Micro: 0.7248, F1 Macro: 0.7071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1777, Accuracy: 0.8884, F1 Micro: 0.7538, F1 Macro: 0.7484\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.8934, F1 Micro: 0.7502, F1 Macro: 0.7406\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.892, F1 Micro: 0.7351, F1 Macro: 0.726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.8933, F1 Micro: 0.7548, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.8988, F1 Micro: 0.7553, F1 Macro: 0.7428\n",
      "Epoch 10/10, Train Loss: 0.0505, Accuracy: 0.895, F1 Micro: 0.7395, F1 Macro: 0.727\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8988, F1 Micro: 0.7553, F1 Macro: 0.7428\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.88       370\n",
      "                sara       0.73      0.51      0.60       248\n",
      "         radikalisme       0.77      0.75      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.73      0.76      1365\n",
      "           macro avg       0.78      0.72      0.74      1365\n",
      "        weighted avg       0.78      0.73      0.75      1365\n",
      "         samples avg       0.43      0.42      0.41      1365\n",
      "\n",
      "Training completed in 84.63610529899597 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.885, F1 Micro: 0.7296, F1 Macro: 0.7231\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 89.41365790367126 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4786, Accuracy: 0.8491, F1 Micro: 0.5096, F1 Macro: 0.4389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3219, Accuracy: 0.8817, F1 Micro: 0.7121, F1 Macro: 0.7063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2531, Accuracy: 0.8906, F1 Micro: 0.7346, F1 Macro: 0.7328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.8963, F1 Micro: 0.7622, F1 Macro: 0.7581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.8922, F1 Micro: 0.7643, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1267, Accuracy: 0.8977, F1 Micro: 0.7678, F1 Macro: 0.7664\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.8948, F1 Micro: 0.7578, F1 Macro: 0.7524\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.8992, F1 Micro: 0.7576, F1 Macro: 0.7453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9033, F1 Micro: 0.7723, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.8992, F1 Micro: 0.7571, F1 Macro: 0.7499\n",
      "Model 1 - Iteration 1969: Accuracy: 0.9033, F1 Micro: 0.7723, F1 Macro: 0.7669\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.72      0.64      0.68       248\n",
      "         radikalisme       0.77      0.76      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.76      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 97.00345802307129 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4756, Accuracy: 0.8505, F1 Micro: 0.5436, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3158, Accuracy: 0.8758, F1 Micro: 0.691, F1 Macro: 0.6839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.256, Accuracy: 0.8869, F1 Micro: 0.7358, F1 Macro: 0.735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.8908, F1 Micro: 0.7543, F1 Macro: 0.7515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.8908, F1 Micro: 0.7638, F1 Macro: 0.7596\n",
      "Epoch 6/10, Train Loss: 0.1267, Accuracy: 0.8883, F1 Micro: 0.7575, F1 Macro: 0.756\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.8944, F1 Micro: 0.7496, F1 Macro: 0.7384\n",
      "Epoch 8/10, Train Loss: 0.0761, Accuracy: 0.8913, F1 Micro: 0.7281, F1 Macro: 0.7072\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.8931, F1 Micro: 0.7555, F1 Macro: 0.7506\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.8967, F1 Micro: 0.7562, F1 Macro: 0.7505\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8908, F1 Micro: 0.7638, F1 Macro: 0.7596\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.83      0.91      0.87       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.73      0.82      0.77       243\n",
      "pencemaran_nama_baik       0.65      0.84      0.73       504\n",
      "\n",
      "           micro avg       0.71      0.83      0.76      1365\n",
      "           macro avg       0.71      0.81      0.76      1365\n",
      "        weighted avg       0.71      0.83      0.76      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 95.03507280349731 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4759, Accuracy: 0.8469, F1 Micro: 0.482, F1 Macro: 0.4066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3201, Accuracy: 0.8802, F1 Micro: 0.7091, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2545, Accuracy: 0.892, F1 Micro: 0.7432, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.8903, F1 Micro: 0.7622, F1 Macro: 0.7599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1545, Accuracy: 0.89, F1 Micro: 0.7644, F1 Macro: 0.7633\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.8984, F1 Micro: 0.7609, F1 Macro: 0.7584\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.8953, F1 Micro: 0.7261, F1 Macro: 0.7118\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.8956, F1 Micro: 0.7621, F1 Macro: 0.7577\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.8978, F1 Micro: 0.7611, F1 Macro: 0.7591\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.8964, F1 Micro: 0.7536, F1 Macro: 0.7483\n",
      "Model 3 - Iteration 1969: Accuracy: 0.89, F1 Micro: 0.7644, F1 Macro: 0.7633\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.92      0.88       370\n",
      "                sara       0.62      0.74      0.68       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.64      0.83      0.72       504\n",
      "\n",
      "           micro avg       0.70      0.84      0.76      1365\n",
      "           macro avg       0.71      0.83      0.76      1365\n",
      "        weighted avg       0.71      0.84      0.77      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 94.60461783409119 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8874, F1 Micro: 0.7389, F1 Macro: 0.7331\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 80.7558662891388 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4609, Accuracy: 0.8648, F1 Micro: 0.6342, F1 Macro: 0.559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2999, Accuracy: 0.8817, F1 Micro: 0.6916, F1 Macro: 0.6787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2472, Accuracy: 0.8933, F1 Micro: 0.7273, F1 Macro: 0.7113\n",
      "Epoch 4/10, Train Loss: 0.1925, Accuracy: 0.8952, F1 Micro: 0.7258, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.8986, F1 Micro: 0.7533, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.12, Accuracy: 0.9014, F1 Micro: 0.7739, F1 Macro: 0.7694\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9019, F1 Micro: 0.7655, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.9014, F1 Micro: 0.7603, F1 Macro: 0.7506\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.8998, F1 Micro: 0.7604, F1 Macro: 0.7559\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9013, F1 Micro: 0.7685, F1 Macro: 0.7663\n",
      "Model 1 - Iteration 2394: Accuracy: 0.9014, F1 Micro: 0.7739, F1 Macro: 0.7694\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.69      0.67      0.68       248\n",
      "         radikalisme       0.70      0.85      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 107.55909442901611 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4583, Accuracy: 0.8655, F1 Micro: 0.6303, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2905, Accuracy: 0.8816, F1 Micro: 0.7133, F1 Macro: 0.7059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2457, Accuracy: 0.8909, F1 Micro: 0.7356, F1 Macro: 0.726\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.8923, F1 Micro: 0.7142, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1624, Accuracy: 0.8978, F1 Micro: 0.7558, F1 Macro: 0.7477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.898, F1 Micro: 0.7579, F1 Macro: 0.7513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0923, Accuracy: 0.8997, F1 Micro: 0.7588, F1 Macro: 0.754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.8995, F1 Micro: 0.7659, F1 Macro: 0.7587\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9005, F1 Micro: 0.7549, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.8978, F1 Micro: 0.7692, F1 Macro: 0.7644\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8978, F1 Micro: 0.7692, F1 Macro: 0.7644\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.70      0.84      0.76       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 112.90694975852966 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4592, Accuracy: 0.8625, F1 Micro: 0.6011, F1 Macro: 0.5121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2955, Accuracy: 0.882, F1 Micro: 0.7137, F1 Macro: 0.7026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.8938, F1 Micro: 0.7461, F1 Macro: 0.7372\n",
      "Epoch 4/10, Train Loss: 0.1974, Accuracy: 0.893, F1 Micro: 0.7185, F1 Macro: 0.7019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9005, F1 Micro: 0.7686, F1 Macro: 0.7651\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.8997, F1 Micro: 0.7601, F1 Macro: 0.7512\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.8978, F1 Micro: 0.7498, F1 Macro: 0.7432\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9011, F1 Micro: 0.7675, F1 Macro: 0.7628\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.8984, F1 Micro: 0.7461, F1 Macro: 0.7415\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9008, F1 Micro: 0.7628, F1 Macro: 0.7573\n",
      "Model 3 - Iteration 2394: Accuracy: 0.9005, F1 Micro: 0.7686, F1 Macro: 0.7651\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.71      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 107.24794435501099 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8899, F1 Micro: 0.7452, F1 Macro: 0.7398\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 73.681321144104 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4483, Accuracy: 0.8614, F1 Micro: 0.5842, F1 Macro: 0.5208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2849, Accuracy: 0.8898, F1 Micro: 0.7279, F1 Macro: 0.7262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.8948, F1 Micro: 0.7682, F1 Macro: 0.7662\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.9014, F1 Micro: 0.7523, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.9045, F1 Micro: 0.7759, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9039, F1 Micro: 0.7777, F1 Macro: 0.7743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9016, F1 Micro: 0.778, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9048, F1 Micro: 0.7675, F1 Macro: 0.7525\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9027, F1 Micro: 0.7668, F1 Macro: 0.7547\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9034, F1 Micro: 0.7718, F1 Macro: 0.7653\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9016, F1 Micro: 0.778, F1 Macro: 0.7688\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.90       370\n",
      "                sara       0.70      0.65      0.67       248\n",
      "         radikalisme       0.76      0.75      0.75       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 121.24660897254944 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4397, Accuracy: 0.8677, F1 Micro: 0.6247, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.283, Accuracy: 0.8872, F1 Micro: 0.7296, F1 Macro: 0.7265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9011, F1 Micro: 0.7743, F1 Macro: 0.7698\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9033, F1 Micro: 0.7667, F1 Macro: 0.7605\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9013, F1 Micro: 0.7593, F1 Macro: 0.7545\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.903, F1 Micro: 0.7652, F1 Macro: 0.7553\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.8986, F1 Micro: 0.7666, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9008, F1 Micro: 0.7788, F1 Macro: 0.7747\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9013, F1 Micro: 0.7767, F1 Macro: 0.772\n",
      "Epoch 10/10, Train Loss: 0.0386, Accuracy: 0.8973, F1 Micro: 0.7617, F1 Macro: 0.7552\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9008, F1 Micro: 0.7788, F1 Macro: 0.7747\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.70      0.85      0.76       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 118.51130270957947 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4484, Accuracy: 0.8627, F1 Micro: 0.5864, F1 Macro: 0.5248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2877, Accuracy: 0.8889, F1 Micro: 0.7332, F1 Macro: 0.7322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.8939, F1 Micro: 0.7685, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9005, F1 Micro: 0.7698, F1 Macro: 0.7642\n",
      "Epoch 5/10, Train Loss: 0.1501, Accuracy: 0.8981, F1 Micro: 0.7603, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.903, F1 Micro: 0.7743, F1 Macro: 0.7666\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.8973, F1 Micro: 0.7437, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9044, F1 Micro: 0.7755, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9028, F1 Micro: 0.7688, F1 Macro: 0.7619\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.8988, F1 Micro: 0.7532, F1 Macro: 0.7472\n",
      "Model 3 - Iteration 2777: Accuracy: 0.9044, F1 Micro: 0.7755, F1 Macro: 0.7685\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.72      0.60      0.65       248\n",
      "         radikalisme       0.73      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 120.977872133255 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.892, F1 Micro: 0.7506, F1 Macro: 0.7449\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 66.24087715148926 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.432, Accuracy: 0.8737, F1 Micro: 0.6535, F1 Macro: 0.6478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2828, Accuracy: 0.8941, F1 Micro: 0.7509, F1 Macro: 0.7434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2245, Accuracy: 0.8972, F1 Micro: 0.7648, F1 Macro: 0.7659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7701\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9009, F1 Micro: 0.756, F1 Macro: 0.7434\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9016, F1 Micro: 0.7719, F1 Macro: 0.7633\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9013, F1 Micro: 0.7756, F1 Macro: 0.7711\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9028, F1 Micro: 0.7759, F1 Macro: 0.7703\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9025, F1 Micro: 0.7711, F1 Macro: 0.7711\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.9017, F1 Micro: 0.765, F1 Macro: 0.7588\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7701\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 128.1649763584137 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4313, Accuracy: 0.8683, F1 Micro: 0.6355, F1 Macro: 0.6323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2835, Accuracy: 0.8909, F1 Micro: 0.7542, F1 Macro: 0.748\n",
      "Epoch 3/10, Train Loss: 0.2285, Accuracy: 0.8964, F1 Micro: 0.7534, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9016, F1 Micro: 0.7772, F1 Macro: 0.7699\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.8978, F1 Micro: 0.751, F1 Macro: 0.7383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.9028, F1 Micro: 0.7808, F1 Macro: 0.7763\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.903, F1 Micro: 0.7792, F1 Macro: 0.779\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9017, F1 Micro: 0.7687, F1 Macro: 0.7609\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9027, F1 Micro: 0.7763, F1 Macro: 0.7759\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9055, F1 Micro: 0.7781, F1 Macro: 0.771\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9028, F1 Micro: 0.7808, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.90       370\n",
      "                sara       0.70      0.69      0.69       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 127.98430752754211 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4373, Accuracy: 0.8736, F1 Micro: 0.6556, F1 Macro: 0.6476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2825, Accuracy: 0.8919, F1 Micro: 0.7521, F1 Macro: 0.744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2285, Accuracy: 0.8975, F1 Micro: 0.753, F1 Macro: 0.7515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9027, F1 Micro: 0.775, F1 Macro: 0.7638\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9009, F1 Micro: 0.7554, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9016, F1 Micro: 0.7785, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9033, F1 Micro: 0.7717, F1 Macro: 0.7679\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9028, F1 Micro: 0.7693, F1 Macro: 0.7619\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9022, F1 Micro: 0.765, F1 Macro: 0.7624\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.9017, F1 Micro: 0.7767, F1 Macro: 0.7724\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9016, F1 Micro: 0.7785, F1 Macro: 0.7726\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 129.3324248790741 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8934, F1 Micro: 0.7547, F1 Macro: 0.7489\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 59.93614888191223 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4277, Accuracy: 0.8781, F1 Micro: 0.6742, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2823, Accuracy: 0.8958, F1 Micro: 0.7518, F1 Macro: 0.7459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9008, F1 Micro: 0.7652, F1 Macro: 0.765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9041, F1 Micro: 0.7706, F1 Macro: 0.7658\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9045, F1 Micro: 0.7665, F1 Macro: 0.7598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9086, F1 Micro: 0.7808, F1 Macro: 0.7713\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9055, F1 Micro: 0.7777, F1 Macro: 0.7687\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9036, F1 Micro: 0.7685, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.044, Accuracy: 0.9067, F1 Micro: 0.7691, F1 Macro: 0.7545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9072, F1 Micro: 0.781, F1 Macro: 0.7762\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9072, F1 Micro: 0.781, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.71      0.62      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.78      1365\n",
      "        weighted avg       0.79      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 139.5335533618927 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4243, Accuracy: 0.8733, F1 Micro: 0.6611, F1 Macro: 0.6349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2887, Accuracy: 0.8927, F1 Micro: 0.7565, F1 Macro: 0.7522\n",
      "Epoch 3/10, Train Loss: 0.2218, Accuracy: 0.8988, F1 Micro: 0.7562, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.9008, F1 Micro: 0.7615, F1 Macro: 0.7551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9038, F1 Micro: 0.7703, F1 Macro: 0.7635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.9047, F1 Micro: 0.7756, F1 Macro: 0.7641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9045, F1 Micro: 0.7797, F1 Macro: 0.7728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9047, F1 Micro: 0.7804, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9047, F1 Micro: 0.7814, F1 Macro: 0.7775\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.9016, F1 Micro: 0.7684, F1 Macro: 0.7623\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9047, F1 Micro: 0.7814, F1 Macro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.66      0.69      0.68       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 142.47779417037964 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4318, Accuracy: 0.8748, F1 Micro: 0.6639, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2882, Accuracy: 0.8955, F1 Micro: 0.7623, F1 Macro: 0.7555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.222, Accuracy: 0.8989, F1 Micro: 0.7627, F1 Macro: 0.7634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1916, Accuracy: 0.902, F1 Micro: 0.7665, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.905, F1 Micro: 0.7766, F1 Macro: 0.7718\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9045, F1 Micro: 0.7736, F1 Macro: 0.7636\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9025, F1 Micro: 0.762, F1 Macro: 0.7553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.9061, F1 Micro: 0.7814, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.9052, F1 Micro: 0.7615, F1 Macro: 0.7512\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.9042, F1 Micro: 0.7708, F1 Macro: 0.7652\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9061, F1 Micro: 0.7814, F1 Macro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 139.12616324424744 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.895, F1 Micro: 0.758, F1 Macro: 0.7525\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.803715467453 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4201, Accuracy: 0.8783, F1 Micro: 0.7053, F1 Macro: 0.6949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2683, Accuracy: 0.8947, F1 Micro: 0.7643, F1 Macro: 0.7586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.219, Accuracy: 0.9009, F1 Micro: 0.776, F1 Macro: 0.771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9009, F1 Micro: 0.7797, F1 Macro: 0.7758\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9008, F1 Micro: 0.7555, F1 Macro: 0.7412\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.9084, F1 Micro: 0.7909, F1 Macro: 0.7883\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9041, F1 Micro: 0.7852, F1 Macro: 0.785\n",
      "Epoch 9/10, Train Loss: 0.0449, Accuracy: 0.9017, F1 Micro: 0.7806, F1 Macro: 0.78\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.9017, F1 Micro: 0.7762, F1 Macro: 0.7746\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9084, F1 Micro: 0.7909, F1 Macro: 0.7883\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.74      0.70       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 144.97347927093506 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4138, Accuracy: 0.8727, F1 Micro: 0.6852, F1 Macro: 0.6762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.8963, F1 Micro: 0.7664, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2184, Accuracy: 0.8981, F1 Micro: 0.7733, F1 Macro: 0.7694\n",
      "Epoch 4/10, Train Loss: 0.1831, Accuracy: 0.9008, F1 Micro: 0.7708, F1 Macro: 0.7644\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.897, F1 Micro: 0.7488, F1 Macro: 0.7343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9041, F1 Micro: 0.7736, F1 Macro: 0.7634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9009, F1 Micro: 0.7772, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9042, F1 Micro: 0.7773, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.9053, F1 Micro: 0.7769, F1 Macro: 0.7694\n",
      "Epoch 10/10, Train Loss: 0.0375, Accuracy: 0.9055, F1 Micro: 0.7753, F1 Macro: 0.7724\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9042, F1 Micro: 0.7773, F1 Macro: 0.7732\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 147.2661304473877 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4211, Accuracy: 0.8806, F1 Micro: 0.7097, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.8945, F1 Micro: 0.7651, F1 Macro: 0.7576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.8997, F1 Micro: 0.7741, F1 Macro: 0.7676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9027, F1 Micro: 0.7761, F1 Macro: 0.7714\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.8995, F1 Micro: 0.7501, F1 Macro: 0.7346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9064, F1 Micro: 0.7767, F1 Macro: 0.7712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9106, F1 Micro: 0.7918, F1 Macro: 0.7892\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9066, F1 Micro: 0.7801, F1 Macro: 0.7798\n",
      "Epoch 9/10, Train Loss: 0.0397, Accuracy: 0.905, F1 Micro: 0.7721, F1 Macro: 0.7666\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.9048, F1 Micro: 0.7684, F1 Macro: 0.7669\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9106, F1 Micro: 0.7918, F1 Macro: 0.7892\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.69      0.70      0.69       248\n",
      "         radikalisme       0.77      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.75      0.74      0.75       504\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.79      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 147.84649395942688 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8964, F1 Micro: 0.7612, F1 Macro: 0.756\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 49.19533157348633 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.418, Accuracy: 0.8659, F1 Micro: 0.5941, F1 Macro: 0.5598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2741, Accuracy: 0.8955, F1 Micro: 0.7625, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2173, Accuracy: 0.902, F1 Micro: 0.7697, F1 Macro: 0.766\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.9017, F1 Micro: 0.7647, F1 Macro: 0.7472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.9028, F1 Micro: 0.7796, F1 Macro: 0.7751\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.9011, F1 Micro: 0.7532, F1 Macro: 0.748\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9047, F1 Micro: 0.7783, F1 Macro: 0.769\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.907, F1 Micro: 0.7787, F1 Macro: 0.7744\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9039, F1 Micro: 0.7735, F1 Macro: 0.7721\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9033, F1 Micro: 0.7696, F1 Macro: 0.7628\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9028, F1 Micro: 0.7796, F1 Macro: 0.7751\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.93      0.89       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 149.38309049606323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.416, Accuracy: 0.868, F1 Micro: 0.6216, F1 Macro: 0.5961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.8913, F1 Micro: 0.7608, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.9006, F1 Micro: 0.763, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9022, F1 Micro: 0.7659, F1 Macro: 0.747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9022, F1 Micro: 0.7819, F1 Macro: 0.7779\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.9006, F1 Micro: 0.751, F1 Macro: 0.7421\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9031, F1 Micro: 0.7792, F1 Macro: 0.7749\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9028, F1 Micro: 0.7593, F1 Macro: 0.7498\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.905, F1 Micro: 0.7758, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9067, F1 Micro: 0.7859, F1 Macro: 0.7806\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9067, F1 Micro: 0.7859, F1 Macro: 0.7806\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.68      0.63      0.66       248\n",
      "         radikalisme       0.76      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 153.32114219665527 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4214, Accuracy: 0.8691, F1 Micro: 0.6069, F1 Macro: 0.5579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.8917, F1 Micro: 0.7616, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9022, F1 Micro: 0.77, F1 Macro: 0.7705\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.8997, F1 Micro: 0.7583, F1 Macro: 0.7379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9013, F1 Micro: 0.783, F1 Macro: 0.7819\n",
      "Epoch 6/10, Train Loss: 0.113, Accuracy: 0.9053, F1 Micro: 0.7769, F1 Macro: 0.7783\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.9061, F1 Micro: 0.7783, F1 Macro: 0.7705\n",
      "Epoch 8/10, Train Loss: 0.0586, Accuracy: 0.9042, F1 Micro: 0.773, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9047, F1 Micro: 0.7803, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9023, F1 Micro: 0.758, F1 Macro: 0.7501\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9013, F1 Micro: 0.783, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 149.97869443893433 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8971, F1 Micro: 0.7634, F1 Macro: 0.7583\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 46.27676558494568 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3974, Accuracy: 0.882, F1 Micro: 0.7106, F1 Macro: 0.7018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.8991, F1 Micro: 0.7704, F1 Macro: 0.7655\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.9023, F1 Micro: 0.7669, F1 Macro: 0.7544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.8989, F1 Micro: 0.778, F1 Macro: 0.7746\n",
      "Epoch 5/10, Train Loss: 0.1306, Accuracy: 0.8995, F1 Micro: 0.7541, F1 Macro: 0.747\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9059, F1 Micro: 0.7659, F1 Macro: 0.7562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.907, F1 Micro: 0.784, F1 Macro: 0.7801\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.9027, F1 Micro: 0.7816, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9058, F1 Micro: 0.7827, F1 Macro: 0.7781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9034, F1 Micro: 0.7842, F1 Macro: 0.7852\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9034, F1 Micro: 0.7842, F1 Macro: 0.7852\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.65      0.75      0.70       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 159.4257197380066 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3933, Accuracy: 0.8777, F1 Micro: 0.7086, F1 Macro: 0.7001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.8975, F1 Micro: 0.7675, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.902, F1 Micro: 0.7732, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1694, Accuracy: 0.8983, F1 Micro: 0.7748, F1 Macro: 0.7705\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.8997, F1 Micro: 0.7626, F1 Macro: 0.7569\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.902, F1 Micro: 0.7654, F1 Macro: 0.7598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9056, F1 Micro: 0.7807, F1 Macro: 0.7774\n",
      "Epoch 8/10, Train Loss: 0.0549, Accuracy: 0.9014, F1 Micro: 0.7776, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.7743\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9039, F1 Micro: 0.7738, F1 Macro: 0.7685\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9056, F1 Micro: 0.7807, F1 Macro: 0.7774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.75      0.72      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 158.63914394378662 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3973, Accuracy: 0.8798, F1 Micro: 0.7132, F1 Macro: 0.7023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.266, Accuracy: 0.8989, F1 Micro: 0.7693, F1 Macro: 0.7645\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9006, F1 Micro: 0.7658, F1 Macro: 0.748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1711, Accuracy: 0.8984, F1 Micro: 0.7803, F1 Macro: 0.7814\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.902, F1 Micro: 0.7684, F1 Macro: 0.765\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9064, F1 Micro: 0.7708, F1 Macro: 0.7638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9067, F1 Micro: 0.7825, F1 Macro: 0.7818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9036, F1 Micro: 0.7854, F1 Macro: 0.786\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9059, F1 Micro: 0.7831, F1 Macro: 0.781\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9061, F1 Micro: 0.7848, F1 Macro: 0.7836\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9036, F1 Micro: 0.7854, F1 Macro: 0.786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.66      0.76      0.71       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 159.11500597000122 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8978, F1 Micro: 0.7652, F1 Macro: 0.7605\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.01753044128418 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4076, Accuracy: 0.883, F1 Micro: 0.7219, F1 Macro: 0.7209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2611, Accuracy: 0.8981, F1 Micro: 0.7599, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2176, Accuracy: 0.9041, F1 Micro: 0.7642, F1 Macro: 0.7537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9022, F1 Micro: 0.7811, F1 Macro: 0.7767\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9059, F1 Micro: 0.7732, F1 Macro: 0.7637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9062, F1 Micro: 0.7828, F1 Macro: 0.7751\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9069, F1 Micro: 0.7807, F1 Macro: 0.7755\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9056, F1 Micro: 0.7807, F1 Macro: 0.7743\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9033, F1 Micro: 0.7662, F1 Macro: 0.7562\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9052, F1 Micro: 0.7763, F1 Macro: 0.7707\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9062, F1 Micro: 0.7828, F1 Macro: 0.7751\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.67      0.62      0.65       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 165.7328917980194 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.403, Accuracy: 0.8797, F1 Micro: 0.707, F1 Macro: 0.7069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.8958, F1 Micro: 0.7579, F1 Macro: 0.755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9025, F1 Micro: 0.7685, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1795, Accuracy: 0.903, F1 Micro: 0.777, F1 Macro: 0.7702\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.9011, F1 Micro: 0.7721, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9038, F1 Micro: 0.772, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9069, F1 Micro: 0.7791, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.9025, F1 Micro: 0.7795, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9006, F1 Micro: 0.7627, F1 Macro: 0.7558\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.9028, F1 Micro: 0.7708, F1 Macro: 0.7652\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9025, F1 Micro: 0.7795, F1 Macro: 0.7755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.89       370\n",
      "                sara       0.68      0.65      0.67       248\n",
      "         radikalisme       0.76      0.85      0.81       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 165.59426140785217 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4081, Accuracy: 0.8841, F1 Micro: 0.7229, F1 Macro: 0.7207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.8959, F1 Micro: 0.7627, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2208, Accuracy: 0.9069, F1 Micro: 0.7768, F1 Macro: 0.7734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9034, F1 Micro: 0.7824, F1 Macro: 0.7786\n",
      "Epoch 5/10, Train Loss: 0.1372, Accuracy: 0.9047, F1 Micro: 0.765, F1 Macro: 0.7531\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.9067, F1 Micro: 0.7808, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9053, F1 Micro: 0.7683, F1 Macro: 0.7634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.9033, F1 Micro: 0.7851, F1 Macro: 0.7844\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.9028, F1 Micro: 0.7628, F1 Macro: 0.7581\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9022, F1 Micro: 0.7719, F1 Macro: 0.769\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9033, F1 Micro: 0.7851, F1 Macro: 0.7844\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.75      0.87      0.81       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 164.6663854122162 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8983, F1 Micro: 0.7666, F1 Macro: 0.762\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.208709716796875 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8783, F1 Micro: 0.6658, F1 Macro: 0.6605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2677, Accuracy: 0.8998, F1 Micro: 0.7598, F1 Macro: 0.7485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9038, F1 Micro: 0.7771, F1 Macro: 0.7694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9031, F1 Micro: 0.7866, F1 Macro: 0.7829\n",
      "Epoch 5/10, Train Loss: 0.1289, Accuracy: 0.9069, F1 Micro: 0.7756, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9102, F1 Micro: 0.7829, F1 Macro: 0.7794\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9067, F1 Micro: 0.7786, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9056, F1 Micro: 0.774, F1 Macro: 0.7686\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9033, F1 Micro: 0.7787, F1 Macro: 0.7749\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.8997, F1 Micro: 0.7594, F1 Macro: 0.7469\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9031, F1 Micro: 0.7866, F1 Macro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.63      0.75      0.68       248\n",
      "         radikalisme       0.70      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 170.20416975021362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4021, Accuracy: 0.8786, F1 Micro: 0.6806, F1 Macro: 0.6778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.8964, F1 Micro: 0.7574, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9016, F1 Micro: 0.7763, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.8975, F1 Micro: 0.7772, F1 Macro: 0.7758\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.9, F1 Micro: 0.7645, F1 Macro: 0.7602\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.898, F1 Micro: 0.7539, F1 Macro: 0.752\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9023, F1 Micro: 0.7649, F1 Macro: 0.7573\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.9023, F1 Micro: 0.7832, F1 Macro: 0.7848\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9041, F1 Micro: 0.7731, F1 Macro: 0.7719\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9023, F1 Micro: 0.7832, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.62      0.80      0.70       248\n",
      "         radikalisme       0.73      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 172.33289742469788 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4059, Accuracy: 0.8813, F1 Micro: 0.6903, F1 Macro: 0.6872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2683, Accuracy: 0.8958, F1 Micro: 0.7567, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9023, F1 Micro: 0.7736, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9, F1 Micro: 0.7826, F1 Macro: 0.7817\n",
      "Epoch 5/10, Train Loss: 0.1365, Accuracy: 0.9048, F1 Micro: 0.7732, F1 Macro: 0.7713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.9066, F1 Micro: 0.7843, F1 Macro: 0.7858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9064, F1 Micro: 0.7861, F1 Macro: 0.7846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9091, F1 Micro: 0.7903, F1 Macro: 0.7905\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.9044, F1 Micro: 0.7765, F1 Macro: 0.773\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9066, F1 Micro: 0.7825, F1 Macro: 0.7828\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9091, F1 Micro: 0.7903, F1 Macro: 0.7905\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.68      0.72      0.70       248\n",
      "         radikalisme       0.77      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 175.0139286518097 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8988, F1 Micro: 0.7682, F1 Macro: 0.7639\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.15637993812561 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4031, Accuracy: 0.8791, F1 Micro: 0.7318, F1 Macro: 0.7327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.8998, F1 Micro: 0.7747, F1 Macro: 0.7742\n",
      "Epoch 3/10, Train Loss: 0.2161, Accuracy: 0.9039, F1 Micro: 0.7666, F1 Macro: 0.7637\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.905, F1 Micro: 0.7731, F1 Macro: 0.769\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9067, F1 Micro: 0.7696, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.9036, F1 Micro: 0.7786, F1 Macro: 0.7745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9027, F1 Micro: 0.7852, F1 Macro: 0.7899\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.9041, F1 Micro: 0.7744, F1 Macro: 0.7683\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.9019, F1 Micro: 0.776, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.9027, F1 Micro: 0.7838, F1 Macro: 0.7858\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9027, F1 Micro: 0.7852, F1 Macro: 0.7899\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.63      0.83      0.71       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 173.42312908172607 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3944, Accuracy: 0.878, F1 Micro: 0.7194, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.8964, F1 Micro: 0.7694, F1 Macro: 0.7683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.9045, F1 Micro: 0.7729, F1 Macro: 0.7673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9059, F1 Micro: 0.7784, F1 Macro: 0.7733\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.9066, F1 Micro: 0.7764, F1 Macro: 0.7674\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9011, F1 Micro: 0.7751, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9005, F1 Micro: 0.7813, F1 Macro: 0.7827\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9017, F1 Micro: 0.7756, F1 Macro: 0.7725\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9042, F1 Micro: 0.7759, F1 Macro: 0.7688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9061, F1 Micro: 0.7867, F1 Macro: 0.787\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9061, F1 Micro: 0.7867, F1 Macro: 0.787\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.79      0.83      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 176.8870780467987 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3961, Accuracy: 0.8792, F1 Micro: 0.7256, F1 Macro: 0.719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2603, Accuracy: 0.9003, F1 Micro: 0.7777, F1 Macro: 0.7774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2173, Accuracy: 0.9053, F1 Micro: 0.7796, F1 Macro: 0.7786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.177, Accuracy: 0.9062, F1 Micro: 0.7799, F1 Macro: 0.7774\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9069, F1 Micro: 0.7692, F1 Macro: 0.7575\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9033, F1 Micro: 0.7703, F1 Macro: 0.7641\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9023, F1 Micro: 0.7792, F1 Macro: 0.7807\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.9031, F1 Micro: 0.7767, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9033, F1 Micro: 0.7745, F1 Macro: 0.7731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9078, F1 Micro: 0.7858, F1 Macro: 0.7864\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9078, F1 Micro: 0.7858, F1 Macro: 0.7864\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.72      0.70       248\n",
      "         radikalisme       0.79      0.83      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 175.0582504272461 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8993, F1 Micro: 0.7695, F1 Macro: 0.7656\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 31.11939287185669 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3932, Accuracy: 0.8869, F1 Micro: 0.7362, F1 Macro: 0.7281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.9006, F1 Micro: 0.7573, F1 Macro: 0.7372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9038, F1 Micro: 0.77, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1663, Accuracy: 0.9042, F1 Micro: 0.7856, F1 Macro: 0.7832\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9055, F1 Micro: 0.773, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9042, F1 Micro: 0.7714, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9064, F1 Micro: 0.7848, F1 Macro: 0.7788\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9061, F1 Micro: 0.7825, F1 Macro: 0.782\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.9045, F1 Micro: 0.7751, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.9069, F1 Micro: 0.7861, F1 Macro: 0.7821\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9069, F1 Micro: 0.7861, F1 Macro: 0.7821\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.72      0.64      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 180.9995698928833 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.8836, F1 Micro: 0.7327, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2619, Accuracy: 0.8978, F1 Micro: 0.761, F1 Macro: 0.7477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9042, F1 Micro: 0.773, F1 Macro: 0.7676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.9031, F1 Micro: 0.7811, F1 Macro: 0.7811\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9022, F1 Micro: 0.7622, F1 Macro: 0.7542\n",
      "Epoch 6/10, Train Loss: 0.0944, Accuracy: 0.8995, F1 Micro: 0.7628, F1 Macro: 0.7502\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9028, F1 Micro: 0.7701, F1 Macro: 0.7651\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9016, F1 Micro: 0.7739, F1 Macro: 0.7676\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9072, F1 Micro: 0.781, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9028, F1 Micro: 0.7782, F1 Macro: 0.7754\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9031, F1 Micro: 0.7811, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.65      0.74      0.69       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 179.24168276786804 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3877, Accuracy: 0.8853, F1 Micro: 0.7352, F1 Macro: 0.7245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2607, Accuracy: 0.9005, F1 Micro: 0.765, F1 Macro: 0.7526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2123, Accuracy: 0.9044, F1 Micro: 0.7727, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9005, F1 Micro: 0.7853, F1 Macro: 0.7867\n",
      "Epoch 5/10, Train Loss: 0.1328, Accuracy: 0.9053, F1 Micro: 0.7764, F1 Macro: 0.768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.908, F1 Micro: 0.7876, F1 Macro: 0.7844\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9061, F1 Micro: 0.7796, F1 Macro: 0.775\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.9058, F1 Micro: 0.7756, F1 Macro: 0.7738\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.905, F1 Micro: 0.7738, F1 Macro: 0.7665\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9055, F1 Micro: 0.7825, F1 Macro: 0.7789\n",
      "Model 3 - Iteration 4863: Accuracy: 0.908, F1 Micro: 0.7876, F1 Macro: 0.7844\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.71      0.67      0.69       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 182.56931018829346 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8997, F1 Micro: 0.7705, F1 Macro: 0.7667\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 26.633984088897705 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3922, Accuracy: 0.8894, F1 Micro: 0.7273, F1 Macro: 0.7207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9041, F1 Micro: 0.7749, F1 Macro: 0.7646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2086, Accuracy: 0.9045, F1 Micro: 0.7761, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1729, Accuracy: 0.9055, F1 Micro: 0.789, F1 Macro: 0.7873\n",
      "Epoch 5/10, Train Loss: 0.1212, Accuracy: 0.9016, F1 Micro: 0.7752, F1 Macro: 0.7731\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.903, F1 Micro: 0.7788, F1 Macro: 0.7778\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9056, F1 Micro: 0.7861, F1 Macro: 0.7816\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9059, F1 Micro: 0.7836, F1 Macro: 0.7853\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9034, F1 Micro: 0.7728, F1 Macro: 0.771\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9039, F1 Micro: 0.7811, F1 Macro: 0.7813\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9055, F1 Micro: 0.789, F1 Macro: 0.7873\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 187.08682894706726 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3923, Accuracy: 0.8848, F1 Micro: 0.7199, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.9017, F1 Micro: 0.7717, F1 Macro: 0.7637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9052, F1 Micro: 0.7827, F1 Macro: 0.7767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1736, Accuracy: 0.9041, F1 Micro: 0.7874, F1 Macro: 0.7834\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9014, F1 Micro: 0.7684, F1 Macro: 0.7615\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9025, F1 Micro: 0.7779, F1 Macro: 0.7778\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.8989, F1 Micro: 0.7663, F1 Macro: 0.7627\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9064, F1 Micro: 0.781, F1 Macro: 0.7788\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.7697\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9008, F1 Micro: 0.7776, F1 Macro: 0.7762\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9041, F1 Micro: 0.7874, F1 Macro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.75      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.85      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 186.27538323402405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3912, Accuracy: 0.8878, F1 Micro: 0.7251, F1 Macro: 0.7213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2659, Accuracy: 0.9014, F1 Micro: 0.7659, F1 Macro: 0.758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9045, F1 Micro: 0.7806, F1 Macro: 0.7765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9052, F1 Micro: 0.7857, F1 Macro: 0.7836\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9059, F1 Micro: 0.7774, F1 Macro: 0.7679\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9066, F1 Micro: 0.7847, F1 Macro: 0.7834\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9048, F1 Micro: 0.7703, F1 Macro: 0.7639\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9042, F1 Micro: 0.7775, F1 Macro: 0.7756\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9013, F1 Micro: 0.7751, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9038, F1 Micro: 0.7834, F1 Macro: 0.7837\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9052, F1 Micro: 0.7857, F1 Macro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.78      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 186.94423151016235 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.9, F1 Micro: 0.7715, F1 Macro: 0.7678\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.639057874679565 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.8897, F1 Micro: 0.7473, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.8998, F1 Micro: 0.7634, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2083, Accuracy: 0.9006, F1 Micro: 0.7856, F1 Macro: 0.7856\n",
      "Epoch 4/10, Train Loss: 0.1688, Accuracy: 0.9048, F1 Micro: 0.774, F1 Macro: 0.7682\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9017, F1 Micro: 0.7833, F1 Macro: 0.7836\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9036, F1 Micro: 0.7805, F1 Macro: 0.7792\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9042, F1 Micro: 0.7813, F1 Macro: 0.7812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.9105, F1 Micro: 0.7914, F1 Macro: 0.7907\n",
      "Epoch 9/10, Train Loss: 0.0393, Accuracy: 0.9053, F1 Micro: 0.7812, F1 Macro: 0.78\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9034, F1 Micro: 0.7785, F1 Macro: 0.7767\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9105, F1 Micro: 0.7914, F1 Macro: 0.7907\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.73      0.70       248\n",
      "         radikalisme       0.79      0.83      0.81       243\n",
      "pencemaran_nama_baik       0.75      0.73      0.74       504\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.79      0.80      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 192.36112928390503 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.8867, F1 Micro: 0.7399, F1 Macro: 0.7353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9003, F1 Micro: 0.7576, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9019, F1 Micro: 0.7819, F1 Macro: 0.7793\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9036, F1 Micro: 0.7653, F1 Macro: 0.7573\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9006, F1 Micro: 0.7765, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9034, F1 Micro: 0.7821, F1 Macro: 0.7789\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.9086, F1 Micro: 0.7795, F1 Macro: 0.7757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.9055, F1 Micro: 0.7846, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9034, F1 Micro: 0.7813, F1 Macro: 0.7788\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.9002, F1 Micro: 0.7785, F1 Macro: 0.7783\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9055, F1 Micro: 0.7846, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 195.01281213760376 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.8866, F1 Micro: 0.747, F1 Macro: 0.7421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.9006, F1 Micro: 0.7627, F1 Macro: 0.7563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2097, Accuracy: 0.902, F1 Micro: 0.7845, F1 Macro: 0.7822\n",
      "Epoch 4/10, Train Loss: 0.1692, Accuracy: 0.9016, F1 Micro: 0.7571, F1 Macro: 0.7471\n",
      "Epoch 5/10, Train Loss: 0.1352, Accuracy: 0.9038, F1 Micro: 0.7757, F1 Macro: 0.7704\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9047, F1 Micro: 0.7695, F1 Macro: 0.7627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9081, F1 Micro: 0.7846, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0516, Accuracy: 0.908, F1 Micro: 0.7902, F1 Macro: 0.7894\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.9042, F1 Micro: 0.789, F1 Macro: 0.7894\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.9056, F1 Micro: 0.7751, F1 Macro: 0.7717\n",
      "Model 3 - Iteration 5263: Accuracy: 0.908, F1 Micro: 0.7902, F1 Macro: 0.7894\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.76      0.70       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.82      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 195.0904734134674 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.9005, F1 Micro: 0.7725, F1 Macro: 0.769\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.24230432510376 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3869, Accuracy: 0.8905, F1 Micro: 0.7334, F1 Macro: 0.7304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2551, Accuracy: 0.9013, F1 Micro: 0.7789, F1 Macro: 0.7753\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9045, F1 Micro: 0.7748, F1 Macro: 0.7673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1683, Accuracy: 0.9041, F1 Micro: 0.7877, F1 Macro: 0.7852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1294, Accuracy: 0.9072, F1 Micro: 0.7892, F1 Macro: 0.7878\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.9027, F1 Micro: 0.7777, F1 Macro: 0.7741\n",
      "Epoch 7/10, Train Loss: 0.0672, Accuracy: 0.9027, F1 Micro: 0.7781, F1 Macro: 0.775\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9064, F1 Micro: 0.7723, F1 Macro: 0.7708\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9045, F1 Micro: 0.7702, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.9039, F1 Micro: 0.7871, F1 Macro: 0.7866\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9072, F1 Micro: 0.7892, F1 Macro: 0.7878\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.71      0.70       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 197.48061299324036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3832, Accuracy: 0.8853, F1 Micro: 0.7157, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.9, F1 Micro: 0.7798, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9042, F1 Micro: 0.7802, F1 Macro: 0.773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9028, F1 Micro: 0.7864, F1 Macro: 0.783\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9041, F1 Micro: 0.7861, F1 Macro: 0.7843\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.9031, F1 Micro: 0.7806, F1 Macro: 0.776\n",
      "Epoch 7/10, Train Loss: 0.0696, Accuracy: 0.9061, F1 Micro: 0.7773, F1 Macro: 0.7733\n",
      "Epoch 8/10, Train Loss: 0.0512, Accuracy: 0.903, F1 Micro: 0.7744, F1 Macro: 0.7736\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9044, F1 Micro: 0.7713, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9047, F1 Micro: 0.781, F1 Macro: 0.7787\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9028, F1 Micro: 0.7864, F1 Macro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 197.63911628723145 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.8869, F1 Micro: 0.7272, F1 Macro: 0.7277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.8961, F1 Micro: 0.7737, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.9034, F1 Micro: 0.7804, F1 Macro: 0.7743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9034, F1 Micro: 0.7881, F1 Macro: 0.7848\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9027, F1 Micro: 0.7813, F1 Macro: 0.7798\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.902, F1 Micro: 0.7774, F1 Macro: 0.7737\n",
      "Epoch 7/10, Train Loss: 0.0697, Accuracy: 0.9039, F1 Micro: 0.7735, F1 Macro: 0.7662\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9027, F1 Micro: 0.7737, F1 Macro: 0.7678\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9034, F1 Micro: 0.768, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9002, F1 Micro: 0.7757, F1 Macro: 0.7724\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9034, F1 Micro: 0.7881, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       370\n",
      "                sara       0.67      0.72      0.69       248\n",
      "         radikalisme       0.71      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 197.20648312568665 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.9007, F1 Micro: 0.7734, F1 Macro: 0.7699\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.061351537704468 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8905, F1 Micro: 0.751, F1 Macro: 0.7478\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.8948, F1 Micro: 0.7349, F1 Macro: 0.7222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.9053, F1 Micro: 0.7843, F1 Macro: 0.7833\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9058, F1 Micro: 0.7705, F1 Macro: 0.7575\n",
      "Epoch 5/10, Train Loss: 0.1294, Accuracy: 0.9009, F1 Micro: 0.7763, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.9017, F1 Micro: 0.778, F1 Macro: 0.7746\n",
      "Epoch 7/10, Train Loss: 0.0647, Accuracy: 0.9053, F1 Micro: 0.7734, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.0531, Accuracy: 0.9005, F1 Micro: 0.7736, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9039, F1 Micro: 0.774, F1 Macro: 0.7703\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9048, F1 Micro: 0.777, F1 Macro: 0.7737\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9053, F1 Micro: 0.7843, F1 Macro: 0.7833\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.72      0.89      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 200.90038132667542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3691, Accuracy: 0.8881, F1 Micro: 0.7477, F1 Macro: 0.7462\n",
      "Epoch 2/10, Train Loss: 0.2511, Accuracy: 0.897, F1 Micro: 0.7443, F1 Macro: 0.7335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.903, F1 Micro: 0.781, F1 Macro: 0.7789\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9066, F1 Micro: 0.7765, F1 Macro: 0.7671\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9014, F1 Micro: 0.7781, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9056, F1 Micro: 0.7797, F1 Macro: 0.7759\n",
      "Epoch 7/10, Train Loss: 0.0701, Accuracy: 0.9044, F1 Micro: 0.7805, F1 Macro: 0.7744\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.9027, F1 Micro: 0.7732, F1 Macro: 0.7684\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9042, F1 Micro: 0.7702, F1 Macro: 0.7634\n",
      "Epoch 10/10, Train Loss: 0.0291, Accuracy: 0.9033, F1 Micro: 0.7771, F1 Macro: 0.7682\n",
      "Model 2 - Iteration 5641: Accuracy: 0.903, F1 Micro: 0.781, F1 Macro: 0.7789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.64      0.73      0.69       248\n",
      "         radikalisme       0.69      0.90      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 199.7198429107666 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8916, F1 Micro: 0.753, F1 Macro: 0.7504\n",
      "Epoch 2/10, Train Loss: 0.2527, Accuracy: 0.893, F1 Micro: 0.7302, F1 Macro: 0.7147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9048, F1 Micro: 0.781, F1 Macro: 0.7793\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.9061, F1 Micro: 0.7752, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9025, F1 Micro: 0.7844, F1 Macro: 0.784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9045, F1 Micro: 0.787, F1 Macro: 0.7864\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.9044, F1 Micro: 0.7784, F1 Macro: 0.7767\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9011, F1 Micro: 0.7784, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9069, F1 Micro: 0.7804, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9031, F1 Micro: 0.7714, F1 Macro: 0.7611\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9045, F1 Micro: 0.787, F1 Macro: 0.7864\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.63      0.73      0.68       248\n",
      "         radikalisme       0.75      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 203.27121996879578 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.9009, F1 Micro: 0.774, F1 Macro: 0.7706\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.321690082550049 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3669, Accuracy: 0.8823, F1 Micro: 0.6829, F1 Macro: 0.6662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2456, Accuracy: 0.9048, F1 Micro: 0.7685, F1 Macro: 0.7573\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.9036, F1 Micro: 0.7598, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1593, Accuracy: 0.903, F1 Micro: 0.7699, F1 Macro: 0.7622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.8988, F1 Micro: 0.7784, F1 Macro: 0.7754\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9041, F1 Micro: 0.7616, F1 Macro: 0.752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9064, F1 Micro: 0.7838, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0507, Accuracy: 0.9078, F1 Micro: 0.7887, F1 Macro: 0.7883\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.905, F1 Micro: 0.7832, F1 Macro: 0.7806\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9052, F1 Micro: 0.783, F1 Macro: 0.7767\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9078, F1 Micro: 0.7887, F1 Macro: 0.7883\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.77      0.71       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 212.18083310127258 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3635, Accuracy: 0.8813, F1 Micro: 0.6744, F1 Macro: 0.6582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2475, Accuracy: 0.9014, F1 Micro: 0.7593, F1 Macro: 0.7426\n",
      "Epoch 3/10, Train Loss: 0.2024, Accuracy: 0.9028, F1 Micro: 0.7565, F1 Macro: 0.7486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1616, Accuracy: 0.9022, F1 Micro: 0.7664, F1 Macro: 0.7593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.8963, F1 Micro: 0.7767, F1 Macro: 0.7755\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9034, F1 Micro: 0.7764, F1 Macro: 0.7748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.8998, F1 Micro: 0.7801, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0496, Accuracy: 0.9055, F1 Micro: 0.784, F1 Macro: 0.7843\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9022, F1 Micro: 0.7734, F1 Macro: 0.771\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9023, F1 Micro: 0.7728, F1 Macro: 0.7689\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9055, F1 Micro: 0.784, F1 Macro: 0.7843\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.80      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 212.15383672714233 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3668, Accuracy: 0.8809, F1 Micro: 0.6746, F1 Macro: 0.6571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2488, Accuracy: 0.9023, F1 Micro: 0.7684, F1 Macro: 0.7581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9062, F1 Micro: 0.7713, F1 Macro: 0.7669\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.9017, F1 Micro: 0.7712, F1 Macro: 0.764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.8998, F1 Micro: 0.7831, F1 Macro: 0.7811\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.903, F1 Micro: 0.7765, F1 Macro: 0.7742\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9064, F1 Micro: 0.7829, F1 Macro: 0.7804\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9006, F1 Micro: 0.7746, F1 Macro: 0.7731\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9028, F1 Micro: 0.7802, F1 Macro: 0.7781\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9022, F1 Micro: 0.7763, F1 Macro: 0.7674\n",
      "Model 3 - Iteration 5841: Accuracy: 0.8998, F1 Micro: 0.7831, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 209.13012790679932 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.9011, F1 Micro: 0.7745, F1 Macro: 0.7713\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.188024282455444 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3673, Accuracy: 0.8923, F1 Micro: 0.7514, F1 Macro: 0.7483\n",
      "Epoch 2/10, Train Loss: 0.2404, Accuracy: 0.8984, F1 Micro: 0.7475, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.905, F1 Micro: 0.7931, F1 Macro: 0.7945\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9081, F1 Micro: 0.7857, F1 Macro: 0.7843\n",
      "Epoch 5/10, Train Loss: 0.1187, Accuracy: 0.9041, F1 Micro: 0.7774, F1 Macro: 0.768\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9031, F1 Micro: 0.7704, F1 Macro: 0.7584\n",
      "Epoch 7/10, Train Loss: 0.0662, Accuracy: 0.9027, F1 Micro: 0.7847, F1 Macro: 0.7838\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9028, F1 Micro: 0.7756, F1 Macro: 0.7712\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.9036, F1 Micro: 0.7778, F1 Macro: 0.7743\n",
      "Model 1 - Iteration 6041: Accuracy: 0.905, F1 Micro: 0.7931, F1 Macro: 0.7945\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.63      0.82      0.71       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.75      0.85      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.80      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 211.2883858680725 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3647, Accuracy: 0.8888, F1 Micro: 0.7396, F1 Macro: 0.7367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2425, Accuracy: 0.8984, F1 Micro: 0.7529, F1 Macro: 0.7335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9003, F1 Micro: 0.784, F1 Macro: 0.782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1627, Accuracy: 0.9075, F1 Micro: 0.7866, F1 Macro: 0.7823\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9069, F1 Micro: 0.7865, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9047, F1 Micro: 0.7632, F1 Macro: 0.7457\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9053, F1 Micro: 0.7853, F1 Macro: 0.783\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9033, F1 Micro: 0.7663, F1 Macro: 0.7606\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.902, F1 Micro: 0.7706, F1 Macro: 0.7653\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9059, F1 Micro: 0.7809, F1 Macro: 0.7785\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9075, F1 Micro: 0.7866, F1 Macro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 216.38498497009277 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3679, Accuracy: 0.8908, F1 Micro: 0.7494, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9008, F1 Micro: 0.7545, F1 Macro: 0.7371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9028, F1 Micro: 0.7899, F1 Macro: 0.7922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9094, F1 Micro: 0.7918, F1 Macro: 0.792\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9067, F1 Micro: 0.7907, F1 Macro: 0.7858\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9052, F1 Micro: 0.7817, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0659, Accuracy: 0.9039, F1 Micro: 0.7895, F1 Macro: 0.7912\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9066, F1 Micro: 0.7825, F1 Macro: 0.7777\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9072, F1 Micro: 0.7842, F1 Macro: 0.7803\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9048, F1 Micro: 0.7718, F1 Macro: 0.7682\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9094, F1 Micro: 0.7918, F1 Macro: 0.792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.73      0.70       248\n",
      "         radikalisme       0.78      0.86      0.82       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.81      0.79      1365\n",
      "           macro avg       0.78      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 214.90109252929688 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.9014, F1 Micro: 0.7753, F1 Macro: 0.7721\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 3.9860856533050537 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3592, Accuracy: 0.8905, F1 Micro: 0.7273, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2363, Accuracy: 0.8975, F1 Micro: 0.7353, F1 Macro: 0.7303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9022, F1 Micro: 0.7871, F1 Macro: 0.7856\n",
      "Epoch 4/10, Train Loss: 0.1497, Accuracy: 0.9067, F1 Micro: 0.7866, F1 Macro: 0.7844\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9025, F1 Micro: 0.7729, F1 Macro: 0.7692\n",
      "Epoch 6/10, Train Loss: 0.0845, Accuracy: 0.9005, F1 Micro: 0.7794, F1 Macro: 0.7774\n",
      "Epoch 7/10, Train Loss: 0.0633, Accuracy: 0.9025, F1 Micro: 0.7814, F1 Macro: 0.7793\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9009, F1 Micro: 0.7741, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.036, Accuracy: 0.9027, F1 Micro: 0.776, F1 Macro: 0.7723\n",
      "Epoch 10/10, Train Loss: 0.0273, Accuracy: 0.9028, F1 Micro: 0.77, F1 Macro: 0.7662\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9022, F1 Micro: 0.7871, F1 Macro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.62      0.79      0.70       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.82      0.76       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 218.63303112983704 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3556, Accuracy: 0.8895, F1 Micro: 0.7294, F1 Macro: 0.6997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2362, Accuracy: 0.8969, F1 Micro: 0.7343, F1 Macro: 0.7272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1944, Accuracy: 0.9022, F1 Micro: 0.7875, F1 Macro: 0.7874\n",
      "Epoch 4/10, Train Loss: 0.1488, Accuracy: 0.9066, F1 Micro: 0.7795, F1 Macro: 0.7748\n",
      "Epoch 5/10, Train Loss: 0.1145, Accuracy: 0.9019, F1 Micro: 0.7839, F1 Macro: 0.7832\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.9006, F1 Micro: 0.7746, F1 Macro: 0.7725\n",
      "Epoch 7/10, Train Loss: 0.0623, Accuracy: 0.9022, F1 Micro: 0.7796, F1 Macro: 0.7785\n",
      "Epoch 8/10, Train Loss: 0.0506, Accuracy: 0.9044, F1 Micro: 0.7757, F1 Macro: 0.7702\n",
      "Epoch 9/10, Train Loss: 0.0385, Accuracy: 0.9025, F1 Micro: 0.783, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.0256, Accuracy: 0.9039, F1 Micro: 0.7671, F1 Macro: 0.7633\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9022, F1 Micro: 0.7875, F1 Macro: 0.7874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.61      0.81      0.70       248\n",
      "         radikalisme       0.72      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 218.36133694648743 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3572, Accuracy: 0.8905, F1 Micro: 0.7297, F1 Macro: 0.7009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.8991, F1 Micro: 0.7422, F1 Macro: 0.7352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1972, Accuracy: 0.9058, F1 Micro: 0.7901, F1 Macro: 0.7883\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.9067, F1 Micro: 0.7741, F1 Macro: 0.7698\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9025, F1 Micro: 0.7792, F1 Macro: 0.7749\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.9011, F1 Micro: 0.7712, F1 Macro: 0.7693\n",
      "Epoch 7/10, Train Loss: 0.0611, Accuracy: 0.9016, F1 Micro: 0.7763, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9055, F1 Micro: 0.7888, F1 Macro: 0.7876\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9023, F1 Micro: 0.7817, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.0265, Accuracy: 0.9048, F1 Micro: 0.7793, F1 Macro: 0.7761\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9058, F1 Micro: 0.7901, F1 Macro: 0.7883\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.75      0.70       248\n",
      "         radikalisme       0.73      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 219.10371375083923 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9015, F1 Micro: 0.7759, F1 Macro: 0.7728\n",
      "Total sampling time: 978.45 seconds\n",
      "Total runtime: 11583.938933610916 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b18331",
   "metadata": {
    "papermill": {
     "duration": 0.130302,
     "end_time": "2025-03-13T10:00:44.810883",
     "exception": false,
     "start_time": "2025-03-13T10:00:44.680581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5895, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4426, Accuracy: 0.7894, F1 Micro: 0.0246, F1 Macro: 0.022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3847, Accuracy: 0.818, F1 Micro: 0.2556, F1 Macro: 0.1754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3668, Accuracy: 0.8298, F1 Micro: 0.3605, F1 Macro: 0.2793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3013, Accuracy: 0.8477, F1 Micro: 0.5288, F1 Macro: 0.4896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2561, Accuracy: 0.852, F1 Micro: 0.5414, F1 Macro: 0.5173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2332, Accuracy: 0.8598, F1 Micro: 0.6081, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1801, Accuracy: 0.8694, F1 Micro: 0.6596, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1445, Accuracy: 0.8716, F1 Micro: 0.6907, F1 Macro: 0.6854\n",
      "Epoch 10/10, Train Loss: 0.125, Accuracy: 0.8741, F1 Micro: 0.6832, F1 Macro: 0.6774\n",
      "Model 1 - Iteration 388: Accuracy: 0.8716, F1 Micro: 0.6907, F1 Macro: 0.6854\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.59      0.54      0.56       248\n",
      "         radikalisme       0.67      0.71      0.69       243\n",
      "pencemaran_nama_baik       0.65      0.60      0.62       504\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1365\n",
      "           macro avg       0.70      0.67      0.69      1365\n",
      "        weighted avg       0.71      0.67      0.69      1365\n",
      "         samples avg       0.38      0.37      0.36      1365\n",
      "\n",
      "Training completed in 58.758134603500366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6012, Accuracy: 0.7864, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4516, Accuracy: 0.7872, F1 Micro: 0.0044, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.398, Accuracy: 0.8172, F1 Micro: 0.251, F1 Macro: 0.1728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3939, Accuracy: 0.8272, F1 Micro: 0.3494, F1 Macro: 0.2421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3201, Accuracy: 0.8402, F1 Micro: 0.4607, F1 Macro: 0.3833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2709, Accuracy: 0.8581, F1 Micro: 0.5873, F1 Macro: 0.5595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2413, Accuracy: 0.8652, F1 Micro: 0.6233, F1 Macro: 0.612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1826, Accuracy: 0.8681, F1 Micro: 0.6415, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1453, Accuracy: 0.8694, F1 Micro: 0.6596, F1 Macro: 0.6474\n",
      "Epoch 10/10, Train Loss: 0.1256, Accuracy: 0.8712, F1 Micro: 0.6584, F1 Macro: 0.6519\n",
      "Model 2 - Iteration 388: Accuracy: 0.8694, F1 Micro: 0.6596, F1 Macro: 0.6474\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.85      0.87       370\n",
      "                sara       0.65      0.43      0.51       248\n",
      "         radikalisme       0.68      0.63      0.65       243\n",
      "pencemaran_nama_baik       0.68      0.47      0.56       504\n",
      "\n",
      "           micro avg       0.74      0.59      0.66      1365\n",
      "           macro avg       0.72      0.59      0.65      1365\n",
      "        weighted avg       0.73      0.59      0.65      1365\n",
      "         samples avg       0.35      0.33      0.33      1365\n",
      "\n",
      "Training completed in 58.41703724861145 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6051, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.44, Accuracy: 0.7872, F1 Micro: 0.0044, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3793, Accuracy: 0.815, F1 Micro: 0.2342, F1 Macro: 0.1642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3661, Accuracy: 0.8283, F1 Micro: 0.3599, F1 Macro: 0.2676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2997, Accuracy: 0.8441, F1 Micro: 0.4944, F1 Macro: 0.4593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2552, Accuracy: 0.8545, F1 Micro: 0.5812, F1 Macro: 0.5695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2256, Accuracy: 0.8572, F1 Micro: 0.5875, F1 Macro: 0.5736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1775, Accuracy: 0.8647, F1 Micro: 0.6182, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1353, Accuracy: 0.8702, F1 Micro: 0.6742, F1 Macro: 0.6692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1188, Accuracy: 0.8702, F1 Micro: 0.6805, F1 Macro: 0.677\n",
      "Model 3 - Iteration 388: Accuracy: 0.8702, F1 Micro: 0.6805, F1 Macro: 0.677\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.84      0.86       370\n",
      "                sara       0.58      0.51      0.54       248\n",
      "         radikalisme       0.69      0.73      0.71       243\n",
      "pencemaran_nama_baik       0.65      0.54      0.59       504\n",
      "\n",
      "           micro avg       0.72      0.65      0.68      1365\n",
      "           macro avg       0.70      0.65      0.68      1365\n",
      "        weighted avg       0.71      0.65      0.68      1365\n",
      "         samples avg       0.37      0.36      0.36      1365\n",
      "\n",
      "Training completed in 60.77446222305298 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8704, F1 Micro: 0.677, F1 Macro: 0.6699\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 111.47351932525635 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5273, Accuracy: 0.8189, F1 Micro: 0.266, F1 Macro: 0.1937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3823, Accuracy: 0.8444, F1 Micro: 0.4818, F1 Macro: 0.3894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2859, Accuracy: 0.8562, F1 Micro: 0.5534, F1 Macro: 0.5141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2294, Accuracy: 0.8652, F1 Micro: 0.598, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1888, Accuracy: 0.8813, F1 Micro: 0.6948, F1 Macro: 0.6921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1595, Accuracy: 0.8834, F1 Micro: 0.7235, F1 Macro: 0.7233\n",
      "Epoch 7/10, Train Loss: 0.1258, Accuracy: 0.8856, F1 Micro: 0.7063, F1 Macro: 0.7029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.8889, F1 Micro: 0.73, F1 Macro: 0.7268\n",
      "Epoch 9/10, Train Loss: 0.083, Accuracy: 0.8898, F1 Micro: 0.729, F1 Macro: 0.7233\n",
      "Epoch 10/10, Train Loss: 0.056, Accuracy: 0.8864, F1 Micro: 0.7253, F1 Macro: 0.7206\n",
      "Model 1 - Iteration 971: Accuracy: 0.8889, F1 Micro: 0.73, F1 Macro: 0.7268\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.84      0.88       370\n",
      "                sara       0.66      0.59      0.62       248\n",
      "         radikalisme       0.71      0.78      0.74       243\n",
      "pencemaran_nama_baik       0.72      0.62      0.67       504\n",
      "\n",
      "           micro avg       0.76      0.70      0.73      1365\n",
      "           macro avg       0.75      0.71      0.73      1365\n",
      "        weighted avg       0.76      0.70      0.73      1365\n",
      "         samples avg       0.40      0.40      0.39      1365\n",
      "\n",
      "Training completed in 70.2169554233551 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5358, Accuracy: 0.815, F1 Micro: 0.2381, F1 Macro: 0.1774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4097, Accuracy: 0.8398, F1 Micro: 0.445, F1 Macro: 0.3108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3125, Accuracy: 0.8567, F1 Micro: 0.5422, F1 Macro: 0.4867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2448, Accuracy: 0.8703, F1 Micro: 0.6298, F1 Macro: 0.6074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1933, Accuracy: 0.8803, F1 Micro: 0.7005, F1 Macro: 0.6941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1605, Accuracy: 0.8844, F1 Micro: 0.7023, F1 Macro: 0.7004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1355, Accuracy: 0.8873, F1 Micro: 0.7239, F1 Macro: 0.72\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.8889, F1 Micro: 0.7215, F1 Macro: 0.7145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.8891, F1 Micro: 0.7378, F1 Macro: 0.7317\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.8875, F1 Micro: 0.7366, F1 Macro: 0.7283\n",
      "Model 2 - Iteration 971: Accuracy: 0.8891, F1 Micro: 0.7378, F1 Macro: 0.7317\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       370\n",
      "                sara       0.66      0.58      0.62       248\n",
      "         radikalisme       0.72      0.76      0.74       243\n",
      "pencemaran_nama_baik       0.69      0.71      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.73      0.74      1365\n",
      "           macro avg       0.74      0.72      0.73      1365\n",
      "        weighted avg       0.75      0.73      0.74      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 73.00200891494751 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.8103, F1 Micro: 0.2045, F1 Macro: 0.1556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3881, Accuracy: 0.8366, F1 Micro: 0.4182, F1 Macro: 0.3254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2923, Accuracy: 0.8548, F1 Micro: 0.5353, F1 Macro: 0.4846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2364, Accuracy: 0.8708, F1 Micro: 0.6329, F1 Macro: 0.6198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.189, Accuracy: 0.8806, F1 Micro: 0.7039, F1 Macro: 0.6997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1586, Accuracy: 0.8844, F1 Micro: 0.7184, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1303, Accuracy: 0.8864, F1 Micro: 0.7194, F1 Macro: 0.7164\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.8855, F1 Micro: 0.7088, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.8903, F1 Micro: 0.7317, F1 Macro: 0.7262\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.8869, F1 Micro: 0.727, F1 Macro: 0.7255\n",
      "Model 3 - Iteration 971: Accuracy: 0.8903, F1 Micro: 0.7317, F1 Macro: 0.7262\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.67      0.54      0.60       248\n",
      "         radikalisme       0.72      0.79      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.62      0.67       504\n",
      "\n",
      "           micro avg       0.76      0.70      0.73      1365\n",
      "           macro avg       0.76      0.70      0.73      1365\n",
      "        weighted avg       0.76      0.70      0.73      1365\n",
      "         samples avg       0.40      0.39      0.39      1365\n",
      "\n",
      "Training completed in 72.42057180404663 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8799, F1 Micro: 0.705, F1 Macro: 0.6991\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 100.40713858604431 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5016, Accuracy: 0.8356, F1 Micro: 0.3982, F1 Macro: 0.2866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3535, Accuracy: 0.8691, F1 Micro: 0.6403, F1 Macro: 0.6255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2621, Accuracy: 0.8819, F1 Micro: 0.7058, F1 Macro: 0.7036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2188, Accuracy: 0.8905, F1 Micro: 0.7459, F1 Macro: 0.7399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1815, Accuracy: 0.8913, F1 Micro: 0.7539, F1 Macro: 0.75\n",
      "Epoch 6/10, Train Loss: 0.1347, Accuracy: 0.8955, F1 Micro: 0.7538, F1 Macro: 0.7448\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.8948, F1 Micro: 0.7364, F1 Macro: 0.7252\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.8934, F1 Micro: 0.7525, F1 Macro: 0.7468\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.8923, F1 Micro: 0.7265, F1 Macro: 0.7123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.8964, F1 Micro: 0.7549, F1 Macro: 0.7444\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8964, F1 Micro: 0.7549, F1 Macro: 0.7444\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.89      0.89       370\n",
      "                sara       0.68      0.55      0.61       248\n",
      "         radikalisme       0.73      0.80      0.76       243\n",
      "pencemaran_nama_baik       0.72      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.75      0.75      1365\n",
      "           macro avg       0.75      0.74      0.74      1365\n",
      "        weighted avg       0.76      0.75      0.75      1365\n",
      "         samples avg       0.44      0.43      0.42      1365\n",
      "\n",
      "Training completed in 82.64480900764465 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5132, Accuracy: 0.8338, F1 Micro: 0.3906, F1 Macro: 0.2731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3648, Accuracy: 0.8712, F1 Micro: 0.6688, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2725, Accuracy: 0.8813, F1 Micro: 0.6982, F1 Macro: 0.6932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2227, Accuracy: 0.8869, F1 Micro: 0.7481, F1 Macro: 0.7415\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.8892, F1 Micro: 0.7467, F1 Macro: 0.7382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.893, F1 Micro: 0.7484, F1 Macro: 0.7402\n",
      "Epoch 7/10, Train Loss: 0.1013, Accuracy: 0.8905, F1 Micro: 0.7344, F1 Macro: 0.7257\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.8936, F1 Micro: 0.7386, F1 Macro: 0.7305\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.8916, F1 Micro: 0.7462, F1 Macro: 0.7399\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.887, F1 Micro: 0.7441, F1 Macro: 0.74\n",
      "Model 2 - Iteration 1496: Accuracy: 0.893, F1 Micro: 0.7484, F1 Macro: 0.7402\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.89      0.88       370\n",
      "                sara       0.69      0.58      0.63       248\n",
      "         radikalisme       0.72      0.76      0.74       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.75      0.74      0.74      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 80.63098239898682 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.504, Accuracy: 0.8336, F1 Micro: 0.3911, F1 Macro: 0.2725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3551, Accuracy: 0.8661, F1 Micro: 0.6279, F1 Macro: 0.6047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2663, Accuracy: 0.8795, F1 Micro: 0.7013, F1 Macro: 0.6971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2245, Accuracy: 0.8845, F1 Micro: 0.7408, F1 Macro: 0.7381\n",
      "Epoch 5/10, Train Loss: 0.1855, Accuracy: 0.8873, F1 Micro: 0.7344, F1 Macro: 0.7229\n",
      "Epoch 6/10, Train Loss: 0.1393, Accuracy: 0.8886, F1 Micro: 0.7356, F1 Macro: 0.7293\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.8903, F1 Micro: 0.7285, F1 Macro: 0.7219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.8942, F1 Micro: 0.7456, F1 Macro: 0.7402\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.8931, F1 Micro: 0.7351, F1 Macro: 0.7256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.8959, F1 Micro: 0.7502, F1 Macro: 0.7371\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8959, F1 Micro: 0.7502, F1 Macro: 0.7371\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.89      0.89       370\n",
      "                sara       0.71      0.52      0.60       248\n",
      "         radikalisme       0.73      0.78      0.75       243\n",
      "pencemaran_nama_baik       0.73      0.70      0.71       504\n",
      "\n",
      "           micro avg       0.77      0.73      0.75      1365\n",
      "           macro avg       0.76      0.72      0.74      1365\n",
      "        weighted avg       0.77      0.73      0.75      1365\n",
      "         samples avg       0.42      0.41      0.41      1365\n",
      "\n",
      "Training completed in 82.56548357009888 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.885, F1 Micro: 0.7204, F1 Macro: 0.7129\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 89.88368821144104 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4808, Accuracy: 0.8505, F1 Micro: 0.5297, F1 Macro: 0.4505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3113, Accuracy: 0.8777, F1 Micro: 0.7141, F1 Macro: 0.7064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2508, Accuracy: 0.8892, F1 Micro: 0.7282, F1 Macro: 0.7161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2011, Accuracy: 0.8922, F1 Micro: 0.7471, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.8928, F1 Micro: 0.7507, F1 Macro: 0.7437\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.8925, F1 Micro: 0.7384, F1 Macro: 0.732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.8952, F1 Micro: 0.7512, F1 Macro: 0.7466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.8953, F1 Micro: 0.7513, F1 Macro: 0.7465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.8963, F1 Micro: 0.7561, F1 Macro: 0.7509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.895, F1 Micro: 0.7612, F1 Macro: 0.7571\n",
      "Model 1 - Iteration 1969: Accuracy: 0.895, F1 Micro: 0.7612, F1 Macro: 0.7571\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       370\n",
      "                sara       0.64      0.67      0.66       248\n",
      "         radikalisme       0.71      0.81      0.75       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.74      0.78      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 99.6593029499054 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4905, Accuracy: 0.8456, F1 Micro: 0.4838, F1 Macro: 0.3638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3178, Accuracy: 0.8794, F1 Micro: 0.7174, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2504, Accuracy: 0.8889, F1 Micro: 0.7285, F1 Macro: 0.7148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.8928, F1 Micro: 0.7438, F1 Macro: 0.7382\n",
      "Epoch 5/10, Train Loss: 0.1545, Accuracy: 0.8942, F1 Micro: 0.7435, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.8928, F1 Micro: 0.7455, F1 Macro: 0.7413\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.8917, F1 Micro: 0.7242, F1 Macro: 0.712\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.8947, F1 Micro: 0.7418, F1 Macro: 0.7334\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.8925, F1 Micro: 0.7454, F1 Macro: 0.7382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.8903, F1 Micro: 0.7457, F1 Macro: 0.7404\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8903, F1 Micro: 0.7457, F1 Macro: 0.7404\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.63      0.65      0.64       248\n",
      "         radikalisme       0.71      0.76      0.73       243\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       504\n",
      "\n",
      "           micro avg       0.74      0.75      0.75      1365\n",
      "           macro avg       0.73      0.75      0.74      1365\n",
      "        weighted avg       0.74      0.75      0.75      1365\n",
      "         samples avg       0.42      0.42      0.42      1365\n",
      "\n",
      "Training completed in 94.61288380622864 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4811, Accuracy: 0.8436, F1 Micro: 0.4885, F1 Macro: 0.4152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3168, Accuracy: 0.8775, F1 Micro: 0.7086, F1 Macro: 0.7055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2501, Accuracy: 0.8838, F1 Micro: 0.7127, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.893, F1 Micro: 0.749, F1 Macro: 0.7444\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.8908, F1 Micro: 0.7433, F1 Macro: 0.7325\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.8939, F1 Micro: 0.7373, F1 Macro: 0.7349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.892, F1 Micro: 0.7551, F1 Macro: 0.7508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.8955, F1 Micro: 0.7606, F1 Macro: 0.7597\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.8917, F1 Micro: 0.7417, F1 Macro: 0.7381\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.8936, F1 Micro: 0.7392, F1 Macro: 0.728\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8955, F1 Micro: 0.7606, F1 Macro: 0.7597\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.64      0.69      0.66       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 95.01753687858582 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8871, F1 Micro: 0.7293, F1 Macro: 0.7228\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 80.86301231384277 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4765, Accuracy: 0.8608, F1 Micro: 0.5963, F1 Macro: 0.5247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2982, Accuracy: 0.8852, F1 Micro: 0.7098, F1 Macro: 0.7062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2521, Accuracy: 0.8903, F1 Micro: 0.723, F1 Macro: 0.7132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2028, Accuracy: 0.8964, F1 Micro: 0.7468, F1 Macro: 0.7445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.8948, F1 Micro: 0.769, F1 Macro: 0.7674\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.8955, F1 Micro: 0.7579, F1 Macro: 0.755\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.8984, F1 Micro: 0.7635, F1 Macro: 0.7651\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.895, F1 Micro: 0.7526, F1 Macro: 0.7458\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.8978, F1 Micro: 0.7653, F1 Macro: 0.763\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.8933, F1 Micro: 0.7586, F1 Macro: 0.7547\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8948, F1 Micro: 0.769, F1 Macro: 0.7674\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.69      0.86      0.76       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.82      0.77      1365\n",
      "           macro avg       0.73      0.82      0.77      1365\n",
      "        weighted avg       0.73      0.82      0.77      1365\n",
      "         samples avg       0.44      0.46      0.44      1365\n",
      "\n",
      "Training completed in 104.39550495147705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4938, Accuracy: 0.852, F1 Micro: 0.5518, F1 Macro: 0.4467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3132, Accuracy: 0.8848, F1 Micro: 0.7051, F1 Macro: 0.6978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2548, Accuracy: 0.8905, F1 Micro: 0.723, F1 Macro: 0.7146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2035, Accuracy: 0.8956, F1 Micro: 0.7376, F1 Macro: 0.7356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1562, Accuracy: 0.8983, F1 Micro: 0.7542, F1 Macro: 0.7458\n",
      "Epoch 6/10, Train Loss: 0.1204, Accuracy: 0.8953, F1 Micro: 0.7535, F1 Macro: 0.752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.093, Accuracy: 0.8969, F1 Micro: 0.7628, F1 Macro: 0.7596\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.8977, F1 Micro: 0.76, F1 Macro: 0.755\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.8956, F1 Micro: 0.7531, F1 Macro: 0.7477\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.8938, F1 Micro: 0.7604, F1 Macro: 0.759\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8969, F1 Micro: 0.7628, F1 Macro: 0.7596\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       370\n",
      "                sara       0.63      0.69      0.66       248\n",
      "         radikalisme       0.72      0.81      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.76      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 106.12732148170471 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4835, Accuracy: 0.8566, F1 Micro: 0.5785, F1 Macro: 0.5043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3065, Accuracy: 0.8792, F1 Micro: 0.6799, F1 Macro: 0.6688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2574, Accuracy: 0.8861, F1 Micro: 0.7111, F1 Macro: 0.7\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2018, Accuracy: 0.8936, F1 Micro: 0.7372, F1 Macro: 0.7328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.8992, F1 Micro: 0.7608, F1 Macro: 0.7561\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.895, F1 Micro: 0.7423, F1 Macro: 0.739\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.8963, F1 Micro: 0.7408, F1 Macro: 0.7378\n",
      "Epoch 8/10, Train Loss: 0.0716, Accuracy: 0.8983, F1 Micro: 0.7458, F1 Macro: 0.7378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.8952, F1 Micro: 0.7618, F1 Macro: 0.7617\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.895, F1 Micro: 0.757, F1 Macro: 0.7519\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8952, F1 Micro: 0.7618, F1 Macro: 0.7617\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 106.34637117385864 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8888, F1 Micro: 0.7363, F1 Macro: 0.7308\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 73.30375480651855 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4496, Accuracy: 0.8656, F1 Micro: 0.6087, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.283, Accuracy: 0.8914, F1 Micro: 0.7481, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2418, Accuracy: 0.8945, F1 Micro: 0.7611, F1 Macro: 0.762\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.8983, F1 Micro: 0.7599, F1 Macro: 0.7538\n",
      "Epoch 5/10, Train Loss: 0.148, Accuracy: 0.8992, F1 Micro: 0.7392, F1 Macro: 0.7304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.8998, F1 Micro: 0.7721, F1 Macro: 0.7719\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.8941, F1 Micro: 0.7689, F1 Macro: 0.7693\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.8988, F1 Micro: 0.76, F1 Macro: 0.7584\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.8992, F1 Micro: 0.7664, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.8991, F1 Micro: 0.7704, F1 Macro: 0.769\n",
      "Model 1 - Iteration 2777: Accuracy: 0.8998, F1 Micro: 0.7721, F1 Macro: 0.7719\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.69      0.68      0.69       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.44      0.45      0.44      1365\n",
      "\n",
      "Training completed in 114.29756546020508 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4659, Accuracy: 0.8639, F1 Micro: 0.5962, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2933, Accuracy: 0.8917, F1 Micro: 0.7481, F1 Macro: 0.7458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2415, Accuracy: 0.8917, F1 Micro: 0.7529, F1 Macro: 0.7508\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.8938, F1 Micro: 0.742, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.8989, F1 Micro: 0.7601, F1 Macro: 0.7573\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.8939, F1 Micro: 0.7598, F1 Macro: 0.7573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.8944, F1 Micro: 0.7674, F1 Macro: 0.7678\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.8998, F1 Micro: 0.7646, F1 Macro: 0.7616\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9017, F1 Micro: 0.7673, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.8961, F1 Micro: 0.7674, F1 Macro: 0.7686\n",
      "Model 2 - Iteration 2777: Accuracy: 0.8961, F1 Micro: 0.7674, F1 Macro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.70      0.87      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.74      0.70       504\n",
      "\n",
      "           micro avg       0.73      0.80      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 117.28999185562134 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4527, Accuracy: 0.8586, F1 Micro: 0.5596, F1 Macro: 0.5343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2872, Accuracy: 0.89, F1 Micro: 0.7455, F1 Macro: 0.7413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2428, Accuracy: 0.8923, F1 Micro: 0.7547, F1 Macro: 0.7547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1927, Accuracy: 0.8986, F1 Micro: 0.7579, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9027, F1 Micro: 0.7588, F1 Macro: 0.7483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.121, Accuracy: 0.8967, F1 Micro: 0.7649, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.8986, F1 Micro: 0.7673, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.898, F1 Micro: 0.768, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9003, F1 Micro: 0.7646, F1 Macro: 0.7575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.8975, F1 Micro: 0.7687, F1 Macro: 0.7707\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8975, F1 Micro: 0.7687, F1 Macro: 0.7707\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.75      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.80      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 121.34755396842957 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8903, F1 Micro: 0.7418, F1 Macro: 0.7374\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.23320651054382 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4428, Accuracy: 0.8737, F1 Micro: 0.6794, F1 Macro: 0.6759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.8909, F1 Micro: 0.7278, F1 Macro: 0.7235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.8945, F1 Micro: 0.7536, F1 Macro: 0.7478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.8947, F1 Micro: 0.7638, F1 Macro: 0.7648\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9009, F1 Micro: 0.76, F1 Macro: 0.7537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.8992, F1 Micro: 0.7645, F1 Macro: 0.7548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.9016, F1 Micro: 0.7724, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9002, F1 Micro: 0.7617, F1 Macro: 0.7581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.9016, F1 Micro: 0.7758, F1 Macro: 0.7721\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9008, F1 Micro: 0.7749, F1 Macro: 0.772\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9016, F1 Micro: 0.7758, F1 Macro: 0.7721\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 127.51112484931946 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4549, Accuracy: 0.8778, F1 Micro: 0.6842, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2869, Accuracy: 0.8889, F1 Micro: 0.7159, F1 Macro: 0.7088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2224, Accuracy: 0.8992, F1 Micro: 0.7624, F1 Macro: 0.7568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1789, Accuracy: 0.8973, F1 Micro: 0.7653, F1 Macro: 0.7657\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.8991, F1 Micro: 0.7496, F1 Macro: 0.7408\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.9019, F1 Micro: 0.7639, F1 Macro: 0.7523\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.8969, F1 Micro: 0.7609, F1 Macro: 0.7537\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.898, F1 Micro: 0.7529, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9011, F1 Micro: 0.7758, F1 Macro: 0.7768\n",
      "Epoch 10/10, Train Loss: 0.0402, Accuracy: 0.9011, F1 Micro: 0.7711, F1 Macro: 0.7685\n",
      "Model 2 - Iteration 3122: Accuracy: 0.9011, F1 Micro: 0.7758, F1 Macro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 124.28009462356567 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4488, Accuracy: 0.8714, F1 Micro: 0.6753, F1 Macro: 0.6655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.8902, F1 Micro: 0.7229, F1 Macro: 0.7183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.8986, F1 Micro: 0.7557, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.8967, F1 Micro: 0.767, F1 Macro: 0.7676\n",
      "Epoch 5/10, Train Loss: 0.1527, Accuracy: 0.8991, F1 Micro: 0.7397, F1 Macro: 0.7279\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.8992, F1 Micro: 0.7649, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.8983, F1 Micro: 0.7673, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.8992, F1 Micro: 0.7558, F1 Macro: 0.7472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9013, F1 Micro: 0.7738, F1 Macro: 0.7723\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.898, F1 Micro: 0.754, F1 Macro: 0.7464\n",
      "Model 3 - Iteration 3122: Accuracy: 0.9013, F1 Micro: 0.7738, F1 Macro: 0.7723\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 126.33865547180176 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8919, F1 Micro: 0.7466, F1 Macro: 0.7426\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.62245798110962 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4326, Accuracy: 0.875, F1 Micro: 0.6808, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.8944, F1 Micro: 0.7635, F1 Macro: 0.7541\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.8964, F1 Micro: 0.7518, F1 Macro: 0.7343\n",
      "Epoch 4/10, Train Loss: 0.1985, Accuracy: 0.897, F1 Micro: 0.7246, F1 Macro: 0.718\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.8967, F1 Micro: 0.7499, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9028, F1 Micro: 0.7751, F1 Macro: 0.7728\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9028, F1 Micro: 0.7662, F1 Macro: 0.7657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9042, F1 Micro: 0.7799, F1 Macro: 0.7767\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9039, F1 Micro: 0.7696, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.9038, F1 Micro: 0.7823, F1 Macro: 0.7802\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9038, F1 Micro: 0.7823, F1 Macro: 0.7802\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.64      0.70      0.67       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 132.68618083000183 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4384, Accuracy: 0.8763, F1 Micro: 0.6692, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2843, Accuracy: 0.8948, F1 Micro: 0.7534, F1 Macro: 0.7412\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.8966, F1 Micro: 0.7506, F1 Macro: 0.7367\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.8958, F1 Micro: 0.7254, F1 Macro: 0.7171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.8992, F1 Micro: 0.765, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.9025, F1 Micro: 0.7692, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.8995, F1 Micro: 0.7468, F1 Macro: 0.7408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9014, F1 Micro: 0.7817, F1 Macro: 0.7804\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9036, F1 Micro: 0.7683, F1 Macro: 0.7652\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9034, F1 Micro: 0.7735, F1 Macro: 0.772\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9014, F1 Micro: 0.7817, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 132.99469089508057 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4354, Accuracy: 0.8747, F1 Micro: 0.6705, F1 Macro: 0.6539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2831, Accuracy: 0.897, F1 Micro: 0.7576, F1 Macro: 0.7479\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.8947, F1 Micro: 0.7443, F1 Macro: 0.7246\n",
      "Epoch 4/10, Train Loss: 0.1965, Accuracy: 0.8969, F1 Micro: 0.7291, F1 Macro: 0.7197\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.8978, F1 Micro: 0.7515, F1 Macro: 0.7459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9031, F1 Micro: 0.7739, F1 Macro: 0.7723\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9016, F1 Micro: 0.7619, F1 Macro: 0.7564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.7724\n",
      "Epoch 9/10, Train Loss: 0.0467, Accuracy: 0.9009, F1 Micro: 0.76, F1 Macro: 0.751\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9042, F1 Micro: 0.7689, F1 Macro: 0.7619\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.7724\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 131.18331503868103 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8932, F1 Micro: 0.7507, F1 Macro: 0.747\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.55023908615112 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4269, Accuracy: 0.8736, F1 Micro: 0.6616, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.8956, F1 Micro: 0.7498, F1 Macro: 0.7467\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.8995, F1 Micro: 0.7441, F1 Macro: 0.7314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9016, F1 Micro: 0.7615, F1 Macro: 0.7489\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9017, F1 Micro: 0.7463, F1 Macro: 0.7378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9009, F1 Micro: 0.7779, F1 Macro: 0.7771\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.8997, F1 Micro: 0.7458, F1 Macro: 0.7304\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9022, F1 Micro: 0.7652, F1 Macro: 0.7652\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.8991, F1 Micro: 0.7721, F1 Macro: 0.771\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9016, F1 Micro: 0.7714, F1 Macro: 0.7707\n",
      "Model 1 - Iteration 3711: Accuracy: 0.9009, F1 Micro: 0.7779, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.63      0.73      0.68       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 139.48122024536133 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4354, Accuracy: 0.8797, F1 Micro: 0.6969, F1 Macro: 0.6954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2744, Accuracy: 0.8956, F1 Micro: 0.7509, F1 Macro: 0.7479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2257, Accuracy: 0.9, F1 Micro: 0.7546, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1808, Accuracy: 0.9006, F1 Micro: 0.7596, F1 Macro: 0.7479\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9014, F1 Micro: 0.7561, F1 Macro: 0.7453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.9005, F1 Micro: 0.7767, F1 Macro: 0.7764\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9003, F1 Micro: 0.7482, F1 Macro: 0.7336\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.9025, F1 Micro: 0.7689, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9033, F1 Micro: 0.7774, F1 Macro: 0.7771\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.9022, F1 Micro: 0.7699, F1 Macro: 0.7647\n",
      "Model 2 - Iteration 3711: Accuracy: 0.9033, F1 Micro: 0.7774, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 142.03440952301025 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4278, Accuracy: 0.8717, F1 Micro: 0.6694, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2726, Accuracy: 0.8955, F1 Micro: 0.7453, F1 Macro: 0.7425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9016, F1 Micro: 0.7558, F1 Macro: 0.7456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9019, F1 Micro: 0.7598, F1 Macro: 0.7464\n",
      "Epoch 5/10, Train Loss: 0.1401, Accuracy: 0.9016, F1 Micro: 0.7486, F1 Macro: 0.7366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.8972, F1 Micro: 0.7726, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9016, F1 Micro: 0.7637, F1 Macro: 0.7525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9052, F1 Micro: 0.7763, F1 Macro: 0.7757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.9039, F1 Micro: 0.7789, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.9033, F1 Micro: 0.7803, F1 Macro: 0.7813\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9033, F1 Micro: 0.7803, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 145.1278474330902 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8943, F1 Micro: 0.7538, F1 Macro: 0.7505\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 49.52346181869507 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4207, Accuracy: 0.877, F1 Micro: 0.6794, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2736, Accuracy: 0.8958, F1 Micro: 0.7389, F1 Macro: 0.7358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.9009, F1 Micro: 0.7598, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.8959, F1 Micro: 0.7711, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9023, F1 Micro: 0.7786, F1 Macro: 0.7749\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9023, F1 Micro: 0.7689, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9034, F1 Micro: 0.7731, F1 Macro: 0.7675\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9009, F1 Micro: 0.7711, F1 Macro: 0.7663\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9055, F1 Micro: 0.7768, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.9022, F1 Micro: 0.7785, F1 Macro: 0.7762\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9023, F1 Micro: 0.7786, F1 Macro: 0.7749\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 145.34890699386597 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.429, Accuracy: 0.8684, F1 Micro: 0.6073, F1 Macro: 0.5608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.8956, F1 Micro: 0.7332, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2207, Accuracy: 0.8998, F1 Micro: 0.7553, F1 Macro: 0.7513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.8978, F1 Micro: 0.7681, F1 Macro: 0.7646\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.8973, F1 Micro: 0.7367, F1 Macro: 0.7229\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9011, F1 Micro: 0.7641, F1 Macro: 0.7564\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9013, F1 Micro: 0.7676, F1 Macro: 0.76\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.8998, F1 Micro: 0.7739, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0495, Accuracy: 0.9036, F1 Micro: 0.7816, F1 Macro: 0.7805\n",
      "Epoch 10/10, Train Loss: 0.038, Accuracy: 0.9005, F1 Micro: 0.7657, F1 Macro: 0.7631\n",
      "Model 2 - Iteration 3886: Accuracy: 0.9036, F1 Micro: 0.7816, F1 Macro: 0.7805\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.65      0.76      0.70       248\n",
      "         radikalisme       0.75      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 147.40017700195312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4249, Accuracy: 0.8739, F1 Micro: 0.6591, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2782, Accuracy: 0.892, F1 Micro: 0.7261, F1 Macro: 0.7219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.8995, F1 Micro: 0.7691, F1 Macro: 0.765\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.8956, F1 Micro: 0.7685, F1 Macro: 0.7669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9033, F1 Micro: 0.77, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.111, Accuracy: 0.8986, F1 Micro: 0.7658, F1 Macro: 0.7611\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.903, F1 Micro: 0.7692, F1 Macro: 0.7657\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9031, F1 Micro: 0.7695, F1 Macro: 0.7628\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.902, F1 Micro: 0.7595, F1 Macro: 0.7555\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.9025, F1 Micro: 0.7684, F1 Macro: 0.765\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9033, F1 Micro: 0.77, F1 Macro: 0.7648\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.69      0.60      0.64       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1365\n",
      "           macro avg       0.78      0.75      0.76      1365\n",
      "        weighted avg       0.78      0.76      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 142.99191570281982 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8951, F1 Micro: 0.7561, F1 Macro: 0.7528\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 46.83146572113037 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.412, Accuracy: 0.8794, F1 Micro: 0.6998, F1 Macro: 0.6911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.8955, F1 Micro: 0.7541, F1 Macro: 0.7482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.8989, F1 Micro: 0.7745, F1 Macro: 0.7722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.902, F1 Micro: 0.7773, F1 Macro: 0.7744\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9009, F1 Micro: 0.7678, F1 Macro: 0.7644\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.8992, F1 Micro: 0.7556, F1 Macro: 0.7442\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9036, F1 Micro: 0.7666, F1 Macro: 0.7638\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.8989, F1 Micro: 0.7731, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9009, F1 Micro: 0.7633, F1 Macro: 0.7528\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.9028, F1 Micro: 0.7669, F1 Macro: 0.7594\n",
      "Model 1 - Iteration 4120: Accuracy: 0.902, F1 Micro: 0.7773, F1 Macro: 0.7744\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.71      0.87      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.45      1365\n",
      "\n",
      "Training completed in 150.26340222358704 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.42, Accuracy: 0.8791, F1 Micro: 0.6902, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.8972, F1 Micro: 0.7545, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2083, Accuracy: 0.9011, F1 Micro: 0.7777, F1 Macro: 0.7721\n",
      "Epoch 4/10, Train Loss: 0.1768, Accuracy: 0.8952, F1 Micro: 0.7668, F1 Macro: 0.7673\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.8989, F1 Micro: 0.766, F1 Macro: 0.762\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.8959, F1 Micro: 0.7647, F1 Macro: 0.7606\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9041, F1 Micro: 0.7709, F1 Macro: 0.7696\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.8975, F1 Micro: 0.7732, F1 Macro: 0.7742\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.8978, F1 Micro: 0.7731, F1 Macro: 0.7687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9016, F1 Micro: 0.7794, F1 Macro: 0.7772\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9016, F1 Micro: 0.7794, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.63      0.72      0.67       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 150.1863968372345 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4129, Accuracy: 0.8761, F1 Micro: 0.6869, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.8973, F1 Micro: 0.756, F1 Macro: 0.7459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9008, F1 Micro: 0.7703, F1 Macro: 0.7631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.9034, F1 Micro: 0.7759, F1 Macro: 0.7729\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.8998, F1 Micro: 0.7658, F1 Macro: 0.7599\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9025, F1 Micro: 0.7716, F1 Macro: 0.7672\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9027, F1 Micro: 0.771, F1 Macro: 0.7705\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.8992, F1 Micro: 0.7672, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9017, F1 Micro: 0.7743, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9034, F1 Micro: 0.7684, F1 Macro: 0.7659\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9034, F1 Micro: 0.7759, F1 Macro: 0.7729\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.91       370\n",
      "                sara       0.68      0.65      0.66       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 150.67887711524963 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8958, F1 Micro: 0.758, F1 Macro: 0.7548\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 42.16431760787964 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4009, Accuracy: 0.8753, F1 Micro: 0.673, F1 Macro: 0.666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2629, Accuracy: 0.8944, F1 Micro: 0.7659, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9006, F1 Micro: 0.7675, F1 Macro: 0.764\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.9022, F1 Micro: 0.7661, F1 Macro: 0.7599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9016, F1 Micro: 0.7745, F1 Macro: 0.7699\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9002, F1 Micro: 0.7655, F1 Macro: 0.7611\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9006, F1 Micro: 0.7604, F1 Macro: 0.7552\n",
      "Epoch 8/10, Train Loss: 0.0555, Accuracy: 0.9019, F1 Micro: 0.7701, F1 Macro: 0.7623\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.9025, F1 Micro: 0.7689, F1 Macro: 0.7632\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.9025, F1 Micro: 0.7706, F1 Macro: 0.7676\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9016, F1 Micro: 0.7745, F1 Macro: 0.7699\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.63      0.68      0.65       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 155.67542815208435 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8769, F1 Micro: 0.6868, F1 Macro: 0.6817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2636, Accuracy: 0.8961, F1 Micro: 0.7656, F1 Macro: 0.7664\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9006, F1 Micro: 0.7651, F1 Macro: 0.7597\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9011, F1 Micro: 0.7649, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9056, F1 Micro: 0.7804, F1 Macro: 0.7758\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9041, F1 Micro: 0.7678, F1 Macro: 0.7608\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9045, F1 Micro: 0.7726, F1 Macro: 0.7697\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.9002, F1 Micro: 0.7665, F1 Macro: 0.7601\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9044, F1 Micro: 0.7682, F1 Macro: 0.7641\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9039, F1 Micro: 0.7777, F1 Macro: 0.7769\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9056, F1 Micro: 0.7804, F1 Macro: 0.7758\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.69      0.66      0.67       248\n",
      "         radikalisme       0.78      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.44      0.44      0.44      1365\n",
      "\n",
      "Training completed in 153.83481216430664 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.8747, F1 Micro: 0.6697, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.8958, F1 Micro: 0.7659, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2116, Accuracy: 0.9014, F1 Micro: 0.7683, F1 Macro: 0.7641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1743, Accuracy: 0.9016, F1 Micro: 0.774, F1 Macro: 0.7709\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9047, F1 Micro: 0.7705, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.1063, Accuracy: 0.9034, F1 Micro: 0.7657, F1 Macro: 0.7596\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9036, F1 Micro: 0.7731, F1 Macro: 0.7746\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9041, F1 Micro: 0.7635, F1 Macro: 0.7567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9053, F1 Micro: 0.7754, F1 Macro: 0.7723\n",
      "Epoch 10/10, Train Loss: 0.0354, Accuracy: 0.9036, F1 Micro: 0.7741, F1 Macro: 0.7673\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9053, F1 Micro: 0.7754, F1 Macro: 0.7723\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.79      0.77      0.78      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 156.70777249336243 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8965, F1 Micro: 0.7596, F1 Macro: 0.7563\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 38.93305540084839 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3994, Accuracy: 0.8764, F1 Micro: 0.6683, F1 Macro: 0.6498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2573, Accuracy: 0.895, F1 Micro: 0.7683, F1 Macro: 0.7612\n",
      "Epoch 3/10, Train Loss: 0.2168, Accuracy: 0.9013, F1 Micro: 0.7652, F1 Macro: 0.7542\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9011, F1 Micro: 0.7523, F1 Macro: 0.7391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.8936, F1 Micro: 0.7746, F1 Macro: 0.7753\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.8967, F1 Micro: 0.7417, F1 Macro: 0.7387\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9044, F1 Micro: 0.7613, F1 Macro: 0.7515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9009, F1 Micro: 0.7789, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.8995, F1 Micro: 0.7708, F1 Macro: 0.7701\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.905, F1 Micro: 0.7707, F1 Macro: 0.7676\n",
      "Model 1 - Iteration 4530: Accuracy: 0.9009, F1 Micro: 0.7789, F1 Macro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.63      0.69      0.66       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 162.94145822525024 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4085, Accuracy: 0.8706, F1 Micro: 0.6294, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.8975, F1 Micro: 0.7701, F1 Macro: 0.7621\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.9016, F1 Micro: 0.7647, F1 Macro: 0.7528\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9028, F1 Micro: 0.7559, F1 Macro: 0.7452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.897, F1 Micro: 0.7727, F1 Macro: 0.7719\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.8983, F1 Micro: 0.7623, F1 Macro: 0.7593\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9044, F1 Micro: 0.7677, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9022, F1 Micro: 0.7763, F1 Macro: 0.7718\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.9031, F1 Micro: 0.7692, F1 Macro: 0.7656\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9034, F1 Micro: 0.7691, F1 Macro: 0.7644\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9022, F1 Micro: 0.7763, F1 Macro: 0.7718\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.66      0.66       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 162.80153441429138 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4018, Accuracy: 0.8761, F1 Micro: 0.6683, F1 Macro: 0.6522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.8923, F1 Micro: 0.7625, F1 Macro: 0.7555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.9039, F1 Micro: 0.7743, F1 Macro: 0.7633\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.9013, F1 Micro: 0.7554, F1 Macro: 0.742\n",
      "Epoch 5/10, Train Loss: 0.1401, Accuracy: 0.8966, F1 Micro: 0.7731, F1 Macro: 0.7726\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9017, F1 Micro: 0.7634, F1 Macro: 0.7611\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9056, F1 Micro: 0.7691, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9009, F1 Micro: 0.7782, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.9017, F1 Micro: 0.7717, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.905, F1 Micro: 0.7775, F1 Macro: 0.7756\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9009, F1 Micro: 0.7782, F1 Macro: 0.7778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.63      0.70      0.66       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.47639679908752 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8969, F1 Micro: 0.761, F1 Macro: 0.7577\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.77294111251831 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4003, Accuracy: 0.8828, F1 Micro: 0.7066, F1 Macro: 0.6778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.8991, F1 Micro: 0.7688, F1 Macro: 0.7597\n",
      "Epoch 3/10, Train Loss: 0.2149, Accuracy: 0.9006, F1 Micro: 0.7514, F1 Macro: 0.7349\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9016, F1 Micro: 0.7623, F1 Macro: 0.7526\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9019, F1 Micro: 0.7669, F1 Macro: 0.7606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.8991, F1 Micro: 0.7765, F1 Macro: 0.7756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9017, F1 Micro: 0.7789, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9056, F1 Micro: 0.7823, F1 Macro: 0.78\n",
      "Epoch 9/10, Train Loss: 0.043, Accuracy: 0.9047, F1 Micro: 0.7722, F1 Macro: 0.766\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9064, F1 Micro: 0.7732, F1 Macro: 0.7676\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9056, F1 Micro: 0.7823, F1 Macro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 165.5171821117401 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4146, Accuracy: 0.8842, F1 Micro: 0.7145, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.9023, F1 Micro: 0.7674, F1 Macro: 0.7532\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.902, F1 Micro: 0.7563, F1 Macro: 0.7444\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.9023, F1 Micro: 0.7505, F1 Macro: 0.7405\n",
      "Epoch 5/10, Train Loss: 0.1295, Accuracy: 0.9013, F1 Micro: 0.7647, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9006, F1 Micro: 0.7826, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9025, F1 Micro: 0.7746, F1 Macro: 0.7696\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9003, F1 Micro: 0.772, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0441, Accuracy: 0.9038, F1 Micro: 0.7729, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0379, Accuracy: 0.9016, F1 Micro: 0.768, F1 Macro: 0.7651\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9006, F1 Micro: 0.7826, F1 Macro: 0.7819\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.84      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 162.05330657958984 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8792, F1 Micro: 0.7039, F1 Macro: 0.6842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.9, F1 Micro: 0.7654, F1 Macro: 0.7546\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.8966, F1 Micro: 0.7331, F1 Macro: 0.7159\n",
      "Epoch 4/10, Train Loss: 0.1764, Accuracy: 0.903, F1 Micro: 0.7564, F1 Macro: 0.7422\n",
      "Epoch 5/10, Train Loss: 0.1392, Accuracy: 0.9006, F1 Micro: 0.7574, F1 Macro: 0.7512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9003, F1 Micro: 0.7806, F1 Macro: 0.7811\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.8981, F1 Micro: 0.7746, F1 Macro: 0.7735\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9025, F1 Micro: 0.7682, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.9036, F1 Micro: 0.7699, F1 Macro: 0.7615\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9031, F1 Micro: 0.766, F1 Macro: 0.7601\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9003, F1 Micro: 0.7806, F1 Macro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.75      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 162.10972547531128 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8973, F1 Micro: 0.7625, F1 Macro: 0.7594\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.362563610076904 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3966, Accuracy: 0.8866, F1 Micro: 0.7346, F1 Macro: 0.7304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2638, Accuracy: 0.8989, F1 Micro: 0.747, F1 Macro: 0.7309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.8973, F1 Micro: 0.7553, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9047, F1 Micro: 0.7724, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9053, F1 Micro: 0.784, F1 Macro: 0.7808\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.897, F1 Micro: 0.7573, F1 Macro: 0.7495\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9044, F1 Micro: 0.7753, F1 Macro: 0.7727\n",
      "Epoch 8/10, Train Loss: 0.051, Accuracy: 0.9008, F1 Micro: 0.771, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.8988, F1 Micro: 0.7699, F1 Macro: 0.7642\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9014, F1 Micro: 0.7706, F1 Macro: 0.7701\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9053, F1 Micro: 0.784, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.76      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 171.07694959640503 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4027, Accuracy: 0.8844, F1 Micro: 0.7249, F1 Macro: 0.7162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2634, Accuracy: 0.9002, F1 Micro: 0.7528, F1 Macro: 0.7357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.9, F1 Micro: 0.7598, F1 Macro: 0.7511\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9014, F1 Micro: 0.7581, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9053, F1 Micro: 0.7854, F1 Macro: 0.7812\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9023, F1 Micro: 0.7621, F1 Macro: 0.752\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.9027, F1 Micro: 0.7819, F1 Macro: 0.7813\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9033, F1 Micro: 0.7651, F1 Macro: 0.7606\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9008, F1 Micro: 0.7731, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.9041, F1 Micro: 0.778, F1 Macro: 0.7759\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9053, F1 Micro: 0.7854, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.77      0.79      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 169.03343534469604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3982, Accuracy: 0.8811, F1 Micro: 0.7197, F1 Macro: 0.7147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2652, Accuracy: 0.8991, F1 Micro: 0.7422, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2109, Accuracy: 0.9005, F1 Micro: 0.7547, F1 Macro: 0.7508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9023, F1 Micro: 0.7731, F1 Macro: 0.7623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.8994, F1 Micro: 0.7785, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.8995, F1 Micro: 0.7491, F1 Macro: 0.7386\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.8994, F1 Micro: 0.7718, F1 Macro: 0.7666\n",
      "Epoch 8/10, Train Loss: 0.0531, Accuracy: 0.9041, F1 Micro: 0.7761, F1 Macro: 0.7734\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9027, F1 Micro: 0.7639, F1 Macro: 0.7587\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.8998, F1 Micro: 0.7744, F1 Macro: 0.7721\n",
      "Model 3 - Iteration 4863: Accuracy: 0.8994, F1 Micro: 0.7785, F1 Macro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.90       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 170.46927881240845 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8977, F1 Micro: 0.7638, F1 Macro: 0.7607\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.394862413406372 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3955, Accuracy: 0.8798, F1 Micro: 0.68, F1 Macro: 0.662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.8984, F1 Micro: 0.7735, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2053, Accuracy: 0.9059, F1 Micro: 0.7825, F1 Macro: 0.774\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9044, F1 Micro: 0.7727, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.8992, F1 Micro: 0.7603, F1 Macro: 0.7496\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9048, F1 Micro: 0.7659, F1 Macro: 0.754\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9016, F1 Micro: 0.764, F1 Macro: 0.7571\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.9002, F1 Micro: 0.767, F1 Macro: 0.7625\n",
      "Epoch 9/10, Train Loss: 0.0377, Accuracy: 0.9041, F1 Micro: 0.7648, F1 Macro: 0.7601\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9009, F1 Micro: 0.7582, F1 Macro: 0.7486\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9059, F1 Micro: 0.7825, F1 Macro: 0.774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.68      0.61      0.64       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 174.11906790733337 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4043, Accuracy: 0.8839, F1 Micro: 0.7043, F1 Macro: 0.6891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2591, Accuracy: 0.8983, F1 Micro: 0.7725, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9053, F1 Micro: 0.7798, F1 Macro: 0.77\n",
      "Epoch 4/10, Train Loss: 0.1692, Accuracy: 0.9011, F1 Micro: 0.768, F1 Macro: 0.7609\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9039, F1 Micro: 0.7758, F1 Macro: 0.7679\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9002, F1 Micro: 0.758, F1 Macro: 0.7459\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9036, F1 Micro: 0.7746, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.8998, F1 Micro: 0.7685, F1 Macro: 0.7642\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.9025, F1 Micro: 0.7672, F1 Macro: 0.7606\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9041, F1 Micro: 0.7714, F1 Macro: 0.765\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9053, F1 Micro: 0.7798, F1 Macro: 0.77\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.68      0.59      0.63       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.78      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 175.30905175209045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3987, Accuracy: 0.8792, F1 Micro: 0.6812, F1 Macro: 0.6584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.9, F1 Micro: 0.775, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2042, Accuracy: 0.9056, F1 Micro: 0.7755, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.9052, F1 Micro: 0.7756, F1 Macro: 0.7737\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9027, F1 Micro: 0.7666, F1 Macro: 0.7597\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9033, F1 Micro: 0.764, F1 Macro: 0.7566\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9003, F1 Micro: 0.758, F1 Macro: 0.752\n",
      "Epoch 8/10, Train Loss: 0.0546, Accuracy: 0.9031, F1 Micro: 0.7717, F1 Macro: 0.767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9047, F1 Micro: 0.7843, F1 Macro: 0.7824\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.8998, F1 Micro: 0.7752, F1 Macro: 0.7715\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9047, F1 Micro: 0.7843, F1 Macro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.64      0.71      0.68       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 177.3450345993042 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8981, F1 Micro: 0.765, F1 Macro: 0.7617\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.15090298652649 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3834, Accuracy: 0.8872, F1 Micro: 0.7135, F1 Macro: 0.6889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.9005, F1 Micro: 0.7601, F1 Macro: 0.751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.9039, F1 Micro: 0.772, F1 Macro: 0.7681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9025, F1 Micro: 0.782, F1 Macro: 0.7791\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.902, F1 Micro: 0.7481, F1 Macro: 0.7352\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9013, F1 Micro: 0.7647, F1 Macro: 0.7591\n",
      "Epoch 7/10, Train Loss: 0.0693, Accuracy: 0.9027, F1 Micro: 0.7692, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.9039, F1 Micro: 0.7818, F1 Macro: 0.7789\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9041, F1 Micro: 0.7662, F1 Macro: 0.7565\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9023, F1 Micro: 0.7728, F1 Macro: 0.7641\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9025, F1 Micro: 0.782, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.64      0.70      0.67       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.3001675605774 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3954, Accuracy: 0.8844, F1 Micro: 0.7052, F1 Macro: 0.6769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2551, Accuracy: 0.9025, F1 Micro: 0.7587, F1 Macro: 0.7509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9041, F1 Micro: 0.77, F1 Macro: 0.7607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1668, Accuracy: 0.8986, F1 Micro: 0.7781, F1 Macro: 0.7771\n",
      "Epoch 5/10, Train Loss: 0.1276, Accuracy: 0.9027, F1 Micro: 0.7675, F1 Macro: 0.7563\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9031, F1 Micro: 0.7705, F1 Macro: 0.7666\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.9028, F1 Micro: 0.7688, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.7635\n",
      "Epoch 9/10, Train Loss: 0.044, Accuracy: 0.9013, F1 Micro: 0.7752, F1 Macro: 0.7726\n",
      "Epoch 10/10, Train Loss: 0.0329, Accuracy: 0.9017, F1 Micro: 0.7693, F1 Macro: 0.7608\n",
      "Model 2 - Iteration 5263: Accuracy: 0.8986, F1 Micro: 0.7781, F1 Macro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.62      0.73      0.67       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 181.23521947860718 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.387, Accuracy: 0.8841, F1 Micro: 0.7018, F1 Macro: 0.6801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.8995, F1 Micro: 0.7587, F1 Macro: 0.7543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2083, Accuracy: 0.9036, F1 Micro: 0.7707, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9056, F1 Micro: 0.7854, F1 Macro: 0.7815\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.8986, F1 Micro: 0.7326, F1 Macro: 0.7111\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.903, F1 Micro: 0.7792, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9064, F1 Micro: 0.7841, F1 Macro: 0.7817\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9033, F1 Micro: 0.7836, F1 Macro: 0.7807\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9058, F1 Micro: 0.7771, F1 Macro: 0.7756\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9008, F1 Micro: 0.7733, F1 Macro: 0.7656\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9056, F1 Micro: 0.7854, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.65      0.67      0.66       248\n",
      "         radikalisme       0.75      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 181.06592535972595 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8984, F1 Micro: 0.766, F1 Macro: 0.7627\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 20.07908868789673 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3894, Accuracy: 0.8908, F1 Micro: 0.7408, F1 Macro: 0.7265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2531, Accuracy: 0.8997, F1 Micro: 0.7577, F1 Macro: 0.7424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2055, Accuracy: 0.9044, F1 Micro: 0.7621, F1 Macro: 0.7518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1664, Accuracy: 0.9031, F1 Micro: 0.7644, F1 Macro: 0.7612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1328, Accuracy: 0.9055, F1 Micro: 0.7809, F1 Macro: 0.7727\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9023, F1 Micro: 0.7754, F1 Macro: 0.7677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9077, F1 Micro: 0.7826, F1 Macro: 0.7765\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9044, F1 Micro: 0.7619, F1 Macro: 0.7581\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9034, F1 Micro: 0.7758, F1 Macro: 0.773\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9023, F1 Micro: 0.782, F1 Macro: 0.7768\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9077, F1 Micro: 0.7826, F1 Macro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.70      0.60      0.65       248\n",
      "         radikalisme       0.77      0.84      0.81       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1365\n",
      "           macro avg       0.78      0.77      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 190.53466057777405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3996, Accuracy: 0.8891, F1 Micro: 0.7401, F1 Macro: 0.7276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2524, Accuracy: 0.9025, F1 Micro: 0.7672, F1 Macro: 0.7496\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9042, F1 Micro: 0.7588, F1 Macro: 0.752\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.9003, F1 Micro: 0.7605, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9017, F1 Micro: 0.7705, F1 Macro: 0.7658\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9023, F1 Micro: 0.7646, F1 Macro: 0.7542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9036, F1 Micro: 0.7759, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0548, Accuracy: 0.9058, F1 Micro: 0.7797, F1 Macro: 0.7773\n",
      "Epoch 9/10, Train Loss: 0.0411, Accuracy: 0.9013, F1 Micro: 0.7789, F1 Macro: 0.7773\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9027, F1 Micro: 0.7705, F1 Macro: 0.7717\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9058, F1 Micro: 0.7797, F1 Macro: 0.7773\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.77      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.44      0.44      1365\n",
      "\n",
      "Training completed in 189.02344155311584 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3936, Accuracy: 0.8867, F1 Micro: 0.7434, F1 Macro: 0.7397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9019, F1 Micro: 0.7628, F1 Macro: 0.7525\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.9041, F1 Micro: 0.7607, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9041, F1 Micro: 0.7671, F1 Macro: 0.7659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.9059, F1 Micro: 0.7817, F1 Macro: 0.7755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9005, F1 Micro: 0.7833, F1 Macro: 0.7829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.9048, F1 Micro: 0.7846, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0547, Accuracy: 0.9008, F1 Micro: 0.7807, F1 Macro: 0.7792\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9019, F1 Micro: 0.7703, F1 Macro: 0.7687\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.903, F1 Micro: 0.7805, F1 Macro: 0.7807\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9048, F1 Micro: 0.7846, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.78      0.81      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 191.32255244255066 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.8988, F1 Micro: 0.7669, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.29149055480957 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3824, Accuracy: 0.8897, F1 Micro: 0.7528, F1 Macro: 0.7512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2444, Accuracy: 0.9011, F1 Micro: 0.7704, F1 Macro: 0.7639\n",
      "Epoch 3/10, Train Loss: 0.2071, Accuracy: 0.9038, F1 Micro: 0.7622, F1 Macro: 0.7582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9045, F1 Micro: 0.7885, F1 Macro: 0.788\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9059, F1 Micro: 0.7792, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9023, F1 Micro: 0.7756, F1 Macro: 0.7722\n",
      "Epoch 7/10, Train Loss: 0.0683, Accuracy: 0.9061, F1 Micro: 0.7703, F1 Macro: 0.762\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.9041, F1 Micro: 0.7693, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9052, F1 Micro: 0.7795, F1 Macro: 0.7761\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9056, F1 Micro: 0.7861, F1 Macro: 0.7856\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9045, F1 Micro: 0.7885, F1 Macro: 0.788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.86      0.90       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.73      0.89      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.83      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 190.92752361297607 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3904, Accuracy: 0.887, F1 Micro: 0.7509, F1 Macro: 0.7511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2444, Accuracy: 0.902, F1 Micro: 0.7694, F1 Macro: 0.7608\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.9023, F1 Micro: 0.7674, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1615, Accuracy: 0.903, F1 Micro: 0.7817, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1172, Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.7664\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9045, F1 Micro: 0.7622, F1 Macro: 0.7535\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9053, F1 Micro: 0.7764, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9039, F1 Micro: 0.7841, F1 Macro: 0.7835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7814\n",
      "Model 2 - Iteration 5641: Accuracy: 0.9064, F1 Micro: 0.7852, F1 Macro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.66      0.66      0.66       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 193.42297625541687 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8881, F1 Micro: 0.7441, F1 Macro: 0.7383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2463, Accuracy: 0.9011, F1 Micro: 0.773, F1 Macro: 0.7674\n",
      "Epoch 3/10, Train Loss: 0.2086, Accuracy: 0.9016, F1 Micro: 0.756, F1 Macro: 0.7526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.905, F1 Micro: 0.7876, F1 Macro: 0.7859\n",
      "Epoch 5/10, Train Loss: 0.1248, Accuracy: 0.9023, F1 Micro: 0.7751, F1 Macro: 0.7651\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9011, F1 Micro: 0.7646, F1 Macro: 0.7605\n",
      "Epoch 7/10, Train Loss: 0.0685, Accuracy: 0.9052, F1 Micro: 0.7679, F1 Macro: 0.7618\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9047, F1 Micro: 0.7777, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9036, F1 Micro: 0.7794, F1 Macro: 0.7766\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.905, F1 Micro: 0.7815, F1 Macro: 0.7744\n",
      "Model 3 - Iteration 5641: Accuracy: 0.905, F1 Micro: 0.7876, F1 Macro: 0.7859\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.71      0.68       248\n",
      "         radikalisme       0.74      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 189.88997602462769 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.8991, F1 Micro: 0.7679, F1 Macro: 0.7647\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.495314836502075 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3712, Accuracy: 0.8895, F1 Micro: 0.7263, F1 Macro: 0.7134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.9003, F1 Micro: 0.7578, F1 Macro: 0.753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9013, F1 Micro: 0.783, F1 Macro: 0.7785\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9048, F1 Micro: 0.7737, F1 Macro: 0.7646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9013, F1 Micro: 0.783, F1 Macro: 0.7812\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.8967, F1 Micro: 0.7754, F1 Macro: 0.7745\n",
      "Epoch 7/10, Train Loss: 0.0715, Accuracy: 0.9014, F1 Micro: 0.775, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.8972, F1 Micro: 0.7711, F1 Macro: 0.7672\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.905, F1 Micro: 0.773, F1 Macro: 0.7681\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9031, F1 Micro: 0.776, F1 Macro: 0.7689\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9013, F1 Micro: 0.783, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.63      0.70      0.66       248\n",
      "         radikalisme       0.74      0.88      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 196.6933856010437 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3797, Accuracy: 0.8906, F1 Micro: 0.7328, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2411, Accuracy: 0.9022, F1 Micro: 0.7652, F1 Macro: 0.7609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1981, Accuracy: 0.9044, F1 Micro: 0.7816, F1 Macro: 0.7763\n",
      "Epoch 4/10, Train Loss: 0.1616, Accuracy: 0.9034, F1 Micro: 0.7761, F1 Macro: 0.7706\n",
      "Epoch 5/10, Train Loss: 0.1261, Accuracy: 0.8984, F1 Micro: 0.7808, F1 Macro: 0.7824\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.8989, F1 Micro: 0.776, F1 Macro: 0.7711\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9036, F1 Micro: 0.7811, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.9038, F1 Micro: 0.7732, F1 Macro: 0.7681\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9059, F1 Micro: 0.7744, F1 Macro: 0.7667\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9031, F1 Micro: 0.7765, F1 Macro: 0.7711\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9044, F1 Micro: 0.7816, F1 Macro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.64      0.66      0.65       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 195.20040488243103 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3741, Accuracy: 0.8883, F1 Micro: 0.7228, F1 Macro: 0.7073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2408, Accuracy: 0.903, F1 Micro: 0.7694, F1 Macro: 0.7654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.905, F1 Micro: 0.7856, F1 Macro: 0.7841\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9059, F1 Micro: 0.7795, F1 Macro: 0.7691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1275, Accuracy: 0.9061, F1 Micro: 0.7859, F1 Macro: 0.7835\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.903, F1 Micro: 0.7832, F1 Macro: 0.7803\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.9039, F1 Micro: 0.7776, F1 Macro: 0.7728\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.9052, F1 Micro: 0.7722, F1 Macro: 0.7682\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.905, F1 Micro: 0.7808, F1 Macro: 0.7782\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.903, F1 Micro: 0.7746, F1 Macro: 0.7715\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9061, F1 Micro: 0.7859, F1 Macro: 0.7835\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 197.03324604034424 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.8994, F1 Micro: 0.7687, F1 Macro: 0.7655\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.468140363693237 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3665, Accuracy: 0.8905, F1 Micro: 0.7545, F1 Macro: 0.7525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.242, Accuracy: 0.9011, F1 Micro: 0.7648, F1 Macro: 0.7503\n",
      "Epoch 3/10, Train Loss: 0.1997, Accuracy: 0.9034, F1 Micro: 0.7463, F1 Macro: 0.7365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.159, Accuracy: 0.9056, F1 Micro: 0.7834, F1 Macro: 0.7807\n",
      "Epoch 5/10, Train Loss: 0.112, Accuracy: 0.9011, F1 Micro: 0.777, F1 Macro: 0.7756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.9048, F1 Micro: 0.7899, F1 Macro: 0.7903\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.9044, F1 Micro: 0.7834, F1 Macro: 0.7801\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9059, F1 Micro: 0.7754, F1 Macro: 0.7681\n",
      "Epoch 9/10, Train Loss: 0.037, Accuracy: 0.9025, F1 Micro: 0.7763, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.902, F1 Micro: 0.7757, F1 Macro: 0.7684\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9048, F1 Micro: 0.7899, F1 Macro: 0.7903\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.64      0.79      0.71       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.75      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 201.48811197280884 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3771, Accuracy: 0.888, F1 Micro: 0.7392, F1 Macro: 0.7326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2408, Accuracy: 0.9025, F1 Micro: 0.7675, F1 Macro: 0.7551\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.9047, F1 Micro: 0.7532, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.9027, F1 Micro: 0.7773, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.11, Accuracy: 0.9055, F1 Micro: 0.7801, F1 Macro: 0.7742\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.905, F1 Micro: 0.7753, F1 Macro: 0.77\n",
      "Epoch 7/10, Train Loss: 0.066, Accuracy: 0.9061, F1 Micro: 0.7785, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9019, F1 Micro: 0.7806, F1 Macro: 0.778\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.8988, F1 Micro: 0.7747, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.9028, F1 Micro: 0.7779, F1 Macro: 0.771\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9019, F1 Micro: 0.7806, F1 Macro: 0.778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 204.749986410141 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3692, Accuracy: 0.888, F1 Micro: 0.7464, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2427, Accuracy: 0.9031, F1 Micro: 0.7707, F1 Macro: 0.7608\n",
      "Epoch 3/10, Train Loss: 0.1981, Accuracy: 0.9028, F1 Micro: 0.7482, F1 Macro: 0.7381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9028, F1 Micro: 0.7775, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9045, F1 Micro: 0.7806, F1 Macro: 0.7751\n",
      "Epoch 6/10, Train Loss: 0.0844, Accuracy: 0.9013, F1 Micro: 0.7803, F1 Macro: 0.7811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9083, F1 Micro: 0.7854, F1 Macro: 0.7817\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.9066, F1 Micro: 0.7814, F1 Macro: 0.7772\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9009, F1 Micro: 0.7852, F1 Macro: 0.7863\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9028, F1 Micro: 0.7703, F1 Macro: 0.7634\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9083, F1 Micro: 0.7854, F1 Macro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.68      0.69      0.68       248\n",
      "         radikalisme       0.78      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.74      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1365\n",
      "           macro avg       0.78      0.78      0.78      1365\n",
      "        weighted avg       0.79      0.79      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 203.86966800689697 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.8997, F1 Micro: 0.7695, F1 Macro: 0.7663\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.353123188018799 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3573, Accuracy: 0.8878, F1 Micro: 0.7184, F1 Macro: 0.7031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.8961, F1 Micro: 0.7712, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9045, F1 Micro: 0.7789, F1 Macro: 0.7704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.9089, F1 Micro: 0.7893, F1 Macro: 0.7856\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9073, F1 Micro: 0.7765, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.0831, Accuracy: 0.9009, F1 Micro: 0.7766, F1 Macro: 0.7736\n",
      "Epoch 7/10, Train Loss: 0.0627, Accuracy: 0.9031, F1 Micro: 0.7792, F1 Macro: 0.7737\n",
      "Epoch 8/10, Train Loss: 0.0476, Accuracy: 0.9041, F1 Micro: 0.7813, F1 Macro: 0.7794\n",
      "Epoch 9/10, Train Loss: 0.0403, Accuracy: 0.9034, F1 Micro: 0.7813, F1 Macro: 0.78\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9038, F1 Micro: 0.77, F1 Macro: 0.7622\n",
      "Model 1 - Iteration 6218: Accuracy: 0.9089, F1 Micro: 0.7893, F1 Macro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.79      0.83      0.81       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.76       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.79      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 208.03636598587036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3667, Accuracy: 0.8936, F1 Micro: 0.7429, F1 Macro: 0.7353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.8992, F1 Micro: 0.7796, F1 Macro: 0.78\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9034, F1 Micro: 0.7696, F1 Macro: 0.7582\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9045, F1 Micro: 0.7759, F1 Macro: 0.769\n",
      "Epoch 5/10, Train Loss: 0.1175, Accuracy: 0.9039, F1 Micro: 0.7678, F1 Macro: 0.7556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.9036, F1 Micro: 0.7813, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0613, Accuracy: 0.9027, F1 Micro: 0.7832, F1 Macro: 0.7825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0472, Accuracy: 0.9069, F1 Micro: 0.7839, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0362, Accuracy: 0.9017, F1 Micro: 0.7727, F1 Macro: 0.7685\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9053, F1 Micro: 0.7831, F1 Macro: 0.7806\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9069, F1 Micro: 0.7839, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.67      0.70      0.69       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.74      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 210.08044052124023 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3604, Accuracy: 0.8906, F1 Micro: 0.7338, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2293, Accuracy: 0.8969, F1 Micro: 0.7718, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1869, Accuracy: 0.905, F1 Micro: 0.7778, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1499, Accuracy: 0.9064, F1 Micro: 0.7821, F1 Macro: 0.7788\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9019, F1 Micro: 0.7696, F1 Macro: 0.7608\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.903, F1 Micro: 0.7799, F1 Macro: 0.7755\n",
      "Epoch 7/10, Train Loss: 0.0584, Accuracy: 0.9072, F1 Micro: 0.7757, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9042, F1 Micro: 0.7829, F1 Macro: 0.7796\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.9022, F1 Micro: 0.7751, F1 Macro: 0.771\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.898, F1 Micro: 0.7763, F1 Macro: 0.7751\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9042, F1 Micro: 0.7829, F1 Macro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.69      0.67       248\n",
      "         radikalisme       0.73      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 209.11012625694275 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.9, F1 Micro: 0.7702, F1 Macro: 0.767\n",
      "Total sampling time: 992.16 seconds\n",
      "Total runtime: 11145.135002613068 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c124ed",
   "metadata": {
    "papermill": {
     "duration": 0.12839,
     "end_time": "2025-03-13T10:00:45.333552",
     "exception": false,
     "start_time": "2025-03-13T10:00:45.205162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922becc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6257, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4449, Accuracy: 0.7873, F1 Micro: 0.0058, F1 Macro: 0.0053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3901, Accuracy: 0.8056, F1 Micro: 0.1629, F1 Macro: 0.1232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3715, Accuracy: 0.8298, F1 Micro: 0.3702, F1 Macro: 0.2792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3107, Accuracy: 0.8478, F1 Micro: 0.5173, F1 Macro: 0.4639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.269, Accuracy: 0.858, F1 Micro: 0.5821, F1 Macro: 0.5628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2396, Accuracy: 0.8644, F1 Micro: 0.6341, F1 Macro: 0.6306\n",
      "Epoch 8/10, Train Loss: 0.1893, Accuracy: 0.8645, F1 Micro: 0.6199, F1 Macro: 0.6103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1519, Accuracy: 0.8764, F1 Micro: 0.6801, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1377, Accuracy: 0.8766, F1 Micro: 0.7037, F1 Macro: 0.7025\n",
      "Model 1 - Iteration 388: Accuracy: 0.8766, F1 Micro: 0.7037, F1 Macro: 0.7025\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       370\n",
      "                sara       0.60      0.59      0.60       248\n",
      "         radikalisme       0.67      0.79      0.72       243\n",
      "pencemaran_nama_baik       0.68      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.72      0.69      0.70      1365\n",
      "           macro avg       0.71      0.70      0.70      1365\n",
      "        weighted avg       0.72      0.69      0.70      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 59.47039604187012 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.641, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4523, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.7945, F1 Micro: 0.0707, F1 Macro: 0.0595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3985, Accuracy: 0.8206, F1 Micro: 0.2743, F1 Macro: 0.1959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3282, Accuracy: 0.8359, F1 Micro: 0.4312, F1 Macro: 0.3544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.291, Accuracy: 0.8492, F1 Micro: 0.5304, F1 Macro: 0.4884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2575, Accuracy: 0.8558, F1 Micro: 0.5733, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2031, Accuracy: 0.8644, F1 Micro: 0.6331, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1705, Accuracy: 0.8687, F1 Micro: 0.6754, F1 Macro: 0.6686\n",
      "Epoch 10/10, Train Loss: 0.1513, Accuracy: 0.8719, F1 Micro: 0.6743, F1 Macro: 0.6681\n",
      "Model 2 - Iteration 388: Accuracy: 0.8687, F1 Micro: 0.6754, F1 Macro: 0.6686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.80      0.85       370\n",
      "                sara       0.58      0.50      0.54       248\n",
      "         radikalisme       0.66      0.67      0.67       243\n",
      "pencemaran_nama_baik       0.67      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.71      0.64      0.68      1365\n",
      "           macro avg       0.70      0.64      0.67      1365\n",
      "        weighted avg       0.71      0.64      0.67      1365\n",
      "         samples avg       0.36      0.36      0.35      1365\n",
      "\n",
      "Training completed in 56.56459856033325 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5978, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4434, Accuracy: 0.7872, F1 Micro: 0.0044, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3804, Accuracy: 0.8202, F1 Micro: 0.2738, F1 Macro: 0.1854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3666, Accuracy: 0.8278, F1 Micro: 0.3464, F1 Macro: 0.2469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3085, Accuracy: 0.8375, F1 Micro: 0.4372, F1 Macro: 0.3739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2698, Accuracy: 0.8508, F1 Micro: 0.538, F1 Macro: 0.5159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2493, Accuracy: 0.853, F1 Micro: 0.5597, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1995, Accuracy: 0.8572, F1 Micro: 0.5981, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1665, Accuracy: 0.8678, F1 Micro: 0.6513, F1 Macro: 0.6482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1477, Accuracy: 0.873, F1 Micro: 0.6661, F1 Macro: 0.6587\n",
      "Model 3 - Iteration 388: Accuracy: 0.873, F1 Micro: 0.6661, F1 Macro: 0.6587\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.83      0.87       370\n",
      "                sara       0.67      0.46      0.54       248\n",
      "         radikalisme       0.68      0.67      0.67       243\n",
      "pencemaran_nama_baik       0.71      0.45      0.55       504\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1365\n",
      "           macro avg       0.74      0.60      0.66      1365\n",
      "        weighted avg       0.75      0.59      0.66      1365\n",
      "         samples avg       0.35      0.33      0.33      1365\n",
      "\n",
      "Training completed in 59.10523986816406 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8728, F1 Micro: 0.6817, F1 Macro: 0.6766\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5544, Accuracy: 0.8134, F1 Micro: 0.2297, F1 Macro: 0.1727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3967, Accuracy: 0.8547, F1 Micro: 0.5567, F1 Macro: 0.5067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2916, Accuracy: 0.8684, F1 Micro: 0.6297, F1 Macro: 0.6186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2382, Accuracy: 0.8808, F1 Micro: 0.6874, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1917, Accuracy: 0.8828, F1 Micro: 0.7161, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.8914, F1 Micro: 0.7252, F1 Macro: 0.7168\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.883, F1 Micro: 0.6979, F1 Macro: 0.6952\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.8831, F1 Micro: 0.6972, F1 Macro: 0.6938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.8861, F1 Micro: 0.7307, F1 Macro: 0.7267\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.8853, F1 Micro: 0.6895, F1 Macro: 0.6736\n",
      "Model 1 - Iteration 971: Accuracy: 0.8861, F1 Micro: 0.7307, F1 Macro: 0.7267\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       370\n",
      "                sara       0.65      0.59      0.62       248\n",
      "         radikalisme       0.70      0.78      0.74       243\n",
      "pencemaran_nama_baik       0.69      0.66      0.67       504\n",
      "\n",
      "           micro avg       0.74      0.72      0.73      1365\n",
      "           macro avg       0.73      0.72      0.73      1365\n",
      "        weighted avg       0.74      0.72      0.73      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 69.91167783737183 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5797, Accuracy: 0.7998, F1 Micro: 0.1184, F1 Macro: 0.0921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4322, Accuracy: 0.842, F1 Micro: 0.4648, F1 Macro: 0.3939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3212, Accuracy: 0.8592, F1 Micro: 0.576, F1 Macro: 0.544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2603, Accuracy: 0.8794, F1 Micro: 0.6917, F1 Macro: 0.6859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2144, Accuracy: 0.8784, F1 Micro: 0.7028, F1 Macro: 0.7021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1637, Accuracy: 0.8872, F1 Micro: 0.7225, F1 Macro: 0.7186\n",
      "Epoch 7/10, Train Loss: 0.1262, Accuracy: 0.8795, F1 Micro: 0.674, F1 Macro: 0.6717\n",
      "Epoch 8/10, Train Loss: 0.1036, Accuracy: 0.8856, F1 Micro: 0.7147, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.8875, F1 Micro: 0.725, F1 Macro: 0.7186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.8894, F1 Micro: 0.7256, F1 Macro: 0.7218\n",
      "Model 2 - Iteration 971: Accuracy: 0.8894, F1 Micro: 0.7256, F1 Macro: 0.7218\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.83      0.88       370\n",
      "                sara       0.70      0.54      0.61       248\n",
      "         radikalisme       0.71      0.77      0.74       243\n",
      "pencemaran_nama_baik       0.71      0.61      0.66       504\n",
      "\n",
      "           micro avg       0.77      0.69      0.73      1365\n",
      "           macro avg       0.76      0.69      0.72      1365\n",
      "        weighted avg       0.77      0.69      0.72      1365\n",
      "         samples avg       0.39      0.38      0.37      1365\n",
      "\n",
      "Training completed in 71.14880681037903 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5286, Accuracy: 0.8189, F1 Micro: 0.2688, F1 Macro: 0.1954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.396, Accuracy: 0.8478, F1 Micro: 0.5, F1 Macro: 0.461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3043, Accuracy: 0.8608, F1 Micro: 0.5783, F1 Macro: 0.549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2409, Accuracy: 0.8798, F1 Micro: 0.7004, F1 Macro: 0.6924\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.8795, F1 Micro: 0.6836, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1574, Accuracy: 0.8872, F1 Micro: 0.7225, F1 Macro: 0.7205\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.8863, F1 Micro: 0.7055, F1 Macro: 0.7034\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.8866, F1 Micro: 0.7096, F1 Macro: 0.7023\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.8858, F1 Micro: 0.7146, F1 Macro: 0.7042\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.8872, F1 Micro: 0.7206, F1 Macro: 0.7114\n",
      "Model 3 - Iteration 971: Accuracy: 0.8872, F1 Micro: 0.7225, F1 Macro: 0.7205\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.82      0.87       370\n",
      "                sara       0.69      0.58      0.63       248\n",
      "         radikalisme       0.70      0.77      0.73       243\n",
      "pencemaran_nama_baik       0.71      0.61      0.66       504\n",
      "\n",
      "           micro avg       0.76      0.69      0.72      1365\n",
      "           macro avg       0.75      0.69      0.72      1365\n",
      "        weighted avg       0.76      0.69      0.72      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 66.04653596878052 s\n",
      "Averaged - Iteration 971: Accuracy: 0.8802, F1 Micro: 0.704, F1 Macro: 0.6998\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5124, Accuracy: 0.8325, F1 Micro: 0.3803, F1 Macro: 0.2788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3359, Accuracy: 0.8747, F1 Micro: 0.6756, F1 Macro: 0.657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2607, Accuracy: 0.8817, F1 Micro: 0.717, F1 Macro: 0.7109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2179, Accuracy: 0.8884, F1 Micro: 0.743, F1 Macro: 0.737\n",
      "Epoch 5/10, Train Loss: 0.1897, Accuracy: 0.8833, F1 Micro: 0.6687, F1 Macro: 0.6469\n",
      "Epoch 6/10, Train Loss: 0.1449, Accuracy: 0.8922, F1 Micro: 0.742, F1 Macro: 0.7331\n",
      "Epoch 7/10, Train Loss: 0.1026, Accuracy: 0.8917, F1 Micro: 0.724, F1 Macro: 0.716\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.8927, F1 Micro: 0.7344, F1 Macro: 0.7235\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.8906, F1 Micro: 0.7346, F1 Macro: 0.7232\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.8886, F1 Micro: 0.7276, F1 Macro: 0.7223\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8884, F1 Micro: 0.743, F1 Macro: 0.737\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.82      0.86       370\n",
      "                sara       0.65      0.60      0.62       248\n",
      "         radikalisme       0.70      0.79      0.74       243\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.76      0.74      1365\n",
      "           macro avg       0.73      0.74      0.74      1365\n",
      "        weighted avg       0.74      0.76      0.74      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 78.35748600959778 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.8197, F1 Micro: 0.336, F1 Macro: 0.2416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3638, Accuracy: 0.8636, F1 Micro: 0.6005, F1 Macro: 0.5441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2773, Accuracy: 0.8827, F1 Micro: 0.7072, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2282, Accuracy: 0.8889, F1 Micro: 0.7377, F1 Macro: 0.7347\n",
      "Epoch 5/10, Train Loss: 0.1973, Accuracy: 0.8822, F1 Micro: 0.6643, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1535, Accuracy: 0.8939, F1 Micro: 0.7464, F1 Macro: 0.7345\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.8909, F1 Micro: 0.7075, F1 Macro: 0.6886\n",
      "Epoch 8/10, Train Loss: 0.0848, Accuracy: 0.8927, F1 Micro: 0.7416, F1 Macro: 0.7317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.8939, F1 Micro: 0.7514, F1 Macro: 0.7438\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.8895, F1 Micro: 0.7307, F1 Macro: 0.7208\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8939, F1 Micro: 0.7514, F1 Macro: 0.7438\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.68      0.60      0.64       248\n",
      "         radikalisme       0.71      0.75      0.73       243\n",
      "pencemaran_nama_baik       0.69      0.74      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.75      0.74      0.74      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 82.64295411109924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5024, Accuracy: 0.8316, F1 Micro: 0.4031, F1 Macro: 0.2721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3469, Accuracy: 0.8662, F1 Micro: 0.612, F1 Macro: 0.5817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.8798, F1 Micro: 0.7039, F1 Macro: 0.6953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2192, Accuracy: 0.8891, F1 Micro: 0.7361, F1 Macro: 0.727\n",
      "Epoch 5/10, Train Loss: 0.185, Accuracy: 0.8839, F1 Micro: 0.6785, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1491, Accuracy: 0.8895, F1 Micro: 0.7417, F1 Macro: 0.7288\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.8894, F1 Micro: 0.7117, F1 Macro: 0.693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0817, Accuracy: 0.8906, F1 Micro: 0.744, F1 Macro: 0.7379\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.8897, F1 Micro: 0.7374, F1 Macro: 0.7238\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.8881, F1 Micro: 0.7227, F1 Macro: 0.7176\n",
      "Model 3 - Iteration 1496: Accuracy: 0.8906, F1 Micro: 0.744, F1 Macro: 0.7379\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.89       370\n",
      "                sara       0.65      0.60      0.62       248\n",
      "         radikalisme       0.75      0.72      0.74       243\n",
      "pencemaran_nama_baik       0.67      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.75      0.74      1365\n",
      "           macro avg       0.75      0.73      0.74      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 80.60224652290344 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8838, F1 Micro: 0.718, F1 Macro: 0.713\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4846, Accuracy: 0.8536, F1 Micro: 0.5493, F1 Macro: 0.491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3193, Accuracy: 0.8786, F1 Micro: 0.6915, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2516, Accuracy: 0.8905, F1 Micro: 0.7358, F1 Macro: 0.7243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2194, Accuracy: 0.8891, F1 Micro: 0.7568, F1 Macro: 0.7529\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.8956, F1 Micro: 0.7472, F1 Macro: 0.7322\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.8919, F1 Micro: 0.746, F1 Macro: 0.7362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.8988, F1 Micro: 0.7645, F1 Macro: 0.7559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.8983, F1 Micro: 0.7686, F1 Macro: 0.7663\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.8988, F1 Micro: 0.7607, F1 Macro: 0.7557\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.8975, F1 Micro: 0.7477, F1 Macro: 0.7384\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8983, F1 Micro: 0.7686, F1 Macro: 0.7663\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       370\n",
      "                sara       0.64      0.68      0.66       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 92.94789409637451 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5063, Accuracy: 0.8383, F1 Micro: 0.436, F1 Macro: 0.3664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3386, Accuracy: 0.8763, F1 Micro: 0.6714, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2599, Accuracy: 0.8889, F1 Micro: 0.7332, F1 Macro: 0.7159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.227, Accuracy: 0.8888, F1 Micro: 0.7516, F1 Macro: 0.7416\n",
      "Epoch 5/10, Train Loss: 0.1837, Accuracy: 0.893, F1 Micro: 0.7374, F1 Macro: 0.7193\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.8959, F1 Micro: 0.7407, F1 Macro: 0.7233\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.8964, F1 Micro: 0.7374, F1 Macro: 0.7188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0827, Accuracy: 0.8983, F1 Micro: 0.7581, F1 Macro: 0.752\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.8973, F1 Micro: 0.752, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.8978, F1 Micro: 0.7651, F1 Macro: 0.7626\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8978, F1 Micro: 0.7651, F1 Macro: 0.7626\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.65      0.66      0.65       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 93.77658438682556 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4754, Accuracy: 0.8395, F1 Micro: 0.4409, F1 Macro: 0.3707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.323, Accuracy: 0.8722, F1 Micro: 0.6603, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2524, Accuracy: 0.8889, F1 Micro: 0.7344, F1 Macro: 0.7172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2202, Accuracy: 0.8867, F1 Micro: 0.7508, F1 Macro: 0.7451\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.8927, F1 Micro: 0.7409, F1 Macro: 0.73\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.8969, F1 Micro: 0.7508, F1 Macro: 0.7378\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.8934, F1 Micro: 0.7315, F1 Macro: 0.7056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.897, F1 Micro: 0.7558, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.8956, F1 Micro: 0.7646, F1 Macro: 0.7636\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.8973, F1 Micro: 0.7614, F1 Macro: 0.7577\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8956, F1 Micro: 0.7646, F1 Macro: 0.7636\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       370\n",
      "                sara       0.62      0.72      0.66       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.74      0.80      0.76      1365\n",
      "        weighted avg       0.74      0.79      0.77      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 93.02429628372192 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8871, F1 Micro: 0.7301, F1 Macro: 0.7258\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4752, Accuracy: 0.8511, F1 Micro: 0.515, F1 Macro: 0.4486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2967, Accuracy: 0.8833, F1 Micro: 0.703, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.8927, F1 Micro: 0.7503, F1 Macro: 0.7398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2109, Accuracy: 0.8944, F1 Micro: 0.7518, F1 Macro: 0.7479\n",
      "Epoch 5/10, Train Loss: 0.1571, Accuracy: 0.8923, F1 Micro: 0.7357, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.8975, F1 Micro: 0.7602, F1 Macro: 0.7599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.898, F1 Micro: 0.7642, F1 Macro: 0.7648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.8938, F1 Micro: 0.7676, F1 Macro: 0.769\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.8967, F1 Micro: 0.7549, F1 Macro: 0.7504\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.8948, F1 Micro: 0.7351, F1 Macro: 0.7245\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8938, F1 Micro: 0.7676, F1 Macro: 0.769\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.65      0.73      0.69       248\n",
      "         radikalisme       0.75      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.64      0.84      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.82      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.73      0.82      0.77      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 108.02677607536316 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4973, Accuracy: 0.8456, F1 Micro: 0.4912, F1 Macro: 0.4041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3197, Accuracy: 0.8792, F1 Micro: 0.697, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.253, Accuracy: 0.8939, F1 Micro: 0.7482, F1 Macro: 0.7394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2164, Accuracy: 0.8938, F1 Micro: 0.7491, F1 Macro: 0.7372\n",
      "Epoch 5/10, Train Loss: 0.1631, Accuracy: 0.8942, F1 Micro: 0.7448, F1 Macro: 0.722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.8977, F1 Micro: 0.7635, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.8967, F1 Micro: 0.7604, F1 Macro: 0.7597\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.8942, F1 Micro: 0.7629, F1 Macro: 0.7649\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.8973, F1 Micro: 0.7535, F1 Macro: 0.7532\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.895, F1 Micro: 0.7395, F1 Macro: 0.7294\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8977, F1 Micro: 0.7635, F1 Macro: 0.764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.73      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.73      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.76      0.77      0.77      1365\n",
      "         samples avg       0.43      0.44      0.42      1365\n",
      "\n",
      "Training completed in 104.42824792861938 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4703, Accuracy: 0.8508, F1 Micro: 0.5237, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3055, Accuracy: 0.8827, F1 Micro: 0.7002, F1 Macro: 0.7001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2439, Accuracy: 0.8913, F1 Micro: 0.7505, F1 Macro: 0.7436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2113, Accuracy: 0.8933, F1 Micro: 0.756, F1 Macro: 0.7484\n",
      "Epoch 5/10, Train Loss: 0.1615, Accuracy: 0.8922, F1 Micro: 0.7332, F1 Macro: 0.7078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1256, Accuracy: 0.8989, F1 Micro: 0.7601, F1 Macro: 0.7544\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.8975, F1 Micro: 0.7601, F1 Macro: 0.7578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.8981, F1 Micro: 0.7634, F1 Macro: 0.7585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.8984, F1 Micro: 0.7724, F1 Macro: 0.772\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.897, F1 Micro: 0.7495, F1 Macro: 0.7388\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8984, F1 Micro: 0.7724, F1 Macro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.88       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 108.0623288154602 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.889, F1 Micro: 0.7376, F1 Macro: 0.7343\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4504, Accuracy: 0.8608, F1 Micro: 0.5763, F1 Macro: 0.5181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2879, Accuracy: 0.8834, F1 Micro: 0.7142, F1 Macro: 0.7171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.239, Accuracy: 0.8878, F1 Micro: 0.736, F1 Macro: 0.7296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.8977, F1 Micro: 0.765, F1 Macro: 0.763\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.8991, F1 Micro: 0.7625, F1 Macro: 0.7548\n",
      "Epoch 6/10, Train Loss: 0.1176, Accuracy: 0.8972, F1 Micro: 0.7647, F1 Macro: 0.7651\n",
      "Epoch 7/10, Train Loss: 0.0912, Accuracy: 0.893, F1 Micro: 0.7572, F1 Macro: 0.7462\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.8992, F1 Micro: 0.7647, F1 Macro: 0.7646\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.8981, F1 Micro: 0.7626, F1 Macro: 0.7612\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.8966, F1 Micro: 0.7545, F1 Macro: 0.7466\n",
      "Model 1 - Iteration 2777: Accuracy: 0.8977, F1 Micro: 0.765, F1 Macro: 0.763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.89       370\n",
      "                sara       0.64      0.69      0.67       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.76      1365\n",
      "           macro avg       0.74      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 112.93592858314514 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4756, Accuracy: 0.8534, F1 Micro: 0.5442, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.8827, F1 Micro: 0.6978, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2506, Accuracy: 0.8938, F1 Micro: 0.7434, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.199, Accuracy: 0.898, F1 Micro: 0.767, F1 Macro: 0.7627\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.898, F1 Micro: 0.7598, F1 Macro: 0.7524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9008, F1 Micro: 0.7688, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.0913, Accuracy: 0.8959, F1 Micro: 0.7642, F1 Macro: 0.7609\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.8969, F1 Micro: 0.765, F1 Macro: 0.7674\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.8983, F1 Micro: 0.7602, F1 Macro: 0.7563\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.8983, F1 Micro: 0.7661, F1 Macro: 0.7598\n",
      "Model 2 - Iteration 2777: Accuracy: 0.9008, F1 Micro: 0.7688, F1 Macro: 0.767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.77      0.77      1365\n",
      "           macro avg       0.76      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 113.88830590248108 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4543, Accuracy: 0.8662, F1 Micro: 0.6148, F1 Macro: 0.5887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2916, Accuracy: 0.8831, F1 Micro: 0.7119, F1 Macro: 0.7125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2467, Accuracy: 0.8853, F1 Micro: 0.717, F1 Macro: 0.7029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1908, Accuracy: 0.8956, F1 Micro: 0.7672, F1 Macro: 0.7662\n",
      "Epoch 5/10, Train Loss: 0.1592, Accuracy: 0.8906, F1 Micro: 0.7521, F1 Macro: 0.7442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.8991, F1 Micro: 0.7703, F1 Macro: 0.7689\n",
      "Epoch 7/10, Train Loss: 0.0857, Accuracy: 0.8955, F1 Micro: 0.7556, F1 Macro: 0.7468\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.8975, F1 Micro: 0.7644, F1 Macro: 0.7612\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.8988, F1 Micro: 0.7632, F1 Macro: 0.7583\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.8941, F1 Micro: 0.758, F1 Macro: 0.7505\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8991, F1 Micro: 0.7703, F1 Macro: 0.7689\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.66      0.72      0.69       248\n",
      "         radikalisme       0.71      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 114.20555329322815 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8907, F1 Micro: 0.7427, F1 Macro: 0.7396\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4425, Accuracy: 0.8695, F1 Micro: 0.6261, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2852, Accuracy: 0.8902, F1 Micro: 0.7503, F1 Macro: 0.739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2357, Accuracy: 0.8945, F1 Micro: 0.7516, F1 Macro: 0.746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.8998, F1 Micro: 0.77, F1 Macro: 0.7679\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.8942, F1 Micro: 0.7319, F1 Macro: 0.7158\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.8981, F1 Micro: 0.7629, F1 Macro: 0.758\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.8964, F1 Micro: 0.7671, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.8988, F1 Micro: 0.7532, F1 Macro: 0.7462\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.8984, F1 Micro: 0.7693, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.8966, F1 Micro: 0.7487, F1 Macro: 0.7381\n",
      "Model 1 - Iteration 3122: Accuracy: 0.8998, F1 Micro: 0.77, F1 Macro: 0.7679\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       370\n",
      "                sara       0.69      0.63      0.66       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.76      0.78      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.44      0.45      0.43      1365\n",
      "\n",
      "Training completed in 121.37144470214844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.463, Accuracy: 0.8558, F1 Micro: 0.5415, F1 Macro: 0.4598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3005, Accuracy: 0.8894, F1 Micro: 0.7384, F1 Macro: 0.7242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2409, Accuracy: 0.8934, F1 Micro: 0.7463, F1 Macro: 0.7434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.8981, F1 Micro: 0.7553, F1 Macro: 0.7488\n",
      "Epoch 5/10, Train Loss: 0.1677, Accuracy: 0.8961, F1 Micro: 0.7373, F1 Macro: 0.7228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.8972, F1 Micro: 0.7636, F1 Macro: 0.7576\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.8913, F1 Micro: 0.7507, F1 Macro: 0.7442\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9006, F1 Micro: 0.7576, F1 Macro: 0.7528\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.8963, F1 Micro: 0.7617, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.898, F1 Micro: 0.7539, F1 Macro: 0.7466\n",
      "Model 2 - Iteration 3122: Accuracy: 0.8972, F1 Micro: 0.7636, F1 Macro: 0.7576\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       370\n",
      "                sara       0.67      0.57      0.62       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.78      0.76      1365\n",
      "           macro avg       0.75      0.77      0.76      1365\n",
      "        weighted avg       0.75      0.78      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 123.74859976768494 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4377, Accuracy: 0.8648, F1 Micro: 0.6144, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2881, Accuracy: 0.8897, F1 Micro: 0.7395, F1 Macro: 0.729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2355, Accuracy: 0.8922, F1 Micro: 0.7427, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1865, Accuracy: 0.8983, F1 Micro: 0.7639, F1 Macro: 0.7557\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.893, F1 Micro: 0.7196, F1 Macro: 0.6956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.8983, F1 Micro: 0.7692, F1 Macro: 0.7625\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.898, F1 Micro: 0.7604, F1 Macro: 0.7529\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.8948, F1 Micro: 0.7438, F1 Macro: 0.732\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.8981, F1 Micro: 0.7643, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.8998, F1 Micro: 0.7663, F1 Macro: 0.7651\n",
      "Model 3 - Iteration 3122: Accuracy: 0.8983, F1 Micro: 0.7692, F1 Macro: 0.7625\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.89      0.89       370\n",
      "                sara       0.67      0.60      0.64       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.75      0.78      0.76      1365\n",
      "        weighted avg       0.75      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 123.04690647125244 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8918, F1 Micro: 0.7462, F1 Macro: 0.7429\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4361, Accuracy: 0.873, F1 Micro: 0.6891, F1 Macro: 0.6859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.8941, F1 Micro: 0.7613, F1 Macro: 0.7527\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.8983, F1 Micro: 0.7607, F1 Macro: 0.7511\n",
      "Epoch 4/10, Train Loss: 0.1907, Accuracy: 0.8989, F1 Micro: 0.7578, F1 Macro: 0.7467\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.9005, F1 Micro: 0.761, F1 Macro: 0.7583\n",
      "Epoch 6/10, Train Loss: 0.1133, Accuracy: 0.8967, F1 Micro: 0.7597, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.8995, F1 Micro: 0.7628, F1 Macro: 0.7579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9006, F1 Micro: 0.7643, F1 Macro: 0.7624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.8953, F1 Micro: 0.7656, F1 Macro: 0.7621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9002, F1 Micro: 0.7722, F1 Macro: 0.7708\n",
      "Model 1 - Iteration 3432: Accuracy: 0.9002, F1 Micro: 0.7722, F1 Macro: 0.7708\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.66      0.63      0.64       248\n",
      "         radikalisme       0.75      0.89      0.82       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 132.01656413078308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4549, Accuracy: 0.8711, F1 Micro: 0.6623, F1 Macro: 0.6356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2881, Accuracy: 0.8905, F1 Micro: 0.7517, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.8994, F1 Micro: 0.7658, F1 Macro: 0.7557\n",
      "Epoch 4/10, Train Loss: 0.1932, Accuracy: 0.8973, F1 Micro: 0.746, F1 Macro: 0.7282\n",
      "Epoch 5/10, Train Loss: 0.1433, Accuracy: 0.8967, F1 Micro: 0.747, F1 Macro: 0.7444\n",
      "Epoch 6/10, Train Loss: 0.1123, Accuracy: 0.9005, F1 Micro: 0.7594, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9031, F1 Micro: 0.7747, F1 Macro: 0.7698\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.8988, F1 Micro: 0.7593, F1 Macro: 0.7586\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.8992, F1 Micro: 0.7677, F1 Macro: 0.7634\n",
      "Epoch 10/10, Train Loss: 0.0402, Accuracy: 0.8983, F1 Micro: 0.7699, F1 Macro: 0.7665\n",
      "Model 2 - Iteration 3432: Accuracy: 0.9031, F1 Micro: 0.7747, F1 Macro: 0.7698\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.68      0.63      0.65       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.77      0.78      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 129.83694863319397 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4359, Accuracy: 0.8681, F1 Micro: 0.6524, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.284, Accuracy: 0.8922, F1 Micro: 0.7599, F1 Macro: 0.7525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2379, Accuracy: 0.897, F1 Micro: 0.7634, F1 Macro: 0.756\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.8969, F1 Micro: 0.749, F1 Macro: 0.7383\n",
      "Epoch 5/10, Train Loss: 0.1409, Accuracy: 0.8964, F1 Micro: 0.7576, F1 Macro: 0.7526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9016, F1 Micro: 0.7668, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9003, F1 Micro: 0.7746, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9011, F1 Micro: 0.7667, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.8964, F1 Micro: 0.7713, F1 Macro: 0.7672\n",
      "Epoch 10/10, Train Loss: 0.0393, Accuracy: 0.8966, F1 Micro: 0.7632, F1 Macro: 0.7544\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9003, F1 Micro: 0.7746, F1 Macro: 0.7704\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.90       370\n",
      "                sara       0.65      0.66      0.65       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 131.72928643226624 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.893, F1 Micro: 0.7497, F1 Macro: 0.7464\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4336, Accuracy: 0.8769, F1 Micro: 0.6917, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.8928, F1 Micro: 0.7581, F1 Macro: 0.7555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2287, Accuracy: 0.8992, F1 Micro: 0.7642, F1 Macro: 0.7582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1932, Accuracy: 0.893, F1 Micro: 0.768, F1 Macro: 0.7664\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.8984, F1 Micro: 0.7659, F1 Macro: 0.7609\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.8964, F1 Micro: 0.759, F1 Macro: 0.7565\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.8958, F1 Micro: 0.7414, F1 Macro: 0.7329\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9003, F1 Micro: 0.751, F1 Macro: 0.7456\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.8955, F1 Micro: 0.755, F1 Macro: 0.7505\n",
      "Epoch 10/10, Train Loss: 0.0418, Accuracy: 0.897, F1 Micro: 0.7575, F1 Macro: 0.755\n",
      "Model 1 - Iteration 3711: Accuracy: 0.893, F1 Micro: 0.768, F1 Macro: 0.7664\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       370\n",
      "                sara       0.59      0.74      0.66       248\n",
      "         radikalisme       0.74      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.71      0.83      0.77      1365\n",
      "           macro avg       0.72      0.82      0.77      1365\n",
      "        weighted avg       0.72      0.83      0.77      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 136.66464114189148 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4526, Accuracy: 0.8683, F1 Micro: 0.6352, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2876, Accuracy: 0.8927, F1 Micro: 0.7592, F1 Macro: 0.7577\n",
      "Epoch 3/10, Train Loss: 0.2331, Accuracy: 0.9003, F1 Micro: 0.7591, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1967, Accuracy: 0.8986, F1 Micro: 0.779, F1 Macro: 0.7776\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9011, F1 Micro: 0.7672, F1 Macro: 0.7596\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9027, F1 Micro: 0.7546, F1 Macro: 0.7474\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9031, F1 Micro: 0.7628, F1 Macro: 0.7603\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9014, F1 Micro: 0.7643, F1 Macro: 0.7616\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.8988, F1 Micro: 0.7705, F1 Macro: 0.7689\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9006, F1 Micro: 0.7596, F1 Macro: 0.7583\n",
      "Model 2 - Iteration 3711: Accuracy: 0.8986, F1 Micro: 0.779, F1 Macro: 0.7776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 135.527592420578 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4329, Accuracy: 0.8728, F1 Micro: 0.686, F1 Macro: 0.6794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2794, Accuracy: 0.8914, F1 Micro: 0.7564, F1 Macro: 0.7522\n",
      "Epoch 3/10, Train Loss: 0.2302, Accuracy: 0.8964, F1 Micro: 0.7533, F1 Macro: 0.7452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1904, Accuracy: 0.8955, F1 Micro: 0.7691, F1 Macro: 0.766\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.8969, F1 Micro: 0.761, F1 Macro: 0.754\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.8963, F1 Micro: 0.7468, F1 Macro: 0.7341\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.8963, F1 Micro: 0.7555, F1 Macro: 0.7526\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9009, F1 Micro: 0.7606, F1 Macro: 0.7528\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.897, F1 Micro: 0.761, F1 Macro: 0.7573\n",
      "Epoch 10/10, Train Loss: 0.0398, Accuracy: 0.8984, F1 Micro: 0.764, F1 Macro: 0.7603\n",
      "Model 3 - Iteration 3711: Accuracy: 0.8955, F1 Micro: 0.7691, F1 Macro: 0.766\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.91      0.89       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1365\n",
      "           macro avg       0.73      0.81      0.77      1365\n",
      "        weighted avg       0.73      0.82      0.77      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 135.80301117897034 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8933, F1 Micro: 0.7522, F1 Macro: 0.749\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4205, Accuracy: 0.8781, F1 Micro: 0.7043, F1 Macro: 0.6936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.8927, F1 Micro: 0.7357, F1 Macro: 0.7334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.8984, F1 Micro: 0.7675, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9033, F1 Micro: 0.7722, F1 Macro: 0.7697\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.8988, F1 Micro: 0.7694, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.8908, F1 Micro: 0.7698, F1 Macro: 0.7711\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.8991, F1 Micro: 0.7708, F1 Macro: 0.7701\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9033, F1 Micro: 0.7707, F1 Macro: 0.771\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.895, F1 Micro: 0.7655, F1 Macro: 0.7634\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.8972, F1 Micro: 0.7711, F1 Macro: 0.7721\n",
      "Model 1 - Iteration 3886: Accuracy: 0.9033, F1 Micro: 0.7722, F1 Macro: 0.7697\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.90      0.89       370\n",
      "                sara       0.70      0.65      0.67       248\n",
      "         radikalisme       0.77      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.70      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 142.05858039855957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4379, Accuracy: 0.8752, F1 Micro: 0.6593, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2723, Accuracy: 0.8913, F1 Micro: 0.7366, F1 Macro: 0.7343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2319, Accuracy: 0.8994, F1 Micro: 0.7638, F1 Macro: 0.7558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.8994, F1 Micro: 0.7801, F1 Macro: 0.7783\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.8963, F1 Micro: 0.7741, F1 Macro: 0.7733\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.897, F1 Micro: 0.7791, F1 Macro: 0.7825\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.8964, F1 Micro: 0.7621, F1 Macro: 0.7606\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.8978, F1 Micro: 0.7716, F1 Macro: 0.7724\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9002, F1 Micro: 0.7696, F1 Macro: 0.7689\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9006, F1 Micro: 0.7689, F1 Macro: 0.7674\n",
      "Model 2 - Iteration 3886: Accuracy: 0.8994, F1 Micro: 0.7801, F1 Macro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       370\n",
      "                sara       0.63      0.71      0.67       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 142.21158933639526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4199, Accuracy: 0.8717, F1 Micro: 0.6723, F1 Macro: 0.662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.8931, F1 Micro: 0.7518, F1 Macro: 0.7484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2295, Accuracy: 0.8984, F1 Micro: 0.764, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9, F1 Micro: 0.781, F1 Macro: 0.7772\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.8978, F1 Micro: 0.7694, F1 Macro: 0.7645\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.8977, F1 Micro: 0.7745, F1 Macro: 0.7743\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.898, F1 Micro: 0.7667, F1 Macro: 0.7638\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9014, F1 Micro: 0.7698, F1 Macro: 0.7679\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.9003, F1 Micro: 0.7647, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.8997, F1 Micro: 0.7608, F1 Macro: 0.7564\n",
      "Model 3 - Iteration 3886: Accuracy: 0.9, F1 Micro: 0.781, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.45      0.47      0.45      1365\n",
      "\n",
      "Training completed in 142.50278878211975 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8941, F1 Micro: 0.7547, F1 Macro: 0.7516\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4129, Accuracy: 0.8747, F1 Micro: 0.671, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2744, Accuracy: 0.8888, F1 Micro: 0.7147, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.8995, F1 Micro: 0.7737, F1 Macro: 0.7678\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.8981, F1 Micro: 0.7425, F1 Macro: 0.72\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.9031, F1 Micro: 0.7731, F1 Macro: 0.773\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9011, F1 Micro: 0.7674, F1 Macro: 0.7632\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9, F1 Micro: 0.7647, F1 Macro: 0.7614\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.8964, F1 Micro: 0.7678, F1 Macro: 0.769\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8978, F1 Micro: 0.7627, F1 Macro: 0.7632\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9023, F1 Micro: 0.7632, F1 Macro: 0.7614\n",
      "Model 1 - Iteration 4120: Accuracy: 0.8995, F1 Micro: 0.7737, F1 Macro: 0.7678\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       370\n",
      "                sara       0.67      0.65      0.66       248\n",
      "         radikalisme       0.74      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.77      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 147.57885766029358 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4369, Accuracy: 0.8745, F1 Micro: 0.6582, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2777, Accuracy: 0.8906, F1 Micro: 0.7159, F1 Macro: 0.7023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9006, F1 Micro: 0.7711, F1 Macro: 0.7601\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9005, F1 Micro: 0.7483, F1 Macro: 0.7246\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9017, F1 Micro: 0.7607, F1 Macro: 0.7577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.9034, F1 Micro: 0.779, F1 Macro: 0.7754\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.8973, F1 Micro: 0.7691, F1 Macro: 0.7651\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9033, F1 Micro: 0.7771, F1 Macro: 0.771\n",
      "Epoch 9/10, Train Loss: 0.0467, Accuracy: 0.9013, F1 Micro: 0.7608, F1 Macro: 0.7599\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9003, F1 Micro: 0.7695, F1 Macro: 0.7672\n",
      "Model 2 - Iteration 4120: Accuracy: 0.9034, F1 Micro: 0.779, F1 Macro: 0.7754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.79      0.77      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 149.27135801315308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4126, Accuracy: 0.8728, F1 Micro: 0.6545, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2737, Accuracy: 0.8906, F1 Micro: 0.7138, F1 Macro: 0.702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.8967, F1 Micro: 0.7643, F1 Macro: 0.7535\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.897, F1 Micro: 0.7396, F1 Macro: 0.713\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9003, F1 Micro: 0.7637, F1 Macro: 0.7572\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.8975, F1 Micro: 0.7506, F1 Macro: 0.736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0807, Accuracy: 0.9011, F1 Micro: 0.7687, F1 Macro: 0.7626\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.8998, F1 Micro: 0.7646, F1 Macro: 0.7546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.9033, F1 Micro: 0.7768, F1 Macro: 0.7736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9023, F1 Micro: 0.7794, F1 Macro: 0.7787\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9023, F1 Micro: 0.7794, F1 Macro: 0.7787\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 151.896906375885 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8948, F1 Micro: 0.7568, F1 Macro: 0.7536\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4089, Accuracy: 0.88, F1 Micro: 0.7098, F1 Macro: 0.7092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2558, Accuracy: 0.8964, F1 Micro: 0.7676, F1 Macro: 0.7664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.8908, F1 Micro: 0.7725, F1 Macro: 0.7725\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9003, F1 Micro: 0.7554, F1 Macro: 0.7428\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.8988, F1 Micro: 0.7477, F1 Macro: 0.7356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9013, F1 Micro: 0.7789, F1 Macro: 0.7784\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.8997, F1 Micro: 0.7751, F1 Macro: 0.7783\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.8991, F1 Micro: 0.7602, F1 Macro: 0.7583\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9014, F1 Micro: 0.7652, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9009, F1 Micro: 0.7667, F1 Macro: 0.7617\n",
      "Model 1 - Iteration 4330: Accuracy: 0.9013, F1 Micro: 0.7789, F1 Macro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.67      0.70      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 154.11370158195496 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4287, Accuracy: 0.8742, F1 Micro: 0.675, F1 Macro: 0.6662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.8952, F1 Micro: 0.7592, F1 Macro: 0.7553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2281, Accuracy: 0.8925, F1 Micro: 0.7702, F1 Macro: 0.7713\n",
      "Epoch 4/10, Train Loss: 0.1876, Accuracy: 0.9011, F1 Micro: 0.7618, F1 Macro: 0.7489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9009, F1 Micro: 0.7713, F1 Macro: 0.7654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.8963, F1 Micro: 0.777, F1 Macro: 0.7777\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.8995, F1 Micro: 0.7745, F1 Macro: 0.7746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.9067, F1 Micro: 0.7811, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9033, F1 Micro: 0.7755, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.9009, F1 Micro: 0.7673, F1 Macro: 0.7612\n",
      "Model 2 - Iteration 4330: Accuracy: 0.9067, F1 Micro: 0.7811, F1 Macro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.72      0.69       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.75      0.69      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.78      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 157.10358881950378 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4059, Accuracy: 0.875, F1 Micro: 0.6916, F1 Macro: 0.6885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2549, Accuracy: 0.8916, F1 Micro: 0.7561, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2251, Accuracy: 0.8908, F1 Micro: 0.7701, F1 Macro: 0.77\n",
      "Epoch 4/10, Train Loss: 0.1864, Accuracy: 0.9005, F1 Micro: 0.7519, F1 Macro: 0.7341\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9009, F1 Micro: 0.7611, F1 Macro: 0.7486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9009, F1 Micro: 0.7783, F1 Macro: 0.773\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.8975, F1 Micro: 0.7628, F1 Macro: 0.7567\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.8988, F1 Micro: 0.7736, F1 Macro: 0.7752\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.9027, F1 Micro: 0.772, F1 Macro: 0.767\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.902, F1 Micro: 0.7711, F1 Macro: 0.7649\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9009, F1 Micro: 0.7783, F1 Macro: 0.773\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.65      0.68      0.67       248\n",
      "         radikalisme       0.72      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 154.72077655792236 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8954, F1 Micro: 0.7587, F1 Macro: 0.7556\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3984, Accuracy: 0.8822, F1 Micro: 0.7155, F1 Macro: 0.7157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2632, Accuracy: 0.8973, F1 Micro: 0.7525, F1 Macro: 0.7408\n",
      "Epoch 3/10, Train Loss: 0.216, Accuracy: 0.8947, F1 Micro: 0.7217, F1 Macro: 0.7051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.8992, F1 Micro: 0.7787, F1 Macro: 0.7767\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.9027, F1 Micro: 0.7662, F1 Macro: 0.7592\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.8995, F1 Micro: 0.7567, F1 Macro: 0.7473\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9025, F1 Micro: 0.7704, F1 Macro: 0.7678\n",
      "Epoch 8/10, Train Loss: 0.0578, Accuracy: 0.8997, F1 Micro: 0.7681, F1 Macro: 0.7596\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7715\n",
      "Epoch 10/10, Train Loss: 0.0353, Accuracy: 0.902, F1 Micro: 0.7757, F1 Macro: 0.7753\n",
      "Model 1 - Iteration 4530: Accuracy: 0.8992, F1 Micro: 0.7787, F1 Macro: 0.7767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.62      0.71      0.67       248\n",
      "         radikalisme       0.72      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 160.14714360237122 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4157, Accuracy: 0.8803, F1 Micro: 0.697, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.8981, F1 Micro: 0.749, F1 Macro: 0.7336\n",
      "Epoch 3/10, Train Loss: 0.2195, Accuracy: 0.8948, F1 Micro: 0.7252, F1 Macro: 0.7016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9027, F1 Micro: 0.7807, F1 Macro: 0.7775\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.8994, F1 Micro: 0.7663, F1 Macro: 0.7611\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9005, F1 Micro: 0.7509, F1 Macro: 0.7394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.902, F1 Micro: 0.7824, F1 Macro: 0.7814\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.9017, F1 Micro: 0.7759, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9017, F1 Micro: 0.7727, F1 Macro: 0.7716\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.902, F1 Micro: 0.7822, F1 Macro: 0.7819\n",
      "Model 2 - Iteration 4530: Accuracy: 0.902, F1 Micro: 0.7824, F1 Macro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.70878958702087 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3978, Accuracy: 0.8797, F1 Micro: 0.6935, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2669, Accuracy: 0.8963, F1 Micro: 0.7509, F1 Macro: 0.7389\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.8944, F1 Micro: 0.7256, F1 Macro: 0.7079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.8997, F1 Micro: 0.773, F1 Macro: 0.767\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.8997, F1 Micro: 0.7682, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.8991, F1 Micro: 0.7542, F1 Macro: 0.7453\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.8939, F1 Micro: 0.7728, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9009, F1 Micro: 0.758, F1 Macro: 0.7494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0411, Accuracy: 0.8995, F1 Micro: 0.7762, F1 Macro: 0.7751\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.898, F1 Micro: 0.7682, F1 Macro: 0.7665\n",
      "Model 3 - Iteration 4530: Accuracy: 0.8995, F1 Micro: 0.7762, F1 Macro: 0.7751\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.63      0.74      0.68       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.74      0.82      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 161.5226743221283 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8958, F1 Micro: 0.7602, F1 Macro: 0.7573\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3997, Accuracy: 0.877, F1 Micro: 0.6948, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.8953, F1 Micro: 0.7474, F1 Macro: 0.7313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.8981, F1 Micro: 0.7736, F1 Macro: 0.7721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.9039, F1 Micro: 0.7826, F1 Macro: 0.7807\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9013, F1 Micro: 0.7666, F1 Macro: 0.7655\n",
      "Epoch 6/10, Train Loss: 0.1072, Accuracy: 0.8997, F1 Micro: 0.7626, F1 Macro: 0.7603\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9039, F1 Micro: 0.78, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9, F1 Micro: 0.7659, F1 Macro: 0.7593\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.9011, F1 Micro: 0.7634, F1 Macro: 0.7576\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.902, F1 Micro: 0.7672, F1 Macro: 0.7609\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9039, F1 Micro: 0.7826, F1 Macro: 0.7807\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.76      0.82      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 164.58986639976501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4206, Accuracy: 0.8777, F1 Micro: 0.6906, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.8986, F1 Micro: 0.7618, F1 Macro: 0.7504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.9017, F1 Micro: 0.7832, F1 Macro: 0.7845\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9028, F1 Micro: 0.7816, F1 Macro: 0.7797\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9023, F1 Micro: 0.77, F1 Macro: 0.7697\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9002, F1 Micro: 0.7568, F1 Macro: 0.749\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9025, F1 Micro: 0.7812, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.9038, F1 Micro: 0.7724, F1 Macro: 0.7692\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.903, F1 Micro: 0.7741, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9006, F1 Micro: 0.7773, F1 Macro: 0.777\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9017, F1 Micro: 0.7832, F1 Macro: 0.7845\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       370\n",
      "                sara       0.61      0.77      0.68       248\n",
      "         radikalisme       0.75      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 162.34704542160034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3994, Accuracy: 0.8783, F1 Micro: 0.7028, F1 Macro: 0.7007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.8953, F1 Micro: 0.7507, F1 Macro: 0.7311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.8983, F1 Micro: 0.7734, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.8998, F1 Micro: 0.781, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.8984, F1 Micro: 0.7628, F1 Macro: 0.7625\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.8997, F1 Micro: 0.7617, F1 Macro: 0.7589\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9031, F1 Micro: 0.777, F1 Macro: 0.7725\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.9009, F1 Micro: 0.7602, F1 Macro: 0.7502\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.903, F1 Micro: 0.777, F1 Macro: 0.7717\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9023, F1 Micro: 0.772, F1 Macro: 0.769\n",
      "Model 3 - Iteration 4663: Accuracy: 0.8998, F1 Micro: 0.781, F1 Macro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1365\n",
      "           macro avg       0.74      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.84      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 164.35637712478638 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8962, F1 Micro: 0.7618, F1 Macro: 0.759\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8859, F1 Micro: 0.727, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.8961, F1 Micro: 0.7368, F1 Macro: 0.7174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9023, F1 Micro: 0.7711, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9036, F1 Micro: 0.7872, F1 Macro: 0.7864\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.7723\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9006, F1 Micro: 0.7682, F1 Macro: 0.7612\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9034, F1 Micro: 0.7673, F1 Macro: 0.7674\n",
      "Epoch 9/10, Train Loss: 0.0404, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.7724\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.9053, F1 Micro: 0.774, F1 Macro: 0.7721\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9036, F1 Micro: 0.7872, F1 Macro: 0.7864\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.75      0.70       248\n",
      "         radikalisme       0.73      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 171.2702317237854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4121, Accuracy: 0.8834, F1 Micro: 0.7183, F1 Macro: 0.717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.8955, F1 Micro: 0.7359, F1 Macro: 0.7074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.902, F1 Micro: 0.7707, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1791, Accuracy: 0.9034, F1 Micro: 0.7791, F1 Macro: 0.7772\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9005, F1 Micro: 0.7761, F1 Macro: 0.7747\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9011, F1 Micro: 0.7614, F1 Macro: 0.761\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.8997, F1 Micro: 0.7622, F1 Macro: 0.7575\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9006, F1 Micro: 0.7521, F1 Macro: 0.7442\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.902, F1 Micro: 0.7668, F1 Macro: 0.7573\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.8995, F1 Micro: 0.7693, F1 Macro: 0.7699\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9034, F1 Micro: 0.7791, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       370\n",
      "                sara       0.69      0.68      0.68       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 171.89912486076355 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3906, Accuracy: 0.8823, F1 Micro: 0.7089, F1 Macro: 0.6988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2573, Accuracy: 0.8953, F1 Micro: 0.7377, F1 Macro: 0.7134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9017, F1 Micro: 0.7666, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9019, F1 Micro: 0.7752, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9052, F1 Micro: 0.7883, F1 Macro: 0.7877\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9028, F1 Micro: 0.7624, F1 Macro: 0.7588\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9008, F1 Micro: 0.7715, F1 Macro: 0.7677\n",
      "Epoch 8/10, Train Loss: 0.0549, Accuracy: 0.8998, F1 Micro: 0.752, F1 Macro: 0.749\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9017, F1 Micro: 0.7761, F1 Macro: 0.7748\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.9014, F1 Micro: 0.7731, F1 Macro: 0.7731\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9052, F1 Micro: 0.7883, F1 Macro: 0.7877\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.77      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 172.00083780288696 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.8968, F1 Micro: 0.7634, F1 Macro: 0.7607\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3933, Accuracy: 0.8883, F1 Micro: 0.7301, F1 Macro: 0.7217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.252, Accuracy: 0.8975, F1 Micro: 0.7498, F1 Macro: 0.7397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9019, F1 Micro: 0.7686, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9047, F1 Micro: 0.7727, F1 Macro: 0.7661\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9031, F1 Micro: 0.7687, F1 Macro: 0.7642\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9, F1 Micro: 0.7494, F1 Macro: 0.7373\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.8945, F1 Micro: 0.7612, F1 Macro: 0.7572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.8997, F1 Micro: 0.7752, F1 Macro: 0.7752\n",
      "Epoch 9/10, Train Loss: 0.043, Accuracy: 0.9009, F1 Micro: 0.7591, F1 Macro: 0.7535\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.9013, F1 Micro: 0.7751, F1 Macro: 0.7729\n",
      "Model 1 - Iteration 5063: Accuracy: 0.8997, F1 Micro: 0.7752, F1 Macro: 0.7752\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 178.68854522705078 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4136, Accuracy: 0.882, F1 Micro: 0.7141, F1 Macro: 0.7107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.8984, F1 Micro: 0.7519, F1 Macro: 0.7415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2165, Accuracy: 0.9016, F1 Micro: 0.7709, F1 Macro: 0.7652\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9031, F1 Micro: 0.7687, F1 Macro: 0.764\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9016, F1 Micro: 0.7603, F1 Macro: 0.75\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9006, F1 Micro: 0.7663, F1 Macro: 0.761\n",
      "Epoch 7/10, Train Loss: 0.0722, Accuracy: 0.8997, F1 Micro: 0.7692, F1 Macro: 0.7663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.9042, F1 Micro: 0.783, F1 Macro: 0.7818\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9011, F1 Micro: 0.7727, F1 Macro: 0.7711\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.8977, F1 Micro: 0.7753, F1 Macro: 0.7755\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9042, F1 Micro: 0.783, F1 Macro: 0.7818\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.65      0.71      0.68       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 176.6992416381836 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3892, Accuracy: 0.8881, F1 Micro: 0.7373, F1 Macro: 0.729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.258, Accuracy: 0.8975, F1 Micro: 0.7508, F1 Macro: 0.7367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.902, F1 Micro: 0.7665, F1 Macro: 0.7542\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.9014, F1 Micro: 0.7583, F1 Macro: 0.752\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.8988, F1 Micro: 0.7513, F1 Macro: 0.7354\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9008, F1 Micro: 0.7637, F1 Macro: 0.7538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9013, F1 Micro: 0.7707, F1 Macro: 0.7667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.8978, F1 Micro: 0.7716, F1 Macro: 0.7706\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9008, F1 Micro: 0.7623, F1 Macro: 0.7583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9036, F1 Micro: 0.7788, F1 Macro: 0.772\n",
      "Model 3 - Iteration 5063: Accuracy: 0.9036, F1 Micro: 0.7788, F1 Macro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.64      0.65       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 180.27600574493408 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8971, F1 Micro: 0.7643, F1 Macro: 0.7617\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3919, Accuracy: 0.8881, F1 Micro: 0.7244, F1 Macro: 0.717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.8983, F1 Micro: 0.7616, F1 Macro: 0.7531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2085, Accuracy: 0.905, F1 Micro: 0.7711, F1 Macro: 0.7587\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.902, F1 Micro: 0.7685, F1 Macro: 0.7587\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.905, F1 Micro: 0.764, F1 Macro: 0.7534\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9027, F1 Micro: 0.7662, F1 Macro: 0.7632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9031, F1 Micro: 0.7737, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.9052, F1 Micro: 0.7797, F1 Macro: 0.778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9042, F1 Micro: 0.7797, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9045, F1 Micro: 0.7707, F1 Macro: 0.7667\n",
      "Model 1 - Iteration 5263: Accuracy: 0.9042, F1 Micro: 0.7797, F1 Macro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.66      0.63      0.64       248\n",
      "         radikalisme       0.80      0.82      0.81       243\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 185.52651596069336 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.41, Accuracy: 0.8819, F1 Micro: 0.7058, F1 Macro: 0.6994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2619, Accuracy: 0.8978, F1 Micro: 0.7656, F1 Macro: 0.7541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9064, F1 Micro: 0.7746, F1 Macro: 0.7645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.9031, F1 Micro: 0.7815, F1 Macro: 0.7752\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9058, F1 Micro: 0.768, F1 Macro: 0.7551\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9014, F1 Micro: 0.7705, F1 Macro: 0.7614\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9009, F1 Micro: 0.7812, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9014, F1 Micro: 0.7718, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.9034, F1 Micro: 0.7829, F1 Macro: 0.7791\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.9034, F1 Micro: 0.7769, F1 Macro: 0.7739\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9034, F1 Micro: 0.7829, F1 Macro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.66      0.68      0.67       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.78      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 183.5207324028015 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3941, Accuracy: 0.8841, F1 Micro: 0.7097, F1 Macro: 0.6965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.8963, F1 Micro: 0.7644, F1 Macro: 0.7579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2119, Accuracy: 0.903, F1 Micro: 0.7673, F1 Macro: 0.7525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9003, F1 Micro: 0.7744, F1 Macro: 0.7676\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.9014, F1 Micro: 0.7503, F1 Macro: 0.7307\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.902, F1 Micro: 0.7593, F1 Macro: 0.7492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9036, F1 Micro: 0.7767, F1 Macro: 0.775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9047, F1 Micro: 0.7795, F1 Macro: 0.7791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9047, F1 Micro: 0.7834, F1 Macro: 0.7795\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.9058, F1 Micro: 0.779, F1 Macro: 0.7732\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9047, F1 Micro: 0.7834, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.65      0.68      0.66       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 187.0333776473999 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8975, F1 Micro: 0.7654, F1 Macro: 0.7626\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.8877, F1 Micro: 0.732, F1 Macro: 0.7204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2581, Accuracy: 0.9008, F1 Micro: 0.7754, F1 Macro: 0.7745\n",
      "Epoch 3/10, Train Loss: 0.2091, Accuracy: 0.8998, F1 Micro: 0.7611, F1 Macro: 0.7537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9038, F1 Micro: 0.7762, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1325, Accuracy: 0.9011, F1 Micro: 0.7699, F1 Macro: 0.7688\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.9009, F1 Micro: 0.7719, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9056, F1 Micro: 0.7799, F1 Macro: 0.7785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0518, Accuracy: 0.9014, F1 Micro: 0.7807, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.9025, F1 Micro: 0.7716, F1 Macro: 0.7644\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.9017, F1 Micro: 0.7705, F1 Macro: 0.7628\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9014, F1 Micro: 0.7807, F1 Macro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.69      0.79      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 189.7946174144745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3992, Accuracy: 0.8852, F1 Micro: 0.7152, F1 Macro: 0.6917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.8984, F1 Micro: 0.7682, F1 Macro: 0.7655\n",
      "Epoch 3/10, Train Loss: 0.2103, Accuracy: 0.9016, F1 Micro: 0.7619, F1 Macro: 0.7496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1717, Accuracy: 0.905, F1 Micro: 0.7693, F1 Macro: 0.7566\n",
      "Epoch 5/10, Train Loss: 0.1396, Accuracy: 0.9003, F1 Micro: 0.769, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9069, F1 Micro: 0.7771, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.8945, F1 Micro: 0.7801, F1 Macro: 0.7816\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9042, F1 Micro: 0.7752, F1 Macro: 0.7723\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.9044, F1 Micro: 0.7661, F1 Macro: 0.7517\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9033, F1 Micro: 0.7707, F1 Macro: 0.7644\n",
      "Model 2 - Iteration 5441: Accuracy: 0.8945, F1 Micro: 0.7801, F1 Macro: 0.7816\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.55      0.85      0.67       248\n",
      "         radikalisme       0.74      0.86      0.80       243\n",
      "pencemaran_nama_baik       0.66      0.86      0.75       504\n",
      "\n",
      "           micro avg       0.70      0.88      0.78      1365\n",
      "           macro avg       0.71      0.87      0.78      1365\n",
      "        weighted avg       0.72      0.88      0.79      1365\n",
      "         samples avg       0.47      0.49      0.47      1365\n",
      "\n",
      "Training completed in 188.7442548274994 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3853, Accuracy: 0.8861, F1 Micro: 0.7262, F1 Macro: 0.7073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9013, F1 Micro: 0.768, F1 Macro: 0.7641\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.8986, F1 Micro: 0.7579, F1 Macro: 0.7466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9033, F1 Micro: 0.7696, F1 Macro: 0.7628\n",
      "Epoch 5/10, Train Loss: 0.1372, Accuracy: 0.8997, F1 Micro: 0.7652, F1 Macro: 0.7628\n",
      "Epoch 6/10, Train Loss: 0.0959, Accuracy: 0.9019, F1 Micro: 0.7678, F1 Macro: 0.7635\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9019, F1 Micro: 0.7573, F1 Macro: 0.7465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.9056, F1 Micro: 0.7775, F1 Macro: 0.7754\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9031, F1 Micro: 0.7659, F1 Macro: 0.7523\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9044, F1 Micro: 0.7671, F1 Macro: 0.7599\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9056, F1 Micro: 0.7775, F1 Macro: 0.7754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.67      0.68      0.68       248\n",
      "         radikalisme       0.77      0.80      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1365\n",
      "           macro avg       0.78      0.77      0.78      1365\n",
      "        weighted avg       0.78      0.77      0.78      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 186.9197428226471 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.8977, F1 Micro: 0.7661, F1 Macro: 0.7635\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3819, Accuracy: 0.8856, F1 Micro: 0.7012, F1 Macro: 0.6946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2558, Accuracy: 0.8989, F1 Micro: 0.7668, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9008, F1 Micro: 0.777, F1 Macro: 0.7709\n",
      "Epoch 4/10, Train Loss: 0.1753, Accuracy: 0.905, F1 Micro: 0.7723, F1 Macro: 0.7625\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9053, F1 Micro: 0.7701, F1 Macro: 0.7643\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9025, F1 Micro: 0.7762, F1 Macro: 0.7759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9022, F1 Micro: 0.7788, F1 Macro: 0.7777\n",
      "Epoch 8/10, Train Loss: 0.0553, Accuracy: 0.9039, F1 Micro: 0.7578, F1 Macro: 0.7442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.7776\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9027, F1 Micro: 0.7719, F1 Macro: 0.7675\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.7776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.66      0.69      0.68       248\n",
      "         radikalisme       0.77      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.78      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 194.5326611995697 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3956, Accuracy: 0.8759, F1 Micro: 0.6545, F1 Macro: 0.6317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.9002, F1 Micro: 0.7599, F1 Macro: 0.7484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2141, Accuracy: 0.905, F1 Micro: 0.781, F1 Macro: 0.772\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9078, F1 Micro: 0.772, F1 Macro: 0.7615\n",
      "Epoch 5/10, Train Loss: 0.126, Accuracy: 0.9047, F1 Micro: 0.7775, F1 Macro: 0.7731\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9041, F1 Micro: 0.7766, F1 Macro: 0.7755\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.9013, F1 Micro: 0.7764, F1 Macro: 0.7769\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9023, F1 Micro: 0.7711, F1 Macro: 0.7641\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.9053, F1 Micro: 0.7756, F1 Macro: 0.7734\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9041, F1 Micro: 0.7729, F1 Macro: 0.7655\n",
      "Model 2 - Iteration 5641: Accuracy: 0.905, F1 Micro: 0.781, F1 Macro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.69      0.61      0.65       248\n",
      "         radikalisme       0.70      0.89      0.78       243\n",
      "pencemaran_nama_baik       0.74      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.76      0.79      0.77      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 191.2318832874298 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8773, F1 Micro: 0.6597, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2563, Accuracy: 0.8978, F1 Micro: 0.7596, F1 Macro: 0.751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9038, F1 Micro: 0.7787, F1 Macro: 0.7713\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9033, F1 Micro: 0.7653, F1 Macro: 0.754\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9027, F1 Micro: 0.7627, F1 Macro: 0.7533\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9028, F1 Micro: 0.7769, F1 Macro: 0.7741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9036, F1 Micro: 0.781, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9036, F1 Micro: 0.7705, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.9055, F1 Micro: 0.7849, F1 Macro: 0.7854\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.9038, F1 Micro: 0.7757, F1 Macro: 0.7686\n",
      "Model 3 - Iteration 5641: Accuracy: 0.9055, F1 Micro: 0.7849, F1 Macro: 0.7854\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.76      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.45      1365\n",
      "\n",
      "Training completed in 194.3523290157318 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.8981, F1 Micro: 0.767, F1 Macro: 0.7643\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.376, Accuracy: 0.8883, F1 Micro: 0.7299, F1 Macro: 0.7192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.252, Accuracy: 0.8978, F1 Micro: 0.7642, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2014, Accuracy: 0.9055, F1 Micro: 0.775, F1 Macro: 0.7726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9017, F1 Micro: 0.7881, F1 Macro: 0.7875\n",
      "Epoch 5/10, Train Loss: 0.1328, Accuracy: 0.9017, F1 Micro: 0.7803, F1 Macro: 0.7759\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.903, F1 Micro: 0.78, F1 Macro: 0.7789\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9062, F1 Micro: 0.7825, F1 Macro: 0.7834\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9041, F1 Micro: 0.7748, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.0362, Accuracy: 0.9073, F1 Micro: 0.7851, F1 Macro: 0.7814\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9023, F1 Micro: 0.7799, F1 Macro: 0.7781\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9017, F1 Micro: 0.7881, F1 Macro: 0.7875\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.94      0.91       370\n",
      "                sara       0.62      0.80      0.70       248\n",
      "         radikalisme       0.71      0.90      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.73      0.86      0.79      1365\n",
      "           macro avg       0.73      0.86      0.79      1365\n",
      "        weighted avg       0.74      0.86      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 197.66688656806946 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3926, Accuracy: 0.888, F1 Micro: 0.7231, F1 Macro: 0.7149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2551, Accuracy: 0.8997, F1 Micro: 0.764, F1 Macro: 0.7646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.9058, F1 Micro: 0.7867, F1 Macro: 0.7857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9052, F1 Micro: 0.7902, F1 Macro: 0.7879\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9034, F1 Micro: 0.7881, F1 Macro: 0.7849\n",
      "Epoch 6/10, Train Loss: 0.0968, Accuracy: 0.9072, F1 Micro: 0.7872, F1 Macro: 0.7807\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.9059, F1 Micro: 0.7782, F1 Macro: 0.7752\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.9061, F1 Micro: 0.7814, F1 Macro: 0.7761\n",
      "Epoch 9/10, Train Loss: 0.0356, Accuracy: 0.9067, F1 Micro: 0.7855, F1 Macro: 0.781\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9023, F1 Micro: 0.7789, F1 Macro: 0.7748\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9052, F1 Micro: 0.7902, F1 Macro: 0.7879\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.72      0.89      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 198.60256791114807 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8883, F1 Micro: 0.7223, F1 Macro: 0.7108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2529, Accuracy: 0.8994, F1 Micro: 0.7658, F1 Macro: 0.7648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2016, Accuracy: 0.9059, F1 Micro: 0.782, F1 Macro: 0.7793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9039, F1 Micro: 0.7879, F1 Macro: 0.786\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9019, F1 Micro: 0.7806, F1 Macro: 0.7756\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9031, F1 Micro: 0.7768, F1 Macro: 0.7732\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9058, F1 Micro: 0.7732, F1 Macro: 0.7673\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9053, F1 Micro: 0.7783, F1 Macro: 0.7741\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9038, F1 Micro: 0.7842, F1 Macro: 0.7832\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.9058, F1 Micro: 0.7784, F1 Macro: 0.7721\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9039, F1 Micro: 0.7879, F1 Macro: 0.786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.71      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.84      0.79      1365\n",
      "           macro avg       0.74      0.84      0.79      1365\n",
      "        weighted avg       0.75      0.84      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 198.0443410873413 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.8984, F1 Micro: 0.7681, F1 Macro: 0.7655\n",
      "Launching training on 2 GPUs.\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3676, Accuracy: 0.8923, F1 Micro: 0.7353, F1 Macro: 0.7243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.241, Accuracy: 0.9025, F1 Micro: 0.7557, F1 Macro: 0.7433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1972, Accuracy: 0.905, F1 Micro: 0.7805, F1 Macro: 0.7767\n",
      "Epoch 4/10, Train Loss: 0.1544, Accuracy: 0.9045, F1 Micro: 0.7724, F1 Macro: 0.7595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9091, F1 Micro: 0.783, F1 Macro: 0.7815\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9036, F1 Micro: 0.7727, F1 Macro: 0.7684\n",
      "Epoch 7/10, Train Loss: 0.0665, Accuracy: 0.9052, F1 Micro: 0.779, F1 Macro: 0.7755\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.9052, F1 Micro: 0.7763, F1 Macro: 0.7744\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9039, F1 Micro: 0.7675, F1 Macro: 0.7653\n",
      "Epoch 10/10, Train Loss: 0.0292, Accuracy: 0.9033, F1 Micro: 0.7779, F1 Macro: 0.7743\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9091, F1 Micro: 0.783, F1 Macro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.68      0.66      0.67       248\n",
      "         radikalisme       0.77      0.86      0.82       243\n",
      "pencemaran_nama_baik       0.76      0.69      0.73       504\n",
      "\n",
      "           micro avg       0.80      0.77      0.78      1365\n",
      "           macro avg       0.79      0.78      0.78      1365\n",
      "        weighted avg       0.80      0.77      0.78      1365\n",
      "         samples avg       0.44      0.43      0.43      1365\n",
      "\n",
      "Training completed in 203.0396227836609 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8906, F1 Micro: 0.7159, F1 Macro: 0.7071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2468, Accuracy: 0.898, F1 Micro: 0.7342, F1 Macro: 0.7175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.9059, F1 Micro: 0.772, F1 Macro: 0.7634\n",
      "Epoch 4/10, Train Loss: 0.1613, Accuracy: 0.9017, F1 Micro: 0.7661, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.126, Accuracy: 0.9052, F1 Micro: 0.786, F1 Macro: 0.7863\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.7653\n",
      "Epoch 7/10, Train Loss: 0.066, Accuracy: 0.9016, F1 Micro: 0.7603, F1 Macro: 0.755\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9022, F1 Micro: 0.7715, F1 Macro: 0.7676\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.9027, F1 Micro: 0.7784, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.9027, F1 Micro: 0.7739, F1 Macro: 0.7658\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9052, F1 Micro: 0.786, F1 Macro: 0.7863\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.63      0.77      0.70       248\n",
      "         radikalisme       0.73      0.89      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.83      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 203.2099313735962 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.37, Accuracy: 0.8909, F1 Micro: 0.7338, F1 Macro: 0.7218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2409, Accuracy: 0.8998, F1 Micro: 0.7469, F1 Macro: 0.7335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1967, Accuracy: 0.9064, F1 Micro: 0.7737, F1 Macro: 0.7675\n",
      "Epoch 4/10, Train Loss: 0.1559, Accuracy: 0.9034, F1 Micro: 0.7735, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.905, F1 Micro: 0.7813, F1 Macro: 0.7791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9052, F1 Micro: 0.7845, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.065, Accuracy: 0.905, F1 Micro: 0.769, F1 Macro: 0.7593\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.9033, F1 Micro: 0.7679, F1 Macro: 0.7635\n",
      "Epoch 9/10, Train Loss: 0.0377, Accuracy: 0.9039, F1 Micro: 0.7703, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9072, F1 Micro: 0.7831, F1 Macro: 0.7765\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9052, F1 Micro: 0.7845, F1 Macro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.75      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 205.472421169281 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.8988, F1 Micro: 0.7688, F1 Macro: 0.7663\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.574284315109253 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3618, Accuracy: 0.8797, F1 Micro: 0.6559, F1 Macro: 0.6245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2306, Accuracy: 0.8978, F1 Micro: 0.7322, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.9005, F1 Micro: 0.7724, F1 Macro: 0.7666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.8994, F1 Micro: 0.7788, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.9009, F1 Micro: 0.7645, F1 Macro: 0.7559\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9027, F1 Micro: 0.7742, F1 Macro: 0.771\n",
      "Epoch 7/10, Train Loss: 0.0667, Accuracy: 0.8981, F1 Micro: 0.7651, F1 Macro: 0.7609\n",
      "Epoch 8/10, Train Loss: 0.0481, Accuracy: 0.8972, F1 Micro: 0.7683, F1 Macro: 0.7677\n",
      "Epoch 9/10, Train Loss: 0.0337, Accuracy: 0.8997, F1 Micro: 0.7719, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.9009, F1 Micro: 0.7772, F1 Macro: 0.7774\n",
      "Model 1 - Iteration 6218: Accuracy: 0.8994, F1 Micro: 0.7788, F1 Macro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.69      0.90      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 207.8944320678711 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3775, Accuracy: 0.8822, F1 Micro: 0.6736, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2337, Accuracy: 0.8959, F1 Micro: 0.7195, F1 Macro: 0.7031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.9003, F1 Micro: 0.7746, F1 Macro: 0.7699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.8984, F1 Micro: 0.7786, F1 Macro: 0.7761\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9038, F1 Micro: 0.7732, F1 Macro: 0.7668\n",
      "Epoch 6/10, Train Loss: 0.0887, Accuracy: 0.9053, F1 Micro: 0.7691, F1 Macro: 0.7564\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.9047, F1 Micro: 0.7659, F1 Macro: 0.7559\n",
      "Epoch 8/10, Train Loss: 0.0495, Accuracy: 0.9044, F1 Micro: 0.7699, F1 Macro: 0.767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9045, F1 Micro: 0.7805, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0283, Accuracy: 0.9013, F1 Micro: 0.7773, F1 Macro: 0.7757\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9045, F1 Micro: 0.7805, F1 Macro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.68      0.63      0.65       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 210.14538383483887 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3645, Accuracy: 0.8814, F1 Micro: 0.6749, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.8975, F1 Micro: 0.7314, F1 Macro: 0.7152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1934, Accuracy: 0.8997, F1 Micro: 0.772, F1 Macro: 0.7682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1511, Accuracy: 0.9019, F1 Micro: 0.7765, F1 Macro: 0.7732\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.8966, F1 Micro: 0.7564, F1 Macro: 0.7513\n",
      "Epoch 6/10, Train Loss: 0.089, Accuracy: 0.9031, F1 Micro: 0.7749, F1 Macro: 0.7705\n",
      "Epoch 7/10, Train Loss: 0.0641, Accuracy: 0.8983, F1 Micro: 0.7639, F1 Macro: 0.7572\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9008, F1 Micro: 0.759, F1 Macro: 0.7527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.9009, F1 Micro: 0.7774, F1 Macro: 0.7715\n",
      "Epoch 10/10, Train Loss: 0.0277, Accuracy: 0.902, F1 Micro: 0.7702, F1 Macro: 0.7636\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9009, F1 Micro: 0.7774, F1 Macro: 0.7715\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       370\n",
      "                sara       0.64      0.64      0.64       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.77      1365\n",
      "        weighted avg       0.75      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 209.8761384487152 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.8989, F1 Micro: 0.7693, F1 Macro: 0.7667\n",
      "Total sampling time: 1008.69 seconds\n",
      "Total runtime: 11105.223088741302 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c470132",
   "metadata": {
    "papermill": {
     "duration": 0.127956,
     "end_time": "2025-03-13T10:00:45.862168",
     "exception": false,
     "start_time": "2025-03-13T10:00:45.734212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436474da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5781, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.432, Accuracy: 0.788, F1 Micro: 0.0117, F1 Macro: 0.0106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3729, Accuracy: 0.8223, F1 Micro: 0.2889, F1 Macro: 0.2126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3601, Accuracy: 0.83, F1 Micro: 0.3747, F1 Macro: 0.2926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2934, Accuracy: 0.8384, F1 Micro: 0.4405, F1 Macro: 0.3659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2592, Accuracy: 0.8566, F1 Micro: 0.5816, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2344, Accuracy: 0.8592, F1 Micro: 0.588, F1 Macro: 0.5698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1829, Accuracy: 0.8694, F1 Micro: 0.6384, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1544, Accuracy: 0.8728, F1 Micro: 0.6921, F1 Macro: 0.6849\n",
      "Epoch 10/10, Train Loss: 0.1323, Accuracy: 0.8736, F1 Micro: 0.6645, F1 Macro: 0.6516\n",
      "Model 1 - Iteration 388: Accuracy: 0.8728, F1 Micro: 0.6921, F1 Macro: 0.6849\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.82      0.86       370\n",
      "                sara       0.58      0.52      0.55       248\n",
      "         radikalisme       0.67      0.72      0.70       243\n",
      "pencemaran_nama_baik       0.68      0.61      0.64       504\n",
      "\n",
      "           micro avg       0.72      0.67      0.69      1365\n",
      "           macro avg       0.70      0.67      0.68      1365\n",
      "        weighted avg       0.72      0.67      0.69      1365\n",
      "         samples avg       0.38      0.37      0.36      1365\n",
      "\n",
      "Training completed in 57.44107723236084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5897, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4473, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3959, Accuracy: 0.8066, F1 Micro: 0.1702, F1 Macro: 0.1278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3926, Accuracy: 0.8258, F1 Micro: 0.3197, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3251, Accuracy: 0.8391, F1 Micro: 0.4372, F1 Macro: 0.3627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2762, Accuracy: 0.8562, F1 Micro: 0.5611, F1 Macro: 0.5364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2538, Accuracy: 0.8605, F1 Micro: 0.6061, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1942, Accuracy: 0.8698, F1 Micro: 0.6481, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1598, Accuracy: 0.8733, F1 Micro: 0.6915, F1 Macro: 0.6872\n",
      "Epoch 10/10, Train Loss: 0.1373, Accuracy: 0.8755, F1 Micro: 0.6886, F1 Macro: 0.6828\n",
      "Model 2 - Iteration 388: Accuracy: 0.8733, F1 Micro: 0.6915, F1 Macro: 0.6872\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.84      0.86       370\n",
      "                sara       0.60      0.54      0.57       248\n",
      "         radikalisme       0.67      0.74      0.70       243\n",
      "pencemaran_nama_baik       0.68      0.57      0.62       504\n",
      "\n",
      "           micro avg       0.72      0.67      0.69      1365\n",
      "           macro avg       0.71      0.67      0.69      1365\n",
      "        weighted avg       0.72      0.67      0.69      1365\n",
      "         samples avg       0.37      0.37      0.36      1365\n",
      "\n",
      "Training completed in 58.01144480705261 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5583, Accuracy: 0.7867, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.43, Accuracy: 0.7978, F1 Micro: 0.0989, F1 Macro: 0.0805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.363, Accuracy: 0.8225, F1 Micro: 0.2996, F1 Macro: 0.2036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3502, Accuracy: 0.8367, F1 Micro: 0.4397, F1 Macro: 0.3464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2882, Accuracy: 0.85, F1 Micro: 0.5289, F1 Macro: 0.4943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.24, Accuracy: 0.857, F1 Micro: 0.6037, F1 Macro: 0.5962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2211, Accuracy: 0.8647, F1 Micro: 0.6264, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1673, Accuracy: 0.867, F1 Micro: 0.6352, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1339, Accuracy: 0.8692, F1 Micro: 0.6926, F1 Macro: 0.6863\n",
      "Epoch 10/10, Train Loss: 0.1203, Accuracy: 0.8734, F1 Micro: 0.6865, F1 Macro: 0.6788\n",
      "Model 3 - Iteration 388: Accuracy: 0.8692, F1 Micro: 0.6926, F1 Macro: 0.6863\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.85      0.87       370\n",
      "                sara       0.56      0.56      0.56       248\n",
      "         radikalisme       0.67      0.70      0.68       243\n",
      "pencemaran_nama_baik       0.64      0.63      0.64       504\n",
      "\n",
      "           micro avg       0.69      0.69      0.69      1365\n",
      "           macro avg       0.69      0.69      0.69      1365\n",
      "        weighted avg       0.70      0.69      0.69      1365\n",
      "         samples avg       0.38      0.38      0.37      1365\n",
      "\n",
      "Training completed in 58.40836501121521 s\n",
      "Averaged - Iteration 388: Accuracy: 0.8718, F1 Micro: 0.6921, F1 Macro: 0.6861\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 123.12998557090759 seconds\n",
      "New train size: 971\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.8175, F1 Micro: 0.2561, F1 Macro: 0.1877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3734, Accuracy: 0.8433, F1 Micro: 0.4522, F1 Macro: 0.366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3072, Accuracy: 0.8644, F1 Micro: 0.5985, F1 Macro: 0.5626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2386, Accuracy: 0.8809, F1 Micro: 0.6885, F1 Macro: 0.6774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2008, Accuracy: 0.8845, F1 Micro: 0.708, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.169, Accuracy: 0.8897, F1 Micro: 0.7276, F1 Macro: 0.7207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1303, Accuracy: 0.892, F1 Micro: 0.7276, F1 Macro: 0.7151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.8877, F1 Micro: 0.7383, F1 Macro: 0.7259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0849, Accuracy: 0.8872, F1 Micro: 0.7408, F1 Macro: 0.7328\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.8877, F1 Micro: 0.7132, F1 Macro: 0.7011\n",
      "Model 1 - Iteration 971: Accuracy: 0.8872, F1 Micro: 0.7408, F1 Macro: 0.7328\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.69      0.58      0.63       248\n",
      "         radikalisme       0.73      0.70      0.71       243\n",
      "pencemaran_nama_baik       0.64      0.79      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.76      0.74      1365\n",
      "           macro avg       0.74      0.73      0.73      1365\n",
      "        weighted avg       0.74      0.76      0.74      1365\n",
      "         samples avg       0.43      0.43      0.42      1365\n",
      "\n",
      "Training completed in 75.21419358253479 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5478, Accuracy: 0.8145, F1 Micro: 0.2376, F1 Macro: 0.1772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3898, Accuracy: 0.8445, F1 Micro: 0.4749, F1 Macro: 0.3673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3078, Accuracy: 0.8642, F1 Micro: 0.5997, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2422, Accuracy: 0.8792, F1 Micro: 0.6851, F1 Macro: 0.6683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1967, Accuracy: 0.8845, F1 Micro: 0.7071, F1 Macro: 0.6958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1633, Accuracy: 0.8889, F1 Micro: 0.7256, F1 Macro: 0.713\n",
      "Epoch 7/10, Train Loss: 0.1287, Accuracy: 0.8881, F1 Micro: 0.7174, F1 Macro: 0.7023\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.8894, F1 Micro: 0.7224, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.8845, F1 Micro: 0.7306, F1 Macro: 0.7196\n",
      "Epoch 10/10, Train Loss: 0.058, Accuracy: 0.8863, F1 Micro: 0.7194, F1 Macro: 0.7077\n",
      "Model 2 - Iteration 971: Accuracy: 0.8845, F1 Micro: 0.7306, F1 Macro: 0.7196\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       370\n",
      "                sara       0.67      0.51      0.58       248\n",
      "         radikalisme       0.74      0.70      0.72       243\n",
      "pencemaran_nama_baik       0.64      0.76      0.69       504\n",
      "\n",
      "           micro avg       0.73      0.73      0.73      1365\n",
      "           macro avg       0.74      0.71      0.72      1365\n",
      "        weighted avg       0.73      0.73      0.73      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 71.4118971824646 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5151, Accuracy: 0.8189, F1 Micro: 0.2954, F1 Macro: 0.2167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3611, Accuracy: 0.852, F1 Micro: 0.5462, F1 Macro: 0.4672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.292, Accuracy: 0.8691, F1 Micro: 0.6357, F1 Macro: 0.6296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2304, Accuracy: 0.8778, F1 Micro: 0.6872, F1 Macro: 0.6776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1954, Accuracy: 0.8847, F1 Micro: 0.7088, F1 Macro: 0.698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1651, Accuracy: 0.8898, F1 Micro: 0.7243, F1 Macro: 0.7123\n",
      "Epoch 7/10, Train Loss: 0.1268, Accuracy: 0.8894, F1 Micro: 0.7173, F1 Macro: 0.7007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.8913, F1 Micro: 0.7413, F1 Macro: 0.729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0849, Accuracy: 0.8867, F1 Micro: 0.7439, F1 Macro: 0.7357\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.8872, F1 Micro: 0.6997, F1 Macro: 0.6831\n",
      "Model 3 - Iteration 971: Accuracy: 0.8867, F1 Micro: 0.7439, F1 Macro: 0.7357\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       370\n",
      "                sara       0.65      0.58      0.62       248\n",
      "         radikalisme       0.72      0.74      0.73       243\n",
      "pencemaran_nama_baik       0.64      0.79      0.71       504\n",
      "\n",
      "           micro avg       0.72      0.77      0.74      1365\n",
      "           macro avg       0.72      0.75      0.74      1365\n",
      "        weighted avg       0.72      0.77      0.74      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 72.87666058540344 s\n",
      "Averaged - Iteration 971: Accuracy: 0.879, F1 Micro: 0.7153, F1 Macro: 0.7077\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 110.10929274559021 seconds\n",
      "New train size: 1496\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4889, Accuracy: 0.8384, F1 Micro: 0.4441, F1 Macro: 0.3492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3305, Accuracy: 0.8706, F1 Micro: 0.648, F1 Macro: 0.6207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2661, Accuracy: 0.8817, F1 Micro: 0.7056, F1 Macro: 0.6998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2066, Accuracy: 0.8877, F1 Micro: 0.7141, F1 Macro: 0.7038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1671, Accuracy: 0.8884, F1 Micro: 0.7291, F1 Macro: 0.7153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.8905, F1 Micro: 0.7323, F1 Macro: 0.7205\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.8922, F1 Micro: 0.722, F1 Macro: 0.7093\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.8886, F1 Micro: 0.7294, F1 Macro: 0.7222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.8909, F1 Micro: 0.7368, F1 Macro: 0.7321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.8911, F1 Micro: 0.7425, F1 Macro: 0.7393\n",
      "Model 1 - Iteration 1496: Accuracy: 0.8911, F1 Micro: 0.7425, F1 Macro: 0.7393\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       370\n",
      "                sara       0.66      0.61      0.64       248\n",
      "         radikalisme       0.71      0.80      0.75       243\n",
      "pencemaran_nama_baik       0.70      0.66      0.68       504\n",
      "\n",
      "           micro avg       0.75      0.74      0.74      1365\n",
      "           macro avg       0.74      0.74      0.74      1365\n",
      "        weighted avg       0.75      0.74      0.74      1365\n",
      "         samples avg       0.41      0.41      0.40      1365\n",
      "\n",
      "Training completed in 85.58010077476501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4999, Accuracy: 0.8278, F1 Micro: 0.3659, F1 Macro: 0.2587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3346, Accuracy: 0.8763, F1 Micro: 0.6845, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.26, Accuracy: 0.883, F1 Micro: 0.71, F1 Macro: 0.6979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2016, Accuracy: 0.887, F1 Micro: 0.7172, F1 Macro: 0.6962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1665, Accuracy: 0.8859, F1 Micro: 0.7262, F1 Macro: 0.7081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.8956, F1 Micro: 0.7524, F1 Macro: 0.7486\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.8955, F1 Micro: 0.7369, F1 Macro: 0.7249\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.8917, F1 Micro: 0.7481, F1 Macro: 0.7393\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.8916, F1 Micro: 0.7318, F1 Macro: 0.7223\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.8889, F1 Micro: 0.7494, F1 Macro: 0.7458\n",
      "Model 2 - Iteration 1496: Accuracy: 0.8956, F1 Micro: 0.7524, F1 Macro: 0.7486\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       370\n",
      "                sara       0.69      0.61      0.65       248\n",
      "         radikalisme       0.71      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.71      0.69      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.74      0.75      1365\n",
      "           macro avg       0.76      0.74      0.75      1365\n",
      "        weighted avg       0.76      0.74      0.75      1365\n",
      "         samples avg       0.42      0.42      0.41      1365\n",
      "\n",
      "Training completed in 82.5954384803772 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4712, Accuracy: 0.8331, F1 Micro: 0.4093, F1 Macro: 0.2869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3227, Accuracy: 0.877, F1 Micro: 0.6856, F1 Macro: 0.6764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.258, Accuracy: 0.8811, F1 Micro: 0.7116, F1 Macro: 0.7032\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.8859, F1 Micro: 0.7101, F1 Macro: 0.6912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1599, Accuracy: 0.8905, F1 Micro: 0.7385, F1 Macro: 0.7303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.8933, F1 Micro: 0.74, F1 Macro: 0.7292\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.8916, F1 Micro: 0.7312, F1 Macro: 0.718\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.8881, F1 Micro: 0.7106, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0695, Accuracy: 0.893, F1 Micro: 0.7492, F1 Macro: 0.7409\n",
      "Epoch 10/10, Train Loss: 0.0534, Accuracy: 0.8916, F1 Micro: 0.7323, F1 Macro: 0.7245\n",
      "Model 3 - Iteration 1496: Accuracy: 0.893, F1 Micro: 0.7492, F1 Macro: 0.7409\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.67      0.56      0.61       248\n",
      "         radikalisme       0.71      0.82      0.76       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.75      0.75      1365\n",
      "           macro avg       0.74      0.74      0.74      1365\n",
      "        weighted avg       0.75      0.75      0.75      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 83.62594413757324 s\n",
      "Averaged - Iteration 1496: Accuracy: 0.8837, F1 Micro: 0.7262, F1 Macro: 0.7195\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 99.46295166015625 seconds\n",
      "New train size: 1969\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4712, Accuracy: 0.8361, F1 Micro: 0.422, F1 Macro: 0.32\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3265, Accuracy: 0.8777, F1 Micro: 0.717, F1 Macro: 0.7155\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.8839, F1 Micro: 0.7137, F1 Macro: 0.7147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2184, Accuracy: 0.8873, F1 Micro: 0.7297, F1 Macro: 0.7185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1721, Accuracy: 0.8925, F1 Micro: 0.7335, F1 Macro: 0.7284\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.8902, F1 Micro: 0.7055, F1 Macro: 0.6994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.8889, F1 Micro: 0.7344, F1 Macro: 0.7312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0782, Accuracy: 0.8911, F1 Micro: 0.7492, F1 Macro: 0.7433\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.8884, F1 Micro: 0.7468, F1 Macro: 0.7449\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.8919, F1 Micro: 0.7408, F1 Macro: 0.7339\n",
      "Model 1 - Iteration 1969: Accuracy: 0.8911, F1 Micro: 0.7492, F1 Macro: 0.7433\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       370\n",
      "                sara       0.63      0.59      0.61       248\n",
      "         radikalisme       0.73      0.80      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.74      0.71       504\n",
      "\n",
      "           micro avg       0.74      0.76      0.75      1365\n",
      "           macro avg       0.74      0.75      0.74      1365\n",
      "        weighted avg       0.74      0.76      0.75      1365\n",
      "         samples avg       0.42      0.43      0.42      1365\n",
      "\n",
      "Training completed in 94.19397616386414 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4846, Accuracy: 0.8352, F1 Micro: 0.4181, F1 Macro: 0.2992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3298, Accuracy: 0.8739, F1 Micro: 0.709, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.248, Accuracy: 0.8856, F1 Micro: 0.7193, F1 Macro: 0.7189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2168, Accuracy: 0.8881, F1 Micro: 0.732, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.8928, F1 Micro: 0.744, F1 Macro: 0.7355\n",
      "Epoch 6/10, Train Loss: 0.1427, Accuracy: 0.8969, F1 Micro: 0.7334, F1 Macro: 0.7225\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.8938, F1 Micro: 0.7399, F1 Macro: 0.7339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.8934, F1 Micro: 0.7482, F1 Macro: 0.7436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.8948, F1 Micro: 0.7482, F1 Macro: 0.7467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.8916, F1 Micro: 0.7551, F1 Macro: 0.753\n",
      "Model 2 - Iteration 1969: Accuracy: 0.8916, F1 Micro: 0.7551, F1 Macro: 0.753\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       370\n",
      "                sara       0.64      0.65      0.64       248\n",
      "         radikalisme       0.70      0.87      0.77       243\n",
      "pencemaran_nama_baik       0.67      0.75      0.71       504\n",
      "\n",
      "           micro avg       0.73      0.78      0.76      1365\n",
      "           macro avg       0.73      0.78      0.75      1365\n",
      "        weighted avg       0.74      0.78      0.76      1365\n",
      "         samples avg       0.43      0.44      0.43      1365\n",
      "\n",
      "Training completed in 97.01163721084595 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4565, Accuracy: 0.8427, F1 Micro: 0.4817, F1 Macro: 0.3861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.321, Accuracy: 0.8753, F1 Micro: 0.717, F1 Macro: 0.7137\n",
      "Epoch 3/10, Train Loss: 0.2465, Accuracy: 0.8825, F1 Micro: 0.7099, F1 Macro: 0.7113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2166, Accuracy: 0.8906, F1 Micro: 0.739, F1 Macro: 0.7351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.8953, F1 Micro: 0.7452, F1 Macro: 0.7392\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.8894, F1 Micro: 0.7035, F1 Macro: 0.6903\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.892, F1 Micro: 0.7382, F1 Macro: 0.7344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.8936, F1 Micro: 0.7523, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.8905, F1 Micro: 0.761, F1 Macro: 0.7623\n",
      "Epoch 10/10, Train Loss: 0.0505, Accuracy: 0.8945, F1 Micro: 0.7415, F1 Macro: 0.7354\n",
      "Model 3 - Iteration 1969: Accuracy: 0.8905, F1 Micro: 0.761, F1 Macro: 0.7623\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       370\n",
      "                sara       0.63      0.75      0.68       248\n",
      "         radikalisme       0.70      0.84      0.76       243\n",
      "pencemaran_nama_baik       0.65      0.80      0.72       504\n",
      "\n",
      "           micro avg       0.71      0.82      0.76      1365\n",
      "           macro avg       0.72      0.82      0.76      1365\n",
      "        weighted avg       0.72      0.82      0.76      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 93.84664011001587 s\n",
      "Averaged - Iteration 1969: Accuracy: 0.8855, F1 Micro: 0.7334, F1 Macro: 0.7278\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 89.18811392784119 seconds\n",
      "New train size: 2394\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4678, Accuracy: 0.8531, F1 Micro: 0.5446, F1 Macro: 0.4532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2953, Accuracy: 0.8806, F1 Micro: 0.6909, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2489, Accuracy: 0.8834, F1 Micro: 0.7505, F1 Macro: 0.7441\n",
      "Epoch 4/10, Train Loss: 0.2046, Accuracy: 0.8933, F1 Micro: 0.7356, F1 Macro: 0.7331\n",
      "Epoch 5/10, Train Loss: 0.1621, Accuracy: 0.8956, F1 Micro: 0.7504, F1 Macro: 0.7414\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.8952, F1 Micro: 0.745, F1 Macro: 0.7316\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.8953, F1 Micro: 0.7397, F1 Macro: 0.7353\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.8948, F1 Micro: 0.7484, F1 Macro: 0.7444\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.8953, F1 Micro: 0.7481, F1 Macro: 0.7453\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.8933, F1 Micro: 0.7356, F1 Macro: 0.7313\n",
      "Model 1 - Iteration 2394: Accuracy: 0.8834, F1 Micro: 0.7505, F1 Macro: 0.7441\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.84      0.91      0.87       370\n",
      "                sara       0.64      0.60      0.62       248\n",
      "         radikalisme       0.67      0.89      0.76       243\n",
      "pencemaran_nama_baik       0.63      0.83      0.71       504\n",
      "\n",
      "           micro avg       0.69      0.82      0.75      1365\n",
      "           macro avg       0.69      0.81      0.74      1365\n",
      "        weighted avg       0.69      0.82      0.75      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 102.1421639919281 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4787, Accuracy: 0.8536, F1 Micro: 0.5341, F1 Macro: 0.4501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2973, Accuracy: 0.882, F1 Micro: 0.7015, F1 Macro: 0.7\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2496, Accuracy: 0.8858, F1 Micro: 0.754, F1 Macro: 0.7502\n",
      "Epoch 4/10, Train Loss: 0.2047, Accuracy: 0.8945, F1 Micro: 0.735, F1 Macro: 0.7311\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.8959, F1 Micro: 0.7519, F1 Macro: 0.7413\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.8961, F1 Micro: 0.7457, F1 Macro: 0.7325\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.8978, F1 Micro: 0.751, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.8941, F1 Micro: 0.7599, F1 Macro: 0.7542\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.8906, F1 Micro: 0.7362, F1 Macro: 0.7285\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.898, F1 Micro: 0.7503, F1 Macro: 0.7394\n",
      "Model 2 - Iteration 2394: Accuracy: 0.8941, F1 Micro: 0.7599, F1 Macro: 0.7542\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       370\n",
      "                sara       0.63      0.62      0.63       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       504\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1365\n",
      "           macro avg       0.73      0.78      0.75      1365\n",
      "        weighted avg       0.74      0.79      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 104.03716373443604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4568, Accuracy: 0.852, F1 Micro: 0.5423, F1 Macro: 0.4399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2948, Accuracy: 0.8803, F1 Micro: 0.6991, F1 Macro: 0.6979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2441, Accuracy: 0.8863, F1 Micro: 0.7525, F1 Macro: 0.744\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.8942, F1 Micro: 0.7401, F1 Macro: 0.7381\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.8978, F1 Micro: 0.7465, F1 Macro: 0.7347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.8983, F1 Micro: 0.7554, F1 Macro: 0.7461\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.8959, F1 Micro: 0.744, F1 Macro: 0.745\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.8945, F1 Micro: 0.7552, F1 Macro: 0.748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.8959, F1 Micro: 0.7583, F1 Macro: 0.7544\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.8956, F1 Micro: 0.7454, F1 Macro: 0.741\n",
      "Model 3 - Iteration 2394: Accuracy: 0.8959, F1 Micro: 0.7583, F1 Macro: 0.7544\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.66      0.61      0.63       248\n",
      "         radikalisme       0.71      0.86      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       504\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1365\n",
      "           macro avg       0.75      0.76      0.75      1365\n",
      "        weighted avg       0.76      0.77      0.76      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 105.55590081214905 s\n",
      "Averaged - Iteration 2394: Accuracy: 0.8867, F1 Micro: 0.738, F1 Macro: 0.7324\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 80.79927515983582 seconds\n",
      "New train size: 2777\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4406, Accuracy: 0.8619, F1 Micro: 0.5885, F1 Macro: 0.523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2835, Accuracy: 0.8878, F1 Micro: 0.7301, F1 Macro: 0.7287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2411, Accuracy: 0.8864, F1 Micro: 0.7439, F1 Macro: 0.7418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.9025, F1 Micro: 0.7663, F1 Macro: 0.7583\n",
      "Epoch 5/10, Train Loss: 0.1481, Accuracy: 0.8955, F1 Micro: 0.7655, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.1163, Accuracy: 0.8939, F1 Micro: 0.7506, F1 Macro: 0.7347\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.8956, F1 Micro: 0.7594, F1 Macro: 0.7518\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.8955, F1 Micro: 0.7527, F1 Macro: 0.7475\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.8973, F1 Micro: 0.7505, F1 Macro: 0.7419\n",
      "Epoch 10/10, Train Loss: 0.0402, Accuracy: 0.8963, F1 Micro: 0.7522, F1 Macro: 0.7457\n",
      "Model 1 - Iteration 2777: Accuracy: 0.9025, F1 Micro: 0.7663, F1 Macro: 0.7583\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.69      0.57      0.63       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.75      0.69      0.72       504\n",
      "\n",
      "           micro avg       0.78      0.75      0.77      1365\n",
      "           macro avg       0.77      0.75      0.76      1365\n",
      "        weighted avg       0.79      0.75      0.76      1365\n",
      "         samples avg       0.43      0.42      0.42      1365\n",
      "\n",
      "Training completed in 115.47609233856201 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4507, Accuracy: 0.8606, F1 Micro: 0.572, F1 Macro: 0.5096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2838, Accuracy: 0.8895, F1 Micro: 0.7286, F1 Macro: 0.7229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2395, Accuracy: 0.8886, F1 Micro: 0.7465, F1 Macro: 0.7399\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.8975, F1 Micro: 0.7447, F1 Macro: 0.7346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.8983, F1 Micro: 0.7544, F1 Macro: 0.7459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9008, F1 Micro: 0.7606, F1 Macro: 0.7512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.895, F1 Micro: 0.7652, F1 Macro: 0.7625\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.898, F1 Micro: 0.7605, F1 Macro: 0.7547\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.8972, F1 Micro: 0.7614, F1 Macro: 0.7527\n",
      "Epoch 10/10, Train Loss: 0.0375, Accuracy: 0.8989, F1 Micro: 0.7556, F1 Macro: 0.7507\n",
      "Model 2 - Iteration 2777: Accuracy: 0.895, F1 Micro: 0.7652, F1 Macro: 0.7625\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.65      0.66      0.65       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.66      0.80      0.72       504\n",
      "\n",
      "           micro avg       0.73      0.80      0.77      1365\n",
      "           macro avg       0.74      0.79      0.76      1365\n",
      "        weighted avg       0.74      0.80      0.77      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 119.20260548591614 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4291, Accuracy: 0.8744, F1 Micro: 0.657, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.8886, F1 Micro: 0.7302, F1 Macro: 0.7267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2391, Accuracy: 0.8905, F1 Micro: 0.7519, F1 Macro: 0.7477\n",
      "Epoch 4/10, Train Loss: 0.1835, Accuracy: 0.8998, F1 Micro: 0.7485, F1 Macro: 0.7403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.8998, F1 Micro: 0.7637, F1 Macro: 0.7605\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.8981, F1 Micro: 0.7534, F1 Macro: 0.7399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.8952, F1 Micro: 0.7704, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.8964, F1 Micro: 0.7499, F1 Macro: 0.7471\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.8988, F1 Micro: 0.7586, F1 Macro: 0.7528\n",
      "Epoch 10/10, Train Loss: 0.0381, Accuracy: 0.9009, F1 Micro: 0.7693, F1 Macro: 0.7672\n",
      "Model 3 - Iteration 2777: Accuracy: 0.8952, F1 Micro: 0.7704, F1 Macro: 0.7689\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.64      0.85      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.82      0.77      1365\n",
      "           macro avg       0.74      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.77      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 117.27783989906311 s\n",
      "Averaged - Iteration 2777: Accuracy: 0.8885, F1 Micro: 0.7429, F1 Macro: 0.7376\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 72.70602941513062 seconds\n",
      "New train size: 3122\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4397, Accuracy: 0.8723, F1 Micro: 0.6617, F1 Macro: 0.6572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8797, F1 Micro: 0.7447, F1 Macro: 0.7486\n",
      "Epoch 3/10, Train Loss: 0.2423, Accuracy: 0.8958, F1 Micro: 0.7404, F1 Macro: 0.7313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1974, Accuracy: 0.9008, F1 Micro: 0.7671, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1615, Accuracy: 0.8991, F1 Micro: 0.769, F1 Macro: 0.762\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9005, F1 Micro: 0.7688, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.8978, F1 Micro: 0.7523, F1 Macro: 0.7385\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.8997, F1 Micro: 0.7577, F1 Macro: 0.7502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9017, F1 Micro: 0.769, F1 Macro: 0.7647\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.8953, F1 Micro: 0.7656, F1 Macro: 0.7642\n",
      "Model 1 - Iteration 3122: Accuracy: 0.9017, F1 Micro: 0.769, F1 Macro: 0.7647\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.66      0.63      0.64       248\n",
      "         radikalisme       0.74      0.83      0.78       243\n",
      "pencemaran_nama_baik       0.73      0.72      0.72       504\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1365\n",
      "           macro avg       0.77      0.77      0.76      1365\n",
      "        weighted avg       0.77      0.77      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Training completed in 126.20595955848694 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4478, Accuracy: 0.8708, F1 Micro: 0.6661, F1 Macro: 0.6657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.285, Accuracy: 0.8839, F1 Micro: 0.7465, F1 Macro: 0.7529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.8981, F1 Micro: 0.7511, F1 Macro: 0.7465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.8995, F1 Micro: 0.764, F1 Macro: 0.7542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.8959, F1 Micro: 0.7711, F1 Macro: 0.7686\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.9014, F1 Micro: 0.7622, F1 Macro: 0.7553\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.8989, F1 Micro: 0.7629, F1 Macro: 0.7565\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.8988, F1 Micro: 0.7605, F1 Macro: 0.751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9028, F1 Micro: 0.7756, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.8975, F1 Micro: 0.7756, F1 Macro: 0.7743\n",
      "Model 2 - Iteration 3122: Accuracy: 0.8975, F1 Micro: 0.7756, F1 Macro: 0.7743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.65      0.68      0.66       248\n",
      "         radikalisme       0.73      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.65      0.84      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.77      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 129.85546922683716 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4281, Accuracy: 0.8728, F1 Micro: 0.6688, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2764, Accuracy: 0.882, F1 Micro: 0.7467, F1 Macro: 0.751\n",
      "Epoch 3/10, Train Loss: 0.2387, Accuracy: 0.8959, F1 Micro: 0.7427, F1 Macro: 0.7368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1904, Accuracy: 0.9016, F1 Micro: 0.7633, F1 Macro: 0.7534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.8959, F1 Micro: 0.7696, F1 Macro: 0.7686\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.8988, F1 Micro: 0.7621, F1 Macro: 0.7581\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.8972, F1 Micro: 0.7574, F1 Macro: 0.7522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9002, F1 Micro: 0.7763, F1 Macro: 0.7766\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.8989, F1 Micro: 0.767, F1 Macro: 0.7679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0379, Accuracy: 0.8998, F1 Micro: 0.7796, F1 Macro: 0.7775\n",
      "Model 3 - Iteration 3122: Accuracy: 0.8998, F1 Micro: 0.7796, F1 Macro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.66      0.70      0.68       248\n",
      "         radikalisme       0.72      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.66      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.74      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 127.54531168937683 s\n",
      "Averaged - Iteration 3122: Accuracy: 0.8901, F1 Micro: 0.7474, F1 Macro: 0.7425\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 66.54523301124573 seconds\n",
      "New train size: 3432\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.8805, F1 Micro: 0.6941, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2836, Accuracy: 0.8902, F1 Micro: 0.712, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.8966, F1 Micro: 0.7631, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1954, Accuracy: 0.902, F1 Micro: 0.7635, F1 Macro: 0.7564\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.8977, F1 Micro: 0.7509, F1 Macro: 0.7457\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9002, F1 Micro: 0.7528, F1 Macro: 0.7411\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.8989, F1 Micro: 0.7464, F1 Macro: 0.739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.8981, F1 Micro: 0.7636, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.8966, F1 Micro: 0.7506, F1 Macro: 0.7426\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.8988, F1 Micro: 0.7557, F1 Macro: 0.753\n",
      "Model 1 - Iteration 3432: Accuracy: 0.8981, F1 Micro: 0.7636, F1 Macro: 0.7613\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       370\n",
      "                sara       0.65      0.68      0.66       248\n",
      "         radikalisme       0.76      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.71      0.70       504\n",
      "\n",
      "           micro avg       0.76      0.77      0.76      1365\n",
      "           macro avg       0.75      0.77      0.76      1365\n",
      "        weighted avg       0.76      0.77      0.76      1365\n",
      "         samples avg       0.44      0.44      0.43      1365\n",
      "\n",
      "Training completed in 135.08714056015015 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4336, Accuracy: 0.8753, F1 Micro: 0.7011, F1 Macro: 0.6865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2874, Accuracy: 0.8922, F1 Micro: 0.7247, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2319, Accuracy: 0.8994, F1 Micro: 0.769, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1935, Accuracy: 0.9045, F1 Micro: 0.7728, F1 Macro: 0.7659\n",
      "Epoch 5/10, Train Loss: 0.1531, Accuracy: 0.9014, F1 Micro: 0.7676, F1 Macro: 0.7637\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.9005, F1 Micro: 0.7636, F1 Macro: 0.7559\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.8988, F1 Micro: 0.7418, F1 Macro: 0.7267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.902, F1 Micro: 0.7739, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9014, F1 Micro: 0.7689, F1 Macro: 0.759\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.9014, F1 Micro: 0.7718, F1 Macro: 0.7698\n",
      "Model 2 - Iteration 3432: Accuracy: 0.902, F1 Micro: 0.7739, F1 Macro: 0.7713\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       370\n",
      "                sara       0.65      0.69      0.67       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.71      0.72       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1365\n",
      "           macro avg       0.75      0.79      0.77      1365\n",
      "        weighted avg       0.76      0.79      0.77      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 134.7673795223236 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4112, Accuracy: 0.8741, F1 Micro: 0.7136, F1 Macro: 0.7035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2787, Accuracy: 0.8936, F1 Micro: 0.7235, F1 Macro: 0.7173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.9003, F1 Micro: 0.7705, F1 Macro: 0.7653\n",
      "Epoch 4/10, Train Loss: 0.1896, Accuracy: 0.8977, F1 Micro: 0.7624, F1 Macro: 0.76\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9008, F1 Micro: 0.7621, F1 Macro: 0.7592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9, F1 Micro: 0.7716, F1 Macro: 0.7683\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.8989, F1 Micro: 0.771, F1 Macro: 0.7688\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.8986, F1 Micro: 0.7611, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9033, F1 Micro: 0.7781, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9005, F1 Micro: 0.7671, F1 Macro: 0.7673\n",
      "Model 3 - Iteration 3432: Accuracy: 0.9033, F1 Micro: 0.7781, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.88      0.92       370\n",
      "                sara       0.68      0.67      0.67       248\n",
      "         radikalisme       0.77      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1365\n",
      "           macro avg       0.77      0.78      0.78      1365\n",
      "        weighted avg       0.77      0.79      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 134.74520635604858 s\n",
      "Averaged - Iteration 3432: Accuracy: 0.8915, F1 Micro: 0.7505, F1 Macro: 0.7459\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 60.516804456710815 seconds\n",
      "New train size: 3711\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4121, Accuracy: 0.8777, F1 Micro: 0.6892, F1 Macro: 0.6861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2817, Accuracy: 0.8931, F1 Micro: 0.7405, F1 Macro: 0.7356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2226, Accuracy: 0.8964, F1 Micro: 0.7413, F1 Macro: 0.7338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.903, F1 Micro: 0.7803, F1 Macro: 0.775\n",
      "Epoch 5/10, Train Loss: 0.1524, Accuracy: 0.9, F1 Micro: 0.7624, F1 Macro: 0.7555\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9009, F1 Micro: 0.7678, F1 Macro: 0.7644\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.8903, F1 Micro: 0.763, F1 Macro: 0.7605\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9, F1 Micro: 0.7603, F1 Macro: 0.7547\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.902, F1 Micro: 0.7617, F1 Macro: 0.755\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.9002, F1 Micro: 0.7622, F1 Macro: 0.7581\n",
      "Model 1 - Iteration 3711: Accuracy: 0.903, F1 Micro: 0.7803, F1 Macro: 0.775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.86      0.90       370\n",
      "                sara       0.67      0.67      0.67       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.81      0.76       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.77      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 140.12180495262146 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.8755, F1 Micro: 0.6711, F1 Macro: 0.6632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2786, Accuracy: 0.8995, F1 Micro: 0.7616, F1 Macro: 0.7616\n",
      "Epoch 3/10, Train Loss: 0.2224, Accuracy: 0.8977, F1 Micro: 0.7507, F1 Macro: 0.7438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.8992, F1 Micro: 0.7769, F1 Macro: 0.773\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.8995, F1 Micro: 0.7623, F1 Macro: 0.7554\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.769\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9003, F1 Micro: 0.7623, F1 Macro: 0.7517\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9019, F1 Micro: 0.7741, F1 Macro: 0.7721\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9008, F1 Micro: 0.7544, F1 Macro: 0.7469\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9, F1 Micro: 0.7721, F1 Macro: 0.7671\n",
      "Model 2 - Iteration 3711: Accuracy: 0.8992, F1 Micro: 0.7769, F1 Macro: 0.773\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.64      0.70      0.67       248\n",
      "         radikalisme       0.68      0.89      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.74      0.82      0.77      1365\n",
      "        weighted avg       0.75      0.82      0.78      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 138.52720975875854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4024, Accuracy: 0.8794, F1 Micro: 0.6932, F1 Macro: 0.6875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2785, Accuracy: 0.8925, F1 Micro: 0.7263, F1 Macro: 0.7219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9006, F1 Micro: 0.7712, F1 Macro: 0.7675\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.8997, F1 Micro: 0.7641, F1 Macro: 0.7584\n",
      "Epoch 5/10, Train Loss: 0.1548, Accuracy: 0.9017, F1 Micro: 0.7652, F1 Macro: 0.7594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9011, F1 Micro: 0.7745, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9, F1 Micro: 0.7789, F1 Macro: 0.7772\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9047, F1 Micro: 0.7744, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.9028, F1 Micro: 0.7713, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0363, Accuracy: 0.9036, F1 Micro: 0.7817, F1 Macro: 0.7796\n",
      "Model 3 - Iteration 3711: Accuracy: 0.9036, F1 Micro: 0.7817, F1 Macro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.68      0.68      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 144.89865279197693 s\n",
      "Averaged - Iteration 3711: Accuracy: 0.8926, F1 Micro: 0.7537, F1 Macro: 0.7493\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 54.75490760803223 seconds\n",
      "New train size: 3886\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8755, F1 Micro: 0.6661, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.8927, F1 Micro: 0.7367, F1 Macro: 0.732\n",
      "Epoch 3/10, Train Loss: 0.2244, Accuracy: 0.8945, F1 Micro: 0.7266, F1 Macro: 0.7163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.8959, F1 Micro: 0.7643, F1 Macro: 0.7609\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.8944, F1 Micro: 0.7526, F1 Macro: 0.7409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.8967, F1 Micro: 0.772, F1 Macro: 0.7662\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.8986, F1 Micro: 0.7603, F1 Macro: 0.7537\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.8973, F1 Micro: 0.7619, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.8997, F1 Micro: 0.765, F1 Macro: 0.7598\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.8964, F1 Micro: 0.763, F1 Macro: 0.7624\n",
      "Model 1 - Iteration 3886: Accuracy: 0.8967, F1 Micro: 0.772, F1 Macro: 0.7662\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.61      0.67      0.63       248\n",
      "         radikalisme       0.71      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1365\n",
      "           macro avg       0.73      0.81      0.77      1365\n",
      "        weighted avg       0.74      0.82      0.77      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 144.7135374546051 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.8734, F1 Micro: 0.6547, F1 Macro: 0.6478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.8941, F1 Micro: 0.7388, F1 Macro: 0.7356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.8986, F1 Micro: 0.7417, F1 Macro: 0.7342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.8988, F1 Micro: 0.7633, F1 Macro: 0.7592\n",
      "Epoch 5/10, Train Loss: 0.1396, Accuracy: 0.8972, F1 Micro: 0.7568, F1 Macro: 0.7449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1108, Accuracy: 0.898, F1 Micro: 0.7804, F1 Macro: 0.7786\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9003, F1 Micro: 0.763, F1 Macro: 0.7568\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.8978, F1 Micro: 0.758, F1 Macro: 0.7514\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.9017, F1 Micro: 0.7712, F1 Macro: 0.7654\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.8989, F1 Micro: 0.769, F1 Macro: 0.7652\n",
      "Model 2 - Iteration 3886: Accuracy: 0.898, F1 Micro: 0.7804, F1 Macro: 0.7786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.60      0.78      0.68       248\n",
      "         radikalisme       0.69      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.73      0.85      0.78      1365\n",
      "        weighted avg       0.74      0.85      0.79      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 147.49942660331726 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3984, Accuracy: 0.8781, F1 Micro: 0.6834, F1 Macro: 0.6803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2684, Accuracy: 0.8966, F1 Micro: 0.7462, F1 Macro: 0.7431\n",
      "Epoch 3/10, Train Loss: 0.2224, Accuracy: 0.8959, F1 Micro: 0.7323, F1 Macro: 0.7233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.8998, F1 Micro: 0.7702, F1 Macro: 0.7676\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.8994, F1 Micro: 0.7665, F1 Macro: 0.759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.8986, F1 Micro: 0.7821, F1 Macro: 0.7822\n",
      "Epoch 7/10, Train Loss: 0.0817, Accuracy: 0.9014, F1 Micro: 0.775, F1 Macro: 0.7742\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9014, F1 Micro: 0.7734, F1 Macro: 0.7709\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9022, F1 Micro: 0.7683, F1 Macro: 0.7632\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.9005, F1 Micro: 0.7747, F1 Macro: 0.7694\n",
      "Model 3 - Iteration 3886: Accuracy: 0.8986, F1 Micro: 0.7821, F1 Macro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       370\n",
      "                sara       0.63      0.77      0.69       248\n",
      "         radikalisme       0.72      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.85      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.73      0.85      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 146.06584906578064 s\n",
      "Averaged - Iteration 3886: Accuracy: 0.8931, F1 Micro: 0.7562, F1 Macro: 0.7519\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 51.32608413696289 seconds\n",
      "New train size: 4120\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.408, Accuracy: 0.88, F1 Micro: 0.7191, F1 Macro: 0.7191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.278, Accuracy: 0.8978, F1 Micro: 0.7583, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.8989, F1 Micro: 0.7655, F1 Macro: 0.7559\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.8978, F1 Micro: 0.7423, F1 Macro: 0.7354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.8998, F1 Micro: 0.7678, F1 Macro: 0.759\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.8983, F1 Micro: 0.7668, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.7683\n",
      "Epoch 8/10, Train Loss: 0.0565, Accuracy: 0.9009, F1 Micro: 0.7598, F1 Macro: 0.7551\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.8998, F1 Micro: 0.7713, F1 Macro: 0.768\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.9003, F1 Micro: 0.7703, F1 Macro: 0.7649\n",
      "Model 1 - Iteration 4120: Accuracy: 0.9025, F1 Micro: 0.7731, F1 Macro: 0.7683\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.90       370\n",
      "                sara       0.66      0.65      0.66       248\n",
      "         radikalisme       0.74      0.81      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.77      0.77      0.77      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.45      0.44      0.43      1365\n",
      "\n",
      "Training completed in 154.19515895843506 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4155, Accuracy: 0.8797, F1 Micro: 0.7129, F1 Macro: 0.7138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2793, Accuracy: 0.8978, F1 Micro: 0.7519, F1 Macro: 0.7472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.8983, F1 Micro: 0.763, F1 Macro: 0.7523\n",
      "Epoch 4/10, Train Loss: 0.1754, Accuracy: 0.9014, F1 Micro: 0.7459, F1 Macro: 0.7351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9036, F1 Micro: 0.7777, F1 Macro: 0.774\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.8978, F1 Micro: 0.7587, F1 Macro: 0.7492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.8967, F1 Micro: 0.7793, F1 Macro: 0.7799\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9039, F1 Micro: 0.7694, F1 Macro: 0.7626\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9036, F1 Micro: 0.7749, F1 Macro: 0.7673\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9, F1 Micro: 0.7542, F1 Macro: 0.746\n",
      "Model 2 - Iteration 4120: Accuracy: 0.8967, F1 Micro: 0.7793, F1 Macro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       370\n",
      "                sara       0.60      0.81      0.69       248\n",
      "         radikalisme       0.70      0.88      0.78       243\n",
      "pencemaran_nama_baik       0.67      0.84      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.72      0.85      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.78      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 154.7179081439972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8816, F1 Micro: 0.7301, F1 Macro: 0.7271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.8972, F1 Micro: 0.7547, F1 Macro: 0.748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2163, Accuracy: 0.8995, F1 Micro: 0.7729, F1 Macro: 0.7671\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9034, F1 Micro: 0.7641, F1 Macro: 0.7594\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9008, F1 Micro: 0.7644, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.1127, Accuracy: 0.9002, F1 Micro: 0.7582, F1 Macro: 0.744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.902, F1 Micro: 0.7802, F1 Macro: 0.7775\n",
      "Epoch 8/10, Train Loss: 0.0602, Accuracy: 0.9038, F1 Micro: 0.7722, F1 Macro: 0.7651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9044, F1 Micro: 0.7806, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9016, F1 Micro: 0.7649, F1 Macro: 0.7584\n",
      "Model 3 - Iteration 4120: Accuracy: 0.9044, F1 Micro: 0.7806, F1 Macro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.66      0.67      0.67       248\n",
      "         radikalisme       0.77      0.80      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1365\n",
      "           macro avg       0.76      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 154.66573476791382 s\n",
      "Averaged - Iteration 4120: Accuracy: 0.8939, F1 Micro: 0.7581, F1 Macro: 0.754\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 45.98032212257385 seconds\n",
      "New train size: 4330\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4024, Accuracy: 0.88, F1 Micro: 0.6891, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2565, Accuracy: 0.8972, F1 Micro: 0.7628, F1 Macro: 0.7572\n",
      "Epoch 3/10, Train Loss: 0.2288, Accuracy: 0.9014, F1 Micro: 0.7477, F1 Macro: 0.7313\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.8992, F1 Micro: 0.7507, F1 Macro: 0.7468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9022, F1 Micro: 0.7722, F1 Macro: 0.766\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.8989, F1 Micro: 0.7522, F1 Macro: 0.7452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.8981, F1 Micro: 0.7808, F1 Macro: 0.7799\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9042, F1 Micro: 0.7786, F1 Macro: 0.7715\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9011, F1 Micro: 0.7577, F1 Macro: 0.7548\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9002, F1 Micro: 0.773, F1 Macro: 0.7688\n",
      "Model 1 - Iteration 4330: Accuracy: 0.8981, F1 Micro: 0.7808, F1 Macro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.69      0.86      0.76       243\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.72      0.85      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.78      1365\n",
      "         samples avg       0.46      0.48      0.46      1365\n",
      "\n",
      "Training completed in 156.59266805648804 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4113, Accuracy: 0.8811, F1 Micro: 0.696, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9011, F1 Micro: 0.7719, F1 Macro: 0.7666\n",
      "Epoch 3/10, Train Loss: 0.226, Accuracy: 0.9041, F1 Micro: 0.7571, F1 Macro: 0.7376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.177, Accuracy: 0.9055, F1 Micro: 0.7763, F1 Macro: 0.7725\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9034, F1 Micro: 0.7741, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9019, F1 Micro: 0.7734, F1 Macro: 0.7658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.8964, F1 Micro: 0.7768, F1 Macro: 0.7764\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9023, F1 Micro: 0.7751, F1 Macro: 0.7651\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.8997, F1 Micro: 0.7634, F1 Macro: 0.7542\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9016, F1 Micro: 0.7637, F1 Macro: 0.7539\n",
      "Model 2 - Iteration 4330: Accuracy: 0.8964, F1 Micro: 0.7768, F1 Macro: 0.7764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.92      0.90       370\n",
      "                sara       0.64      0.75      0.69       248\n",
      "         radikalisme       0.75      0.82      0.78       243\n",
      "pencemaran_nama_baik       0.65      0.85      0.73       504\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1365\n",
      "           macro avg       0.73      0.84      0.78      1365\n",
      "        weighted avg       0.73      0.85      0.78      1365\n",
      "         samples avg       0.47      0.48      0.46      1365\n",
      "\n",
      "Training completed in 157.45501136779785 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3971, Accuracy: 0.8828, F1 Micro: 0.699, F1 Macro: 0.6954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.8972, F1 Micro: 0.766, F1 Macro: 0.7602\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.9014, F1 Micro: 0.7459, F1 Macro: 0.7283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1776, Accuracy: 0.902, F1 Micro: 0.7691, F1 Macro: 0.7675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9016, F1 Micro: 0.7719, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9014, F1 Micro: 0.7659, F1 Macro: 0.7603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.8989, F1 Micro: 0.7765, F1 Macro: 0.7755\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9039, F1 Micro: 0.7713, F1 Macro: 0.7614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9033, F1 Micro: 0.7806, F1 Macro: 0.7798\n",
      "Epoch 10/10, Train Loss: 0.0322, Accuracy: 0.9042, F1 Micro: 0.7712, F1 Macro: 0.7665\n",
      "Model 3 - Iteration 4330: Accuracy: 0.9033, F1 Micro: 0.7806, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.84      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.76      0.73       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 160.47104024887085 s\n",
      "Averaged - Iteration 4330: Accuracy: 0.8943, F1 Micro: 0.7599, F1 Macro: 0.756\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 43.151904821395874 seconds\n",
      "New train size: 4530\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.405, Accuracy: 0.8802, F1 Micro: 0.7056, F1 Macro: 0.691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.8925, F1 Micro: 0.7283, F1 Macro: 0.7273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2199, Accuracy: 0.8986, F1 Micro: 0.7738, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.177, Accuracy: 0.8977, F1 Micro: 0.7759, F1 Macro: 0.7755\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.8997, F1 Micro: 0.7604, F1 Macro: 0.7512\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.897, F1 Micro: 0.7682, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.8998, F1 Micro: 0.7582, F1 Macro: 0.7473\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9011, F1 Micro: 0.7722, F1 Macro: 0.7666\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.8994, F1 Micro: 0.7672, F1 Macro: 0.761\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.902, F1 Micro: 0.7749, F1 Macro: 0.7724\n",
      "Model 1 - Iteration 4530: Accuracy: 0.8977, F1 Micro: 0.7759, F1 Macro: 0.7755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       370\n",
      "                sara       0.61      0.77      0.68       248\n",
      "         radikalisme       0.71      0.85      0.77       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1365\n",
      "           macro avg       0.73      0.83      0.78      1365\n",
      "        weighted avg       0.74      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 163.86721205711365 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4092, Accuracy: 0.8834, F1 Micro: 0.707, F1 Macro: 0.6814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.8975, F1 Micro: 0.7384, F1 Macro: 0.732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9045, F1 Micro: 0.7822, F1 Macro: 0.7736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9031, F1 Micro: 0.7844, F1 Macro: 0.7828\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.8997, F1 Micro: 0.7731, F1 Macro: 0.767\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9011, F1 Micro: 0.7714, F1 Macro: 0.7631\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9009, F1 Micro: 0.7749, F1 Macro: 0.7678\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9013, F1 Micro: 0.7735, F1 Macro: 0.7698\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.8988, F1 Micro: 0.7762, F1 Macro: 0.7748\n",
      "Epoch 10/10, Train Loss: 0.0355, Accuracy: 0.8997, F1 Micro: 0.7743, F1 Macro: 0.774\n",
      "Model 2 - Iteration 4530: Accuracy: 0.9031, F1 Micro: 0.7844, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       370\n",
      "                sara       0.63      0.75      0.69       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.45      1365\n",
      "\n",
      "Training completed in 163.2225170135498 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3994, Accuracy: 0.8859, F1 Micro: 0.7222, F1 Macro: 0.7038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2642, Accuracy: 0.8945, F1 Micro: 0.7251, F1 Macro: 0.7225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2193, Accuracy: 0.8992, F1 Micro: 0.7747, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9006, F1 Micro: 0.7801, F1 Macro: 0.7798\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9036, F1 Micro: 0.7756, F1 Macro: 0.7681\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9042, F1 Micro: 0.7801, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.902, F1 Micro: 0.7713, F1 Macro: 0.763\n",
      "Epoch 8/10, Train Loss: 0.0547, Accuracy: 0.9003, F1 Micro: 0.7741, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.9042, F1 Micro: 0.7781, F1 Macro: 0.7715\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7804\n",
      "Model 3 - Iteration 4530: Accuracy: 0.9006, F1 Micro: 0.7801, F1 Macro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.71      0.84      0.77       243\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.83      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.78      1365\n",
      "         samples avg       0.45      0.46      0.45      1365\n",
      "\n",
      "Training completed in 165.20711088180542 s\n",
      "Averaged - Iteration 4530: Accuracy: 0.8948, F1 Micro: 0.7615, F1 Macro: 0.7578\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 37.729575872421265 seconds\n",
      "New train size: 4663\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.875, F1 Micro: 0.6708, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2536, Accuracy: 0.8955, F1 Micro: 0.7346, F1 Macro: 0.7256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.8997, F1 Micro: 0.7736, F1 Macro: 0.7719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9023, F1 Micro: 0.7805, F1 Macro: 0.7737\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.8986, F1 Micro: 0.7678, F1 Macro: 0.7617\n",
      "Epoch 6/10, Train Loss: 0.1115, Accuracy: 0.9033, F1 Micro: 0.7785, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9006, F1 Micro: 0.7741, F1 Macro: 0.771\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.902, F1 Micro: 0.7668, F1 Macro: 0.7599\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9027, F1 Micro: 0.7692, F1 Macro: 0.7653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9038, F1 Micro: 0.7811, F1 Macro: 0.7772\n",
      "Model 1 - Iteration 4663: Accuracy: 0.9038, F1 Micro: 0.7811, F1 Macro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.72      0.83      0.77       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.75      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.78      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 168.95526885986328 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4078, Accuracy: 0.8748, F1 Micro: 0.6721, F1 Macro: 0.6409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2557, Accuracy: 0.9013, F1 Micro: 0.7504, F1 Macro: 0.7432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2114, Accuracy: 0.8986, F1 Micro: 0.7727, F1 Macro: 0.7716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.9047, F1 Micro: 0.7828, F1 Macro: 0.7778\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.8992, F1 Micro: 0.7769, F1 Macro: 0.7748\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9038, F1 Micro: 0.767, F1 Macro: 0.7582\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9027, F1 Micro: 0.7799, F1 Macro: 0.7753\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9017, F1 Micro: 0.7792, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9045, F1 Micro: 0.7676, F1 Macro: 0.7632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9042, F1 Micro: 0.7835, F1 Macro: 0.7784\n",
      "Model 2 - Iteration 4663: Accuracy: 0.9042, F1 Micro: 0.7835, F1 Macro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.64      0.69      0.67       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.81      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.47      0.46      0.46      1365\n",
      "\n",
      "Training completed in 167.47504019737244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.8811, F1 Micro: 0.6828, F1 Macro: 0.6512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2508, Accuracy: 0.8998, F1 Micro: 0.7441, F1 Macro: 0.7402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.897, F1 Micro: 0.7705, F1 Macro: 0.7693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9041, F1 Micro: 0.7729, F1 Macro: 0.7636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.8994, F1 Micro: 0.7784, F1 Macro: 0.7787\n",
      "Epoch 6/10, Train Loss: 0.1108, Accuracy: 0.9039, F1 Micro: 0.7763, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9023, F1 Micro: 0.7831, F1 Macro: 0.7807\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9011, F1 Micro: 0.7794, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9036, F1 Micro: 0.7626, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0358, Accuracy: 0.9023, F1 Micro: 0.7837, F1 Macro: 0.7822\n",
      "Model 3 - Iteration 4663: Accuracy: 0.9023, F1 Micro: 0.7837, F1 Macro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       370\n",
      "                sara       0.64      0.72      0.68       248\n",
      "         radikalisme       0.74      0.85      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 172.15643644332886 s\n",
      "Averaged - Iteration 4663: Accuracy: 0.8954, F1 Micro: 0.763, F1 Macro: 0.7594\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 35.78854465484619 seconds\n",
      "New train size: 4863\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3911, Accuracy: 0.882, F1 Micro: 0.713, F1 Macro: 0.7077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2569, Accuracy: 0.8984, F1 Micro: 0.7521, F1 Macro: 0.7478\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9, F1 Micro: 0.748, F1 Macro: 0.74\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1729, Accuracy: 0.9, F1 Micro: 0.7568, F1 Macro: 0.7517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9041, F1 Micro: 0.7846, F1 Macro: 0.7808\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9014, F1 Micro: 0.7684, F1 Macro: 0.7617\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.902, F1 Micro: 0.7779, F1 Macro: 0.7761\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.9006, F1 Micro: 0.7595, F1 Macro: 0.751\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.903, F1 Micro: 0.7696, F1 Macro: 0.7573\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9033, F1 Micro: 0.7708, F1 Macro: 0.7654\n",
      "Model 1 - Iteration 4863: Accuracy: 0.9041, F1 Micro: 0.7846, F1 Macro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.65      0.70      0.67       248\n",
      "         radikalisme       0.76      0.81      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 169.73777508735657 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3968, Accuracy: 0.8864, F1 Micro: 0.7245, F1 Macro: 0.7174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2544, Accuracy: 0.8998, F1 Micro: 0.7595, F1 Macro: 0.7505\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9023, F1 Micro: 0.7586, F1 Macro: 0.7514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.9013, F1 Micro: 0.7595, F1 Macro: 0.7495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9031, F1 Micro: 0.78, F1 Macro: 0.7771\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9048, F1 Micro: 0.7718, F1 Macro: 0.7659\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9014, F1 Micro: 0.7594, F1 Macro: 0.7522\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9009, F1 Micro: 0.7622, F1 Macro: 0.7556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.044, Accuracy: 0.9064, F1 Micro: 0.7845, F1 Macro: 0.777\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9048, F1 Micro: 0.7699, F1 Macro: 0.756\n",
      "Model 2 - Iteration 4863: Accuracy: 0.9064, F1 Micro: 0.7845, F1 Macro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       370\n",
      "                sara       0.70      0.62      0.65       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.77      0.80      0.78      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 173.0599822998047 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3828, Accuracy: 0.8877, F1 Micro: 0.7238, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2544, Accuracy: 0.9008, F1 Micro: 0.7671, F1 Macro: 0.76\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.8984, F1 Micro: 0.7477, F1 Macro: 0.7414\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9031, F1 Micro: 0.7664, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1355, Accuracy: 0.903, F1 Micro: 0.7823, F1 Macro: 0.7804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9044, F1 Micro: 0.7853, F1 Macro: 0.7824\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9009, F1 Micro: 0.7768, F1 Macro: 0.7749\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.9023, F1 Micro: 0.7743, F1 Macro: 0.7706\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.9052, F1 Micro: 0.7811, F1 Macro: 0.7778\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9055, F1 Micro: 0.7802, F1 Macro: 0.773\n",
      "Model 3 - Iteration 4863: Accuracy: 0.9044, F1 Micro: 0.7853, F1 Macro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.72      0.86      0.78       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 171.29622340202332 s\n",
      "Averaged - Iteration 4863: Accuracy: 0.896, F1 Micro: 0.7644, F1 Macro: 0.7608\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.879920959472656 seconds\n",
      "New train size: 5063\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.391, Accuracy: 0.8819, F1 Micro: 0.6929, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.8964, F1 Micro: 0.7429, F1 Macro: 0.7271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.9, F1 Micro: 0.7638, F1 Macro: 0.7605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9013, F1 Micro: 0.7712, F1 Macro: 0.7641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1275, Accuracy: 0.9008, F1 Micro: 0.7796, F1 Macro: 0.7784\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9031, F1 Micro: 0.7601, F1 Macro: 0.7493\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9047, F1 Micro: 0.7791, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.9044, F1 Micro: 0.7827, F1 Macro: 0.7804\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9003, F1 Micro: 0.7755, F1 Macro: 0.7736\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9055, F1 Micro: 0.7711, F1 Macro: 0.768\n",
      "Model 1 - Iteration 5063: Accuracy: 0.9044, F1 Micro: 0.7827, F1 Macro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.71      0.67       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.71      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1365\n",
      "           macro avg       0.76      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.45      0.45      1365\n",
      "\n",
      "Training completed in 181.74627900123596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3969, Accuracy: 0.887, F1 Micro: 0.7239, F1 Macro: 0.7149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.8988, F1 Micro: 0.7377, F1 Macro: 0.7123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9055, F1 Micro: 0.7802, F1 Macro: 0.7747\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9045, F1 Micro: 0.7736, F1 Macro: 0.768\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.902, F1 Micro: 0.7791, F1 Macro: 0.7759\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9031, F1 Micro: 0.7763, F1 Macro: 0.7745\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9033, F1 Micro: 0.7753, F1 Macro: 0.7697\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9016, F1 Micro: 0.7729, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.8995, F1 Micro: 0.7793, F1 Macro: 0.7821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9027, F1 Micro: 0.7841, F1 Macro: 0.7847\n",
      "Model 2 - Iteration 5063: Accuracy: 0.9027, F1 Micro: 0.7841, F1 Macro: 0.7847\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       370\n",
      "                sara       0.65      0.72      0.68       248\n",
      "         radikalisme       0.77      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 177.36154103279114 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.384, Accuracy: 0.8878, F1 Micro: 0.7149, F1 Macro: 0.7025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2477, Accuracy: 0.8998, F1 Micro: 0.7495, F1 Macro: 0.7328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9034, F1 Micro: 0.7774, F1 Macro: 0.7742\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9045, F1 Micro: 0.7729, F1 Macro: 0.7707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.8984, F1 Micro: 0.7825, F1 Macro: 0.7842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.102, Accuracy: 0.907, F1 Micro: 0.7857, F1 Macro: 0.7828\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9031, F1 Micro: 0.7726, F1 Macro: 0.769\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9067, F1 Micro: 0.783, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9044, F1 Micro: 0.785, F1 Macro: 0.7856\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.9031, F1 Micro: 0.7809, F1 Macro: 0.7806\n",
      "Model 3 - Iteration 5063: Accuracy: 0.907, F1 Micro: 0.7857, F1 Macro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.69      0.68       248\n",
      "         radikalisme       0.74      0.86      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.45      0.44      1365\n",
      "\n",
      "Training completed in 180.5647668838501 s\n",
      "Averaged - Iteration 5063: Accuracy: 0.8966, F1 Micro: 0.7657, F1 Macro: 0.7621\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.248827934265137 seconds\n",
      "New train size: 5263\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3802, Accuracy: 0.8859, F1 Micro: 0.726, F1 Macro: 0.7221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.8967, F1 Micro: 0.7635, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.2125, Accuracy: 0.9005, F1 Micro: 0.753, F1 Macro: 0.7382\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9034, F1 Micro: 0.7607, F1 Macro: 0.7499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1306, Accuracy: 0.9048, F1 Micro: 0.7711, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9031, F1 Micro: 0.7695, F1 Macro: 0.766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.8964, F1 Micro: 0.773, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.9022, F1 Micro: 0.7705, F1 Macro: 0.7689\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.9011, F1 Micro: 0.7672, F1 Macro: 0.7632\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.8981, F1 Micro: 0.7728, F1 Macro: 0.7716\n",
      "Model 1 - Iteration 5263: Accuracy: 0.8964, F1 Micro: 0.773, F1 Macro: 0.7731\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.61      0.74      0.67       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       504\n",
      "\n",
      "           micro avg       0.73      0.83      0.77      1365\n",
      "           macro avg       0.73      0.82      0.77      1365\n",
      "        weighted avg       0.73      0.83      0.78      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 183.5575065612793 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3879, Accuracy: 0.8905, F1 Micro: 0.7407, F1 Macro: 0.7376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.8994, F1 Micro: 0.7646, F1 Macro: 0.7526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.9069, F1 Micro: 0.7677, F1 Macro: 0.7532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9059, F1 Micro: 0.7701, F1 Macro: 0.7599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9025, F1 Micro: 0.7689, F1 Macro: 0.7668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9058, F1 Micro: 0.7802, F1 Macro: 0.7781\n",
      "Epoch 8/10, Train Loss: 0.0547, Accuracy: 0.903, F1 Micro: 0.7629, F1 Macro: 0.7562\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.9064, F1 Micro: 0.7718, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.9022, F1 Micro: 0.7825, F1 Macro: 0.7826\n",
      "Model 2 - Iteration 5263: Accuracy: 0.9022, F1 Micro: 0.7825, F1 Macro: 0.7826\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.64      0.74      0.69       248\n",
      "         radikalisme       0.74      0.84      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 188.50611090660095 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3733, Accuracy: 0.8877, F1 Micro: 0.7381, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.8998, F1 Micro: 0.77, F1 Macro: 0.7639\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9048, F1 Micro: 0.7618, F1 Macro: 0.751\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9042, F1 Micro: 0.7612, F1 Macro: 0.7487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9069, F1 Micro: 0.7802, F1 Macro: 0.7729\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9036, F1 Micro: 0.7802, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9069, F1 Micro: 0.7825, F1 Macro: 0.7797\n",
      "Epoch 8/10, Train Loss: 0.0547, Accuracy: 0.9042, F1 Micro: 0.7691, F1 Macro: 0.7627\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.9034, F1 Micro: 0.7796, F1 Macro: 0.7761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.037, Accuracy: 0.9048, F1 Micro: 0.7877, F1 Macro: 0.785\n",
      "Model 3 - Iteration 5263: Accuracy: 0.9048, F1 Micro: 0.7877, F1 Macro: 0.785\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.67      0.71      0.69       248\n",
      "         radikalisme       0.73      0.85      0.78       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.48      0.47      0.46      1365\n",
      "\n",
      "Training completed in 184.82062673568726 s\n",
      "Averaged - Iteration 5263: Accuracy: 0.8969, F1 Micro: 0.7666, F1 Macro: 0.7632\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 21.90954303741455 seconds\n",
      "New train size: 5441\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3829, Accuracy: 0.8844, F1 Micro: 0.7169, F1 Macro: 0.7061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.8952, F1 Micro: 0.7677, F1 Macro: 0.7655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.8966, F1 Micro: 0.7745, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9025, F1 Micro: 0.7821, F1 Macro: 0.7812\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9022, F1 Micro: 0.7742, F1 Macro: 0.7715\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9033, F1 Micro: 0.7727, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9036, F1 Micro: 0.7673, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.9048, F1 Micro: 0.7837, F1 Macro: 0.782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9066, F1 Micro: 0.7888, F1 Macro: 0.7889\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.905, F1 Micro: 0.7864, F1 Macro: 0.7836\n",
      "Model 1 - Iteration 5441: Accuracy: 0.9066, F1 Micro: 0.7888, F1 Macro: 0.7889\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       370\n",
      "                sara       0.69      0.70      0.69       248\n",
      "         radikalisme       0.78      0.83      0.80       243\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.77      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 193.5627748966217 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3893, Accuracy: 0.8867, F1 Micro: 0.72, F1 Macro: 0.7063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.8989, F1 Micro: 0.774, F1 Macro: 0.7723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9003, F1 Micro: 0.7779, F1 Macro: 0.7766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.161, Accuracy: 0.9056, F1 Micro: 0.7855, F1 Macro: 0.7831\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.7728\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9066, F1 Micro: 0.7821, F1 Macro: 0.7797\n",
      "Epoch 7/10, Train Loss: 0.0679, Accuracy: 0.9038, F1 Micro: 0.7614, F1 Macro: 0.7516\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9034, F1 Micro: 0.7721, F1 Macro: 0.7646\n",
      "Epoch 9/10, Train Loss: 0.0419, Accuracy: 0.9058, F1 Micro: 0.7687, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9033, F1 Micro: 0.7795, F1 Macro: 0.7751\n",
      "Model 2 - Iteration 5441: Accuracy: 0.9056, F1 Micro: 0.7855, F1 Macro: 0.7831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.62      0.74      0.68       248\n",
      "         radikalisme       0.75      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.76      0.81      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.45      0.46      0.44      1365\n",
      "\n",
      "Training completed in 188.19341921806335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3769, Accuracy: 0.8839, F1 Micro: 0.7076, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.8956, F1 Micro: 0.7648, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9005, F1 Micro: 0.7781, F1 Macro: 0.7766\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.8998, F1 Micro: 0.7747, F1 Macro: 0.7724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9044, F1 Micro: 0.7808, F1 Macro: 0.7777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9039, F1 Micro: 0.7852, F1 Macro: 0.7848\n",
      "Epoch 7/10, Train Loss: 0.0693, Accuracy: 0.903, F1 Micro: 0.7645, F1 Macro: 0.7565\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.907, F1 Micro: 0.7831, F1 Macro: 0.7759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9086, F1 Micro: 0.7902, F1 Macro: 0.7888\n",
      "Epoch 10/10, Train Loss: 0.0275, Accuracy: 0.9023, F1 Micro: 0.7765, F1 Macro: 0.7696\n",
      "Model 3 - Iteration 5441: Accuracy: 0.9086, F1 Micro: 0.7902, F1 Macro: 0.7888\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.67      0.72      0.69       248\n",
      "         radikalisme       0.78      0.82      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       504\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1365\n",
      "           macro avg       0.77      0.81      0.79      1365\n",
      "        weighted avg       0.78      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 192.37494826316833 s\n",
      "Averaged - Iteration 5441: Accuracy: 0.8974, F1 Micro: 0.7678, F1 Macro: 0.7645\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.273256063461304 seconds\n",
      "New train size: 5641\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3751, Accuracy: 0.8853, F1 Micro: 0.7101, F1 Macro: 0.701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2522, Accuracy: 0.8903, F1 Micro: 0.771, F1 Macro: 0.774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9055, F1 Micro: 0.7809, F1 Macro: 0.7733\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9023, F1 Micro: 0.7597, F1 Macro: 0.7521\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9034, F1 Micro: 0.7723, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.8981, F1 Micro: 0.7797, F1 Macro: 0.7745\n",
      "Epoch 7/10, Train Loss: 0.0682, Accuracy: 0.8959, F1 Micro: 0.7773, F1 Macro: 0.7758\n",
      "Epoch 8/10, Train Loss: 0.0518, Accuracy: 0.8984, F1 Micro: 0.7808, F1 Macro: 0.7821\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9034, F1 Micro: 0.7799, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.9027, F1 Micro: 0.7683, F1 Macro: 0.759\n",
      "Model 1 - Iteration 5641: Accuracy: 0.9055, F1 Micro: 0.7809, F1 Macro: 0.7733\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       370\n",
      "                sara       0.71      0.61      0.66       248\n",
      "         radikalisme       0.77      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.75       504\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1365\n",
      "           macro avg       0.78      0.77      0.77      1365\n",
      "        weighted avg       0.78      0.79      0.78      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 192.47394371032715 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.8894, F1 Micro: 0.7136, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2487, Accuracy: 0.8961, F1 Micro: 0.7778, F1 Macro: 0.7775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.903, F1 Micro: 0.7849, F1 Macro: 0.7838\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.9055, F1 Micro: 0.778, F1 Macro: 0.7741\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9002, F1 Micro: 0.7471, F1 Macro: 0.727\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9031, F1 Micro: 0.7835, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9011, F1 Micro: 0.7821, F1 Macro: 0.7822\n",
      "Epoch 8/10, Train Loss: 0.0533, Accuracy: 0.8997, F1 Micro: 0.7822, F1 Macro: 0.7815\n",
      "Epoch 9/10, Train Loss: 0.0401, Accuracy: 0.9027, F1 Micro: 0.7776, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9016, F1 Micro: 0.7717, F1 Macro: 0.7633\n",
      "Model 2 - Iteration 5641: Accuracy: 0.903, F1 Micro: 0.7849, F1 Macro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.92       370\n",
      "                sara       0.68      0.69      0.69       248\n",
      "         radikalisme       0.76      0.83      0.79       243\n",
      "pencemaran_nama_baik       0.66      0.83      0.74       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1365\n",
      "           macro avg       0.75      0.82      0.78      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 192.01051115989685 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3694, Accuracy: 0.8892, F1 Micro: 0.7121, F1 Macro: 0.7005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.8945, F1 Micro: 0.7746, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.905, F1 Micro: 0.7852, F1 Macro: 0.7834\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9067, F1 Micro: 0.7775, F1 Macro: 0.775\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9016, F1 Micro: 0.7535, F1 Macro: 0.7394\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9047, F1 Micro: 0.7793, F1 Macro: 0.7734\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.902, F1 Micro: 0.7747, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.9058, F1 Micro: 0.783, F1 Macro: 0.7811\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9025, F1 Micro: 0.7835, F1 Macro: 0.7839\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9048, F1 Micro: 0.7749, F1 Macro: 0.7686\n",
      "Model 3 - Iteration 5641: Accuracy: 0.905, F1 Micro: 0.7852, F1 Macro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.77      0.81      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1365\n",
      "           macro avg       0.77      0.80      0.78      1365\n",
      "        weighted avg       0.77      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 191.82536935806274 s\n",
      "Averaged - Iteration 5641: Accuracy: 0.8978, F1 Micro: 0.7686, F1 Macro: 0.7653\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.598735809326172 seconds\n",
      "New train size: 5841\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3806, Accuracy: 0.8897, F1 Micro: 0.7507, F1 Macro: 0.7498\n",
      "Epoch 2/10, Train Loss: 0.2419, Accuracy: 0.8988, F1 Micro: 0.7502, F1 Macro: 0.7444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9061, F1 Micro: 0.7883, F1 Macro: 0.7848\n",
      "Epoch 4/10, Train Loss: 0.1627, Accuracy: 0.9042, F1 Micro: 0.7783, F1 Macro: 0.7727\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9027, F1 Micro: 0.7766, F1 Macro: 0.7717\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9031, F1 Micro: 0.7832, F1 Macro: 0.7824\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9047, F1 Micro: 0.7774, F1 Macro: 0.7723\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.9044, F1 Micro: 0.7859, F1 Macro: 0.7847\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9061, F1 Micro: 0.788, F1 Macro: 0.7869\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9075, F1 Micro: 0.7825, F1 Macro: 0.7818\n",
      "Model 1 - Iteration 5841: Accuracy: 0.9061, F1 Micro: 0.7883, F1 Macro: 0.7848\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.63      0.72      0.67       248\n",
      "         radikalisme       0.76      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.78      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 195.42607021331787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3865, Accuracy: 0.8902, F1 Micro: 0.7605, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2401, Accuracy: 0.9028, F1 Micro: 0.7613, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2056, Accuracy: 0.9064, F1 Micro: 0.7911, F1 Macro: 0.789\n",
      "Epoch 4/10, Train Loss: 0.1593, Accuracy: 0.9028, F1 Micro: 0.7843, F1 Macro: 0.7816\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9023, F1 Micro: 0.7716, F1 Macro: 0.7658\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9048, F1 Micro: 0.7861, F1 Macro: 0.7846\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.9048, F1 Micro: 0.7684, F1 Macro: 0.7649\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.9014, F1 Micro: 0.7768, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.9023, F1 Micro: 0.7749, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9059, F1 Micro: 0.7828, F1 Macro: 0.7785\n",
      "Model 2 - Iteration 5841: Accuracy: 0.9064, F1 Micro: 0.7911, F1 Macro: 0.789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       370\n",
      "                sara       0.62      0.75      0.68       248\n",
      "         radikalisme       0.75      0.88      0.81       243\n",
      "pencemaran_nama_baik       0.73      0.79      0.75       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 199.73400616645813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3754, Accuracy: 0.8908, F1 Micro: 0.7553, F1 Macro: 0.7542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2406, Accuracy: 0.8992, F1 Micro: 0.7659, F1 Macro: 0.7616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2086, Accuracy: 0.8998, F1 Micro: 0.7819, F1 Macro: 0.7788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9038, F1 Micro: 0.7864, F1 Macro: 0.7834\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9033, F1 Micro: 0.7705, F1 Macro: 0.7629\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9081, F1 Micro: 0.7856, F1 Macro: 0.7833\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9081, F1 Micro: 0.7735, F1 Macro: 0.7695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.9087, F1 Micro: 0.7898, F1 Macro: 0.788\n",
      "Epoch 9/10, Train Loss: 0.0421, Accuracy: 0.9044, F1 Micro: 0.7841, F1 Macro: 0.7817\n",
      "Epoch 10/10, Train Loss: 0.035, Accuracy: 0.9044, F1 Micro: 0.7771, F1 Macro: 0.774\n",
      "Model 3 - Iteration 5841: Accuracy: 0.9087, F1 Micro: 0.7898, F1 Macro: 0.788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.69      0.69      0.69       248\n",
      "         radikalisme       0.80      0.80      0.80       243\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1365\n",
      "           macro avg       0.78      0.80      0.79      1365\n",
      "        weighted avg       0.78      0.80      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 200.42837715148926 s\n",
      "Averaged - Iteration 5841: Accuracy: 0.8983, F1 Micro: 0.7697, F1 Macro: 0.7664\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.85997223854065 seconds\n",
      "New train size: 6041\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3602, Accuracy: 0.8925, F1 Micro: 0.7562, F1 Macro: 0.75\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2407, Accuracy: 0.8988, F1 Micro: 0.7731, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1913, Accuracy: 0.9053, F1 Micro: 0.7846, F1 Macro: 0.7815\n",
      "Epoch 4/10, Train Loss: 0.1589, Accuracy: 0.8997, F1 Micro: 0.7825, F1 Macro: 0.7809\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9078, F1 Micro: 0.7679, F1 Macro: 0.7616\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9041, F1 Micro: 0.7833, F1 Macro: 0.7822\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9034, F1 Micro: 0.7759, F1 Macro: 0.7697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.051, Accuracy: 0.9058, F1 Micro: 0.7885, F1 Macro: 0.7868\n",
      "Epoch 9/10, Train Loss: 0.0385, Accuracy: 0.9036, F1 Micro: 0.7671, F1 Macro: 0.7598\n",
      "Epoch 10/10, Train Loss: 0.0283, Accuracy: 0.903, F1 Micro: 0.78, F1 Macro: 0.779\n",
      "Model 1 - Iteration 6041: Accuracy: 0.9058, F1 Micro: 0.7885, F1 Macro: 0.7868\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       370\n",
      "                sara       0.66      0.71      0.69       248\n",
      "         radikalisme       0.73      0.87      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       504\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1365\n",
      "           macro avg       0.76      0.82      0.79      1365\n",
      "        weighted avg       0.76      0.82      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 204.77322602272034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3687, Accuracy: 0.8919, F1 Micro: 0.7516, F1 Macro: 0.7445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.242, Accuracy: 0.9025, F1 Micro: 0.7787, F1 Macro: 0.7738\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.9028, F1 Micro: 0.7763, F1 Macro: 0.7708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9006, F1 Micro: 0.7841, F1 Macro: 0.7828\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.9061, F1 Micro: 0.7703, F1 Macro: 0.7669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9041, F1 Micro: 0.7865, F1 Macro: 0.7856\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9017, F1 Micro: 0.7657, F1 Macro: 0.7537\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.9075, F1 Micro: 0.7844, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0409, Accuracy: 0.9038, F1 Micro: 0.773, F1 Macro: 0.7678\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.9038, F1 Micro: 0.777, F1 Macro: 0.7728\n",
      "Model 2 - Iteration 6041: Accuracy: 0.9041, F1 Micro: 0.7865, F1 Macro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       370\n",
      "                sara       0.66      0.74      0.70       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.76      0.83      0.79      1365\n",
      "         samples avg       0.46      0.47      0.46      1365\n",
      "\n",
      "Training completed in 204.3746039867401 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3538, Accuracy: 0.8917, F1 Micro: 0.7571, F1 Macro: 0.7495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.9005, F1 Micro: 0.7752, F1 Macro: 0.7709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9044, F1 Micro: 0.7853, F1 Macro: 0.7818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.9031, F1 Micro: 0.789, F1 Macro: 0.7878\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9039, F1 Micro: 0.7645, F1 Macro: 0.7571\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9062, F1 Micro: 0.7828, F1 Macro: 0.7779\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.9061, F1 Micro: 0.7767, F1 Macro: 0.7697\n",
      "Epoch 8/10, Train Loss: 0.0487, Accuracy: 0.9045, F1 Micro: 0.7702, F1 Macro: 0.7634\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9019, F1 Micro: 0.761, F1 Macro: 0.7554\n",
      "Epoch 10/10, Train Loss: 0.0275, Accuracy: 0.905, F1 Micro: 0.7824, F1 Macro: 0.7788\n",
      "Model 3 - Iteration 6041: Accuracy: 0.9031, F1 Micro: 0.789, F1 Macro: 0.7878\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       370\n",
      "                sara       0.63      0.78      0.70       248\n",
      "         radikalisme       0.71      0.87      0.79       243\n",
      "pencemaran_nama_baik       0.69      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.85      0.79      1365\n",
      "           macro avg       0.74      0.85      0.79      1365\n",
      "        weighted avg       0.75      0.85      0.79      1365\n",
      "         samples avg       0.47      0.48      0.47      1365\n",
      "\n",
      "Training completed in 204.2496461868286 s\n",
      "Averaged - Iteration 6041: Accuracy: 0.8985, F1 Micro: 0.7705, F1 Macro: 0.7674\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.0335915088653564 seconds\n",
      "New train size: 6218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3539, Accuracy: 0.8908, F1 Micro: 0.7359, F1 Macro: 0.7158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.227, Accuracy: 0.9013, F1 Micro: 0.7663, F1 Macro: 0.7604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9025, F1 Micro: 0.7807, F1 Macro: 0.7753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.907, F1 Micro: 0.7847, F1 Macro: 0.7817\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9019, F1 Micro: 0.7728, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.084, Accuracy: 0.9031, F1 Micro: 0.7794, F1 Macro: 0.7789\n",
      "Epoch 7/10, Train Loss: 0.0629, Accuracy: 0.9062, F1 Micro: 0.7821, F1 Macro: 0.7796\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.905, F1 Micro: 0.7771, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9048, F1 Micro: 0.7783, F1 Macro: 0.7736\n",
      "Epoch 10/10, Train Loss: 0.0278, Accuracy: 0.9056, F1 Micro: 0.7838, F1 Macro: 0.7814\n",
      "Model 1 - Iteration 6218: Accuracy: 0.907, F1 Micro: 0.7847, F1 Macro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       370\n",
      "                sara       0.67      0.68      0.68       248\n",
      "         radikalisme       0.75      0.85      0.80       243\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       504\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1365\n",
      "           macro avg       0.77      0.79      0.78      1365\n",
      "        weighted avg       0.78      0.79      0.79      1365\n",
      "         samples avg       0.45      0.45      0.44      1365\n",
      "\n",
      "Training completed in 213.19861555099487 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3555, Accuracy: 0.8923, F1 Micro: 0.744, F1 Macro: 0.7352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9023, F1 Micro: 0.7721, F1 Macro: 0.7669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9033, F1 Micro: 0.7863, F1 Macro: 0.7851\n",
      "Epoch 4/10, Train Loss: 0.1593, Accuracy: 0.9041, F1 Micro: 0.7796, F1 Macro: 0.7762\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9041, F1 Micro: 0.7706, F1 Macro: 0.7637\n",
      "Epoch 6/10, Train Loss: 0.0828, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.7648\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9027, F1 Micro: 0.7776, F1 Macro: 0.773\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9008, F1 Micro: 0.7722, F1 Macro: 0.7672\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9036, F1 Micro: 0.7789, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9022, F1 Micro: 0.78, F1 Macro: 0.7777\n",
      "Model 2 - Iteration 6218: Accuracy: 0.9033, F1 Micro: 0.7863, F1 Macro: 0.7851\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       370\n",
      "                sara       0.67      0.72      0.69       248\n",
      "         radikalisme       0.71      0.88      0.79       243\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       504\n",
      "\n",
      "           micro avg       0.74      0.83      0.79      1365\n",
      "           macro avg       0.75      0.83      0.79      1365\n",
      "        weighted avg       0.75      0.83      0.79      1365\n",
      "         samples avg       0.47      0.47      0.46      1365\n",
      "\n",
      "Training completed in 208.115553855896 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3465, Accuracy: 0.8931, F1 Micro: 0.745, F1 Macro: 0.7327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9042, F1 Micro: 0.772, F1 Macro: 0.7647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1941, Accuracy: 0.9017, F1 Micro: 0.783, F1 Macro: 0.7809\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.9027, F1 Micro: 0.7737, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9047, F1 Micro: 0.7712, F1 Macro: 0.7628\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9047, F1 Micro: 0.778, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.065, Accuracy: 0.9039, F1 Micro: 0.7834, F1 Macro: 0.782\n",
      "Epoch 8/10, Train Loss: 0.0503, Accuracy: 0.9042, F1 Micro: 0.7833, F1 Macro: 0.7828\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9041, F1 Micro: 0.7757, F1 Macro: 0.7681\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9038, F1 Micro: 0.7789, F1 Macro: 0.7768\n",
      "Model 3 - Iteration 6218: Accuracy: 0.9039, F1 Micro: 0.7834, F1 Macro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       370\n",
      "                sara       0.64      0.73      0.68       248\n",
      "         radikalisme       0.76      0.84      0.80       243\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       504\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1365\n",
      "           macro avg       0.75      0.81      0.78      1365\n",
      "        weighted avg       0.76      0.81      0.79      1365\n",
      "         samples avg       0.46      0.46      0.45      1365\n",
      "\n",
      "Training completed in 210.3840970993042 s\n",
      "Averaged - Iteration 6218: Accuracy: 0.8988, F1 Micro: 0.7712, F1 Macro: 0.7681\n",
      "Total sampling time: 1092.99 seconds\n",
      "Total runtime: 11312.146282196045 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6078649,
     "sourceId": 9896348,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11951.280781,
   "end_time": "2025-03-13T10:00:49.726776",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-13T06:41:38.445995",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "020fb174190f4ee693fc53cb8b13f648": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "09c59fb94f2047bb9336d088cab740fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0d74cc48c21841a19bace1fbd382704a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24ef060db71c4975bac6353a651302ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_699285e83f4847498d26dd250dc0e8bd",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9d29d1fccfb94f53b75ae6ee03b4c72b",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡229k/229kâ€‡[00:00&lt;00:00,â€‡8.28MB/s]"
      }
     },
     "28846bf36af04588bb399af185fda8b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e8e1b80ad61c4222916a971c23e46dcc",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_98327640869c453cb7cafb15c1774c13",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "2b5c71c442d04f6bbdc9565f97d768c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2de13be20aed4bb6b775f088db4f9fe9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0d74cc48c21841a19bace1fbd382704a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a85fb7f1f3014bc78baba3ec94475c6e",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1.53k/1.53kâ€‡[00:00&lt;00:00,â€‡168kB/s]"
      }
     },
     "2f1d3e8d77d146429c39e2e86780828b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42052567563d4f3c80a7c7ceb51cf97a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "43244565035c4876909b7c8a5360bdbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "520cc881f8bb47f398019d58ae3b99c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a7cc3a36ce74863806dafd64b637186": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_43244565035c4876909b7c8a5360bdbc",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_d4cb61644e5149768d16fd8e92f7c9f4",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:â€‡100%"
      }
     },
     "699285e83f4847498d26dd250dc0e8bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75795c575f544a2e9c5af8576e226cd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8035b4f4c9a3480f8237ca1c5a72cf1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_520cc881f8bb47f398019d58ae3b99c5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_d4617cf4c69348a89f493d6f2adae538",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:â€‡100%"
      }
     },
     "804e9790ce6947edb12dc736ffea4a64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81c44ab8f5e040878c469db1e9d9d531": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b821cac9f64548bf9530b8525243588d",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75795c575f544a2e9c5af8576e226cd5",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "838c38e718974221aade135289f12c93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_895338bbc4b94967a2f63a1422225922",
        "IPY_MODEL_8e8cd573d8b14c0eb35f9c587ab6b201",
        "IPY_MODEL_2de13be20aed4bb6b775f088db4f9fe9"
       ],
       "layout": "IPY_MODEL_d4e0519cbc9f4bb59c4c346d8a84b29a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "870321547fd64216bdf635910e9c69e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "881ec80bc576416d95372bdfbe0fd714": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c859ef1d9fb54d799a7453ef3ec9b9b5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_020fb174190f4ee693fc53cb8b13f648",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2.00/2.00â€‡[00:00&lt;00:00,â€‡165B/s]"
      }
     },
     "895338bbc4b94967a2f63a1422225922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fcfe8ffa015542c5b42682636389a928",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_cb286e413953497eb1ed67473349a4d4",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:â€‡100%"
      }
     },
     "8e8cd573d8b14c0eb35f9c587ab6b201": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_870321547fd64216bdf635910e9c69e0",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_42052567563d4f3c80a7c7ceb51cf97a",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "98327640869c453cb7cafb15c1774c13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9abec03f47924e31965613df9585d7eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a81e504c05f64aa2949dbdd639a31b99",
        "IPY_MODEL_28846bf36af04588bb399af185fda8b6",
        "IPY_MODEL_24ef060db71c4975bac6353a651302ac"
       ],
       "layout": "IPY_MODEL_f1122d4ce13146d599932b06e0c97f6b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9d29d1fccfb94f53b75ae6ee03b4c72b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a760ac82108140418214ed2b90f87d09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2f1d3e8d77d146429c39e2e86780828b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ba322bfc93044f30abfa65f1d66453eb",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡112/112â€‡[00:00&lt;00:00,â€‡12.4kB/s]"
      }
     },
     "a81e504c05f64aa2949dbdd639a31b99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d9709297998742edb2052a4eaf819ee5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_baeff7e7782f474fa1eb72e0c90d9976",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:â€‡100%"
      }
     },
     "a85fb7f1f3014bc78baba3ec94475c6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b821cac9f64548bf9530b8525243588d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba322bfc93044f30abfa65f1d66453eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baeff7e7782f474fa1eb72e0c90d9976": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c859ef1d9fb54d799a7453ef3ec9b9b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb286e413953497eb1ed67473349a4d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d26a08e317334710bb5022a68ae1735e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2b5c71c442d04f6bbdc9565f97d768c0",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09c59fb94f2047bb9336d088cab740fa",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "d4617cf4c69348a89f493d6f2adae538": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d4cb61644e5149768d16fd8e92f7c9f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d4e0519cbc9f4bb59c4c346d8a84b29a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9709297998742edb2052a4eaf819ee5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8e1b80ad61c4222916a971c23e46dcc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eda92a11dd234951abc518cfee5c8e5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1122d4ce13146d599932b06e0c97f6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2162d30133946488650a54fcef38f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8035b4f4c9a3480f8237ca1c5a72cf1e",
        "IPY_MODEL_d26a08e317334710bb5022a68ae1735e",
        "IPY_MODEL_881ec80bc576416d95372bdfbe0fd714"
       ],
       "layout": "IPY_MODEL_804e9790ce6947edb12dc736ffea4a64",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f38585e4d0964c3498be73400860a327": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a7cc3a36ce74863806dafd64b637186",
        "IPY_MODEL_81c44ab8f5e040878c469db1e9d9d531",
        "IPY_MODEL_a760ac82108140418214ed2b90f87d09"
       ],
       "layout": "IPY_MODEL_eda92a11dd234951abc518cfee5c8e5d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fcfe8ffa015542c5b42682636389a928": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
