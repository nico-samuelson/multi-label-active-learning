{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e801ec79",
   "metadata": {
    "papermill": {
     "duration": 0.011166,
     "end_time": "2025-06-27T15:48:43.974030",
     "exception": false,
     "start_time": "2025-06-27T15:48:43.962864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4f8899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:48:43.995521Z",
     "iopub.status.busy": "2025-06-27T15:48:43.995195Z",
     "iopub.status.idle": "2025-06-27T15:49:09.215581Z",
     "shell.execute_reply": "2025-06-27T15:49:09.214813Z"
    },
    "papermill": {
     "duration": 25.23285,
     "end_time": "2025-06-27T15:49:09.217168",
     "exception": false,
     "start_time": "2025-06-27T15:48:43.984318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23571577",
   "metadata": {
    "papermill": {
     "duration": 0.019971,
     "end_time": "2025-06-27T15:49:09.248208",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.228237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3111420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.271741Z",
     "iopub.status.busy": "2025-06-27T15:49:09.271149Z",
     "iopub.status.idle": "2025-06-27T15:49:09.274910Z",
     "shell.execute_reply": "2025-06-27T15:49:09.274178Z"
    },
    "papermill": {
     "duration": 0.016439,
     "end_time": "2025-06-27T15:49:09.276121",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.259682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9a646f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.298816Z",
     "iopub.status.busy": "2025-06-27T15:49:09.298594Z",
     "iopub.status.idle": "2025-06-27T15:49:09.302271Z",
     "shell.execute_reply": "2025-06-27T15:49:09.301618Z"
    },
    "papermill": {
     "duration": 0.017127,
     "end_time": "2025-06-27T15:49:09.303578",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.286451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1487201a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.324416Z",
     "iopub.status.busy": "2025-06-27T15:49:09.324179Z",
     "iopub.status.idle": "2025-06-27T15:49:09.333364Z",
     "shell.execute_reply": "2025-06-27T15:49:09.332585Z"
    },
    "papermill": {
     "duration": 0.020817,
     "end_time": "2025-06-27T15:49:09.334730",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.313913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d667f16",
   "metadata": {
    "papermill": {
     "duration": 0.009844,
     "end_time": "2025-06-27T15:49:09.355069",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.345225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bfdbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.376664Z",
     "iopub.status.busy": "2025-06-27T15:49:09.376438Z",
     "iopub.status.idle": "2025-06-27T15:49:09.428494Z",
     "shell.execute_reply": "2025-06-27T15:49:09.427100Z"
    },
    "papermill": {
     "duration": 0.064908,
     "end_time": "2025-06-27T15:49:09.429989",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.365081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-kmeans-kfold'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4437a",
   "metadata": {
    "papermill": {
     "duration": 0.009867,
     "end_time": "2025-06-27T15:49:09.449873",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.440006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ef5a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.471769Z",
     "iopub.status.busy": "2025-06-27T15:49:09.471366Z",
     "iopub.status.idle": "2025-06-27T15:49:09.756747Z",
     "shell.execute_reply": "2025-06-27T15:49:09.755888Z"
    },
    "papermill": {
     "duration": 0.29818,
     "end_time": "2025-06-27T15:49:09.758182",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.460002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c5a493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.779832Z",
     "iopub.status.busy": "2025-06-27T15:49:09.779552Z",
     "iopub.status.idle": "2025-06-27T15:49:09.807606Z",
     "shell.execute_reply": "2025-06-27T15:49:09.806615Z"
    },
    "papermill": {
     "duration": 0.040423,
     "end_time": "2025-06-27T15:49:09.809220",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.768797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0aa133",
   "metadata": {
    "papermill": {
     "duration": 0.010648,
     "end_time": "2025-06-27T15:49:09.831951",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.821303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1424cc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:09.854892Z",
     "iopub.status.busy": "2025-06-27T15:49:09.854593Z",
     "iopub.status.idle": "2025-06-27T15:49:13.070560Z",
     "shell.execute_reply": "2025-06-27T15:49:13.069510Z"
    },
    "papermill": {
     "duration": 3.229448,
     "end_time": "2025-06-27T15:49:13.072231",
     "exception": false,
     "start_time": "2025-06-27T15:49:09.842783",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877c55a545eb425b943692680364b5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581c927ee6a64803b57b64d1a56c35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fa081457fe4536a5e7ec5ed21d02ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64b04434d10476c8ef610c23ef6c64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define custom Dataset class\n",
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ace92e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.097849Z",
     "iopub.status.busy": "2025-06-27T15:49:13.097084Z",
     "iopub.status.idle": "2025-06-27T15:49:13.102517Z",
     "shell.execute_reply": "2025-06-27T15:49:13.101693Z"
    },
    "papermill": {
     "duration": 0.019586,
     "end_time": "2025-06-27T15:49:13.103771",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.084185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f193f4e",
   "metadata": {
    "papermill": {
     "duration": 0.011651,
     "end_time": "2025-06-27T15:49:13.127526",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.115875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c104110c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.152008Z",
     "iopub.status.busy": "2025-06-27T15:49:13.151706Z",
     "iopub.status.idle": "2025-06-27T15:49:13.157230Z",
     "shell.execute_reply": "2025-06-27T15:49:13.156252Z"
    },
    "papermill": {
     "duration": 0.019557,
     "end_time": "2025-06-27T15:49:13.158653",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.139096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363a2d05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.184281Z",
     "iopub.status.busy": "2025-06-27T15:49:13.183936Z",
     "iopub.status.idle": "2025-06-27T15:49:13.197599Z",
     "shell.execute_reply": "2025-06-27T15:49:13.196744Z"
    },
    "papermill": {
     "duration": 0.028184,
     "end_time": "2025-06-27T15:49:13.198940",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.170756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples...\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5832f7a",
   "metadata": {
    "papermill": {
     "duration": 0.01164,
     "end_time": "2025-06-27T15:49:13.222720",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.211080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f240d3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.247285Z",
     "iopub.status.busy": "2025-06-27T15:49:13.246947Z",
     "iopub.status.idle": "2025-06-27T15:49:13.253008Z",
     "shell.execute_reply": "2025-06-27T15:49:13.252260Z"
    },
    "papermill": {
     "duration": 0.019991,
     "end_time": "2025-06-27T15:49:13.254330",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.234339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d99e4",
   "metadata": {
    "papermill": {
     "duration": 0.011015,
     "end_time": "2025-06-27T15:49:13.277130",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.266115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18422df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.301041Z",
     "iopub.status.busy": "2025-06-27T15:49:13.300607Z",
     "iopub.status.idle": "2025-06-27T15:49:13.316519Z",
     "shell.execute_reply": "2025-06-27T15:49:13.315788Z"
    },
    "papermill": {
     "duration": 0.029498,
     "end_time": "2025-06-27T15:49:13.317847",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.288349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmeans_clustering_sampling(model, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            outputs = model.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = outputs.last_hidden_state.mean(dim=1)  # Mean of hidden states for vector representation\n",
    "            embeddings.append(hidden_states.cpu().numpy())\n",
    "    \n",
    "    # Convert embeddings list to numpy array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    collected_indices = set()  # Initialize set to store selected indices\n",
    "    thresholds = []\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        target_samples = len(embeddings[:math.ceil(0.1 * len(embeddings))])\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "                \n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train_fold[i] for i in temp],\n",
    "                'pornografi': [y_train_fold[i][0] for i in temp],\n",
    "                'sara': [y_train_fold[i][1] for i in temp],\n",
    "                'radikalisme': [y_train_fold[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train_fold[i][3] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(embeddings)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train_fold[i] for i in temp],\n",
    "                    'pornografi': [y_train_fold[i][0] for i in temp],\n",
    "                    'sara': [y_train_fold[i][1] for i in temp],\n",
    "                    'radikalisme': [y_train_fold[i][2] for i in temp],\n",
    "                    'pencemaran_nama_baik': [y_train_fold[i][3] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        # threshold_data = pd.DataFrame({\n",
    "        #     'Threshold': thresholds\n",
    "        # })\n",
    "        # threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d4f42",
   "metadata": {
    "papermill": {
     "duration": 0.010849,
     "end_time": "2025-06-27T15:49:13.339888",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.329039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58f5ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:49:13.363730Z",
     "iopub.status.busy": "2025-06-27T15:49:13.363432Z",
     "iopub.status.idle": "2025-06-27T21:01:00.782122Z",
     "shell.execute_reply": "2025-06-27T21:01:00.781010Z"
    },
    "papermill": {
     "duration": 18707.432526,
     "end_time": "2025-06-27T21:01:00.783738",
     "exception": false,
     "start_time": "2025-06-27T15:49:13.351212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 388 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11222b0949634cfaa59e0d8d50ab1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5233, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4851, Accuracy: 0.8166, F1 Micro: 0.2367, F1 Macro: 0.1613\n",
      "Epoch 4/10, Train Loss: 0.4274, Accuracy: 0.8252, F1 Micro: 0.3327, F1 Macro: 0.227\n",
      "Epoch 5/10, Train Loss: 0.3978, Accuracy: 0.8386, F1 Micro: 0.4661, F1 Macro: 0.3619\n",
      "Epoch 6/10, Train Loss: 0.349, Accuracy: 0.8506, F1 Micro: 0.5631, F1 Macro: 0.5253\n",
      "Epoch 7/10, Train Loss: 0.3106, Accuracy: 0.862, F1 Micro: 0.652, F1 Macro: 0.6434\n",
      "Epoch 8/10, Train Loss: 0.2486, Accuracy: 0.8617, F1 Micro: 0.6641, F1 Macro: 0.6497\n",
      "Epoch 9/10, Train Loss: 0.2302, Accuracy: 0.8678, F1 Micro: 0.6874, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.8681, F1 Micro: 0.6544, F1 Macro: 0.6387\n",
      "Best result for 388 samples: F1 Micro: 0.6874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.79      0.85       369\n",
      "                sara       0.55      0.57      0.56       262\n",
      "         radikalisme       0.65      0.72      0.68       234\n",
      "pencemaran_nama_baik       0.63      0.67      0.65       478\n",
      "\n",
      "           micro avg       0.68      0.69      0.69      1343\n",
      "           macro avg       0.68      0.69      0.68      1343\n",
      "        weighted avg       0.69      0.69      0.69      1343\n",
      "         samples avg       0.37      0.39      0.37      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 33.081571102142334 seconds\n",
      "\n",
      "Fold 1 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5315, Accuracy: 0.815, F1 Micro: 0.2312, F1 Macro: 0.1686\n",
      "Epoch 2/10, Train Loss: 0.4107, Accuracy: 0.83, F1 Micro: 0.3592, F1 Macro: 0.2442\n",
      "Epoch 3/10, Train Loss: 0.3171, Accuracy: 0.8473, F1 Micro: 0.4898, F1 Macro: 0.3883\n",
      "Epoch 4/10, Train Loss: 0.2693, Accuracy: 0.8727, F1 Micro: 0.6832, F1 Macro: 0.6759\n",
      "Epoch 5/10, Train Loss: 0.1998, Accuracy: 0.8773, F1 Micro: 0.6714, F1 Macro: 0.6569\n",
      "Epoch 6/10, Train Loss: 0.1633, Accuracy: 0.8755, F1 Micro: 0.6506, F1 Macro: 0.6303\n",
      "Epoch 7/10, Train Loss: 0.1441, Accuracy: 0.8773, F1 Micro: 0.6768, F1 Macro: 0.6608\n",
      "Epoch 8/10, Train Loss: 0.1081, Accuracy: 0.878, F1 Micro: 0.6914, F1 Macro: 0.6794\n",
      "Epoch 9/10, Train Loss: 0.0868, Accuracy: 0.8813, F1 Micro: 0.6898, F1 Macro: 0.6777\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.8816, F1 Micro: 0.7067, F1 Macro: 0.6963\n",
      "Best result for 971 samples: F1 Micro: 0.7067\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.83      0.87       369\n",
      "                sara       0.61      0.48      0.54       262\n",
      "         radikalisme       0.72      0.70      0.71       234\n",
      "pencemaran_nama_baik       0.67      0.66      0.66       478\n",
      "\n",
      "           micro avg       0.74      0.68      0.71      1343\n",
      "           macro avg       0.73      0.67      0.70      1343\n",
      "        weighted avg       0.73      0.68      0.71      1343\n",
      "         samples avg       0.39      0.39      0.38      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 33.810707569122314 seconds\n",
      "\n",
      "Fold 1 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4855, Accuracy: 0.8144, F1 Micro: 0.2215, F1 Macro: 0.1668\n",
      "Epoch 2/10, Train Loss: 0.3588, Accuracy: 0.8505, F1 Micro: 0.5436, F1 Macro: 0.4915\n",
      "Epoch 3/10, Train Loss: 0.2901, Accuracy: 0.8736, F1 Micro: 0.6739, F1 Macro: 0.6598\n",
      "Epoch 4/10, Train Loss: 0.2276, Accuracy: 0.8863, F1 Micro: 0.7178, F1 Macro: 0.7044\n",
      "Epoch 5/10, Train Loss: 0.1833, Accuracy: 0.8859, F1 Micro: 0.7171, F1 Macro: 0.7108\n",
      "Epoch 6/10, Train Loss: 0.1469, Accuracy: 0.8905, F1 Micro: 0.7307, F1 Macro: 0.7241\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.8891, F1 Micro: 0.7009, F1 Macro: 0.676\n",
      "Epoch 8/10, Train Loss: 0.0808, Accuracy: 0.8883, F1 Micro: 0.6948, F1 Macro: 0.6724\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.8909, F1 Micro: 0.7258, F1 Macro: 0.7195\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.8872, F1 Micro: 0.6877, F1 Macro: 0.6619\n",
      "Best result for 1496 samples: F1 Micro: 0.7307\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.83      0.88       369\n",
      "                sara       0.62      0.56      0.59       262\n",
      "         radikalisme       0.76      0.72      0.74       234\n",
      "pencemaran_nama_baik       0.70      0.69      0.69       478\n",
      "\n",
      "           micro avg       0.75      0.71      0.73      1343\n",
      "           macro avg       0.75      0.70      0.72      1343\n",
      "        weighted avg       0.76      0.71      0.73      1343\n",
      "         samples avg       0.40      0.40      0.39      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 29.394462823867798 seconds\n",
      "\n",
      "Fold 1 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4697, Accuracy: 0.8278, F1 Micro: 0.3393, F1 Macro: 0.2291\n",
      "Epoch 2/10, Train Loss: 0.3401, Accuracy: 0.8648, F1 Micro: 0.5997, F1 Macro: 0.5612\n",
      "Epoch 3/10, Train Loss: 0.2579, Accuracy: 0.8863, F1 Micro: 0.7007, F1 Macro: 0.688\n",
      "Epoch 4/10, Train Loss: 0.2147, Accuracy: 0.8891, F1 Micro: 0.7292, F1 Macro: 0.7115\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.8905, F1 Micro: 0.7199, F1 Macro: 0.7097\n",
      "Epoch 6/10, Train Loss: 0.137, Accuracy: 0.8911, F1 Micro: 0.7246, F1 Macro: 0.7157\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.8878, F1 Micro: 0.7305, F1 Macro: 0.7266\n",
      "Epoch 8/10, Train Loss: 0.0764, Accuracy: 0.8886, F1 Micro: 0.7218, F1 Macro: 0.715\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.8895, F1 Micro: 0.7115, F1 Macro: 0.7\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.8872, F1 Micro: 0.7278, F1 Macro: 0.7233\n",
      "Best result for 1969 samples: F1 Micro: 0.7305\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.88       369\n",
      "                sara       0.66      0.58      0.62       262\n",
      "         radikalisme       0.75      0.70      0.73       234\n",
      "pencemaran_nama_baik       0.64      0.72      0.68       478\n",
      "\n",
      "           micro avg       0.74      0.72      0.73      1343\n",
      "           macro avg       0.74      0.71      0.73      1343\n",
      "        weighted avg       0.74      0.72      0.73      1343\n",
      "         samples avg       0.42      0.41      0.41      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 26.75754976272583 seconds\n",
      "\n",
      "Fold 1 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4658, Accuracy: 0.8358, F1 Micro: 0.3998, F1 Macro: 0.3142\n",
      "Epoch 2/10, Train Loss: 0.3187, Accuracy: 0.8703, F1 Micro: 0.6186, F1 Macro: 0.6023\n",
      "Epoch 3/10, Train Loss: 0.2569, Accuracy: 0.8869, F1 Micro: 0.6935, F1 Macro: 0.688\n",
      "Epoch 4/10, Train Loss: 0.1994, Accuracy: 0.8933, F1 Micro: 0.7227, F1 Macro: 0.7101\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.8966, F1 Micro: 0.7543, F1 Macro: 0.7531\n",
      "Epoch 6/10, Train Loss: 0.1161, Accuracy: 0.8941, F1 Micro: 0.7244, F1 Macro: 0.7123\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.893, F1 Micro: 0.7437, F1 Macro: 0.7414\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.8967, F1 Micro: 0.7415, F1 Macro: 0.7368\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.8927, F1 Micro: 0.7447, F1 Macro: 0.7435\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.8945, F1 Micro: 0.7301, F1 Macro: 0.7243\n",
      "Best result for 2394 samples: F1 Micro: 0.7543\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       369\n",
      "                sara       0.64      0.67      0.66       262\n",
      "         radikalisme       0.74      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.71      0.70      0.70       478\n",
      "\n",
      "           micro avg       0.75      0.76      0.75      1343\n",
      "           macro avg       0.75      0.76      0.75      1343\n",
      "        weighted avg       0.75      0.76      0.76      1343\n",
      "         samples avg       0.42      0.42      0.41      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 24.274484872817993 seconds\n",
      "\n",
      "Fold 1 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4494, Accuracy: 0.8366, F1 Micro: 0.4117, F1 Macro: 0.294\n",
      "Epoch 2/10, Train Loss: 0.2989, Accuracy: 0.8781, F1 Micro: 0.6493, F1 Macro: 0.6267\n",
      "Epoch 3/10, Train Loss: 0.2343, Accuracy: 0.883, F1 Micro: 0.6511, F1 Macro: 0.6256\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.8975, F1 Micro: 0.723, F1 Macro: 0.7116\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.8994, F1 Micro: 0.7515, F1 Macro: 0.7473\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.8927, F1 Micro: 0.7197, F1 Macro: 0.7026\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.892, F1 Micro: 0.7472, F1 Macro: 0.75\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.8998, F1 Micro: 0.7455, F1 Macro: 0.7374\n",
      "Epoch 9/10, Train Loss: 0.047, Accuracy: 0.8983, F1 Micro: 0.748, F1 Macro: 0.746\n",
      "Epoch 10/10, Train Loss: 0.0354, Accuracy: 0.8956, F1 Micro: 0.738, F1 Macro: 0.7342\n",
      "Best result for 2777 samples: F1 Micro: 0.7515\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       369\n",
      "                sara       0.72      0.58      0.64       262\n",
      "         radikalisme       0.79      0.74      0.76       234\n",
      "pencemaran_nama_baik       0.70      0.67      0.69       478\n",
      "\n",
      "           micro avg       0.78      0.73      0.75      1343\n",
      "           macro avg       0.78      0.72      0.75      1343\n",
      "        weighted avg       0.78      0.73      0.75      1343\n",
      "         samples avg       0.42      0.41      0.40      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 21.67094135284424 seconds\n",
      "\n",
      "Fold 1 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4414, Accuracy: 0.847, F1 Micro: 0.4956, F1 Macro: 0.4325\n",
      "Epoch 2/10, Train Loss: 0.2865, Accuracy: 0.8864, F1 Micro: 0.7074, F1 Macro: 0.6923\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.8928, F1 Micro: 0.7245, F1 Macro: 0.7071\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.8927, F1 Micro: 0.7481, F1 Macro: 0.7452\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.8931, F1 Micro: 0.7206, F1 Macro: 0.706\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.8944, F1 Micro: 0.757, F1 Macro: 0.7585\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.8975, F1 Micro: 0.7405, F1 Macro: 0.7367\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.8963, F1 Micro: 0.7392, F1 Macro: 0.7317\n",
      "Epoch 9/10, Train Loss: 0.0489, Accuracy: 0.8941, F1 Micro: 0.7478, F1 Macro: 0.7491\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.8961, F1 Micro: 0.7518, F1 Macro: 0.7497\n",
      "Best result for 3122 samples: F1 Micro: 0.757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       369\n",
      "                sara       0.67      0.65      0.66       262\n",
      "         radikalisme       0.77      0.78      0.77       234\n",
      "pencemaran_nama_baik       0.63      0.79      0.70       478\n",
      "\n",
      "           micro avg       0.73      0.78      0.76      1343\n",
      "           macro avg       0.75      0.77      0.76      1343\n",
      "        weighted avg       0.74      0.78      0.76      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.728477954864502 seconds\n",
      "\n",
      "Fold 1 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4337, Accuracy: 0.8562, F1 Micro: 0.5867, F1 Macro: 0.5723\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.8834, F1 Micro: 0.6687, F1 Macro: 0.6425\n",
      "Epoch 3/10, Train Loss: 0.2233, Accuracy: 0.8909, F1 Micro: 0.6917, F1 Macro: 0.6758\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.898, F1 Micro: 0.7318, F1 Macro: 0.7223\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.8998, F1 Micro: 0.758, F1 Macro: 0.7575\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9016, F1 Micro: 0.7407, F1 Macro: 0.734\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.8981, F1 Micro: 0.7515, F1 Macro: 0.7494\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9003, F1 Micro: 0.7607, F1 Macro: 0.7597\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.8992, F1 Micro: 0.7518, F1 Macro: 0.7492\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.898, F1 Micro: 0.7553, F1 Macro: 0.7521\n",
      "Best result for 3432 samples: F1 Micro: 0.7607\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       369\n",
      "                sara       0.69      0.63      0.66       262\n",
      "         radikalisme       0.79      0.75      0.77       234\n",
      "pencemaran_nama_baik       0.68      0.74      0.71       478\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1343\n",
      "           macro avg       0.78      0.75      0.76      1343\n",
      "        weighted avg       0.77      0.76      0.76      1343\n",
      "         samples avg       0.44      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 18.045342922210693 seconds\n",
      "\n",
      "Fold 1 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4118, Accuracy: 0.8633, F1 Micro: 0.5943, F1 Macro: 0.5532\n",
      "Epoch 2/10, Train Loss: 0.261, Accuracy: 0.893, F1 Micro: 0.7161, F1 Macro: 0.7018\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.8983, F1 Micro: 0.7528, F1 Macro: 0.7442\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9006, F1 Micro: 0.7444, F1 Macro: 0.7384\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.8989, F1 Micro: 0.7468, F1 Macro: 0.743\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.897, F1 Micro: 0.7542, F1 Macro: 0.7571\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.8998, F1 Micro: 0.7511, F1 Macro: 0.7458\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.8923, F1 Micro: 0.7531, F1 Macro: 0.7554\n",
      "Epoch 9/10, Train Loss: 0.044, Accuracy: 0.9033, F1 Micro: 0.7633, F1 Macro: 0.7605\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9031, F1 Micro: 0.7606, F1 Macro: 0.7571\n",
      "Best result for 3711 samples: F1 Micro: 0.7633\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       369\n",
      "                sara       0.69      0.61      0.64       262\n",
      "         radikalisme       0.77      0.80      0.79       234\n",
      "pencemaran_nama_baik       0.73      0.69      0.71       478\n",
      "\n",
      "           micro avg       0.78      0.74      0.76      1343\n",
      "           macro avg       0.78      0.74      0.76      1343\n",
      "        weighted avg       0.78      0.74      0.76      1343\n",
      "         samples avg       0.43      0.42      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 16.23469305038452 seconds\n",
      "\n",
      "Fold 1 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4219, Accuracy: 0.8642, F1 Micro: 0.6115, F1 Macro: 0.5668\n",
      "Epoch 2/10, Train Loss: 0.2723, Accuracy: 0.8916, F1 Micro: 0.7135, F1 Macro: 0.7016\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.8975, F1 Micro: 0.7549, F1 Macro: 0.7554\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9008, F1 Micro: 0.754, F1 Macro: 0.744\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.8973, F1 Micro: 0.7509, F1 Macro: 0.7464\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9034, F1 Micro: 0.761, F1 Macro: 0.7563\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.8978, F1 Micro: 0.7545, F1 Macro: 0.75\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.8925, F1 Micro: 0.7609, F1 Macro: 0.7634\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9022, F1 Micro: 0.7472, F1 Macro: 0.7351\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.8981, F1 Micro: 0.7485, F1 Macro: 0.7478\n",
      "Best result for 3886 samples: F1 Micro: 0.761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       369\n",
      "                sara       0.66      0.60      0.63       262\n",
      "         radikalisme       0.80      0.75      0.77       234\n",
      "pencemaran_nama_baik       0.74      0.69      0.72       478\n",
      "\n",
      "           micro avg       0.79      0.73      0.76      1343\n",
      "           macro avg       0.79      0.73      0.76      1343\n",
      "        weighted avg       0.79      0.73      0.76      1343\n",
      "         samples avg       0.42      0.42      0.41      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 15.341254949569702 seconds\n",
      "\n",
      "Fold 1 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.8722, F1 Micro: 0.6483, F1 Macro: 0.617\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.8938, F1 Micro: 0.7299, F1 Macro: 0.7182\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.8994, F1 Micro: 0.7488, F1 Macro: 0.7439\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9017, F1 Micro: 0.7638, F1 Macro: 0.7582\n",
      "Epoch 5/10, Train Loss: 0.1277, Accuracy: 0.9002, F1 Micro: 0.7361, F1 Macro: 0.7262\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.8944, F1 Micro: 0.7547, F1 Macro: 0.7536\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.8963, F1 Micro: 0.7408, F1 Macro: 0.7331\n",
      "Epoch 8/10, Train Loss: 0.0481, Accuracy: 0.8988, F1 Micro: 0.7439, F1 Macro: 0.7382\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.9008, F1 Micro: 0.7522, F1 Macro: 0.7474\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.898, F1 Micro: 0.7595, F1 Macro: 0.7596\n",
      "Best result for 4120 samples: F1 Micro: 0.7638\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       369\n",
      "                sara       0.67      0.63      0.65       262\n",
      "         radikalisme       0.77      0.75      0.76       234\n",
      "pencemaran_nama_baik       0.71      0.74      0.72       478\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1343\n",
      "           macro avg       0.77      0.75      0.76      1343\n",
      "        weighted avg       0.77      0.76      0.76      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.760390281677246 seconds\n",
      "\n",
      "Fold 1 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4062, Accuracy: 0.8664, F1 Micro: 0.5957, F1 Macro: 0.5779\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.8916, F1 Micro: 0.7478, F1 Macro: 0.75\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.9009, F1 Micro: 0.7522, F1 Macro: 0.7461\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9009, F1 Micro: 0.7468, F1 Macro: 0.7448\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9013, F1 Micro: 0.764, F1 Macro: 0.7614\n",
      "Epoch 6/10, Train Loss: 0.0939, Accuracy: 0.8973, F1 Micro: 0.7603, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.9055, F1 Micro: 0.7579, F1 Macro: 0.7467\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.8945, F1 Micro: 0.7571, F1 Macro: 0.7562\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.9042, F1 Micro: 0.7505, F1 Macro: 0.7417\n",
      "Epoch 10/10, Train Loss: 0.0333, Accuracy: 0.8991, F1 Micro: 0.7529, F1 Macro: 0.7522\n",
      "Best result for 4330 samples: F1 Micro: 0.764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.67      0.68      0.68       262\n",
      "         radikalisme       0.77      0.72      0.75       234\n",
      "pencemaran_nama_baik       0.69      0.74      0.72       478\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1343\n",
      "           macro avg       0.77      0.75      0.76      1343\n",
      "        weighted avg       0.77      0.76      0.77      1343\n",
      "         samples avg       0.42      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.621489763259888 seconds\n",
      "\n",
      "Fold 1 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4145, Accuracy: 0.8778, F1 Micro: 0.6766, F1 Macro: 0.6527\n",
      "Epoch 2/10, Train Loss: 0.2611, Accuracy: 0.8995, F1 Micro: 0.7626, F1 Macro: 0.7607\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.9003, F1 Micro: 0.751, F1 Macro: 0.7474\n",
      "Epoch 4/10, Train Loss: 0.1757, Accuracy: 0.9014, F1 Micro: 0.7733, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9031, F1 Micro: 0.7518, F1 Macro: 0.75\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.8995, F1 Micro: 0.7635, F1 Macro: 0.7658\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.903, F1 Micro: 0.767, F1 Macro: 0.766\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.9017, F1 Micro: 0.7704, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.9036, F1 Micro: 0.7697, F1 Macro: 0.7685\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.9034, F1 Micro: 0.7661, F1 Macro: 0.7598\n",
      "Best result for 4530 samples: F1 Micro: 0.7733\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       369\n",
      "                sara       0.65      0.70      0.67       262\n",
      "         radikalisme       0.79      0.80      0.79       234\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1343\n",
      "           macro avg       0.76      0.79      0.77      1343\n",
      "        weighted avg       0.76      0.80      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.39547348022461 seconds\n",
      "\n",
      "Fold 1 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4087, Accuracy: 0.8728, F1 Micro: 0.6557, F1 Macro: 0.6448\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.8936, F1 Micro: 0.7594, F1 Macro: 0.7588\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.9038, F1 Micro: 0.7684, F1 Macro: 0.7643\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.907, F1 Micro: 0.7564, F1 Macro: 0.7533\n",
      "Epoch 5/10, Train Loss: 0.136, Accuracy: 0.8991, F1 Micro: 0.7642, F1 Macro: 0.7624\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9028, F1 Micro: 0.7708, F1 Macro: 0.7703\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9038, F1 Micro: 0.7695, F1 Macro: 0.7669\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.9025, F1 Micro: 0.7604, F1 Macro: 0.7574\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.8927, F1 Micro: 0.764, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9039, F1 Micro: 0.7713, F1 Macro: 0.773\n",
      "Best result for 4663 samples: F1 Micro: 0.7713\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.70      0.68      0.69       262\n",
      "         radikalisme       0.76      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.69      0.72      0.71       478\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1343\n",
      "           macro avg       0.77      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.77      0.77      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.618400573730469 seconds\n",
      "\n",
      "Fold 1 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4006, Accuracy: 0.8669, F1 Micro: 0.5939, F1 Macro: 0.5644\n",
      "Epoch 2/10, Train Loss: 0.2541, Accuracy: 0.8941, F1 Micro: 0.7273, F1 Macro: 0.701\n",
      "Epoch 3/10, Train Loss: 0.2052, Accuracy: 0.9025, F1 Micro: 0.75, F1 Macro: 0.7467\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9009, F1 Micro: 0.7742, F1 Macro: 0.7738\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9041, F1 Micro: 0.7692, F1 Macro: 0.7682\n",
      "Epoch 6/10, Train Loss: 0.0939, Accuracy: 0.8995, F1 Micro: 0.7582, F1 Macro: 0.7565\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9011, F1 Micro: 0.7546, F1 Macro: 0.7492\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.9027, F1 Micro: 0.7599, F1 Macro: 0.7572\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9023, F1 Micro: 0.7669, F1 Macro: 0.7664\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8983, F1 Micro: 0.764, F1 Macro: 0.7645\n",
      "Best result for 4863 samples: F1 Micro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       369\n",
      "                sara       0.66      0.68      0.67       262\n",
      "         radikalisme       0.75      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       478\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1343\n",
      "           macro avg       0.75      0.80      0.77      1343\n",
      "        weighted avg       0.75      0.81      0.78      1343\n",
      "         samples avg       0.45      0.46      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.487519025802612 seconds\n",
      "\n",
      "Fold 1 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3867, Accuracy: 0.8828, F1 Micro: 0.6936, F1 Macro: 0.6847\n",
      "Epoch 2/10, Train Loss: 0.2522, Accuracy: 0.8972, F1 Micro: 0.7349, F1 Macro: 0.7274\n",
      "Epoch 3/10, Train Loss: 0.1942, Accuracy: 0.9034, F1 Micro: 0.752, F1 Macro: 0.7377\n",
      "Epoch 4/10, Train Loss: 0.1605, Accuracy: 0.9058, F1 Micro: 0.7619, F1 Macro: 0.7547\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9044, F1 Micro: 0.7619, F1 Macro: 0.7554\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9006, F1 Micro: 0.7716, F1 Macro: 0.7712\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.9045, F1 Micro: 0.7599, F1 Macro: 0.7562\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9042, F1 Micro: 0.7744, F1 Macro: 0.7715\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9028, F1 Micro: 0.7733, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9038, F1 Micro: 0.7717, F1 Macro: 0.7685\n",
      "Best result for 5063 samples: F1 Micro: 0.7744\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       369\n",
      "                sara       0.69      0.61      0.65       262\n",
      "         radikalisme       0.81      0.79      0.80       234\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       478\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1343\n",
      "           macro avg       0.78      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.206481695175171 seconds\n",
      "\n",
      "Fold 1 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3975, Accuracy: 0.8773, F1 Micro: 0.66, F1 Macro: 0.6504\n",
      "Epoch 2/10, Train Loss: 0.2566, Accuracy: 0.8977, F1 Micro: 0.7553, F1 Macro: 0.7475\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9066, F1 Micro: 0.7811, F1 Macro: 0.7798\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9042, F1 Micro: 0.7764, F1 Macro: 0.7716\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.9083, F1 Micro: 0.7699, F1 Macro: 0.762\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9055, F1 Micro: 0.7725, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0696, Accuracy: 0.8986, F1 Micro: 0.7681, F1 Macro: 0.7661\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.8969, F1 Micro: 0.7648, F1 Macro: 0.7627\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9017, F1 Micro: 0.7654, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.895, F1 Micro: 0.7668, F1 Macro: 0.7655\n",
      "Best result for 5263 samples: F1 Micro: 0.7811\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       369\n",
      "                sara       0.68      0.67      0.68       262\n",
      "         radikalisme       0.77      0.81      0.79       234\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       478\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1343\n",
      "           macro avg       0.78      0.79      0.78      1343\n",
      "        weighted avg       0.78      0.79      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.98959493637085 seconds\n",
      "\n",
      "Fold 1 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.398, Accuracy: 0.8853, F1 Micro: 0.6937, F1 Macro: 0.6797\n",
      "Epoch 2/10, Train Loss: 0.2429, Accuracy: 0.8983, F1 Micro: 0.7478, F1 Macro: 0.7415\n",
      "Epoch 3/10, Train Loss: 0.2005, Accuracy: 0.9, F1 Micro: 0.7726, F1 Macro: 0.7709\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9067, F1 Micro: 0.7701, F1 Macro: 0.7639\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.8936, F1 Micro: 0.7661, F1 Macro: 0.7626\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9002, F1 Micro: 0.7681, F1 Macro: 0.7655\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9056, F1 Micro: 0.7765, F1 Macro: 0.7716\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9052, F1 Micro: 0.7746, F1 Macro: 0.7727\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9028, F1 Micro: 0.771, F1 Macro: 0.7643\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9011, F1 Micro: 0.7707, F1 Macro: 0.7688\n",
      "Best result for 5441 samples: F1 Micro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       369\n",
      "                sara       0.69      0.63      0.66       262\n",
      "         radikalisme       0.77      0.80      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       478\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1343\n",
      "           macro avg       0.77      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.0217320919036865 seconds\n",
      "\n",
      "Fold 1 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3857, Accuracy: 0.873, F1 Micro: 0.7128, F1 Macro: 0.7076\n",
      "Epoch 2/10, Train Loss: 0.2439, Accuracy: 0.8992, F1 Micro: 0.7481, F1 Macro: 0.7401\n",
      "Epoch 3/10, Train Loss: 0.2021, Accuracy: 0.9036, F1 Micro: 0.7674, F1 Macro: 0.7651\n",
      "Epoch 4/10, Train Loss: 0.1556, Accuracy: 0.9052, F1 Micro: 0.7746, F1 Macro: 0.7721\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9056, F1 Micro: 0.7738, F1 Macro: 0.7705\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9056, F1 Micro: 0.7689, F1 Macro: 0.7641\n",
      "Epoch 7/10, Train Loss: 0.0693, Accuracy: 0.902, F1 Micro: 0.7677, F1 Macro: 0.7656\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.9016, F1 Micro: 0.7566, F1 Macro: 0.7454\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9036, F1 Micro: 0.7624, F1 Macro: 0.7599\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9013, F1 Micro: 0.7666, F1 Macro: 0.7634\n",
      "Best result for 5641 samples: F1 Micro: 0.7746\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       369\n",
      "                sara       0.66      0.65      0.66       262\n",
      "         radikalisme       0.75      0.85      0.80       234\n",
      "pencemaran_nama_baik       0.73      0.72      0.72       478\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1343\n",
      "           macro avg       0.77      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.77      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.104210138320923 seconds\n",
      "\n",
      "Fold 1 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3722, Accuracy: 0.8777, F1 Micro: 0.6423, F1 Macro: 0.6024\n",
      "Epoch 2/10, Train Loss: 0.2369, Accuracy: 0.8953, F1 Micro: 0.7072, F1 Macro: 0.6771\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9052, F1 Micro: 0.7751, F1 Macro: 0.7756\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9072, F1 Micro: 0.7765, F1 Macro: 0.7731\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9116, F1 Micro: 0.7913, F1 Macro: 0.7903\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9038, F1 Micro: 0.7757, F1 Macro: 0.7746\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.9086, F1 Micro: 0.777, F1 Macro: 0.7754\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.905, F1 Micro: 0.7791, F1 Macro: 0.7793\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.907, F1 Micro: 0.7787, F1 Macro: 0.7788\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9087, F1 Micro: 0.7847, F1 Macro: 0.7844\n",
      "Best result for 5841 samples: F1 Micro: 0.7913\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.71      0.70      0.71       262\n",
      "         radikalisme       0.79      0.79      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       478\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1343\n",
      "           macro avg       0.79      0.79      0.79      1343\n",
      "        weighted avg       0.79      0.80      0.79      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.9688620567321777 seconds\n",
      "\n",
      "Fold 1 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8878, F1 Micro: 0.7191, F1 Macro: 0.701\n",
      "Epoch 2/10, Train Loss: 0.2425, Accuracy: 0.8986, F1 Micro: 0.7518, F1 Macro: 0.7423\n",
      "Epoch 3/10, Train Loss: 0.1994, Accuracy: 0.9044, F1 Micro: 0.7661, F1 Macro: 0.7619\n",
      "Epoch 4/10, Train Loss: 0.1621, Accuracy: 0.9066, F1 Micro: 0.7843, F1 Macro: 0.7827\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.898, F1 Micro: 0.7682, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9025, F1 Micro: 0.7518, F1 Macro: 0.7427\n",
      "Epoch 7/10, Train Loss: 0.0668, Accuracy: 0.9052, F1 Micro: 0.7771, F1 Macro: 0.776\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9008, F1 Micro: 0.7666, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0307, Accuracy: 0.9061, F1 Micro: 0.7707, F1 Macro: 0.7611\n",
      "Best result for 6041 samples: F1 Micro: 0.7843\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.66      0.71      0.69       262\n",
      "         radikalisme       0.74      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       478\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1343\n",
      "           macro avg       0.76      0.81      0.78      1343\n",
      "        weighted avg       0.77      0.81      0.79      1343\n",
      "         samples avg       0.45      0.46      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.1625993251800537 seconds\n",
      "\n",
      "Fold 1 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3639, Accuracy: 0.8878, F1 Micro: 0.7274, F1 Macro: 0.7136\n",
      "Epoch 2/10, Train Loss: 0.2423, Accuracy: 0.9033, F1 Micro: 0.7638, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.1943, Accuracy: 0.9089, F1 Micro: 0.7741, F1 Macro: 0.7666\n",
      "Epoch 4/10, Train Loss: 0.1585, Accuracy: 0.9047, F1 Micro: 0.7764, F1 Macro: 0.7701\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9053, F1 Micro: 0.7669, F1 Macro: 0.759\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.9014, F1 Micro: 0.7708, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.0636, Accuracy: 0.9075, F1 Micro: 0.77, F1 Macro: 0.7633\n",
      "Epoch 8/10, Train Loss: 0.0442, Accuracy: 0.907, F1 Micro: 0.7754, F1 Macro: 0.7712\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9016, F1 Micro: 0.7721, F1 Macro: 0.7699\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.9036, F1 Micro: 0.7734, F1 Macro: 0.7743\n",
      "Best result for 6218 samples: F1 Micro: 0.7764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       369\n",
      "                sara       0.68      0.64      0.66       262\n",
      "         radikalisme       0.81      0.73      0.76       234\n",
      "pencemaran_nama_baik       0.68      0.82      0.74       478\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1343\n",
      "           macro avg       0.78      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.79      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 3742.29 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4954, Accuracy: 0.7953, F1 Micro: 0.099, F1 Macro: 0.08\n",
      "Epoch 3/10, Train Loss: 0.448, Accuracy: 0.8238, F1 Micro: 0.3229, F1 Macro: 0.2108\n",
      "Epoch 4/10, Train Loss: 0.3886, Accuracy: 0.8323, F1 Micro: 0.3975, F1 Macro: 0.3047\n",
      "Epoch 5/10, Train Loss: 0.3358, Accuracy: 0.8384, F1 Micro: 0.4615, F1 Macro: 0.3692\n",
      "Epoch 6/10, Train Loss: 0.2851, Accuracy: 0.8464, F1 Micro: 0.5131, F1 Macro: 0.4513\n",
      "Epoch 7/10, Train Loss: 0.2509, Accuracy: 0.8661, F1 Micro: 0.6355, F1 Macro: 0.5946\n",
      "Epoch 8/10, Train Loss: 0.2376, Accuracy: 0.872, F1 Micro: 0.6667, F1 Macro: 0.6441\n",
      "Epoch 9/10, Train Loss: 0.188, Accuracy: 0.8628, F1 Micro: 0.6132, F1 Macro: 0.5838\n",
      "Epoch 10/10, Train Loss: 0.1613, Accuracy: 0.8709, F1 Micro: 0.6541, F1 Macro: 0.628\n",
      "Best result for 388 samples: F1 Micro: 0.6667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.80      0.86       378\n",
      "                sara       0.62      0.35      0.45       253\n",
      "         radikalisme       0.67      0.62      0.65       234\n",
      "pencemaran_nama_baik       0.72      0.54      0.62       517\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1382\n",
      "           macro avg       0.74      0.58      0.64      1382\n",
      "        weighted avg       0.75      0.59      0.66      1382\n",
      "         samples avg       0.38      0.35      0.35      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 36.220073223114014 seconds\n",
      "\n",
      "Fold 2 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5136, Accuracy: 0.8205, F1 Micro: 0.2929, F1 Macro: 0.208\n",
      "Epoch 2/10, Train Loss: 0.3679, Accuracy: 0.8347, F1 Micro: 0.4083, F1 Macro: 0.2647\n",
      "Epoch 3/10, Train Loss: 0.2893, Accuracy: 0.8556, F1 Micro: 0.5385, F1 Macro: 0.4218\n",
      "Epoch 4/10, Train Loss: 0.2445, Accuracy: 0.8836, F1 Micro: 0.7007, F1 Macro: 0.6873\n",
      "Epoch 5/10, Train Loss: 0.1931, Accuracy: 0.875, F1 Micro: 0.6448, F1 Macro: 0.6215\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.8931, F1 Micro: 0.7448, F1 Macro: 0.7414\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.8883, F1 Micro: 0.7382, F1 Macro: 0.7375\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.8905, F1 Micro: 0.7299, F1 Macro: 0.7169\n",
      "Epoch 9/10, Train Loss: 0.0685, Accuracy: 0.89, F1 Micro: 0.7246, F1 Macro: 0.7177\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.8863, F1 Micro: 0.6951, F1 Macro: 0.6751\n",
      "Best result for 971 samples: F1 Micro: 0.7448\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.86      0.90       378\n",
      "                sara       0.63      0.65      0.64       253\n",
      "         radikalisme       0.72      0.77      0.74       234\n",
      "pencemaran_nama_baik       0.75      0.63      0.69       517\n",
      "\n",
      "           micro avg       0.77      0.72      0.74      1382\n",
      "           macro avg       0.76      0.73      0.74      1382\n",
      "        weighted avg       0.77      0.72      0.74      1382\n",
      "         samples avg       0.41      0.41      0.40      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 32.35719966888428 seconds\n",
      "\n",
      "Fold 2 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4789, Accuracy: 0.8284, F1 Micro: 0.3549, F1 Macro: 0.2418\n",
      "Epoch 2/10, Train Loss: 0.3248, Accuracy: 0.8678, F1 Micro: 0.6319, F1 Macro: 0.532\n",
      "Epoch 3/10, Train Loss: 0.2465, Accuracy: 0.8848, F1 Micro: 0.7029, F1 Macro: 0.6877\n",
      "Epoch 4/10, Train Loss: 0.2143, Accuracy: 0.8848, F1 Micro: 0.6973, F1 Macro: 0.6789\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.8972, F1 Micro: 0.7717, F1 Macro: 0.7646\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.8922, F1 Micro: 0.7427, F1 Macro: 0.7409\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9023, F1 Micro: 0.7777, F1 Macro: 0.7702\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.8905, F1 Micro: 0.7184, F1 Macro: 0.7014\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.892, F1 Micro: 0.7215, F1 Macro: 0.7042\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.8966, F1 Micro: 0.7452, F1 Macro: 0.7359\n",
      "Best result for 1496 samples: F1 Micro: 0.7777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       378\n",
      "                sara       0.65      0.72      0.68       253\n",
      "         radikalisme       0.71      0.76      0.74       234\n",
      "pencemaran_nama_baik       0.74      0.77      0.75       517\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1382\n",
      "           macro avg       0.76      0.78      0.77      1382\n",
      "        weighted avg       0.77      0.79      0.78      1382\n",
      "         samples avg       0.45      0.45      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 28.924194812774658 seconds\n",
      "\n",
      "Fold 2 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4625, Accuracy: 0.8313, F1 Micro: 0.3699, F1 Macro: 0.2593\n",
      "Epoch 2/10, Train Loss: 0.3134, Accuracy: 0.877, F1 Micro: 0.7188, F1 Macro: 0.7139\n",
      "Epoch 3/10, Train Loss: 0.248, Accuracy: 0.8938, F1 Micro: 0.735, F1 Macro: 0.7192\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.8973, F1 Micro: 0.7495, F1 Macro: 0.7365\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.9009, F1 Micro: 0.755, F1 Macro: 0.7413\n",
      "Epoch 6/10, Train Loss: 0.1155, Accuracy: 0.8898, F1 Micro: 0.7117, F1 Macro: 0.6927\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.9025, F1 Micro: 0.7711, F1 Macro: 0.7647\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.8994, F1 Micro: 0.7533, F1 Macro: 0.7457\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9017, F1 Micro: 0.7602, F1 Macro: 0.7488\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.8897, F1 Micro: 0.7085, F1 Macro: 0.6931\n",
      "Best result for 1969 samples: F1 Micro: 0.7711\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       378\n",
      "                sara       0.67      0.68      0.67       253\n",
      "         radikalisme       0.72      0.76      0.74       234\n",
      "pencemaran_nama_baik       0.76      0.71      0.73       517\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1382\n",
      "           macro avg       0.77      0.76      0.76      1382\n",
      "        weighted avg       0.78      0.76      0.77      1382\n",
      "         samples avg       0.44      0.43      0.43      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 26.566391468048096 seconds\n",
      "\n",
      "Fold 2 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4531, Accuracy: 0.8331, F1 Micro: 0.4196, F1 Macro: 0.2826\n",
      "Epoch 2/10, Train Loss: 0.3053, Accuracy: 0.8723, F1 Micro: 0.6295, F1 Macro: 0.6049\n",
      "Epoch 3/10, Train Loss: 0.2314, Accuracy: 0.8984, F1 Micro: 0.7506, F1 Macro: 0.7481\n",
      "Epoch 4/10, Train Loss: 0.1889, Accuracy: 0.8983, F1 Micro: 0.7744, F1 Macro: 0.7736\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9023, F1 Micro: 0.7621, F1 Macro: 0.753\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.8936, F1 Micro: 0.7203, F1 Macro: 0.7051\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9017, F1 Micro: 0.7587, F1 Macro: 0.7512\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9019, F1 Micro: 0.7618, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9019, F1 Micro: 0.7637, F1 Macro: 0.7632\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.897, F1 Micro: 0.7376, F1 Macro: 0.7278\n",
      "Best result for 2394 samples: F1 Micro: 0.7744\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.90       378\n",
      "                sara       0.60      0.80      0.68       253\n",
      "         radikalisme       0.71      0.84      0.77       234\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       517\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1382\n",
      "           macro avg       0.74      0.82      0.77      1382\n",
      "        weighted avg       0.76      0.81      0.78      1382\n",
      "         samples avg       0.45      0.46      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 23.90353512763977 seconds\n",
      "\n",
      "Fold 2 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4353, Accuracy: 0.8487, F1 Micro: 0.5116, F1 Macro: 0.4217\n",
      "Epoch 2/10, Train Loss: 0.275, Accuracy: 0.8955, F1 Micro: 0.7444, F1 Macro: 0.7309\n",
      "Epoch 3/10, Train Loss: 0.2268, Accuracy: 0.8997, F1 Micro: 0.7671, F1 Macro: 0.7643\n",
      "Epoch 4/10, Train Loss: 0.1684, Accuracy: 0.9008, F1 Micro: 0.7651, F1 Macro: 0.7597\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.8991, F1 Micro: 0.7475, F1 Macro: 0.7361\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9055, F1 Micro: 0.773, F1 Macro: 0.7636\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9047, F1 Micro: 0.7665, F1 Macro: 0.74\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9064, F1 Micro: 0.7838, F1 Macro: 0.7772\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9013, F1 Micro: 0.7556, F1 Macro: 0.7547\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9066, F1 Micro: 0.7821, F1 Macro: 0.7753\n",
      "Best result for 2777 samples: F1 Micro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       378\n",
      "                sara       0.68      0.72      0.70       253\n",
      "         radikalisme       0.75      0.73      0.74       234\n",
      "pencemaran_nama_baik       0.74      0.77      0.75       517\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1382\n",
      "           macro avg       0.78      0.78      0.78      1382\n",
      "        weighted avg       0.79      0.79      0.79      1382\n",
      "         samples avg       0.46      0.45      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 21.927470922470093 seconds\n",
      "\n",
      "Fold 2 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4231, Accuracy: 0.8556, F1 Micro: 0.5435, F1 Macro: 0.4581\n",
      "Epoch 2/10, Train Loss: 0.2702, Accuracy: 0.8925, F1 Micro: 0.725, F1 Macro: 0.6996\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.8983, F1 Micro: 0.7548, F1 Macro: 0.7497\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9039, F1 Micro: 0.7748, F1 Macro: 0.7683\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.9027, F1 Micro: 0.779, F1 Macro: 0.7714\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.903, F1 Micro: 0.7708, F1 Macro: 0.7593\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9033, F1 Micro: 0.7737, F1 Macro: 0.7656\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9013, F1 Micro: 0.7539, F1 Macro: 0.7412\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.9022, F1 Micro: 0.7702, F1 Macro: 0.7656\n",
      "Epoch 10/10, Train Loss: 0.0378, Accuracy: 0.9056, F1 Micro: 0.7731, F1 Macro: 0.7655\n",
      "Best result for 3122 samples: F1 Micro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.63      0.70      0.66       253\n",
      "         radikalisme       0.69      0.82      0.75       234\n",
      "pencemaran_nama_baik       0.75      0.76      0.75       517\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1382\n",
      "           macro avg       0.75      0.79      0.77      1382\n",
      "        weighted avg       0.77      0.79      0.78      1382\n",
      "         samples avg       0.45      0.45      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.723005056381226 seconds\n",
      "\n",
      "Fold 2 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.412, Accuracy: 0.8489, F1 Micro: 0.5044, F1 Macro: 0.4219\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.8959, F1 Micro: 0.7368, F1 Macro: 0.7299\n",
      "Epoch 3/10, Train Loss: 0.217, Accuracy: 0.9022, F1 Micro: 0.7583, F1 Macro: 0.7474\n",
      "Epoch 4/10, Train Loss: 0.1816, Accuracy: 0.9022, F1 Micro: 0.7583, F1 Macro: 0.7547\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9083, F1 Micro: 0.7945, F1 Macro: 0.7875\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9108, F1 Micro: 0.788, F1 Macro: 0.7785\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.902, F1 Micro: 0.758, F1 Macro: 0.7545\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9091, F1 Micro: 0.7921, F1 Macro: 0.7886\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9036, F1 Micro: 0.7667, F1 Macro: 0.7616\n",
      "Epoch 10/10, Train Loss: 0.0375, Accuracy: 0.9014, F1 Micro: 0.7583, F1 Macro: 0.7506\n",
      "Best result for 3432 samples: F1 Micro: 0.7945\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       378\n",
      "                sara       0.70      0.69      0.70       253\n",
      "         radikalisme       0.74      0.79      0.77       234\n",
      "pencemaran_nama_baik       0.71      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.79      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.77      0.82      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.825563430786133 seconds\n",
      "\n",
      "Fold 2 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4143, Accuracy: 0.8827, F1 Micro: 0.7097, F1 Macro: 0.7013\n",
      "Epoch 2/10, Train Loss: 0.2625, Accuracy: 0.8998, F1 Micro: 0.7513, F1 Macro: 0.7475\n",
      "Epoch 3/10, Train Loss: 0.2125, Accuracy: 0.8983, F1 Micro: 0.7382, F1 Macro: 0.7296\n",
      "Epoch 4/10, Train Loss: 0.1601, Accuracy: 0.9042, F1 Micro: 0.7627, F1 Macro: 0.7552\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9072, F1 Micro: 0.7789, F1 Macro: 0.7685\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.9053, F1 Micro: 0.7951, F1 Macro: 0.791\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9092, F1 Micro: 0.789, F1 Macro: 0.7801\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9075, F1 Micro: 0.7959, F1 Macro: 0.7929\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9062, F1 Micro: 0.7761, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.9056, F1 Micro: 0.7771, F1 Macro: 0.771\n",
      "Best result for 3711 samples: F1 Micro: 0.7959\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       378\n",
      "                sara       0.67      0.75      0.71       253\n",
      "         radikalisme       0.73      0.83      0.78       234\n",
      "pencemaran_nama_baik       0.71      0.82      0.76       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.83      0.79      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.47      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 16.105306386947632 seconds\n",
      "\n",
      "Fold 2 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4069, Accuracy: 0.8806, F1 Micro: 0.6912, F1 Macro: 0.6685\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.9033, F1 Micro: 0.7676, F1 Macro: 0.7606\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.9089, F1 Micro: 0.7819, F1 Macro: 0.7767\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9097, F1 Micro: 0.7969, F1 Macro: 0.7936\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9072, F1 Micro: 0.7901, F1 Macro: 0.784\n",
      "Epoch 6/10, Train Loss: 0.1057, Accuracy: 0.908, F1 Micro: 0.7765, F1 Macro: 0.7667\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9108, F1 Micro: 0.7869, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.91, F1 Micro: 0.7907, F1 Macro: 0.7858\n",
      "Epoch 9/10, Train Loss: 0.0393, Accuracy: 0.9084, F1 Micro: 0.7909, F1 Macro: 0.7858\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.91, F1 Micro: 0.7882, F1 Macro: 0.7836\n",
      "Best result for 3886 samples: F1 Micro: 0.7969\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       378\n",
      "                sara       0.69      0.75      0.72       253\n",
      "         radikalisme       0.74      0.82      0.78       234\n",
      "pencemaran_nama_baik       0.73      0.80      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.80      1382\n",
      "           macro avg       0.77      0.82      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.985346555709839 seconds\n",
      "\n",
      "Fold 2 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4074, Accuracy: 0.8817, F1 Micro: 0.7035, F1 Macro: 0.6959\n",
      "Epoch 2/10, Train Loss: 0.2515, Accuracy: 0.9041, F1 Micro: 0.7719, F1 Macro: 0.7558\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9073, F1 Micro: 0.7783, F1 Macro: 0.7726\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9089, F1 Micro: 0.7946, F1 Macro: 0.7916\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.9041, F1 Micro: 0.7653, F1 Macro: 0.7497\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9052, F1 Micro: 0.7604, F1 Macro: 0.7408\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9108, F1 Micro: 0.7927, F1 Macro: 0.789\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9056, F1 Micro: 0.7823, F1 Macro: 0.7817\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9098, F1 Micro: 0.7833, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9119, F1 Micro: 0.7925, F1 Macro: 0.7912\n",
      "Best result for 4120 samples: F1 Micro: 0.7946\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       378\n",
      "                sara       0.65      0.79      0.71       253\n",
      "         radikalisme       0.74      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.79      1382\n",
      "           macro avg       0.77      0.82      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.73716688156128 seconds\n",
      "\n",
      "Fold 2 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3892, Accuracy: 0.8827, F1 Micro: 0.6911, F1 Macro: 0.6686\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9002, F1 Micro: 0.7796, F1 Macro: 0.777\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.8952, F1 Micro: 0.7271, F1 Macro: 0.7002\n",
      "Epoch 4/10, Train Loss: 0.1606, Accuracy: 0.9017, F1 Micro: 0.7863, F1 Macro: 0.782\n",
      "Epoch 5/10, Train Loss: 0.1333, Accuracy: 0.9045, F1 Micro: 0.7774, F1 Macro: 0.7728\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9053, F1 Micro: 0.7886, F1 Macro: 0.7812\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9052, F1 Micro: 0.759, F1 Macro: 0.7484\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9089, F1 Micro: 0.789, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9083, F1 Micro: 0.7811, F1 Macro: 0.7724\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9112, F1 Micro: 0.7954, F1 Macro: 0.7911\n",
      "Best result for 4330 samples: F1 Micro: 0.7954\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       378\n",
      "                sara       0.70      0.68      0.69       253\n",
      "         radikalisme       0.77      0.83      0.80       234\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       517\n",
      "\n",
      "           micro avg       0.79      0.80      0.80      1382\n",
      "           macro avg       0.79      0.80      0.79      1382\n",
      "        weighted avg       0.79      0.80      0.79      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.438966989517212 seconds\n",
      "\n",
      "Fold 2 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3844, Accuracy: 0.8894, F1 Micro: 0.7306, F1 Macro: 0.7169\n",
      "Epoch 2/10, Train Loss: 0.2546, Accuracy: 0.9003, F1 Micro: 0.7666, F1 Macro: 0.762\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.9019, F1 Micro: 0.7526, F1 Macro: 0.7326\n",
      "Epoch 4/10, Train Loss: 0.1729, Accuracy: 0.9102, F1 Micro: 0.7805, F1 Macro: 0.7745\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9092, F1 Micro: 0.8003, F1 Macro: 0.7982\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9038, F1 Micro: 0.7546, F1 Macro: 0.739\n",
      "Epoch 7/10, Train Loss: 0.0668, Accuracy: 0.9092, F1 Micro: 0.7877, F1 Macro: 0.7803\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9098, F1 Micro: 0.7913, F1 Macro: 0.7871\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9072, F1 Micro: 0.7752, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.9089, F1 Micro: 0.7888, F1 Macro: 0.777\n",
      "Best result for 4530 samples: F1 Micro: 0.8003\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       378\n",
      "                sara       0.65      0.81      0.72       253\n",
      "         radikalisme       0.70      0.87      0.77       234\n",
      "pencemaran_nama_baik       0.73      0.80      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.85      0.80      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.260940790176392 seconds\n",
      "\n",
      "Fold 2 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3874, Accuracy: 0.8888, F1 Micro: 0.7247, F1 Macro: 0.7142\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.9066, F1 Micro: 0.7814, F1 Macro: 0.7777\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9056, F1 Micro: 0.7852, F1 Macro: 0.7807\n",
      "Epoch 4/10, Train Loss: 0.1652, Accuracy: 0.9105, F1 Micro: 0.8, F1 Macro: 0.791\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9119, F1 Micro: 0.7978, F1 Macro: 0.7894\n",
      "Epoch 6/10, Train Loss: 0.086, Accuracy: 0.9033, F1 Micro: 0.7591, F1 Macro: 0.7488\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9087, F1 Micro: 0.7839, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.9092, F1 Micro: 0.7915, F1 Macro: 0.7885\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9047, F1 Micro: 0.7703, F1 Macro: 0.758\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9091, F1 Micro: 0.7835, F1 Macro: 0.7784\n",
      "Best result for 4663 samples: F1 Micro: 0.8\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.93       378\n",
      "                sara       0.70      0.64      0.67       253\n",
      "         radikalisme       0.76      0.84      0.80       234\n",
      "pencemaran_nama_baik       0.71      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.83      0.80      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.78      0.83      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.517054080963135 seconds\n",
      "\n",
      "Fold 2 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3856, Accuracy: 0.8886, F1 Micro: 0.7341, F1 Macro: 0.7202\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9053, F1 Micro: 0.7828, F1 Macro: 0.768\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.9094, F1 Micro: 0.786, F1 Macro: 0.7753\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9064, F1 Micro: 0.7756, F1 Macro: 0.7642\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9047, F1 Micro: 0.761, F1 Macro: 0.7487\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9073, F1 Micro: 0.7886, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.0648, Accuracy: 0.9078, F1 Micro: 0.7888, F1 Macro: 0.7819\n",
      "Epoch 8/10, Train Loss: 0.0508, Accuracy: 0.9089, F1 Micro: 0.7904, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.9067, F1 Micro: 0.7864, F1 Macro: 0.7851\n",
      "Epoch 10/10, Train Loss: 0.0329, Accuracy: 0.9042, F1 Micro: 0.7686, F1 Macro: 0.7579\n",
      "Best result for 4863 samples: F1 Micro: 0.7904\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.70      0.63      0.66       253\n",
      "         radikalisme       0.72      0.86      0.79       234\n",
      "pencemaran_nama_baik       0.75      0.78      0.76       517\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1382\n",
      "           macro avg       0.78      0.79      0.78      1382\n",
      "        weighted avg       0.79      0.80      0.79      1382\n",
      "         samples avg       0.47      0.46      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.392739295959473 seconds\n",
      "\n",
      "Fold 2 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3847, Accuracy: 0.8936, F1 Micro: 0.7396, F1 Macro: 0.7231\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9048, F1 Micro: 0.7783, F1 Macro: 0.7736\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9086, F1 Micro: 0.7853, F1 Macro: 0.7725\n",
      "Epoch 4/10, Train Loss: 0.1632, Accuracy: 0.9089, F1 Micro: 0.7862, F1 Macro: 0.7753\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9089, F1 Micro: 0.7759, F1 Macro: 0.758\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9116, F1 Micro: 0.7826, F1 Macro: 0.7746\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.9119, F1 Micro: 0.7993, F1 Macro: 0.7935\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9083, F1 Micro: 0.8001, F1 Macro: 0.7952\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9103, F1 Micro: 0.7975, F1 Macro: 0.7939\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.908, F1 Micro: 0.7823, F1 Macro: 0.7763\n",
      "Best result for 5063 samples: F1 Micro: 0.8001\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.93       378\n",
      "                sara       0.65      0.79      0.71       253\n",
      "         radikalisme       0.69      0.86      0.77       234\n",
      "pencemaran_nama_baik       0.73      0.82      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.85      0.80      1382\n",
      "           macro avg       0.75      0.85      0.80      1382\n",
      "        weighted avg       0.76      0.85      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.315060138702393 seconds\n",
      "\n",
      "Fold 2 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3786, Accuracy: 0.8902, F1 Micro: 0.7629, F1 Macro: 0.7569\n",
      "Epoch 2/10, Train Loss: 0.2484, Accuracy: 0.9048, F1 Micro: 0.7685, F1 Macro: 0.7588\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.9062, F1 Micro: 0.7676, F1 Macro: 0.7546\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.9056, F1 Micro: 0.797, F1 Macro: 0.7949\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9075, F1 Micro: 0.782, F1 Macro: 0.7664\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9066, F1 Micro: 0.7792, F1 Macro: 0.7684\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.9048, F1 Micro: 0.7725, F1 Macro: 0.7637\n",
      "Epoch 8/10, Train Loss: 0.0493, Accuracy: 0.9086, F1 Micro: 0.7951, F1 Macro: 0.7893\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9041, F1 Micro: 0.762, F1 Macro: 0.7516\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.9064, F1 Micro: 0.7761, F1 Macro: 0.7695\n",
      "Best result for 5263 samples: F1 Micro: 0.797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       378\n",
      "                sara       0.61      0.83      0.70       253\n",
      "         radikalisme       0.72      0.86      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.74      0.86      0.80      1382\n",
      "           macro avg       0.75      0.86      0.79      1382\n",
      "        weighted avg       0.76      0.86      0.80      1382\n",
      "         samples avg       0.47      0.49      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 7.070289611816406 seconds\n",
      "\n",
      "Fold 2 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3741, Accuracy: 0.8948, F1 Micro: 0.7448, F1 Macro: 0.7276\n",
      "Epoch 2/10, Train Loss: 0.2505, Accuracy: 0.9066, F1 Micro: 0.7718, F1 Macro: 0.7581\n",
      "Epoch 3/10, Train Loss: 0.2021, Accuracy: 0.9105, F1 Micro: 0.7847, F1 Macro: 0.7791\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9103, F1 Micro: 0.784, F1 Macro: 0.7692\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9083, F1 Micro: 0.7822, F1 Macro: 0.7654\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9083, F1 Micro: 0.7934, F1 Macro: 0.7893\n",
      "Epoch 7/10, Train Loss: 0.0601, Accuracy: 0.9086, F1 Micro: 0.792, F1 Macro: 0.7883\n",
      "Epoch 8/10, Train Loss: 0.0524, Accuracy: 0.9097, F1 Micro: 0.7814, F1 Macro: 0.7698\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.9069, F1 Micro: 0.7895, F1 Macro: 0.7844\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9077, F1 Micro: 0.7914, F1 Macro: 0.7885\n",
      "Best result for 5441 samples: F1 Micro: 0.7934\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       378\n",
      "                sara       0.68      0.72      0.70       253\n",
      "         radikalisme       0.74      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.72      0.80      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.79      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.1482603549957275 seconds\n",
      "\n",
      "Fold 2 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3652, Accuracy: 0.893, F1 Micro: 0.7327, F1 Macro: 0.7253\n",
      "Epoch 2/10, Train Loss: 0.2318, Accuracy: 0.907, F1 Micro: 0.7802, F1 Macro: 0.7652\n",
      "Epoch 3/10, Train Loss: 0.1916, Accuracy: 0.9098, F1 Micro: 0.7947, F1 Macro: 0.7873\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9117, F1 Micro: 0.7949, F1 Macro: 0.7924\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9109, F1 Micro: 0.79, F1 Macro: 0.782\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9062, F1 Micro: 0.7854, F1 Macro: 0.7768\n",
      "Epoch 7/10, Train Loss: 0.0608, Accuracy: 0.9064, F1 Micro: 0.7846, F1 Macro: 0.7747\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9102, F1 Micro: 0.7926, F1 Macro: 0.7898\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9089, F1 Micro: 0.7848, F1 Macro: 0.7811\n",
      "Epoch 10/10, Train Loss: 0.0294, Accuracy: 0.9081, F1 Micro: 0.7951, F1 Macro: 0.7895\n",
      "Best result for 5641 samples: F1 Micro: 0.7951\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.94      0.93       378\n",
      "                sara       0.71      0.67      0.69       253\n",
      "         radikalisme       0.74      0.84      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.83      0.80      1382\n",
      "           macro avg       0.77      0.82      0.79      1382\n",
      "        weighted avg       0.77      0.83      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.930648326873779 seconds\n",
      "\n",
      "Fold 2 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3688, Accuracy: 0.8939, F1 Micro: 0.7415, F1 Macro: 0.7339\n",
      "Epoch 2/10, Train Loss: 0.2406, Accuracy: 0.9055, F1 Micro: 0.7879, F1 Macro: 0.7822\n",
      "Epoch 3/10, Train Loss: 0.1998, Accuracy: 0.9077, F1 Micro: 0.7822, F1 Macro: 0.7784\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9086, F1 Micro: 0.7852, F1 Macro: 0.7798\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9103, F1 Micro: 0.7839, F1 Macro: 0.7747\n",
      "Epoch 6/10, Train Loss: 0.0869, Accuracy: 0.91, F1 Micro: 0.7849, F1 Macro: 0.778\n",
      "Epoch 7/10, Train Loss: 0.0666, Accuracy: 0.9086, F1 Micro: 0.7905, F1 Macro: 0.7849\n",
      "Epoch 8/10, Train Loss: 0.0536, Accuracy: 0.9098, F1 Micro: 0.7979, F1 Macro: 0.7933\n",
      "Epoch 9/10, Train Loss: 0.0371, Accuracy: 0.9095, F1 Micro: 0.7877, F1 Macro: 0.7839\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9058, F1 Micro: 0.7897, F1 Macro: 0.7868\n",
      "Best result for 5841 samples: F1 Micro: 0.7979\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       378\n",
      "                sara       0.67      0.73      0.70       253\n",
      "         radikalisme       0.73      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.78      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.80      1382\n",
      "           macro avg       0.77      0.82      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.716560125350952 seconds\n",
      "\n",
      "Fold 2 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3626, Accuracy: 0.8972, F1 Micro: 0.7577, F1 Macro: 0.749\n",
      "Epoch 2/10, Train Loss: 0.2379, Accuracy: 0.9089, F1 Micro: 0.7816, F1 Macro: 0.7696\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.915, F1 Micro: 0.8043, F1 Macro: 0.7994\n",
      "Epoch 4/10, Train Loss: 0.1534, Accuracy: 0.9123, F1 Micro: 0.7962, F1 Macro: 0.7873\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9109, F1 Micro: 0.801, F1 Macro: 0.7986\n",
      "Epoch 6/10, Train Loss: 0.0859, Accuracy: 0.9077, F1 Micro: 0.774, F1 Macro: 0.7598\n",
      "Epoch 7/10, Train Loss: 0.0614, Accuracy: 0.9106, F1 Micro: 0.7938, F1 Macro: 0.792\n",
      "Epoch 8/10, Train Loss: 0.048, Accuracy: 0.9114, F1 Micro: 0.7958, F1 Macro: 0.7904\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9092, F1 Micro: 0.7905, F1 Macro: 0.7891\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.912, F1 Micro: 0.8024, F1 Macro: 0.7966\n",
      "Best result for 6041 samples: F1 Micro: 0.8043\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.93       378\n",
      "                sara       0.69      0.74      0.71       253\n",
      "         radikalisme       0.75      0.83      0.79       234\n",
      "pencemaran_nama_baik       0.78      0.77      0.77       517\n",
      "\n",
      "           micro avg       0.80      0.81      0.80      1382\n",
      "           macro avg       0.79      0.81      0.80      1382\n",
      "        weighted avg       0.80      0.81      0.81      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.0959017276763916 seconds\n",
      "\n",
      "Fold 2 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3607, Accuracy: 0.8872, F1 Micro: 0.6935, F1 Macro: 0.6679\n",
      "Epoch 2/10, Train Loss: 0.2352, Accuracy: 0.9087, F1 Micro: 0.7893, F1 Macro: 0.7856\n",
      "Epoch 3/10, Train Loss: 0.1852, Accuracy: 0.9137, F1 Micro: 0.8044, F1 Macro: 0.7985\n",
      "Epoch 4/10, Train Loss: 0.1509, Accuracy: 0.9084, F1 Micro: 0.7812, F1 Macro: 0.7745\n",
      "Epoch 5/10, Train Loss: 0.1128, Accuracy: 0.9066, F1 Micro: 0.7642, F1 Macro: 0.7552\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9098, F1 Micro: 0.7837, F1 Macro: 0.7702\n",
      "Epoch 7/10, Train Loss: 0.0645, Accuracy: 0.9055, F1 Micro: 0.8, F1 Macro: 0.7976\n",
      "Epoch 8/10, Train Loss: 0.0462, Accuracy: 0.9097, F1 Micro: 0.7997, F1 Macro: 0.7944\n",
      "Epoch 9/10, Train Loss: 0.0347, Accuracy: 0.9098, F1 Micro: 0.7823, F1 Macro: 0.7678\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.908, F1 Micro: 0.7887, F1 Macro: 0.7825\n",
      "Best result for 6218 samples: F1 Micro: 0.8044\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.69      0.73      0.71       253\n",
      "         radikalisme       0.72      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.76      0.80      0.78       517\n",
      "\n",
      "           micro avg       0.79      0.82      0.80      1382\n",
      "           macro avg       0.78      0.82      0.80      1382\n",
      "        weighted avg       0.79      0.82      0.81      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 3716.98 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6513, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4812, Accuracy: 0.7903, F1 Micro: 0.1183, F1 Macro: 0.098\n",
      "Epoch 4/10, Train Loss: 0.4323, Accuracy: 0.8141, F1 Micro: 0.2942, F1 Macro: 0.2249\n",
      "Epoch 5/10, Train Loss: 0.3936, Accuracy: 0.8339, F1 Micro: 0.4907, F1 Macro: 0.409\n",
      "Epoch 6/10, Train Loss: 0.3469, Accuracy: 0.8489, F1 Micro: 0.565, F1 Macro: 0.4941\n",
      "Epoch 7/10, Train Loss: 0.2903, Accuracy: 0.8502, F1 Micro: 0.5889, F1 Macro: 0.5297\n",
      "Epoch 8/10, Train Loss: 0.227, Accuracy: 0.8589, F1 Micro: 0.6596, F1 Macro: 0.6437\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.8583, F1 Micro: 0.6931, F1 Macro: 0.6907\n",
      "Epoch 10/10, Train Loss: 0.1624, Accuracy: 0.8569, F1 Micro: 0.6041, F1 Macro: 0.5588\n",
      "Best result for 388 samples: F1 Micro: 0.6931\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.85      0.85       355\n",
      "                sara       0.58      0.58      0.58       273\n",
      "         radikalisme       0.64      0.70      0.67       281\n",
      "pencemaran_nama_baik       0.62      0.71      0.66       521\n",
      "\n",
      "           micro avg       0.67      0.72      0.69      1430\n",
      "           macro avg       0.67      0.71      0.69      1430\n",
      "        weighted avg       0.68      0.72      0.69      1430\n",
      "         samples avg       0.40      0.40      0.39      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 36.82982897758484 seconds\n",
      "\n",
      "Fold 3 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.561, Accuracy: 0.7884, F1 Micro: 0.1021, F1 Macro: 0.0849\n",
      "Epoch 2/10, Train Loss: 0.4153, Accuracy: 0.8138, F1 Micro: 0.3078, F1 Macro: 0.2288\n",
      "Epoch 3/10, Train Loss: 0.3235, Accuracy: 0.848, F1 Micro: 0.5658, F1 Macro: 0.4698\n",
      "Epoch 4/10, Train Loss: 0.2596, Accuracy: 0.8647, F1 Micro: 0.6705, F1 Macro: 0.6455\n",
      "Epoch 5/10, Train Loss: 0.2084, Accuracy: 0.8683, F1 Micro: 0.6973, F1 Macro: 0.6879\n",
      "Epoch 6/10, Train Loss: 0.1727, Accuracy: 0.8761, F1 Micro: 0.6849, F1 Macro: 0.6614\n",
      "Epoch 7/10, Train Loss: 0.1324, Accuracy: 0.8748, F1 Micro: 0.6848, F1 Macro: 0.6584\n",
      "Epoch 8/10, Train Loss: 0.1039, Accuracy: 0.8783, F1 Micro: 0.7254, F1 Macro: 0.724\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.8763, F1 Micro: 0.7165, F1 Macro: 0.7098\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.8789, F1 Micro: 0.7118, F1 Macro: 0.6985\n",
      "Best result for 971 samples: F1 Micro: 0.7254\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.87      0.88       355\n",
      "                sara       0.62      0.61      0.62       273\n",
      "         radikalisme       0.69      0.76      0.72       281\n",
      "pencemaran_nama_baik       0.71      0.65      0.68       521\n",
      "\n",
      "           micro avg       0.73      0.72      0.73      1430\n",
      "           macro avg       0.73      0.72      0.72      1430\n",
      "        weighted avg       0.73      0.72      0.72      1430\n",
      "         samples avg       0.41      0.41      0.40      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 32.59803247451782 seconds\n",
      "\n",
      "Fold 3 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.524, Accuracy: 0.7978, F1 Micro: 0.1768, F1 Macro: 0.1445\n",
      "Epoch 2/10, Train Loss: 0.3611, Accuracy: 0.8519, F1 Micro: 0.6177, F1 Macro: 0.586\n",
      "Epoch 3/10, Train Loss: 0.2741, Accuracy: 0.8636, F1 Micro: 0.6447, F1 Macro: 0.6456\n",
      "Epoch 4/10, Train Loss: 0.217, Accuracy: 0.8769, F1 Micro: 0.691, F1 Macro: 0.6849\n",
      "Epoch 5/10, Train Loss: 0.1762, Accuracy: 0.8761, F1 Micro: 0.7331, F1 Macro: 0.7353\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.8806, F1 Micro: 0.6934, F1 Macro: 0.6898\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.8792, F1 Micro: 0.6965, F1 Macro: 0.6875\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.8822, F1 Micro: 0.7124, F1 Macro: 0.6991\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.8803, F1 Micro: 0.7344, F1 Macro: 0.738\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.8842, F1 Micro: 0.7406, F1 Macro: 0.7444\n",
      "Best result for 1496 samples: F1 Micro: 0.7406\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       355\n",
      "                sara       0.64      0.68      0.66       273\n",
      "         radikalisme       0.75      0.76      0.75       281\n",
      "pencemaran_nama_baik       0.69      0.68      0.68       521\n",
      "\n",
      "           micro avg       0.74      0.74      0.74      1430\n",
      "           macro avg       0.75      0.74      0.74      1430\n",
      "        weighted avg       0.74      0.74      0.74      1430\n",
      "         samples avg       0.42      0.42      0.41      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 29.10951256752014 seconds\n",
      "\n",
      "Fold 3 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5053, Accuracy: 0.8166, F1 Micro: 0.3143, F1 Macro: 0.2313\n",
      "Epoch 2/10, Train Loss: 0.3341, Accuracy: 0.8647, F1 Micro: 0.6474, F1 Macro: 0.6347\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.8808, F1 Micro: 0.7335, F1 Macro: 0.7352\n",
      "Epoch 4/10, Train Loss: 0.2, Accuracy: 0.8834, F1 Micro: 0.7255, F1 Macro: 0.7278\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.875, F1 Micro: 0.7419, F1 Macro: 0.747\n",
      "Epoch 6/10, Train Loss: 0.1203, Accuracy: 0.8773, F1 Micro: 0.7393, F1 Macro: 0.7419\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.8802, F1 Micro: 0.7323, F1 Macro: 0.732\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.8864, F1 Micro: 0.7192, F1 Macro: 0.7119\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.8886, F1 Micro: 0.7414, F1 Macro: 0.7357\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.8866, F1 Micro: 0.7366, F1 Macro: 0.7352\n",
      "Best result for 1969 samples: F1 Micro: 0.7419\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       355\n",
      "                sara       0.56      0.78      0.65       273\n",
      "         radikalisme       0.70      0.81      0.75       281\n",
      "pencemaran_nama_baik       0.65      0.76      0.70       521\n",
      "\n",
      "           micro avg       0.69      0.80      0.74      1430\n",
      "           macro avg       0.70      0.81      0.75      1430\n",
      "        weighted avg       0.70      0.80      0.75      1430\n",
      "         samples avg       0.44      0.46      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 26.632752895355225 seconds\n",
      "\n",
      "Fold 3 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4797, Accuracy: 0.8236, F1 Micro: 0.4067, F1 Macro: 0.3065\n",
      "Epoch 2/10, Train Loss: 0.3056, Accuracy: 0.8763, F1 Micro: 0.7103, F1 Macro: 0.7006\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.8848, F1 Micro: 0.7241, F1 Macro: 0.7255\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.8917, F1 Micro: 0.7421, F1 Macro: 0.7394\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.892, F1 Micro: 0.7421, F1 Macro: 0.7447\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.8869, F1 Micro: 0.7288, F1 Macro: 0.7302\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.888, F1 Micro: 0.7444, F1 Macro: 0.749\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.8836, F1 Micro: 0.7544, F1 Macro: 0.76\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.8853, F1 Micro: 0.752, F1 Macro: 0.7554\n",
      "Epoch 10/10, Train Loss: 0.0396, Accuracy: 0.8836, F1 Micro: 0.7359, F1 Macro: 0.7378\n",
      "Best result for 2394 samples: F1 Micro: 0.7544\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.89      0.89       355\n",
      "                sara       0.63      0.73      0.67       273\n",
      "         radikalisme       0.73      0.83      0.78       281\n",
      "pencemaran_nama_baik       0.65      0.76      0.70       521\n",
      "\n",
      "           micro avg       0.71      0.80      0.75      1430\n",
      "           macro avg       0.72      0.80      0.76      1430\n",
      "        weighted avg       0.72      0.80      0.76      1430\n",
      "         samples avg       0.45      0.46      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 24.282214403152466 seconds\n",
      "\n",
      "Fold 3 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4746, Accuracy: 0.8448, F1 Micro: 0.5358, F1 Macro: 0.4578\n",
      "Epoch 2/10, Train Loss: 0.3037, Accuracy: 0.8788, F1 Micro: 0.7283, F1 Macro: 0.7276\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.892, F1 Micro: 0.7531, F1 Macro: 0.7541\n",
      "Epoch 4/10, Train Loss: 0.1777, Accuracy: 0.8905, F1 Micro: 0.7524, F1 Macro: 0.7514\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.8877, F1 Micro: 0.758, F1 Macro: 0.7609\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.8903, F1 Micro: 0.7396, F1 Macro: 0.7326\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.8953, F1 Micro: 0.7616, F1 Macro: 0.7609\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.8914, F1 Micro: 0.7593, F1 Macro: 0.7573\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.892, F1 Micro: 0.7531, F1 Macro: 0.7539\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.8914, F1 Micro: 0.7645, F1 Macro: 0.7659\n",
      "Best result for 2777 samples: F1 Micro: 0.7645\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       355\n",
      "                sara       0.65      0.68      0.67       273\n",
      "         radikalisme       0.75      0.82      0.78       281\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1430\n",
      "           macro avg       0.75      0.79      0.77      1430\n",
      "        weighted avg       0.75      0.79      0.77      1430\n",
      "         samples avg       0.45      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 21.798633813858032 seconds\n",
      "\n",
      "Fold 3 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4562, Accuracy: 0.8469, F1 Micro: 0.5369, F1 Macro: 0.455\n",
      "Epoch 2/10, Train Loss: 0.2928, Accuracy: 0.8808, F1 Micro: 0.69, F1 Macro: 0.6679\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.8886, F1 Micro: 0.724, F1 Macro: 0.7123\n",
      "Epoch 4/10, Train Loss: 0.1849, Accuracy: 0.8916, F1 Micro: 0.7435, F1 Macro: 0.7365\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.8883, F1 Micro: 0.7249, F1 Macro: 0.719\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.8917, F1 Micro: 0.751, F1 Macro: 0.7564\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.8906, F1 Micro: 0.7646, F1 Macro: 0.7638\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.8902, F1 Micro: 0.7539, F1 Macro: 0.7506\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.8913, F1 Micro: 0.7621, F1 Macro: 0.7625\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.8903, F1 Micro: 0.7523, F1 Macro: 0.7529\n",
      "Best result for 3122 samples: F1 Micro: 0.7646\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.90      0.89       355\n",
      "                sara       0.63      0.68      0.65       273\n",
      "         radikalisme       0.75      0.81      0.78       281\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.80      0.76      1430\n",
      "           macro avg       0.74      0.79      0.76      1430\n",
      "        weighted avg       0.74      0.80      0.77      1430\n",
      "         samples avg       0.45      0.46      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.918652534484863 seconds\n",
      "\n",
      "Fold 3 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4491, Accuracy: 0.842, F1 Micro: 0.5002, F1 Macro: 0.4227\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.8816, F1 Micro: 0.6929, F1 Macro: 0.6895\n",
      "Epoch 3/10, Train Loss: 0.2218, Accuracy: 0.8927, F1 Micro: 0.7542, F1 Macro: 0.7459\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.8903, F1 Micro: 0.7247, F1 Macro: 0.706\n",
      "Epoch 5/10, Train Loss: 0.1406, Accuracy: 0.8866, F1 Micro: 0.7225, F1 Macro: 0.6916\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.8891, F1 Micro: 0.7558, F1 Macro: 0.76\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.8908, F1 Micro: 0.7657, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.892, F1 Micro: 0.7468, F1 Macro: 0.7418\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.8881, F1 Micro: 0.7561, F1 Macro: 0.7549\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.8861, F1 Micro: 0.759, F1 Macro: 0.7629\n",
      "Best result for 3432 samples: F1 Micro: 0.7657\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       355\n",
      "                sara       0.61      0.79      0.69       273\n",
      "         radikalisme       0.75      0.78      0.76       281\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1430\n",
      "           macro avg       0.75      0.80      0.77      1430\n",
      "        weighted avg       0.75      0.80      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 18.06955122947693 seconds\n",
      "\n",
      "Fold 3 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4338, Accuracy: 0.8566, F1 Micro: 0.5977, F1 Macro: 0.5227\n",
      "Epoch 2/10, Train Loss: 0.2651, Accuracy: 0.8884, F1 Micro: 0.7235, F1 Macro: 0.7118\n",
      "Epoch 3/10, Train Loss: 0.2119, Accuracy: 0.8884, F1 Micro: 0.7422, F1 Macro: 0.7298\n",
      "Epoch 4/10, Train Loss: 0.1783, Accuracy: 0.8936, F1 Micro: 0.7567, F1 Macro: 0.7572\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.895, F1 Micro: 0.7635, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.8911, F1 Micro: 0.7596, F1 Macro: 0.7575\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.8883, F1 Micro: 0.7549, F1 Macro: 0.7544\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.8908, F1 Micro: 0.742, F1 Macro: 0.7386\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.8916, F1 Micro: 0.7498, F1 Macro: 0.7474\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.8891, F1 Micro: 0.7466, F1 Macro: 0.7416\n",
      "Best result for 3711 samples: F1 Micro: 0.7635\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.89       355\n",
      "                sara       0.67      0.63      0.65       273\n",
      "         radikalisme       0.78      0.77      0.77       281\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       521\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1430\n",
      "           macro avg       0.77      0.75      0.76      1430\n",
      "        weighted avg       0.77      0.76      0.76      1430\n",
      "         samples avg       0.45      0.44      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 16.288941144943237 seconds\n",
      "\n",
      "Fold 3 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.8498, F1 Micro: 0.5606, F1 Macro: 0.4781\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.8756, F1 Micro: 0.6484, F1 Macro: 0.6328\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.8892, F1 Micro: 0.7524, F1 Macro: 0.7481\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.8903, F1 Micro: 0.7381, F1 Macro: 0.729\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.885, F1 Micro: 0.7065, F1 Macro: 0.6937\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.8889, F1 Micro: 0.7578, F1 Macro: 0.7537\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.8863, F1 Micro: 0.7557, F1 Macro: 0.7575\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.8917, F1 Micro: 0.7517, F1 Macro: 0.7517\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.8863, F1 Micro: 0.7557, F1 Macro: 0.7567\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.8905, F1 Micro: 0.7459, F1 Macro: 0.7437\n",
      "Best result for 3886 samples: F1 Micro: 0.7578\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.90      0.89       355\n",
      "                sara       0.66      0.62      0.64       273\n",
      "         radikalisme       0.76      0.76      0.76       281\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1430\n",
      "           macro avg       0.74      0.77      0.75      1430\n",
      "        weighted avg       0.74      0.78      0.76      1430\n",
      "         samples avg       0.45      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 15.318231105804443 seconds\n",
      "\n",
      "Fold 3 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4215, Accuracy: 0.8727, F1 Micro: 0.6976, F1 Macro: 0.6893\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.8909, F1 Micro: 0.7491, F1 Macro: 0.7483\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.8923, F1 Micro: 0.7347, F1 Macro: 0.7283\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.8909, F1 Micro: 0.7578, F1 Macro: 0.7547\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.8911, F1 Micro: 0.7534, F1 Macro: 0.7506\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.8883, F1 Micro: 0.7513, F1 Macro: 0.7526\n",
      "Epoch 7/10, Train Loss: 0.0682, Accuracy: 0.8886, F1 Micro: 0.7517, F1 Macro: 0.753\n",
      "Epoch 8/10, Train Loss: 0.0512, Accuracy: 0.8833, F1 Micro: 0.7458, F1 Macro: 0.7454\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.8881, F1 Micro: 0.7493, F1 Macro: 0.7518\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.8895, F1 Micro: 0.7515, F1 Macro: 0.7509\n",
      "Best result for 4120 samples: F1 Micro: 0.7578\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       355\n",
      "                sara       0.71      0.58      0.64       273\n",
      "         radikalisme       0.76      0.76      0.76       281\n",
      "pencemaran_nama_baik       0.67      0.78      0.72       521\n",
      "\n",
      "           micro avg       0.75      0.76      0.76      1430\n",
      "           macro avg       0.76      0.75      0.75      1430\n",
      "        weighted avg       0.76      0.76      0.76      1430\n",
      "         samples avg       0.46      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.852277517318726 seconds\n",
      "\n",
      "Fold 3 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4141, Accuracy: 0.8722, F1 Micro: 0.7279, F1 Macro: 0.7285\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.8894, F1 Micro: 0.7524, F1 Macro: 0.7517\n",
      "Epoch 3/10, Train Loss: 0.2065, Accuracy: 0.893, F1 Micro: 0.7362, F1 Macro: 0.7273\n",
      "Epoch 4/10, Train Loss: 0.1682, Accuracy: 0.8897, F1 Micro: 0.7336, F1 Macro: 0.7239\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.8948, F1 Micro: 0.7596, F1 Macro: 0.7533\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.888, F1 Micro: 0.7252, F1 Macro: 0.7116\n",
      "Epoch 7/10, Train Loss: 0.0685, Accuracy: 0.8888, F1 Micro: 0.7384, F1 Macro: 0.7315\n",
      "Epoch 8/10, Train Loss: 0.0549, Accuracy: 0.8908, F1 Micro: 0.7538, F1 Macro: 0.7566\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.8909, F1 Micro: 0.7462, F1 Macro: 0.7435\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.8903, F1 Micro: 0.746, F1 Macro: 0.7432\n",
      "Best result for 4330 samples: F1 Micro: 0.7596\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       355\n",
      "                sara       0.71      0.56      0.62       273\n",
      "         radikalisme       0.77      0.77      0.77       281\n",
      "pencemaran_nama_baik       0.73      0.72      0.72       521\n",
      "\n",
      "           micro avg       0.78      0.74      0.76      1430\n",
      "           macro avg       0.77      0.74      0.75      1430\n",
      "        weighted avg       0.77      0.74      0.76      1430\n",
      "         samples avg       0.45      0.43      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.780838012695312 seconds\n",
      "\n",
      "Fold 3 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4094, Accuracy: 0.8775, F1 Micro: 0.6896, F1 Macro: 0.6625\n",
      "Epoch 2/10, Train Loss: 0.2583, Accuracy: 0.8911, F1 Micro: 0.7513, F1 Macro: 0.7439\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.8948, F1 Micro: 0.7532, F1 Macro: 0.755\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.8916, F1 Micro: 0.767, F1 Macro: 0.7691\n",
      "Epoch 5/10, Train Loss: 0.1251, Accuracy: 0.8947, F1 Micro: 0.7605, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.0912, Accuracy: 0.8878, F1 Micro: 0.7495, F1 Macro: 0.7492\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.897, F1 Micro: 0.7587, F1 Macro: 0.7588\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.8902, F1 Micro: 0.7446, F1 Macro: 0.7449\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.8884, F1 Micro: 0.7482, F1 Macro: 0.7525\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.888, F1 Micro: 0.749, F1 Macro: 0.7513\n",
      "Best result for 4530 samples: F1 Micro: 0.767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       355\n",
      "                sara       0.69      0.66      0.67       273\n",
      "         radikalisme       0.78      0.79      0.79       281\n",
      "pencemaran_nama_baik       0.65      0.82      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1430\n",
      "           macro avg       0.76      0.79      0.77      1430\n",
      "        weighted avg       0.75      0.80      0.77      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.424024105072021 seconds\n",
      "\n",
      "Fold 3 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4143, Accuracy: 0.8773, F1 Micro: 0.7025, F1 Macro: 0.6925\n",
      "Epoch 2/10, Train Loss: 0.2602, Accuracy: 0.897, F1 Micro: 0.7667, F1 Macro: 0.7676\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.897, F1 Micro: 0.7536, F1 Macro: 0.7505\n",
      "Epoch 4/10, Train Loss: 0.1569, Accuracy: 0.895, F1 Micro: 0.7743, F1 Macro: 0.7768\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.8923, F1 Micro: 0.7468, F1 Macro: 0.7431\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.8905, F1 Micro: 0.7555, F1 Macro: 0.7565\n",
      "Epoch 7/10, Train Loss: 0.0642, Accuracy: 0.8881, F1 Micro: 0.7479, F1 Macro: 0.7474\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.8936, F1 Micro: 0.7525, F1 Macro: 0.7536\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.8903, F1 Micro: 0.7545, F1 Macro: 0.756\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.8952, F1 Micro: 0.7587, F1 Macro: 0.7602\n",
      "Best result for 4663 samples: F1 Micro: 0.7743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.85      0.90       355\n",
      "                sara       0.64      0.75      0.69       273\n",
      "         radikalisme       0.72      0.84      0.77       281\n",
      "pencemaran_nama_baik       0.71      0.79      0.74       521\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1430\n",
      "           macro avg       0.75      0.81      0.78      1430\n",
      "        weighted avg       0.76      0.81      0.78      1430\n",
      "         samples avg       0.47      0.46      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.648853778839111 seconds\n",
      "\n",
      "Fold 3 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4163, Accuracy: 0.8773, F1 Micro: 0.7134, F1 Macro: 0.71\n",
      "Epoch 2/10, Train Loss: 0.2544, Accuracy: 0.8961, F1 Micro: 0.7631, F1 Macro: 0.76\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.8908, F1 Micro: 0.7264, F1 Macro: 0.7152\n",
      "Epoch 4/10, Train Loss: 0.16, Accuracy: 0.8941, F1 Micro: 0.7558, F1 Macro: 0.7519\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.8914, F1 Micro: 0.7648, F1 Macro: 0.7673\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.8905, F1 Micro: 0.7517, F1 Macro: 0.7517\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.8895, F1 Micro: 0.7534, F1 Macro: 0.7523\n",
      "Epoch 8/10, Train Loss: 0.0513, Accuracy: 0.8897, F1 Micro: 0.7502, F1 Macro: 0.7476\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.8881, F1 Micro: 0.7509, F1 Macro: 0.7511\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.89, F1 Micro: 0.7504, F1 Macro: 0.7477\n",
      "Best result for 4863 samples: F1 Micro: 0.7648\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       355\n",
      "                sara       0.63      0.72      0.67       273\n",
      "         radikalisme       0.73      0.82      0.77       281\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       521\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1430\n",
      "           macro avg       0.74      0.79      0.77      1430\n",
      "        weighted avg       0.75      0.79      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.627097606658936 seconds\n",
      "\n",
      "Fold 3 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4032, Accuracy: 0.8739, F1 Micro: 0.6644, F1 Macro: 0.6411\n",
      "Epoch 2/10, Train Loss: 0.2511, Accuracy: 0.8953, F1 Micro: 0.7643, F1 Macro: 0.7637\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.8958, F1 Micro: 0.7767, F1 Macro: 0.778\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.8977, F1 Micro: 0.7783, F1 Macro: 0.7791\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.8934, F1 Micro: 0.7585, F1 Macro: 0.7576\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.8961, F1 Micro: 0.7598, F1 Macro: 0.7589\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.8903, F1 Micro: 0.7453, F1 Macro: 0.7443\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.8944, F1 Micro: 0.7538, F1 Macro: 0.7538\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.8916, F1 Micro: 0.7555, F1 Macro: 0.755\n",
      "Epoch 10/10, Train Loss: 0.0316, Accuracy: 0.8903, F1 Micro: 0.7547, F1 Macro: 0.7564\n",
      "Best result for 5063 samples: F1 Micro: 0.7783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.63      0.72      0.67       273\n",
      "         radikalisme       0.75      0.84      0.79       281\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       521\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1430\n",
      "           macro avg       0.76      0.80      0.78      1430\n",
      "        weighted avg       0.76      0.80      0.78      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.49127745628357 seconds\n",
      "\n",
      "Fold 3 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3933, Accuracy: 0.8827, F1 Micro: 0.7156, F1 Macro: 0.7029\n",
      "Epoch 2/10, Train Loss: 0.2371, Accuracy: 0.8945, F1 Micro: 0.7521, F1 Macro: 0.7514\n",
      "Epoch 3/10, Train Loss: 0.2002, Accuracy: 0.8905, F1 Micro: 0.7195, F1 Macro: 0.7116\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.8944, F1 Micro: 0.7621, F1 Macro: 0.7617\n",
      "Epoch 5/10, Train Loss: 0.1277, Accuracy: 0.8953, F1 Micro: 0.767, F1 Macro: 0.7711\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.8956, F1 Micro: 0.7571, F1 Macro: 0.7536\n",
      "Epoch 7/10, Train Loss: 0.0647, Accuracy: 0.893, F1 Micro: 0.7668, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.89, F1 Micro: 0.7333, F1 Macro: 0.7272\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.8886, F1 Micro: 0.7485, F1 Macro: 0.7462\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.8942, F1 Micro: 0.7668, F1 Macro: 0.771\n",
      "Best result for 5263 samples: F1 Micro: 0.767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       355\n",
      "                sara       0.67      0.72      0.70       273\n",
      "         radikalisme       0.80      0.75      0.77       281\n",
      "pencemaran_nama_baik       0.70      0.73      0.71       521\n",
      "\n",
      "           micro avg       0.76      0.77      0.77      1430\n",
      "           macro avg       0.77      0.77      0.77      1430\n",
      "        weighted avg       0.77      0.77      0.77      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 7.165738344192505 seconds\n",
      "\n",
      "Fold 3 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3891, Accuracy: 0.8798, F1 Micro: 0.7014, F1 Macro: 0.681\n",
      "Epoch 2/10, Train Loss: 0.2382, Accuracy: 0.8961, F1 Micro: 0.7463, F1 Macro: 0.7467\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.903, F1 Micro: 0.784, F1 Macro: 0.7841\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.8989, F1 Micro: 0.7768, F1 Macro: 0.774\n",
      "Epoch 5/10, Train Loss: 0.1207, Accuracy: 0.8934, F1 Micro: 0.7395, F1 Macro: 0.7319\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.8955, F1 Micro: 0.7577, F1 Macro: 0.7571\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.8958, F1 Micro: 0.7651, F1 Macro: 0.7678\n",
      "Epoch 8/10, Train Loss: 0.0442, Accuracy: 0.8895, F1 Micro: 0.7546, F1 Macro: 0.7558\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.895, F1 Micro: 0.7647, F1 Macro: 0.7674\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.8939, F1 Micro: 0.7607, F1 Macro: 0.7634\n",
      "Best result for 5441 samples: F1 Micro: 0.784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       355\n",
      "                sara       0.69      0.70      0.70       273\n",
      "         radikalisme       0.78      0.80      0.79       281\n",
      "pencemaran_nama_baik       0.74      0.75      0.75       521\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1430\n",
      "           macro avg       0.78      0.79      0.78      1430\n",
      "        weighted avg       0.78      0.79      0.78      1430\n",
      "         samples avg       0.47      0.45      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.21710205078125 seconds\n",
      "\n",
      "Fold 3 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.8794, F1 Micro: 0.7456, F1 Macro: 0.7491\n",
      "Epoch 2/10, Train Loss: 0.2397, Accuracy: 0.9002, F1 Micro: 0.7686, F1 Macro: 0.7629\n",
      "Epoch 3/10, Train Loss: 0.196, Accuracy: 0.8955, F1 Micro: 0.7463, F1 Macro: 0.7396\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.8917, F1 Micro: 0.7229, F1 Macro: 0.7129\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.897, F1 Micro: 0.7525, F1 Macro: 0.7502\n",
      "Epoch 6/10, Train Loss: 0.0892, Accuracy: 0.8959, F1 Micro: 0.7541, F1 Macro: 0.7503\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.8955, F1 Micro: 0.7623, F1 Macro: 0.7653\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.8956, F1 Micro: 0.7592, F1 Macro: 0.7613\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.8939, F1 Micro: 0.7593, F1 Macro: 0.7614\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.8963, F1 Micro: 0.7673, F1 Macro: 0.7682\n",
      "Best result for 5641 samples: F1 Micro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       355\n",
      "                sara       0.73      0.59      0.65       273\n",
      "         radikalisme       0.77      0.75      0.76       281\n",
      "pencemaran_nama_baik       0.76      0.72      0.74       521\n",
      "\n",
      "           micro avg       0.80      0.74      0.77      1430\n",
      "           macro avg       0.79      0.74      0.76      1430\n",
      "        weighted avg       0.79      0.74      0.77      1430\n",
      "         samples avg       0.46      0.43      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.987149477005005 seconds\n",
      "\n",
      "Fold 3 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3819, Accuracy: 0.8814, F1 Micro: 0.7483, F1 Macro: 0.7493\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.89, F1 Micro: 0.7692, F1 Macro: 0.7723\n",
      "Epoch 3/10, Train Loss: 0.1928, Accuracy: 0.9003, F1 Micro: 0.7771, F1 Macro: 0.7782\n",
      "Epoch 4/10, Train Loss: 0.1559, Accuracy: 0.8967, F1 Micro: 0.758, F1 Macro: 0.759\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.8952, F1 Micro: 0.7601, F1 Macro: 0.7579\n",
      "Epoch 6/10, Train Loss: 0.0835, Accuracy: 0.8934, F1 Micro: 0.7594, F1 Macro: 0.7593\n",
      "Epoch 7/10, Train Loss: 0.0648, Accuracy: 0.8959, F1 Micro: 0.7608, F1 Macro: 0.7619\n",
      "Epoch 8/10, Train Loss: 0.0461, Accuracy: 0.8917, F1 Micro: 0.7611, F1 Macro: 0.7654\n",
      "Epoch 9/10, Train Loss: 0.0371, Accuracy: 0.8959, F1 Micro: 0.7652, F1 Macro: 0.7664\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.8928, F1 Micro: 0.7579, F1 Macro: 0.7593\n",
      "Best result for 5841 samples: F1 Micro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       355\n",
      "                sara       0.66      0.70      0.68       273\n",
      "         radikalisme       0.78      0.81      0.80       281\n",
      "pencemaran_nama_baik       0.75      0.71      0.73       521\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1430\n",
      "           macro avg       0.77      0.78      0.78      1430\n",
      "        weighted avg       0.78      0.78      0.78      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.885106086730957 seconds\n",
      "\n",
      "Fold 3 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3793, Accuracy: 0.8883, F1 Micro: 0.7274, F1 Macro: 0.7254\n",
      "Epoch 2/10, Train Loss: 0.2354, Accuracy: 0.8961, F1 Micro: 0.7503, F1 Macro: 0.7447\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.8939, F1 Micro: 0.7701, F1 Macro: 0.7729\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.8905, F1 Micro: 0.7358, F1 Macro: 0.7194\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.8953, F1 Micro: 0.7528, F1 Macro: 0.7468\n",
      "Epoch 6/10, Train Loss: 0.091, Accuracy: 0.8905, F1 Micro: 0.759, F1 Macro: 0.7616\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.8947, F1 Micro: 0.7546, F1 Macro: 0.7507\n",
      "Epoch 8/10, Train Loss: 0.0512, Accuracy: 0.8906, F1 Micro: 0.7609, F1 Macro: 0.7618\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.8908, F1 Micro: 0.7631, F1 Macro: 0.7638\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.8947, F1 Micro: 0.7652, F1 Macro: 0.7647\n",
      "Best result for 6041 samples: F1 Micro: 0.7701\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       355\n",
      "                sara       0.66      0.71      0.69       273\n",
      "         radikalisme       0.73      0.84      0.78       281\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1430\n",
      "           macro avg       0.75      0.80      0.77      1430\n",
      "        weighted avg       0.75      0.80      0.77      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.077834129333496 seconds\n",
      "\n",
      "Fold 3 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3687, Accuracy: 0.8898, F1 Micro: 0.7454, F1 Macro: 0.7436\n",
      "Epoch 2/10, Train Loss: 0.2403, Accuracy: 0.8977, F1 Micro: 0.7667, F1 Macro: 0.7613\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.8878, F1 Micro: 0.7716, F1 Macro: 0.7774\n",
      "Epoch 4/10, Train Loss: 0.1514, Accuracy: 0.8963, F1 Micro: 0.7544, F1 Macro: 0.7493\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.8945, F1 Micro: 0.7508, F1 Macro: 0.7549\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.8969, F1 Micro: 0.7574, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.0617, Accuracy: 0.8955, F1 Micro: 0.7692, F1 Macro: 0.771\n",
      "Epoch 8/10, Train Loss: 0.0454, Accuracy: 0.892, F1 Micro: 0.7576, F1 Macro: 0.7558\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.8966, F1 Micro: 0.7589, F1 Macro: 0.7582\n",
      "Epoch 10/10, Train Loss: 0.0279, Accuracy: 0.8963, F1 Micro: 0.7677, F1 Macro: 0.7697\n",
      "Best result for 6218 samples: F1 Micro: 0.7716\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       355\n",
      "                sara       0.63      0.78      0.70       273\n",
      "         radikalisme       0.74      0.83      0.78       281\n",
      "pencemaran_nama_baik       0.63      0.87      0.73       521\n",
      "\n",
      "           micro avg       0.71      0.85      0.77      1430\n",
      "           macro avg       0.73      0.84      0.78      1430\n",
      "        weighted avg       0.72      0.85      0.78      1430\n",
      "         samples avg       0.48      0.49      0.47      1430\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 3738.56 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5795, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4106, Accuracy: 0.8267, F1 Micro: 0.3666, F1 Macro: 0.3071\n",
      "Epoch 4/10, Train Loss: 0.3659, Accuracy: 0.8505, F1 Micro: 0.5812, F1 Macro: 0.5497\n",
      "Epoch 5/10, Train Loss: 0.3165, Accuracy: 0.8614, F1 Micro: 0.6272, F1 Macro: 0.608\n",
      "Epoch 6/10, Train Loss: 0.2503, Accuracy: 0.8612, F1 Micro: 0.655, F1 Macro: 0.6302\n",
      "Epoch 7/10, Train Loss: 0.2107, Accuracy: 0.8628, F1 Micro: 0.6613, F1 Macro: 0.6477\n",
      "Epoch 8/10, Train Loss: 0.1713, Accuracy: 0.8617, F1 Micro: 0.6757, F1 Macro: 0.6675\n",
      "Epoch 9/10, Train Loss: 0.1484, Accuracy: 0.8644, F1 Micro: 0.672, F1 Macro: 0.662\n",
      "Epoch 10/10, Train Loss: 0.1116, Accuracy: 0.8662, F1 Micro: 0.686, F1 Macro: 0.6766\n",
      "Best result for 388 samples: F1 Micro: 0.686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.81      0.86       342\n",
      "                sara       0.54      0.47      0.51       249\n",
      "         radikalisme       0.72      0.63      0.67       302\n",
      "pencemaran_nama_baik       0.64      0.69      0.66       508\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1401\n",
      "           macro avg       0.71      0.65      0.68      1401\n",
      "        weighted avg       0.71      0.67      0.69      1401\n",
      "         samples avg       0.38      0.39      0.37      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 584\n",
      "Sampling duration: 36.65454697608948 seconds\n",
      "\n",
      "Fold 4 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5203, Accuracy: 0.7991, F1 Micro: 0.1551, F1 Macro: 0.1282\n",
      "Epoch 2/10, Train Loss: 0.3748, Accuracy: 0.8448, F1 Micro: 0.4936, F1 Macro: 0.4285\n",
      "Epoch 3/10, Train Loss: 0.287, Accuracy: 0.8637, F1 Micro: 0.6654, F1 Macro: 0.6363\n",
      "Epoch 4/10, Train Loss: 0.2323, Accuracy: 0.8737, F1 Micro: 0.6849, F1 Macro: 0.6767\n",
      "Epoch 5/10, Train Loss: 0.1696, Accuracy: 0.8763, F1 Micro: 0.6993, F1 Macro: 0.6895\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.8778, F1 Micro: 0.7205, F1 Macro: 0.7196\n",
      "Epoch 7/10, Train Loss: 0.1157, Accuracy: 0.88, F1 Micro: 0.7221, F1 Macro: 0.721\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.8789, F1 Micro: 0.7418, F1 Macro: 0.7405\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.8809, F1 Micro: 0.729, F1 Macro: 0.7308\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.8809, F1 Micro: 0.7365, F1 Macro: 0.7364\n",
      "Best result for 972 samples: F1 Micro: 0.7418\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       342\n",
      "                sara       0.57      0.66      0.61       249\n",
      "         radikalisme       0.71      0.78      0.74       302\n",
      "pencemaran_nama_baik       0.64      0.82      0.72       508\n",
      "\n",
      "           micro avg       0.70      0.79      0.74      1401\n",
      "           macro avg       0.71      0.78      0.74      1401\n",
      "        weighted avg       0.71      0.79      0.75      1401\n",
      "         samples avg       0.43      0.45      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 32.917850971221924 seconds\n",
      "\n",
      "Fold 4 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.49, Accuracy: 0.8256, F1 Micro: 0.3868, F1 Macro: 0.2897\n",
      "Epoch 2/10, Train Loss: 0.3291, Accuracy: 0.8606, F1 Micro: 0.6043, F1 Macro: 0.5508\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.8775, F1 Micro: 0.6867, F1 Macro: 0.6781\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.883, F1 Micro: 0.7147, F1 Macro: 0.7095\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.8773, F1 Micro: 0.7311, F1 Macro: 0.7325\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.8698, F1 Micro: 0.7273, F1 Macro: 0.7312\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.885, F1 Micro: 0.7184, F1 Macro: 0.7141\n",
      "Epoch 8/10, Train Loss: 0.0775, Accuracy: 0.8814, F1 Micro: 0.7353, F1 Macro: 0.7378\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.8848, F1 Micro: 0.7297, F1 Macro: 0.7257\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.8872, F1 Micro: 0.7296, F1 Macro: 0.7275\n",
      "Best result for 1497 samples: F1 Micro: 0.7353\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       342\n",
      "                sara       0.57      0.70      0.63       249\n",
      "         radikalisme       0.74      0.73      0.74       302\n",
      "pencemaran_nama_baik       0.67      0.71      0.69       508\n",
      "\n",
      "           micro avg       0.72      0.75      0.74      1401\n",
      "           macro avg       0.73      0.75      0.74      1401\n",
      "        weighted avg       0.73      0.75      0.74      1401\n",
      "         samples avg       0.42      0.43      0.41      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 29.370017290115356 seconds\n",
      "\n",
      "Fold 4 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4567, Accuracy: 0.8366, F1 Micro: 0.4436, F1 Macro: 0.3764\n",
      "Epoch 2/10, Train Loss: 0.309, Accuracy: 0.8669, F1 Micro: 0.6462, F1 Macro: 0.6335\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.8845, F1 Micro: 0.7316, F1 Macro: 0.7252\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.8881, F1 Micro: 0.7316, F1 Macro: 0.7272\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.8872, F1 Micro: 0.7314, F1 Macro: 0.7279\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.8855, F1 Micro: 0.7381, F1 Macro: 0.74\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.8831, F1 Micro: 0.7466, F1 Macro: 0.7488\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.8883, F1 Micro: 0.7464, F1 Macro: 0.7462\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.8894, F1 Micro: 0.7314, F1 Macro: 0.7223\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.8848, F1 Micro: 0.7232, F1 Macro: 0.723\n",
      "Best result for 1970 samples: F1 Micro: 0.7466\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       342\n",
      "                sara       0.58      0.71      0.63       249\n",
      "         radikalisme       0.75      0.76      0.76       302\n",
      "pencemaran_nama_baik       0.64      0.79      0.71       508\n",
      "\n",
      "           micro avg       0.71      0.79      0.75      1401\n",
      "           macro avg       0.73      0.78      0.75      1401\n",
      "        weighted avg       0.72      0.79      0.75      1401\n",
      "         samples avg       0.44      0.45      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 26.801355361938477 seconds\n",
      "\n",
      "Fold 4 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4363, Accuracy: 0.8525, F1 Micro: 0.5572, F1 Macro: 0.4905\n",
      "Epoch 2/10, Train Loss: 0.2869, Accuracy: 0.8806, F1 Micro: 0.7066, F1 Macro: 0.6963\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.8898, F1 Micro: 0.7448, F1 Macro: 0.7419\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.8905, F1 Micro: 0.7569, F1 Macro: 0.7563\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.888, F1 Micro: 0.7552, F1 Macro: 0.7545\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.8939, F1 Micro: 0.7591, F1 Macro: 0.7557\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.8953, F1 Micro: 0.7565, F1 Macro: 0.7525\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.8883, F1 Micro: 0.7537, F1 Macro: 0.7533\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.8898, F1 Micro: 0.7558, F1 Macro: 0.756\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.895, F1 Micro: 0.7502, F1 Macro: 0.7469\n",
      "Best result for 2395 samples: F1 Micro: 0.7591\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       342\n",
      "                sara       0.64      0.63      0.63       249\n",
      "         radikalisme       0.74      0.80      0.77       302\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       508\n",
      "\n",
      "           micro avg       0.75      0.76      0.76      1401\n",
      "           macro avg       0.75      0.76      0.76      1401\n",
      "        weighted avg       0.76      0.76      0.76      1401\n",
      "         samples avg       0.44      0.44      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 24.309183597564697 seconds\n",
      "\n",
      "Fold 4 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4318, Accuracy: 0.8625, F1 Micro: 0.6255, F1 Macro: 0.5793\n",
      "Epoch 2/10, Train Loss: 0.2756, Accuracy: 0.8822, F1 Micro: 0.7326, F1 Macro: 0.732\n",
      "Epoch 3/10, Train Loss: 0.2186, Accuracy: 0.8919, F1 Micro: 0.745, F1 Macro: 0.7444\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.8903, F1 Micro: 0.7457, F1 Macro: 0.7376\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.8917, F1 Micro: 0.7524, F1 Macro: 0.7528\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.8942, F1 Micro: 0.7639, F1 Macro: 0.7629\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.8894, F1 Micro: 0.754, F1 Macro: 0.755\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.8942, F1 Micro: 0.7673, F1 Macro: 0.7665\n",
      "Epoch 9/10, Train Loss: 0.0432, Accuracy: 0.8942, F1 Micro: 0.7644, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.8917, F1 Micro: 0.7631, F1 Macro: 0.7615\n",
      "Best result for 2778 samples: F1 Micro: 0.7673\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.61      0.70      0.65       249\n",
      "         radikalisme       0.77      0.79      0.78       302\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       508\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1401\n",
      "           macro avg       0.74      0.79      0.77      1401\n",
      "        weighted avg       0.75      0.80      0.77      1401\n",
      "         samples avg       0.45      0.46      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 22.0208637714386 seconds\n",
      "\n",
      "Fold 4 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4148, Accuracy: 0.8655, F1 Micro: 0.6524, F1 Macro: 0.6342\n",
      "Epoch 2/10, Train Loss: 0.2634, Accuracy: 0.8886, F1 Micro: 0.7465, F1 Macro: 0.7457\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.8927, F1 Micro: 0.7553, F1 Macro: 0.7549\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.895, F1 Micro: 0.7595, F1 Macro: 0.7596\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.8784, F1 Micro: 0.7519, F1 Macro: 0.7569\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.8944, F1 Micro: 0.7621, F1 Macro: 0.7598\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.888, F1 Micro: 0.7633, F1 Macro: 0.7659\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.8972, F1 Micro: 0.7701, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8955, F1 Micro: 0.7572, F1 Macro: 0.7577\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.8967, F1 Micro: 0.7655, F1 Macro: 0.7665\n",
      "Best result for 3123 samples: F1 Micro: 0.7701\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       342\n",
      "                sara       0.63      0.70      0.66       249\n",
      "         radikalisme       0.77      0.78      0.78       302\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       508\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1401\n",
      "           macro avg       0.76      0.78      0.77      1401\n",
      "        weighted avg       0.76      0.79      0.77      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.912508964538574 seconds\n",
      "\n",
      "Fold 4 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4126, Accuracy: 0.8652, F1 Micro: 0.6427, F1 Macro: 0.6246\n",
      "Epoch 2/10, Train Loss: 0.2714, Accuracy: 0.8952, F1 Micro: 0.7414, F1 Macro: 0.7225\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.8977, F1 Micro: 0.767, F1 Macro: 0.7665\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.8972, F1 Micro: 0.7652, F1 Macro: 0.7664\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.8891, F1 Micro: 0.7585, F1 Macro: 0.7581\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.8969, F1 Micro: 0.7712, F1 Macro: 0.7694\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.902, F1 Micro: 0.7699, F1 Macro: 0.7686\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9006, F1 Micro: 0.7727, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0448, Accuracy: 0.9019, F1 Micro: 0.7765, F1 Macro: 0.7753\n",
      "Epoch 10/10, Train Loss: 0.0359, Accuracy: 0.8958, F1 Micro: 0.7706, F1 Macro: 0.769\n",
      "Best result for 3433 samples: F1 Micro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       342\n",
      "                sara       0.64      0.71      0.68       249\n",
      "         radikalisme       0.80      0.75      0.77       302\n",
      "pencemaran_nama_baik       0.74      0.75      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1401\n",
      "           macro avg       0.78      0.78      0.78      1401\n",
      "        weighted avg       0.78      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 18.097065925598145 seconds\n",
      "\n",
      "Fold 4 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3969, Accuracy: 0.865, F1 Micro: 0.662, F1 Macro: 0.6552\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.8858, F1 Micro: 0.7549, F1 Macro: 0.7569\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.8995, F1 Micro: 0.7681, F1 Macro: 0.7658\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.8961, F1 Micro: 0.761, F1 Macro: 0.7606\n",
      "Epoch 5/10, Train Loss: 0.1316, Accuracy: 0.8972, F1 Micro: 0.7623, F1 Macro: 0.7604\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.8927, F1 Micro: 0.7653, F1 Macro: 0.7643\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.8933, F1 Micro: 0.7556, F1 Macro: 0.7576\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.8925, F1 Micro: 0.7604, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8913, F1 Micro: 0.7647, F1 Macro: 0.7668\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.8931, F1 Micro: 0.7672, F1 Macro: 0.768\n",
      "Best result for 3712 samples: F1 Micro: 0.7681\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.66      0.65      0.66       249\n",
      "         radikalisme       0.79      0.75      0.77       302\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       508\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1401\n",
      "           macro avg       0.78      0.76      0.77      1401\n",
      "        weighted avg       0.78      0.76      0.77      1401\n",
      "         samples avg       0.44      0.44      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 174\n",
      "Sampling duration: 16.50956416130066 seconds\n",
      "\n",
      "Fold 4 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4, Accuracy: 0.8778, F1 Micro: 0.7065, F1 Macro: 0.6982\n",
      "Epoch 2/10, Train Loss: 0.2612, Accuracy: 0.8964, F1 Micro: 0.749, F1 Macro: 0.7464\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.8864, F1 Micro: 0.7653, F1 Macro: 0.7683\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.8892, F1 Micro: 0.7684, F1 Macro: 0.7698\n",
      "Epoch 5/10, Train Loss: 0.1347, Accuracy: 0.9009, F1 Micro: 0.7755, F1 Macro: 0.7724\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.8886, F1 Micro: 0.7594, F1 Macro: 0.7621\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.8911, F1 Micro: 0.7627, F1 Macro: 0.7631\n",
      "Epoch 8/10, Train Loss: 0.053, Accuracy: 0.8942, F1 Micro: 0.7661, F1 Macro: 0.767\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.897, F1 Micro: 0.7512, F1 Macro: 0.7513\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.8983, F1 Micro: 0.7668, F1 Macro: 0.7639\n",
      "Best result for 3886 samples: F1 Micro: 0.7755\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       342\n",
      "                sara       0.65      0.67      0.66       249\n",
      "         radikalisme       0.78      0.77      0.78       302\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       508\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1401\n",
      "           macro avg       0.77      0.78      0.77      1401\n",
      "        weighted avg       0.77      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 15.319970607757568 seconds\n",
      "\n",
      "Fold 4 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3966, Accuracy: 0.8712, F1 Micro: 0.6402, F1 Macro: 0.6148\n",
      "Epoch 2/10, Train Loss: 0.2563, Accuracy: 0.9002, F1 Micro: 0.7631, F1 Macro: 0.7506\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9002, F1 Micro: 0.766, F1 Macro: 0.7623\n",
      "Epoch 4/10, Train Loss: 0.1685, Accuracy: 0.8917, F1 Micro: 0.7681, F1 Macro: 0.7677\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.8964, F1 Micro: 0.7722, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.8963, F1 Micro: 0.7654, F1 Macro: 0.7638\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.8952, F1 Micro: 0.7674, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.8978, F1 Micro: 0.7689, F1 Macro: 0.7694\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.8975, F1 Micro: 0.7632, F1 Macro: 0.7618\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.8945, F1 Micro: 0.7694, F1 Macro: 0.7707\n",
      "Best result for 4120 samples: F1 Micro: 0.7722\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       342\n",
      "                sara       0.63      0.72      0.67       249\n",
      "         radikalisme       0.78      0.81      0.79       302\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       508\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1401\n",
      "           macro avg       0.75      0.80      0.77      1401\n",
      "        weighted avg       0.75      0.80      0.77      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.958310842514038 seconds\n",
      "\n",
      "Fold 4 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.8789, F1 Micro: 0.7048, F1 Macro: 0.6989\n",
      "Epoch 2/10, Train Loss: 0.258, Accuracy: 0.897, F1 Micro: 0.7661, F1 Macro: 0.7648\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.8972, F1 Micro: 0.7602, F1 Macro: 0.7588\n",
      "Epoch 4/10, Train Loss: 0.1571, Accuracy: 0.8925, F1 Micro: 0.7676, F1 Macro: 0.7665\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.8958, F1 Micro: 0.7627, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.8939, F1 Micro: 0.7708, F1 Macro: 0.772\n",
      "Epoch 7/10, Train Loss: 0.0691, Accuracy: 0.898, F1 Micro: 0.7503, F1 Macro: 0.7409\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.898, F1 Micro: 0.7708, F1 Macro: 0.7694\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.8969, F1 Micro: 0.7633, F1 Macro: 0.7636\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.8969, F1 Micro: 0.7704, F1 Macro: 0.7684\n",
      "Best result for 4330 samples: F1 Micro: 0.7708\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       342\n",
      "                sara       0.58      0.78      0.66       249\n",
      "         radikalisme       0.76      0.81      0.78       302\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       508\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1401\n",
      "           macro avg       0.74      0.82      0.77      1401\n",
      "        weighted avg       0.74      0.82      0.78      1401\n",
      "         samples avg       0.45      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.62223768234253 seconds\n",
      "\n",
      "Fold 4 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8786, F1 Micro: 0.6898, F1 Macro: 0.686\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9, F1 Micro: 0.7714, F1 Macro: 0.7682\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.8975, F1 Micro: 0.7687, F1 Macro: 0.7693\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.8994, F1 Micro: 0.7471, F1 Macro: 0.7419\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.8997, F1 Micro: 0.7714, F1 Macro: 0.7677\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.8931, F1 Micro: 0.7662, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.0644, Accuracy: 0.8973, F1 Micro: 0.7702, F1 Macro: 0.771\n",
      "Epoch 8/10, Train Loss: 0.0484, Accuracy: 0.8958, F1 Micro: 0.7683, F1 Macro: 0.7708\n",
      "Epoch 9/10, Train Loss: 0.0377, Accuracy: 0.9011, F1 Micro: 0.7745, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.031, Accuracy: 0.8983, F1 Micro: 0.7669, F1 Macro: 0.7664\n",
      "Best result for 4530 samples: F1 Micro: 0.7745\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       342\n",
      "                sara       0.64      0.73      0.68       249\n",
      "         radikalisme       0.78      0.80      0.79       302\n",
      "pencemaran_nama_baik       0.74      0.70      0.72       508\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1401\n",
      "           macro avg       0.77      0.78      0.78      1401\n",
      "        weighted avg       0.78      0.78      0.78      1401\n",
      "         samples avg       0.45      0.44      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.399546146392822 seconds\n",
      "\n",
      "Fold 4 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3842, Accuracy: 0.8756, F1 Micro: 0.6678, F1 Macro: 0.6571\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.8998, F1 Micro: 0.7629, F1 Macro: 0.7565\n",
      "Epoch 3/10, Train Loss: 0.21, Accuracy: 0.9013, F1 Micro: 0.7757, F1 Macro: 0.7746\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9028, F1 Micro: 0.7617, F1 Macro: 0.7462\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.8977, F1 Micro: 0.7742, F1 Macro: 0.7742\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.898, F1 Micro: 0.7657, F1 Macro: 0.7629\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.8997, F1 Micro: 0.7704, F1 Macro: 0.7683\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.8995, F1 Micro: 0.7698, F1 Macro: 0.7649\n",
      "Epoch 9/10, Train Loss: 0.037, Accuracy: 0.8955, F1 Micro: 0.7718, F1 Macro: 0.7724\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9006, F1 Micro: 0.7696, F1 Macro: 0.765\n",
      "Best result for 4663 samples: F1 Micro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       342\n",
      "                sara       0.62      0.75      0.68       249\n",
      "         radikalisme       0.76      0.79      0.78       302\n",
      "pencemaran_nama_baik       0.78      0.71      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1401\n",
      "           macro avg       0.77      0.79      0.77      1401\n",
      "        weighted avg       0.78      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.64949107170105 seconds\n",
      "\n",
      "Fold 4 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8819, F1 Micro: 0.7206, F1 Macro: 0.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.9011, F1 Micro: 0.775, F1 Macro: 0.7743\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9008, F1 Micro: 0.7832, F1 Macro: 0.7854\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.9047, F1 Micro: 0.7769, F1 Macro: 0.7767\n",
      "Epoch 5/10, Train Loss: 0.124, Accuracy: 0.9016, F1 Micro: 0.782, F1 Macro: 0.778\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.8967, F1 Micro: 0.7768, F1 Macro: 0.7762\n",
      "Epoch 7/10, Train Loss: 0.0648, Accuracy: 0.8981, F1 Micro: 0.7703, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.9022, F1 Micro: 0.7779, F1 Macro: 0.7745\n",
      "Epoch 9/10, Train Loss: 0.0336, Accuracy: 0.9008, F1 Micro: 0.7762, F1 Macro: 0.7721\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.9036, F1 Micro: 0.778, F1 Macro: 0.7758\n",
      "Best result for 4863 samples: F1 Micro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       342\n",
      "                sara       0.58      0.81      0.68       249\n",
      "         radikalisme       0.76      0.85      0.80       302\n",
      "pencemaran_nama_baik       0.74      0.75      0.75       508\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1401\n",
      "           macro avg       0.75      0.83      0.79      1401\n",
      "        weighted avg       0.76      0.82      0.79      1401\n",
      "         samples avg       0.45      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.38191843032837 seconds\n",
      "\n",
      "Fold 4 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.8825, F1 Micro: 0.7152, F1 Macro: 0.7139\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.9017, F1 Micro: 0.774, F1 Macro: 0.774\n",
      "Epoch 3/10, Train Loss: 0.2019, Accuracy: 0.905, F1 Micro: 0.7688, F1 Macro: 0.7646\n",
      "Epoch 4/10, Train Loss: 0.1538, Accuracy: 0.8991, F1 Micro: 0.7786, F1 Macro: 0.7779\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.8973, F1 Micro: 0.7737, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9016, F1 Micro: 0.7764, F1 Macro: 0.7767\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.8955, F1 Micro: 0.7676, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8964, F1 Micro: 0.7774, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0346, Accuracy: 0.8961, F1 Micro: 0.7682, F1 Macro: 0.7667\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9034, F1 Micro: 0.7836, F1 Macro: 0.7839\n",
      "Best result for 5063 samples: F1 Micro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       342\n",
      "                sara       0.63      0.71      0.67       249\n",
      "         radikalisme       0.78      0.83      0.81       302\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1401\n",
      "           macro avg       0.77      0.80      0.78      1401\n",
      "        weighted avg       0.78      0.80      0.79      1401\n",
      "         samples avg       0.47      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.511678218841553 seconds\n",
      "\n",
      "Fold 4 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3701, Accuracy: 0.8869, F1 Micro: 0.7284, F1 Macro: 0.7205\n",
      "Epoch 2/10, Train Loss: 0.2404, Accuracy: 0.8966, F1 Micro: 0.7709, F1 Macro: 0.7739\n",
      "Epoch 3/10, Train Loss: 0.198, Accuracy: 0.9002, F1 Micro: 0.7776, F1 Macro: 0.7783\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.8997, F1 Micro: 0.7715, F1 Macro: 0.7702\n",
      "Epoch 5/10, Train Loss: 0.1181, Accuracy: 0.8952, F1 Micro: 0.7711, F1 Macro: 0.7732\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.8945, F1 Micro: 0.7767, F1 Macro: 0.7782\n",
      "Epoch 7/10, Train Loss: 0.0584, Accuracy: 0.8986, F1 Micro: 0.7686, F1 Macro: 0.7653\n",
      "Epoch 8/10, Train Loss: 0.0462, Accuracy: 0.8981, F1 Micro: 0.7722, F1 Macro: 0.771\n",
      "Epoch 9/10, Train Loss: 0.0358, Accuracy: 0.8989, F1 Micro: 0.7754, F1 Macro: 0.7749\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9005, F1 Micro: 0.781, F1 Macro: 0.7791\n",
      "Best result for 5263 samples: F1 Micro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       342\n",
      "                sara       0.65      0.67      0.66       249\n",
      "         radikalisme       0.81      0.79      0.80       302\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       508\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1401\n",
      "           macro avg       0.76      0.80      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.78      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 7.034552574157715 seconds\n",
      "\n",
      "Fold 4 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3718, Accuracy: 0.8894, F1 Micro: 0.7362, F1 Macro: 0.7197\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.9038, F1 Micro: 0.7798, F1 Macro: 0.7809\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.8947, F1 Micro: 0.777, F1 Macro: 0.7787\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.8894, F1 Micro: 0.7689, F1 Macro: 0.773\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9002, F1 Micro: 0.7741, F1 Macro: 0.7727\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.8945, F1 Micro: 0.7694, F1 Macro: 0.771\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.8952, F1 Micro: 0.7697, F1 Macro: 0.7683\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.8989, F1 Micro: 0.7757, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.034, Accuracy: 0.9033, F1 Micro: 0.7684, F1 Macro: 0.7633\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.9011, F1 Micro: 0.7756, F1 Macro: 0.7753\n",
      "Best result for 5441 samples: F1 Micro: 0.7798\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.84      0.90       342\n",
      "                sara       0.70      0.67      0.69       249\n",
      "         radikalisme       0.76      0.83      0.80       302\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       508\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1401\n",
      "           macro avg       0.79      0.78      0.78      1401\n",
      "        weighted avg       0.79      0.78      0.78      1401\n",
      "         samples avg       0.44      0.44      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.148605108261108 seconds\n",
      "\n",
      "Fold 4 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.369, Accuracy: 0.8783, F1 Micro: 0.7191, F1 Macro: 0.7159\n",
      "Epoch 2/10, Train Loss: 0.2463, Accuracy: 0.89, F1 Micro: 0.7615, F1 Macro: 0.7613\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9031, F1 Micro: 0.7617, F1 Macro: 0.7548\n",
      "Epoch 4/10, Train Loss: 0.1596, Accuracy: 0.8991, F1 Micro: 0.7826, F1 Macro: 0.7825\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.8955, F1 Micro: 0.7728, F1 Macro: 0.773\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.9009, F1 Micro: 0.7686, F1 Macro: 0.7653\n",
      "Epoch 7/10, Train Loss: 0.0619, Accuracy: 0.8969, F1 Micro: 0.7734, F1 Macro: 0.7712\n",
      "Epoch 8/10, Train Loss: 0.0436, Accuracy: 0.8994, F1 Micro: 0.7713, F1 Macro: 0.7703\n",
      "Epoch 9/10, Train Loss: 0.0318, Accuracy: 0.898, F1 Micro: 0.7705, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9038, F1 Micro: 0.7735, F1 Macro: 0.7742\n",
      "Best result for 5641 samples: F1 Micro: 0.7826\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       342\n",
      "                sara       0.60      0.77      0.68       249\n",
      "         radikalisme       0.77      0.82      0.79       302\n",
      "pencemaran_nama_baik       0.70      0.82      0.76       508\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1401\n",
      "           macro avg       0.75      0.83      0.78      1401\n",
      "        weighted avg       0.75      0.83      0.79      1401\n",
      "         samples avg       0.46      0.48      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.120030164718628 seconds\n",
      "\n",
      "Fold 4 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3602, Accuracy: 0.8825, F1 Micro: 0.7204, F1 Macro: 0.7101\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.905, F1 Micro: 0.7702, F1 Macro: 0.7634\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.9038, F1 Micro: 0.7695, F1 Macro: 0.7623\n",
      "Epoch 4/10, Train Loss: 0.1615, Accuracy: 0.8891, F1 Micro: 0.7668, F1 Macro: 0.7666\n",
      "Epoch 5/10, Train Loss: 0.1127, Accuracy: 0.8969, F1 Micro: 0.7763, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.0797, Accuracy: 0.8952, F1 Micro: 0.7663, F1 Macro: 0.7661\n",
      "Epoch 7/10, Train Loss: 0.0617, Accuracy: 0.9002, F1 Micro: 0.774, F1 Macro: 0.7718\n",
      "Epoch 8/10, Train Loss: 0.0457, Accuracy: 0.8966, F1 Micro: 0.7741, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0343, Accuracy: 0.8961, F1 Micro: 0.7717, F1 Macro: 0.7693\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9044, F1 Micro: 0.78, F1 Macro: 0.7788\n",
      "Best result for 5841 samples: F1 Micro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       342\n",
      "                sara       0.67      0.65      0.66       249\n",
      "         radikalisme       0.82      0.78      0.80       302\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       508\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1401\n",
      "           macro avg       0.79      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.77      0.78      1401\n",
      "         samples avg       0.46      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.749391794204712 seconds\n",
      "\n",
      "Fold 4 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3513, Accuracy: 0.883, F1 Micro: 0.7217, F1 Macro: 0.7227\n",
      "Epoch 2/10, Train Loss: 0.2409, Accuracy: 0.9014, F1 Micro: 0.7742, F1 Macro: 0.7739\n",
      "Epoch 3/10, Train Loss: 0.1966, Accuracy: 0.9016, F1 Micro: 0.7809, F1 Macro: 0.7797\n",
      "Epoch 4/10, Train Loss: 0.1544, Accuracy: 0.9036, F1 Micro: 0.7845, F1 Macro: 0.7823\n",
      "Epoch 5/10, Train Loss: 0.1127, Accuracy: 0.9038, F1 Micro: 0.776, F1 Macro: 0.7717\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.8975, F1 Micro: 0.7689, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0612, Accuracy: 0.8953, F1 Micro: 0.7729, F1 Macro: 0.7754\n",
      "Epoch 8/10, Train Loss: 0.0477, Accuracy: 0.8975, F1 Micro: 0.7766, F1 Macro: 0.7769\n",
      "Epoch 9/10, Train Loss: 0.0328, Accuracy: 0.895, F1 Micro: 0.7717, F1 Macro: 0.7739\n",
      "Epoch 10/10, Train Loss: 0.0252, Accuracy: 0.9027, F1 Micro: 0.7809, F1 Macro: 0.779\n",
      "Best result for 6041 samples: F1 Micro: 0.7845\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       342\n",
      "                sara       0.63      0.70      0.66       249\n",
      "         radikalisme       0.77      0.82      0.80       302\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       508\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1401\n",
      "           macro avg       0.77      0.80      0.78      1401\n",
      "        weighted avg       0.77      0.80      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 178\n",
      "Sampling duration: 2.102698802947998 seconds\n",
      "\n",
      "Fold 4 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3464, Accuracy: 0.8903, F1 Micro: 0.7482, F1 Macro: 0.7432\n",
      "Epoch 2/10, Train Loss: 0.2281, Accuracy: 0.9003, F1 Micro: 0.7794, F1 Macro: 0.7779\n",
      "Epoch 3/10, Train Loss: 0.1873, Accuracy: 0.9056, F1 Micro: 0.7804, F1 Macro: 0.7753\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9013, F1 Micro: 0.7792, F1 Macro: 0.7782\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.8961, F1 Micro: 0.7766, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.9023, F1 Micro: 0.7733, F1 Macro: 0.7719\n",
      "Epoch 7/10, Train Loss: 0.0596, Accuracy: 0.8986, F1 Micro: 0.7767, F1 Macro: 0.7776\n",
      "Epoch 8/10, Train Loss: 0.0439, Accuracy: 0.9003, F1 Micro: 0.7705, F1 Macro: 0.7688\n",
      "Epoch 9/10, Train Loss: 0.031, Accuracy: 0.9017, F1 Micro: 0.7661, F1 Macro: 0.7597\n",
      "Epoch 10/10, Train Loss: 0.0258, Accuracy: 0.9028, F1 Micro: 0.7788, F1 Macro: 0.7759\n",
      "Best result for 6219 samples: F1 Micro: 0.7804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       342\n",
      "                sara       0.68      0.63      0.65       249\n",
      "         radikalisme       0.79      0.81      0.80       302\n",
      "pencemaran_nama_baik       0.78      0.71      0.74       508\n",
      "\n",
      "           micro avg       0.80      0.77      0.78      1401\n",
      "           macro avg       0.79      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.77      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 3770.17 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4673, Accuracy: 0.7939, F1 Micro: 0.0449, F1 Macro: 0.0404\n",
      "Epoch 3/10, Train Loss: 0.421, Accuracy: 0.8194, F1 Micro: 0.2561, F1 Macro: 0.1786\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8313, F1 Micro: 0.3933, F1 Macro: 0.3032\n",
      "Epoch 5/10, Train Loss: 0.3375, Accuracy: 0.8416, F1 Micro: 0.4483, F1 Macro: 0.3709\n",
      "Epoch 6/10, Train Loss: 0.2901, Accuracy: 0.8577, F1 Micro: 0.5902, F1 Macro: 0.5609\n",
      "Epoch 7/10, Train Loss: 0.2547, Accuracy: 0.8578, F1 Micro: 0.5852, F1 Macro: 0.5546\n",
      "Epoch 8/10, Train Loss: 0.2265, Accuracy: 0.867, F1 Micro: 0.6411, F1 Macro: 0.6284\n",
      "Epoch 9/10, Train Loss: 0.174, Accuracy: 0.8655, F1 Micro: 0.6682, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.1497, Accuracy: 0.8669, F1 Micro: 0.6611, F1 Macro: 0.6529\n",
      "Best result for 388 samples: F1 Micro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.82      0.84       353\n",
      "                sara       0.54      0.56      0.55       239\n",
      "         radikalisme       0.69      0.58      0.63       273\n",
      "pencemaran_nama_baik       0.66      0.59      0.62       485\n",
      "\n",
      "           micro avg       0.70      0.64      0.67      1350\n",
      "           macro avg       0.69      0.64      0.66      1350\n",
      "        weighted avg       0.70      0.64      0.67      1350\n",
      "         samples avg       0.36      0.36      0.35      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 584\n",
      "Sampling duration: 36.61707615852356 seconds\n",
      "\n",
      "Fold 5 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4892, Accuracy: 0.8098, F1 Micro: 0.1805, F1 Macro: 0.143\n",
      "Epoch 2/10, Train Loss: 0.3631, Accuracy: 0.833, F1 Micro: 0.3678, F1 Macro: 0.2735\n",
      "Epoch 3/10, Train Loss: 0.3015, Accuracy: 0.8537, F1 Micro: 0.5205, F1 Macro: 0.4453\n",
      "Epoch 4/10, Train Loss: 0.2449, Accuracy: 0.877, F1 Micro: 0.6956, F1 Macro: 0.6942\n",
      "Epoch 5/10, Train Loss: 0.1958, Accuracy: 0.8806, F1 Micro: 0.6961, F1 Macro: 0.6881\n",
      "Epoch 6/10, Train Loss: 0.1475, Accuracy: 0.8761, F1 Micro: 0.6542, F1 Macro: 0.646\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.8763, F1 Micro: 0.7097, F1 Macro: 0.7115\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.8797, F1 Micro: 0.693, F1 Macro: 0.6892\n",
      "Epoch 9/10, Train Loss: 0.0755, Accuracy: 0.8838, F1 Micro: 0.689, F1 Macro: 0.676\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.8861, F1 Micro: 0.7122, F1 Macro: 0.7093\n",
      "Best result for 972 samples: F1 Micro: 0.7122\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       353\n",
      "                sara       0.61      0.54      0.57       239\n",
      "         radikalisme       0.76      0.79      0.78       273\n",
      "pencemaran_nama_baik       0.71      0.52      0.60       485\n",
      "\n",
      "           micro avg       0.76      0.67      0.71      1350\n",
      "           macro avg       0.75      0.68      0.71      1350\n",
      "        weighted avg       0.76      0.67      0.71      1350\n",
      "         samples avg       0.39      0.38      0.37      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 32.44292235374451 seconds\n",
      "\n",
      "Fold 5 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.468, Accuracy: 0.8217, F1 Micro: 0.27, F1 Macro: 0.1988\n",
      "Epoch 2/10, Train Loss: 0.3322, Accuracy: 0.86, F1 Micro: 0.584, F1 Macro: 0.5184\n",
      "Epoch 3/10, Train Loss: 0.2628, Accuracy: 0.8786, F1 Micro: 0.7024, F1 Macro: 0.6954\n",
      "Epoch 4/10, Train Loss: 0.2183, Accuracy: 0.8833, F1 Micro: 0.7324, F1 Macro: 0.7327\n",
      "Epoch 5/10, Train Loss: 0.1715, Accuracy: 0.8864, F1 Micro: 0.7095, F1 Macro: 0.7035\n",
      "Epoch 6/10, Train Loss: 0.1282, Accuracy: 0.8847, F1 Micro: 0.6953, F1 Macro: 0.6834\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.8866, F1 Micro: 0.7142, F1 Macro: 0.711\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.8892, F1 Micro: 0.7234, F1 Macro: 0.7171\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.8866, F1 Micro: 0.7195, F1 Macro: 0.7141\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.8898, F1 Micro: 0.723, F1 Macro: 0.7162\n",
      "Best result for 1497 samples: F1 Micro: 0.7324\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.84      0.88       353\n",
      "                sara       0.53      0.65      0.59       239\n",
      "         radikalisme       0.74      0.82      0.78       273\n",
      "pencemaran_nama_baik       0.66      0.71      0.68       485\n",
      "\n",
      "           micro avg       0.71      0.76      0.73      1350\n",
      "           macro avg       0.71      0.76      0.73      1350\n",
      "        weighted avg       0.72      0.76      0.74      1350\n",
      "         samples avg       0.40      0.42      0.40      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 29.40869450569153 seconds\n",
      "\n",
      "Fold 5 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4417, Accuracy: 0.8439, F1 Micro: 0.4568, F1 Macro: 0.3815\n",
      "Epoch 2/10, Train Loss: 0.3057, Accuracy: 0.878, F1 Micro: 0.679, F1 Macro: 0.6649\n",
      "Epoch 3/10, Train Loss: 0.2459, Accuracy: 0.8848, F1 Micro: 0.7133, F1 Macro: 0.702\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.893, F1 Micro: 0.7327, F1 Macro: 0.7314\n",
      "Epoch 5/10, Train Loss: 0.1651, Accuracy: 0.895, F1 Micro: 0.7524, F1 Macro: 0.7517\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.8983, F1 Micro: 0.7462, F1 Macro: 0.7445\n",
      "Epoch 7/10, Train Loss: 0.0919, Accuracy: 0.897, F1 Micro: 0.748, F1 Macro: 0.7463\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.8961, F1 Micro: 0.7499, F1 Macro: 0.7485\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.8967, F1 Micro: 0.752, F1 Macro: 0.7474\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.8978, F1 Micro: 0.7453, F1 Macro: 0.7413\n",
      "Best result for 1970 samples: F1 Micro: 0.7524\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       353\n",
      "                sara       0.58      0.67      0.62       239\n",
      "         radikalisme       0.74      0.85      0.79       273\n",
      "pencemaran_nama_baik       0.71      0.66      0.69       485\n",
      "\n",
      "           micro avg       0.75      0.76      0.75      1350\n",
      "           macro avg       0.74      0.76      0.75      1350\n",
      "        weighted avg       0.76      0.76      0.75      1350\n",
      "         samples avg       0.41      0.41      0.40      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 26.296621322631836 seconds\n",
      "\n",
      "Fold 5 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4402, Accuracy: 0.8416, F1 Micro: 0.4232, F1 Macro: 0.3348\n",
      "Epoch 2/10, Train Loss: 0.2915, Accuracy: 0.8814, F1 Micro: 0.712, F1 Macro: 0.7141\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.8902, F1 Micro: 0.7096, F1 Macro: 0.6993\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.888, F1 Micro: 0.7239, F1 Macro: 0.7161\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.8986, F1 Micro: 0.743, F1 Macro: 0.737\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.8964, F1 Micro: 0.7599, F1 Macro: 0.7614\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.897, F1 Micro: 0.7627, F1 Macro: 0.7641\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.8997, F1 Micro: 0.7504, F1 Macro: 0.7411\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9003, F1 Micro: 0.7628, F1 Macro: 0.7623\n",
      "Epoch 10/10, Train Loss: 0.0387, Accuracy: 0.8989, F1 Micro: 0.7627, F1 Macro: 0.7604\n",
      "Best result for 2395 samples: F1 Micro: 0.7628\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       353\n",
      "                sara       0.64      0.64      0.64       239\n",
      "         radikalisme       0.79      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.68      0.69       485\n",
      "\n",
      "           micro avg       0.77      0.76      0.76      1350\n",
      "           macro avg       0.76      0.76      0.76      1350\n",
      "        weighted avg       0.77      0.76      0.76      1350\n",
      "         samples avg       0.42      0.42      0.41      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 24.2634060382843 seconds\n",
      "\n",
      "Fold 5 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4269, Accuracy: 0.8566, F1 Micro: 0.5424, F1 Macro: 0.4907\n",
      "Epoch 2/10, Train Loss: 0.2838, Accuracy: 0.8902, F1 Micro: 0.7087, F1 Macro: 0.6909\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.8802, F1 Micro: 0.6318, F1 Macro: 0.5939\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.8931, F1 Micro: 0.7598, F1 Macro: 0.7616\n",
      "Epoch 5/10, Train Loss: 0.1462, Accuracy: 0.8981, F1 Micro: 0.7479, F1 Macro: 0.7416\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.8986, F1 Micro: 0.7432, F1 Macro: 0.7349\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.8947, F1 Micro: 0.7509, F1 Macro: 0.7482\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.8972, F1 Micro: 0.7563, F1 Macro: 0.7537\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.8997, F1 Micro: 0.7633, F1 Macro: 0.762\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.8981, F1 Micro: 0.7431, F1 Macro: 0.7382\n",
      "Best result for 2778 samples: F1 Micro: 0.7633\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.93       353\n",
      "                sara       0.64      0.63      0.63       239\n",
      "         radikalisme       0.78      0.80      0.79       273\n",
      "pencemaran_nama_baik       0.68      0.71      0.69       485\n",
      "\n",
      "           micro avg       0.76      0.77      0.76      1350\n",
      "           macro avg       0.76      0.76      0.76      1350\n",
      "        weighted avg       0.76      0.77      0.76      1350\n",
      "         samples avg       0.43      0.43      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 21.85763907432556 seconds\n",
      "\n",
      "Fold 5 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.418, Accuracy: 0.8711, F1 Micro: 0.6421, F1 Macro: 0.6209\n",
      "Epoch 2/10, Train Loss: 0.2671, Accuracy: 0.8923, F1 Micro: 0.7419, F1 Macro: 0.7354\n",
      "Epoch 3/10, Train Loss: 0.2256, Accuracy: 0.8964, F1 Micro: 0.7401, F1 Macro: 0.7329\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.8936, F1 Micro: 0.76, F1 Macro: 0.7605\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.9038, F1 Micro: 0.7674, F1 Macro: 0.7673\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.9005, F1 Micro: 0.7647, F1 Macro: 0.7613\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.902, F1 Micro: 0.767, F1 Macro: 0.7658\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.8995, F1 Micro: 0.7541, F1 Macro: 0.7449\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9039, F1 Micro: 0.7675, F1 Macro: 0.7631\n",
      "Epoch 10/10, Train Loss: 0.0386, Accuracy: 0.8995, F1 Micro: 0.7678, F1 Macro: 0.7673\n",
      "Best result for 3123 samples: F1 Micro: 0.7678\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.59      0.72      0.65       239\n",
      "         radikalisme       0.78      0.80      0.79       273\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       485\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1350\n",
      "           macro avg       0.75      0.79      0.77      1350\n",
      "        weighted avg       0.76      0.79      0.77      1350\n",
      "         samples avg       0.43      0.44      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.646857976913452 seconds\n",
      "\n",
      "Fold 5 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4061, Accuracy: 0.8642, F1 Micro: 0.6952, F1 Macro: 0.6957\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.8947, F1 Micro: 0.739, F1 Macro: 0.738\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.8981, F1 Micro: 0.7306, F1 Macro: 0.7088\n",
      "Epoch 4/10, Train Loss: 0.1775, Accuracy: 0.8992, F1 Micro: 0.7717, F1 Macro: 0.771\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9042, F1 Micro: 0.7684, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9047, F1 Micro: 0.7688, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9041, F1 Micro: 0.7638, F1 Macro: 0.761\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9069, F1 Micro: 0.7605, F1 Macro: 0.7546\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.9045, F1 Micro: 0.7607, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.038, Accuracy: 0.9006, F1 Micro: 0.7679, F1 Macro: 0.7674\n",
      "Best result for 3433 samples: F1 Micro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.91       353\n",
      "                sara       0.61      0.72      0.66       239\n",
      "         radikalisme       0.79      0.79      0.79       273\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       485\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1350\n",
      "           macro avg       0.74      0.80      0.77      1350\n",
      "        weighted avg       0.74      0.81      0.77      1350\n",
      "         samples avg       0.44      0.45      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.90858006477356 seconds\n",
      "\n",
      "Fold 5 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.41, Accuracy: 0.8784, F1 Micro: 0.684, F1 Macro: 0.6785\n",
      "Epoch 2/10, Train Loss: 0.2655, Accuracy: 0.8905, F1 Micro: 0.7545, F1 Macro: 0.757\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.9011, F1 Micro: 0.7665, F1 Macro: 0.7637\n",
      "Epoch 4/10, Train Loss: 0.1701, Accuracy: 0.9033, F1 Micro: 0.7693, F1 Macro: 0.7669\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.9017, F1 Micro: 0.7661, F1 Macro: 0.7637\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9059, F1 Micro: 0.7661, F1 Macro: 0.7602\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.905, F1 Micro: 0.7599, F1 Macro: 0.7515\n",
      "Epoch 8/10, Train Loss: 0.0586, Accuracy: 0.9006, F1 Micro: 0.7674, F1 Macro: 0.7653\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9033, F1 Micro: 0.7712, F1 Macro: 0.7694\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.9017, F1 Micro: 0.7552, F1 Macro: 0.7437\n",
      "Best result for 3712 samples: F1 Micro: 0.7712\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       353\n",
      "                sara       0.66      0.64      0.65       239\n",
      "         radikalisme       0.81      0.79      0.80       273\n",
      "pencemaran_nama_baik       0.69      0.73      0.71       485\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1350\n",
      "           macro avg       0.77      0.77      0.77      1350\n",
      "        weighted avg       0.77      0.77      0.77      1350\n",
      "         samples avg       0.43      0.43      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 174\n",
      "Sampling duration: 16.273026943206787 seconds\n",
      "\n",
      "Fold 5 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8742, F1 Micro: 0.6477, F1 Macro: 0.6387\n",
      "Epoch 2/10, Train Loss: 0.2659, Accuracy: 0.8958, F1 Micro: 0.7463, F1 Macro: 0.7401\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.8997, F1 Micro: 0.7606, F1 Macro: 0.7568\n",
      "Epoch 4/10, Train Loss: 0.1656, Accuracy: 0.9013, F1 Micro: 0.761, F1 Macro: 0.7596\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9036, F1 Micro: 0.7681, F1 Macro: 0.764\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9028, F1 Micro: 0.7585, F1 Macro: 0.75\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9008, F1 Micro: 0.7771, F1 Macro: 0.7767\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9047, F1 Micro: 0.7593, F1 Macro: 0.7511\n",
      "Epoch 9/10, Train Loss: 0.0446, Accuracy: 0.9059, F1 Micro: 0.7728, F1 Macro: 0.7669\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9041, F1 Micro: 0.7734, F1 Macro: 0.7683\n",
      "Best result for 3886 samples: F1 Micro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.63      0.69      0.66       239\n",
      "         radikalisme       0.79      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       485\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1350\n",
      "           macro avg       0.75      0.81      0.78      1350\n",
      "        weighted avg       0.75      0.82      0.78      1350\n",
      "         samples avg       0.45      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 15.218422174453735 seconds\n",
      "\n",
      "Fold 5 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.877, F1 Micro: 0.7158, F1 Macro: 0.7117\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.8961, F1 Micro: 0.7525, F1 Macro: 0.7417\n",
      "Epoch 3/10, Train Loss: 0.1998, Accuracy: 0.8992, F1 Micro: 0.7633, F1 Macro: 0.7609\n",
      "Epoch 4/10, Train Loss: 0.1607, Accuracy: 0.9, F1 Micro: 0.7442, F1 Macro: 0.7313\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.9006, F1 Micro: 0.7639, F1 Macro: 0.7613\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9039, F1 Micro: 0.7764, F1 Macro: 0.7793\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.905, F1 Micro: 0.7707, F1 Macro: 0.768\n",
      "Epoch 8/10, Train Loss: 0.0506, Accuracy: 0.9011, F1 Micro: 0.7775, F1 Macro: 0.7776\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9066, F1 Micro: 0.7704, F1 Macro: 0.7638\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.9036, F1 Micro: 0.7736, F1 Macro: 0.7709\n",
      "Best result for 4120 samples: F1 Micro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       353\n",
      "                sara       0.62      0.69      0.65       239\n",
      "         radikalisme       0.81      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       485\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1350\n",
      "           macro avg       0.75      0.81      0.78      1350\n",
      "        weighted avg       0.75      0.82      0.78      1350\n",
      "         samples avg       0.46      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.74457049369812 seconds\n",
      "\n",
      "Fold 5 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3812, Accuracy: 0.8766, F1 Micro: 0.6601, F1 Macro: 0.6344\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.8988, F1 Micro: 0.7623, F1 Macro: 0.7596\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.8978, F1 Micro: 0.7506, F1 Macro: 0.7491\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.9009, F1 Micro: 0.7703, F1 Macro: 0.7693\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9002, F1 Micro: 0.7752, F1 Macro: 0.7745\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9008, F1 Micro: 0.7603, F1 Macro: 0.7569\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.905, F1 Micro: 0.7799, F1 Macro: 0.7796\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.776\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9052, F1 Micro: 0.7759, F1 Macro: 0.7727\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9034, F1 Micro: 0.7697, F1 Macro: 0.765\n",
      "Best result for 4330 samples: F1 Micro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       353\n",
      "                sara       0.65      0.67      0.66       239\n",
      "         radikalisme       0.77      0.86      0.81       273\n",
      "pencemaran_nama_baik       0.69      0.74      0.71       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.59048318862915 seconds\n",
      "\n",
      "Fold 5 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.88, F1 Micro: 0.6928, F1 Macro: 0.6824\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9016, F1 Micro: 0.7599, F1 Macro: 0.752\n",
      "Epoch 3/10, Train Loss: 0.2058, Accuracy: 0.9038, F1 Micro: 0.7689, F1 Macro: 0.7668\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9061, F1 Micro: 0.7668, F1 Macro: 0.7625\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9086, F1 Micro: 0.78, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9016, F1 Micro: 0.7735, F1 Macro: 0.7729\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.9033, F1 Micro: 0.7566, F1 Macro: 0.7494\n",
      "Epoch 8/10, Train Loss: 0.0556, Accuracy: 0.903, F1 Micro: 0.7697, F1 Macro: 0.765\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.9081, F1 Micro: 0.7759, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9045, F1 Micro: 0.775, F1 Macro: 0.7714\n",
      "Best result for 4530 samples: F1 Micro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.70      0.63      0.66       239\n",
      "         radikalisme       0.78      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.74      0.70      0.72       485\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1350\n",
      "           macro avg       0.79      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.77      0.78      1350\n",
      "         samples avg       0.43      0.43      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.426882266998291 seconds\n",
      "\n",
      "Fold 5 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3718, Accuracy: 0.8753, F1 Micro: 0.7295, F1 Macro: 0.7283\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.8977, F1 Micro: 0.7371, F1 Macro: 0.717\n",
      "Epoch 3/10, Train Loss: 0.1946, Accuracy: 0.8989, F1 Micro: 0.7716, F1 Macro: 0.7702\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.8995, F1 Micro: 0.7708, F1 Macro: 0.7692\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.8969, F1 Micro: 0.774, F1 Macro: 0.7751\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.9025, F1 Micro: 0.7704, F1 Macro: 0.7677\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.8991, F1 Micro: 0.7614, F1 Macro: 0.7569\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.9034, F1 Micro: 0.7652, F1 Macro: 0.7588\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9038, F1 Micro: 0.7727, F1 Macro: 0.7666\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.8981, F1 Micro: 0.7673, F1 Macro: 0.7662\n",
      "Best result for 4663 samples: F1 Micro: 0.774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.56      0.75      0.64       239\n",
      "         radikalisme       0.78      0.85      0.81       273\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       485\n",
      "\n",
      "           micro avg       0.72      0.84      0.77      1350\n",
      "           macro avg       0.73      0.83      0.78      1350\n",
      "        weighted avg       0.73      0.84      0.78      1350\n",
      "         samples avg       0.45      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.829230070114136 seconds\n",
      "\n",
      "Fold 5 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3782, Accuracy: 0.8858, F1 Micro: 0.6818, F1 Macro: 0.6612\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.8992, F1 Micro: 0.7375, F1 Macro: 0.7213\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9042, F1 Micro: 0.7547, F1 Macro: 0.7523\n",
      "Epoch 4/10, Train Loss: 0.1573, Accuracy: 0.9056, F1 Micro: 0.7709, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.1186, Accuracy: 0.9064, F1 Micro: 0.7722, F1 Macro: 0.7681\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.9034, F1 Micro: 0.7799, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.0648, Accuracy: 0.9002, F1 Micro: 0.7669, F1 Macro: 0.7657\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9067, F1 Micro: 0.7689, F1 Macro: 0.7649\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9048, F1 Micro: 0.772, F1 Macro: 0.7651\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8961, F1 Micro: 0.7706, F1 Macro: 0.7734\n",
      "Best result for 4863 samples: F1 Micro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       353\n",
      "                sara       0.63      0.72      0.67       239\n",
      "         radikalisme       0.78      0.84      0.81       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1350\n",
      "           macro avg       0.75      0.81      0.78      1350\n",
      "        weighted avg       0.75      0.81      0.78      1350\n",
      "         samples avg       0.44      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.656932353973389 seconds\n",
      "\n",
      "Fold 5 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.379, Accuracy: 0.887, F1 Micro: 0.7395, F1 Macro: 0.7358\n",
      "Epoch 2/10, Train Loss: 0.2402, Accuracy: 0.8964, F1 Micro: 0.77, F1 Macro: 0.7699\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9052, F1 Micro: 0.7774, F1 Macro: 0.778\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9027, F1 Micro: 0.7766, F1 Macro: 0.775\n",
      "Epoch 5/10, Train Loss: 0.1207, Accuracy: 0.9083, F1 Micro: 0.7766, F1 Macro: 0.7749\n",
      "Epoch 6/10, Train Loss: 0.0813, Accuracy: 0.9053, F1 Micro: 0.7724, F1 Macro: 0.7684\n",
      "Epoch 7/10, Train Loss: 0.0661, Accuracy: 0.9027, F1 Micro: 0.7763, F1 Macro: 0.7758\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9009, F1 Micro: 0.7744, F1 Macro: 0.7738\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.9059, F1 Micro: 0.7757, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9003, F1 Micro: 0.7697, F1 Macro: 0.7608\n",
      "Best result for 5063 samples: F1 Micro: 0.7774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.66      0.70      0.68       239\n",
      "         radikalisme       0.78      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       485\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1350\n",
      "           macro avg       0.77      0.79      0.78      1350\n",
      "        weighted avg       0.77      0.79      0.78      1350\n",
      "         samples avg       0.43      0.43      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.2520272731781 seconds\n",
      "\n",
      "Fold 5 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.8925, F1 Micro: 0.7261, F1 Macro: 0.7156\n",
      "Epoch 2/10, Train Loss: 0.2412, Accuracy: 0.8995, F1 Micro: 0.7445, F1 Macro: 0.7361\n",
      "Epoch 3/10, Train Loss: 0.196, Accuracy: 0.9073, F1 Micro: 0.7664, F1 Macro: 0.7543\n",
      "Epoch 4/10, Train Loss: 0.1586, Accuracy: 0.9016, F1 Micro: 0.7488, F1 Macro: 0.7351\n",
      "Epoch 5/10, Train Loss: 0.1184, Accuracy: 0.9022, F1 Micro: 0.7783, F1 Macro: 0.7777\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.9061, F1 Micro: 0.7614, F1 Macro: 0.7527\n",
      "Epoch 7/10, Train Loss: 0.0629, Accuracy: 0.9038, F1 Micro: 0.7732, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.9045, F1 Micro: 0.7741, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.9053, F1 Micro: 0.7801, F1 Macro: 0.7782\n",
      "Epoch 10/10, Train Loss: 0.0266, Accuracy: 0.8972, F1 Micro: 0.7728, F1 Macro: 0.7709\n",
      "Best result for 5263 samples: F1 Micro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       353\n",
      "                sara       0.63      0.69      0.66       239\n",
      "         radikalisme       0.79      0.79      0.79       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.77      0.79      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.78      1350\n",
      "         samples avg       0.45      0.44      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 7.120199680328369 seconds\n",
      "\n",
      "Fold 5 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3685, Accuracy: 0.8906, F1 Micro: 0.7268, F1 Macro: 0.7036\n",
      "Epoch 2/10, Train Loss: 0.236, Accuracy: 0.9014, F1 Micro: 0.7666, F1 Macro: 0.7675\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.9066, F1 Micro: 0.7616, F1 Macro: 0.7564\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9031, F1 Micro: 0.7644, F1 Macro: 0.7598\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9062, F1 Micro: 0.7825, F1 Macro: 0.782\n",
      "Epoch 6/10, Train Loss: 0.0865, Accuracy: 0.9045, F1 Micro: 0.7756, F1 Macro: 0.7727\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9097, F1 Micro: 0.7749, F1 Macro: 0.7703\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.8995, F1 Micro: 0.777, F1 Macro: 0.7746\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9036, F1 Micro: 0.7783, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9038, F1 Micro: 0.7752, F1 Macro: 0.7728\n",
      "Best result for 5441 samples: F1 Micro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       353\n",
      "                sara       0.66      0.69      0.67       239\n",
      "         radikalisme       0.77      0.86      0.81       273\n",
      "pencemaran_nama_baik       0.71      0.72      0.72       485\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.127741098403931 seconds\n",
      "\n",
      "Fold 5 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3565, Accuracy: 0.8877, F1 Micro: 0.7294, F1 Macro: 0.7228\n",
      "Epoch 2/10, Train Loss: 0.2433, Accuracy: 0.9045, F1 Micro: 0.7803, F1 Macro: 0.7812\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9039, F1 Micro: 0.7731, F1 Macro: 0.7657\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9055, F1 Micro: 0.7685, F1 Macro: 0.7633\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9102, F1 Micro: 0.7898, F1 Macro: 0.7864\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.9075, F1 Micro: 0.7789, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0685, Accuracy: 0.9066, F1 Micro: 0.7711, F1 Macro: 0.7674\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9072, F1 Micro: 0.7803, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9059, F1 Micro: 0.779, F1 Macro: 0.7803\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.905, F1 Micro: 0.7765, F1 Macro: 0.772\n",
      "Best result for 5641 samples: F1 Micro: 0.7898\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       353\n",
      "                sara       0.70      0.65      0.67       239\n",
      "         radikalisme       0.80      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       485\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1350\n",
      "           macro avg       0.78      0.79      0.79      1350\n",
      "        weighted avg       0.78      0.80      0.79      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.0642619132995605 seconds\n",
      "\n",
      "Fold 5 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3646, Accuracy: 0.8936, F1 Micro: 0.7386, F1 Macro: 0.7293\n",
      "Epoch 2/10, Train Loss: 0.2359, Accuracy: 0.9023, F1 Micro: 0.7565, F1 Macro: 0.7497\n",
      "Epoch 3/10, Train Loss: 0.1956, Accuracy: 0.9036, F1 Micro: 0.7834, F1 Macro: 0.7838\n",
      "Epoch 4/10, Train Loss: 0.1554, Accuracy: 0.9072, F1 Micro: 0.7767, F1 Macro: 0.7733\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9062, F1 Micro: 0.7729, F1 Macro: 0.7671\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9052, F1 Micro: 0.7756, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0601, Accuracy: 0.9045, F1 Micro: 0.7785, F1 Macro: 0.7754\n",
      "Epoch 8/10, Train Loss: 0.0463, Accuracy: 0.9025, F1 Micro: 0.7636, F1 Macro: 0.7534\n",
      "Epoch 9/10, Train Loss: 0.0359, Accuracy: 0.9023, F1 Micro: 0.7648, F1 Macro: 0.7587\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9042, F1 Micro: 0.7775, F1 Macro: 0.7755\n",
      "Best result for 5841 samples: F1 Micro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       353\n",
      "                sara       0.67      0.69      0.68       239\n",
      "         radikalisme       0.78      0.84      0.81       273\n",
      "pencemaran_nama_baik       0.66      0.82      0.73       485\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1350\n",
      "           macro avg       0.75      0.82      0.78      1350\n",
      "        weighted avg       0.75      0.83      0.79      1350\n",
      "         samples avg       0.46      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.93745756149292 seconds\n",
      "\n",
      "Fold 5 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3476, Accuracy: 0.8955, F1 Micro: 0.7428, F1 Macro: 0.7383\n",
      "Epoch 2/10, Train Loss: 0.2375, Accuracy: 0.9038, F1 Micro: 0.7586, F1 Macro: 0.7483\n",
      "Epoch 3/10, Train Loss: 0.1973, Accuracy: 0.905, F1 Micro: 0.7786, F1 Macro: 0.7755\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.9044, F1 Micro: 0.7716, F1 Macro: 0.7678\n",
      "Epoch 5/10, Train Loss: 0.1184, Accuracy: 0.907, F1 Micro: 0.7584, F1 Macro: 0.7487\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9048, F1 Micro: 0.7805, F1 Macro: 0.7765\n",
      "Epoch 7/10, Train Loss: 0.0648, Accuracy: 0.9036, F1 Micro: 0.769, F1 Macro: 0.7618\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9017, F1 Micro: 0.7702, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0342, Accuracy: 0.9041, F1 Micro: 0.7756, F1 Macro: 0.7738\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9036, F1 Micro: 0.7752, F1 Macro: 0.7737\n",
      "Best result for 6041 samples: F1 Micro: 0.7805\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.92       353\n",
      "                sara       0.65      0.65      0.65       239\n",
      "         radikalisme       0.80      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.79      0.78      1350\n",
      "        weighted avg       0.76      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 178\n",
      "Sampling duration: 2.1841750144958496 seconds\n",
      "\n",
      "Fold 5 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3493, Accuracy: 0.8934, F1 Micro: 0.7524, F1 Macro: 0.752\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9034, F1 Micro: 0.7638, F1 Macro: 0.7528\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9072, F1 Micro: 0.7762, F1 Macro: 0.7702\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9003, F1 Micro: 0.7782, F1 Macro: 0.7764\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.9077, F1 Micro: 0.7781, F1 Macro: 0.7718\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9064, F1 Micro: 0.7766, F1 Macro: 0.7728\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9078, F1 Micro: 0.7816, F1 Macro: 0.7782\n",
      "Epoch 8/10, Train Loss: 0.0465, Accuracy: 0.9067, F1 Micro: 0.7773, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0329, Accuracy: 0.9095, F1 Micro: 0.7837, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0277, Accuracy: 0.9072, F1 Micro: 0.7835, F1 Macro: 0.7806\n",
      "Best result for 6219 samples: F1 Micro: 0.7837\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.94      0.92       353\n",
      "                sara       0.68      0.63      0.66       239\n",
      "         radikalisme       0.80      0.77      0.79       273\n",
      "pencemaran_nama_baik       0.75      0.73      0.74       485\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.78      0.78      1350\n",
      "         samples avg       0.45      0.44      0.44      1350\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 3739.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[2:6]\n",
    "X = data['processed_text'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Each element in these lists will be a list of metrics for one fold's learning curve\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    # Set seed for reproducibility within the fold\n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    # Adjust checkpoints based on the current fold's training size\n",
    "    checkpoints = [\n",
    "        # int(0.1 * total_train_fold_size)\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # 1. Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(f'{filename}-fold-{fold + 1}-model')\n",
    "        \n",
    "        # 3. Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        sampling_args = (model, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples_shared, fold, X_train_fold, y_train_fold)\n",
    "        notebook_launcher(kmeans_clustering_sampling, sampling_args, num_processes=2)\n",
    "        \n",
    "        # 4. Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3aaffe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T21:01:01.210434Z",
     "iopub.status.busy": "2025-06-27T21:01:01.210022Z",
     "iopub.status.idle": "2025-06-27T21:01:02.005044Z",
     "shell.execute_reply": "2025-06-27T21:01:02.004066Z"
    },
    "papermill": {
     "duration": 1.002247,
     "end_time": "2025-06-27T21:01:02.006439",
     "exception": false,
     "start_time": "2025-06-27T21:01:01.004192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6739372,
     "sourceId": 11761127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18743.919841,
   "end_time": "2025-06-27T21:01:05.033714",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T15:48:41.113873",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01f7b9efed4c46bfb66267037385a669": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3f8ec666546440ad97390c722304d254",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8cc724884994ce0bf5c8bff0f779b89",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "0ac35fac108941b6a68e978feaf210af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0af05c2159564370b7059aa20a860ba7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c31c9a7a7954ec49bdd78e037465b8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9e2232f929a940eeb3c487335cb6958a",
       "placeholder": "​",
       "style": "IPY_MODEL_606dc0cc41f14c19aeb019f0bcef8ef1",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "0cad12bbfa104568a406a93eeb29a129": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab7b91d7f0ec4c6394cf0c0b2a2ea367",
       "placeholder": "​",
       "style": "IPY_MODEL_de7fb064e28f46a980f4629fc9bd5904",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 11.9kB/s]"
      }
     },
     "11222b0949634cfaa59e0d8d50ab1a49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a3cef87ee78c4df1bed091c3418e3bfd",
        "IPY_MODEL_31d011eed6d14442a446ea8ca8a21159",
        "IPY_MODEL_784a73dbf91a4d5f825371842ce562a3"
       ],
       "layout": "IPY_MODEL_21234a79cb3440a1b7b014101368e08a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "185a315b3b8c4564bc9284d0995f8f86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1cc6e0b7946645b8885fb58cefaa1cce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2013f5de9af44395b769177794eb9a10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21234a79cb3440a1b7b014101368e08a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268f18a724054cf1bc186a583082ecb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e2517c9a4a454065a6cf819494bbc00b",
       "placeholder": "​",
       "style": "IPY_MODEL_9545b07e3ece4f108f21f0a6981c5c1c",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: "
      }
     },
     "305b9b54dd06417ebbac288a3f1ff8a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31d011eed6d14442a446ea8ca8a21159": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2013f5de9af44395b769177794eb9a10",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1345effd39146ff96b5a785fc17474d",
       "tabbable": null,
       "tooltip": null,
       "value": 497810400.0
      }
     },
     "334dec9e0cf947f8b7c89cfa11be3f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33bcb583bd704263a20b62f1245b570b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c0bcdef1d5442d681bc6d10930ee17b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3f8ec666546440ad97390c722304d254": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "44acd9dd54e14f04a421e7ffec055ef8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4901b93fb62247b6921e7d5bc40ac636": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "581c927ee6a64803b57b64d1a56c35b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_268f18a724054cf1bc186a583082ecb5",
        "IPY_MODEL_95ecc00eda414ce285ae7ece29834099",
        "IPY_MODEL_a5e32b3177fd4f9a8f42b14b3d3c48f2"
       ],
       "layout": "IPY_MODEL_334dec9e0cf947f8b7c89cfa11be3f40",
       "tabbable": null,
       "tooltip": null
      }
     },
     "600be6087ac047ac85a30ebc86f1c4e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "606dc0cc41f14c19aeb019f0bcef8ef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6bf94da6878241ceb80cd8a0ea0efda2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "6cb9f89e2c034c5a80db0854fbb15256": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f75b301fbed4b8e8de4108621bc8071": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4901b93fb62247b6921e7d5bc40ac636",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9fc570d389c24f328b05516b05eba689",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "71b22988e21148d09106e6cd26a7cd2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_185a315b3b8c4564bc9284d0995f8f86",
       "placeholder": "​",
       "style": "IPY_MODEL_0ac35fac108941b6a68e978feaf210af",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 144B/s]"
      }
     },
     "784a73dbf91a4d5f825371842ce562a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d1f60babcc204b5ab329ff39994c9ca0",
       "placeholder": "​",
       "style": "IPY_MODEL_e97baea19fc84391a49656f416299cdf",
       "tabbable": null,
       "tooltip": null,
       "value": " 498M/498M [00:02&lt;00:00, 219MB/s]"
      }
     },
     "877c55a545eb425b943692680364b5fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f1bcf678bf6e44099d73c1c6bdce8d76",
        "IPY_MODEL_ee3b0c5bf9b04f439bf833280b37219d",
        "IPY_MODEL_71b22988e21148d09106e6cd26a7cd2a"
       ],
       "layout": "IPY_MODEL_ecf1867fb7954d28b9055d2c492282a6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8e55bf080bf44784b2b75f1f71eb6632": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9545b07e3ece4f108f21f0a6981c5c1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95ecc00eda414ce285ae7ece29834099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6bf94da6878241ceb80cd8a0ea0efda2",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dad8fa7ea499471a8d199d7fad6f5d49",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "9e2232f929a940eeb3c487335cb6958a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f6f1a4fcbc345f7a17432458239947a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9fc570d389c24f328b05516b05eba689": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a3cef87ee78c4df1bed091c3418e3bfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f6f1a4fcbc345f7a17432458239947a",
       "placeholder": "​",
       "style": "IPY_MODEL_3c0bcdef1d5442d681bc6d10930ee17b",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin: 100%"
      }
     },
     "a5e32b3177fd4f9a8f42b14b3d3c48f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e55bf080bf44784b2b75f1f71eb6632",
       "placeholder": "​",
       "style": "IPY_MODEL_600be6087ac047ac85a30ebc86f1c4e3",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/? [00:00&lt;00:00, 9.40MB/s]"
      }
     },
     "a8cc724884994ce0bf5c8bff0f779b89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ab7b91d7f0ec4c6394cf0c0b2a2ea367": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1345effd39146ff96b5a785fc17474d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b1fa081457fe4536a5e7ec5ed21d02ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0c31c9a7a7954ec49bdd78e037465b8d",
        "IPY_MODEL_6f75b301fbed4b8e8de4108621bc8071",
        "IPY_MODEL_0cad12bbfa104568a406a93eeb29a129"
       ],
       "layout": "IPY_MODEL_33bcb583bd704263a20b62f1245b570b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c181516cb26c437aa7d0aff93d7b8d66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1cc6e0b7946645b8885fb58cefaa1cce",
       "placeholder": "​",
       "style": "IPY_MODEL_fdbe52ae7d1b4dfdbd3bc27fdce7f544",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/? [00:00&lt;00:00, 118kB/s]"
      }
     },
     "d1f60babcc204b5ab329ff39994c9ca0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d85b978c107b41fca7cf54765b5f6d22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_305b9b54dd06417ebbac288a3f1ff8a0",
       "placeholder": "​",
       "style": "IPY_MODEL_f041730e92a24a20b63d0934762fbb19",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: "
      }
     },
     "dad8fa7ea499471a8d199d7fad6f5d49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "de7fb064e28f46a980f4629fc9bd5904": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e2517c9a4a454065a6cf819494bbc00b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e64b04434d10476c8ef610c23ef6c64a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d85b978c107b41fca7cf54765b5f6d22",
        "IPY_MODEL_01f7b9efed4c46bfb66267037385a669",
        "IPY_MODEL_c181516cb26c437aa7d0aff93d7b8d66"
       ],
       "layout": "IPY_MODEL_0af05c2159564370b7059aa20a860ba7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e936dd68c29b4dd2a3fcf34062096a29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e97baea19fc84391a49656f416299cdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e9a8cf3123564653a0bdb1189569c6ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ecf1867fb7954d28b9055d2c492282a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee3b0c5bf9b04f439bf833280b37219d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9a8cf3123564653a0bdb1189569c6ac",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_44acd9dd54e14f04a421e7ffec055ef8",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "f041730e92a24a20b63d0934762fbb19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1bcf678bf6e44099d73c1c6bdce8d76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6cb9f89e2c034c5a80db0854fbb15256",
       "placeholder": "​",
       "style": "IPY_MODEL_e936dd68c29b4dd2a3fcf34062096a29",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "fdbe52ae7d1b4dfdbd3bc27fdce7f544": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
