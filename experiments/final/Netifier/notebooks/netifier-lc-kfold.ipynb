{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4563945",
   "metadata": {
    "papermill": {
     "duration": 0.013205,
     "end_time": "2025-06-27T15:45:30.975578",
     "exception": false,
     "start_time": "2025-06-27T15:45:30.962373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b04ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:31.001919Z",
     "iopub.status.busy": "2025-06-27T15:45:31.001541Z",
     "iopub.status.idle": "2025-06-27T15:45:39.914109Z",
     "shell.execute_reply": "2025-06-27T15:45:39.913276Z"
    },
    "papermill": {
     "duration": 8.927648,
     "end_time": "2025-06-27T15:45:39.916287",
     "exception": false,
     "start_time": "2025-06-27T15:45:30.988639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e40c1b",
   "metadata": {
    "papermill": {
     "duration": 0.011359,
     "end_time": "2025-06-27T15:45:39.939689",
     "exception": false,
     "start_time": "2025-06-27T15:45:39.928330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96c4eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:39.964019Z",
     "iopub.status.busy": "2025-06-27T15:45:39.963479Z",
     "iopub.status.idle": "2025-06-27T15:45:39.967914Z",
     "shell.execute_reply": "2025-06-27T15:45:39.967099Z"
    },
    "papermill": {
     "duration": 0.018294,
     "end_time": "2025-06-27T15:45:39.969488",
     "exception": false,
     "start_time": "2025-06-27T15:45:39.951194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c8f7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:39.993548Z",
     "iopub.status.busy": "2025-06-27T15:45:39.993259Z",
     "iopub.status.idle": "2025-06-27T15:45:39.998289Z",
     "shell.execute_reply": "2025-06-27T15:45:39.997383Z"
    },
    "papermill": {
     "duration": 0.019322,
     "end_time": "2025-06-27T15:45:40.000203",
     "exception": false,
     "start_time": "2025-06-27T15:45:39.980881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453a5c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:40.023793Z",
     "iopub.status.busy": "2025-06-27T15:45:40.023474Z",
     "iopub.status.idle": "2025-06-27T15:45:40.033437Z",
     "shell.execute_reply": "2025-06-27T15:45:40.032766Z"
    },
    "papermill": {
     "duration": 0.023578,
     "end_time": "2025-06-27T15:45:40.035169",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.011591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfe16c",
   "metadata": {
    "papermill": {
     "duration": 0.011257,
     "end_time": "2025-06-27T15:45:40.058135",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.046878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd0afe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:40.083254Z",
     "iopub.status.busy": "2025-06-27T15:45:40.082440Z",
     "iopub.status.idle": "2025-06-27T15:45:40.133840Z",
     "shell.execute_reply": "2025-06-27T15:45:40.132223Z"
    },
    "papermill": {
     "duration": 0.066467,
     "end_time": "2025-06-27T15:45:40.136158",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.069691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-lc-kfold'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8cd3a",
   "metadata": {
    "papermill": {
     "duration": 0.011261,
     "end_time": "2025-06-27T15:45:40.159383",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.148122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad5795b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:40.184615Z",
     "iopub.status.busy": "2025-06-27T15:45:40.184170Z",
     "iopub.status.idle": "2025-06-27T15:45:40.339153Z",
     "shell.execute_reply": "2025-06-27T15:45:40.338160Z"
    },
    "papermill": {
     "duration": 0.170356,
     "end_time": "2025-06-27T15:45:40.341226",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.170870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-2/netifier/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-2/netifier/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9811d2f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:40.367425Z",
     "iopub.status.busy": "2025-06-27T15:45:40.366758Z",
     "iopub.status.idle": "2025-06-27T15:45:40.388179Z",
     "shell.execute_reply": "2025-06-27T15:45:40.387097Z"
    },
    "papermill": {
     "duration": 0.037143,
     "end_time": "2025-06-27T15:45:40.390405",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.353262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3e181",
   "metadata": {
    "papermill": {
     "duration": 0.01167,
     "end_time": "2025-06-27T15:45:40.415079",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.403409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f9fffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:40.440918Z",
     "iopub.status.busy": "2025-06-27T15:45:40.440131Z",
     "iopub.status.idle": "2025-06-27T15:45:56.074666Z",
     "shell.execute_reply": "2025-06-27T15:45:56.073824Z"
    },
    "papermill": {
     "duration": 15.649885,
     "end_time": "2025-06-27T15:45:56.076860",
     "exception": false,
     "start_time": "2025-06-27T15:45:40.426975",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004c73e790c040be8d55a46ae07fbe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613747e9e6484801a4e1b6f53812cc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570b41cdaeda405e9a8c8263afc421c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3d9dfb882d455e937e31912d7c3376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define custom Dataset class\n",
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed33859c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.104909Z",
     "iopub.status.busy": "2025-06-27T15:45:56.103774Z",
     "iopub.status.idle": "2025-06-27T15:45:56.109964Z",
     "shell.execute_reply": "2025-06-27T15:45:56.109036Z"
    },
    "papermill": {
     "duration": 0.021678,
     "end_time": "2025-06-27T15:45:56.111777",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.090099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda73f",
   "metadata": {
    "papermill": {
     "duration": 0.012475,
     "end_time": "2025-06-27T15:45:56.136790",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.124315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102ead53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.163408Z",
     "iopub.status.busy": "2025-06-27T15:45:56.163069Z",
     "iopub.status.idle": "2025-06-27T15:45:56.169274Z",
     "shell.execute_reply": "2025-06-27T15:45:56.168460Z"
    },
    "papermill": {
     "duration": 0.021706,
     "end_time": "2025-06-27T15:45:56.171302",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.149596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91183955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.198455Z",
     "iopub.status.busy": "2025-06-27T15:45:56.197916Z",
     "iopub.status.idle": "2025-06-27T15:45:56.213667Z",
     "shell.execute_reply": "2025-06-27T15:45:56.212747Z"
    },
    "papermill": {
     "duration": 0.031704,
     "end_time": "2025-06-27T15:45:56.215510",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.183806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples...\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9b747",
   "metadata": {
    "papermill": {
     "duration": 0.012305,
     "end_time": "2025-06-27T15:45:56.240745",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.228440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f4c4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.267529Z",
     "iopub.status.busy": "2025-06-27T15:45:56.266672Z",
     "iopub.status.idle": "2025-06-27T15:45:56.273820Z",
     "shell.execute_reply": "2025-06-27T15:45:56.272956Z"
    },
    "papermill": {
     "duration": 0.022406,
     "end_time": "2025-06-27T15:45:56.275498",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.253092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c642c",
   "metadata": {
    "papermill": {
     "duration": 0.012759,
     "end_time": "2025-06-27T15:45:56.300681",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.287922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6d968ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.328571Z",
     "iopub.status.busy": "2025-06-27T15:45:56.327659Z",
     "iopub.status.idle": "2025-06-27T15:45:56.342189Z",
     "shell.execute_reply": "2025-06-27T15:45:56.341381Z"
    },
    "papermill": {
     "duration": 0.030574,
     "end_time": "2025-06-27T15:45:56.344120",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.313546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def least_confidence_sampling(model, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, n_samples=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    uncertainties = []\n",
    "    for data in dataloader:\n",
    "        input_ids = data['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = data['attention_mask'].to(device, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        for output in outputs:\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            uncertainty = np.absolute(1 - np.max(probs))\n",
    "            uncertainties.append(uncertainty)\n",
    "    \n",
    "    uncertainties = np.array(uncertainties)\n",
    "    sorted_unc = np.argsort(uncertainties)\n",
    "    sorted_unc = sorted_unc[::-1]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        threshold = np.percentile(uncertainties, 90)\n",
    "        items_greater_than_average = uncertainties[uncertainties >= threshold]\n",
    "        num_of_candidates = len(items_greater_than_average)\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "        \n",
    "        if num_of_candidates <= n_samples and n_samples < nearest_cp - current_train_size:\n",
    "            least_confident_indices = sorted_unc[:n_samples]\n",
    "        elif num_of_candidates > n_samples and num_of_candidates < nearest_cp - current_train_size:\n",
    "             least_confident_indices = sorted_unc[:max(n_samples, min(math.ceil(0.1*len(sorted_unc)), num_of_candidates))]\n",
    "        else:\n",
    "            least_confident_indices = sorted_unc[:nearest_cp - current_train_size]\n",
    "    \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend([remaining_indices[i] for i in least_confident_indices])\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train_fold[i] for i in temp],\n",
    "                'pornografi': [y_train_fold[i][0] for i in temp],\n",
    "                'sara': [y_train_fold[i][1] for i in temp],\n",
    "                'radikalisme': [y_train_fold[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train_fold[i][3] for i in temp],\n",
    "            })\n",
    "    \n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "        end_time = time.time() \n",
    "        duration = end_time - start_time\n",
    "    \n",
    "        sampling_dur.append(duration)\n",
    "        for i in least_confident_indices:\n",
    "            new_samples.append(remaining_indices[i])\n",
    "            \n",
    "        print(\"Nearest checkpoint:\", nearest_cp)\n",
    "        print(\"Threshold:\", threshold)\n",
    "        print(\"Samples above threshold:\", num_of_candidates)\n",
    "        print(\"Acquired samples:\", len(least_confident_indices))\n",
    "        print(f\"Sampling duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ea462",
   "metadata": {
    "papermill": {
     "duration": 0.013038,
     "end_time": "2025-06-27T15:45:56.370179",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.357141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6532fe7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:45:56.407094Z",
     "iopub.status.busy": "2025-06-27T15:45:56.406560Z",
     "iopub.status.idle": "2025-06-27T21:04:27.857663Z",
     "shell.execute_reply": "2025-06-27T21:04:27.856401Z"
    },
    "papermill": {
     "duration": 19111.47225,
     "end_time": "2025-06-27T21:04:27.859878",
     "exception": false,
     "start_time": "2025-06-27T15:45:56.387628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 388 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ec50190c124321a18f6f8ed76277d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5233, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4851, Accuracy: 0.8166, F1 Micro: 0.2367, F1 Macro: 0.1613\n",
      "Epoch 4/10, Train Loss: 0.4274, Accuracy: 0.8252, F1 Micro: 0.3327, F1 Macro: 0.227\n",
      "Epoch 5/10, Train Loss: 0.3978, Accuracy: 0.8386, F1 Micro: 0.4661, F1 Macro: 0.3619\n",
      "Epoch 6/10, Train Loss: 0.349, Accuracy: 0.8506, F1 Micro: 0.5631, F1 Macro: 0.5253\n",
      "Epoch 7/10, Train Loss: 0.3106, Accuracy: 0.862, F1 Micro: 0.652, F1 Macro: 0.6434\n",
      "Epoch 8/10, Train Loss: 0.2486, Accuracy: 0.8617, F1 Micro: 0.6641, F1 Macro: 0.6497\n",
      "Epoch 9/10, Train Loss: 0.2302, Accuracy: 0.8678, F1 Micro: 0.6874, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.8681, F1 Micro: 0.6544, F1 Macro: 0.6387\n",
      "Best result for 388 samples: F1 Micro: 0.6874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.79      0.85       369\n",
      "                sara       0.55      0.57      0.56       262\n",
      "         radikalisme       0.65      0.72      0.68       234\n",
      "pencemaran_nama_baik       0.63      0.67      0.65       478\n",
      "\n",
      "           micro avg       0.68      0.69      0.69      1343\n",
      "           macro avg       0.68      0.69      0.68      1343\n",
      "        weighted avg       0.69      0.69      0.69      1343\n",
      "         samples avg       0.37      0.39      0.37      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8855249121785164\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 31.481109857559204 seconds\n",
      "\n",
      "Fold 1 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4318, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.3333, Accuracy: 0.793, F1 Micro: 0.0265, F1 Macro: 0.0233\n",
      "Epoch 3/10, Train Loss: 0.2506, Accuracy: 0.8275, F1 Micro: 0.3285, F1 Macro: 0.2426\n",
      "Epoch 4/10, Train Loss: 0.2165, Accuracy: 0.8475, F1 Micro: 0.6484, F1 Macro: 0.6472\n",
      "Epoch 5/10, Train Loss: 0.1758, Accuracy: 0.8608, F1 Micro: 0.6759, F1 Macro: 0.6649\n",
      "Epoch 6/10, Train Loss: 0.1387, Accuracy: 0.8641, F1 Micro: 0.6559, F1 Macro: 0.6307\n",
      "Epoch 7/10, Train Loss: 0.1297, Accuracy: 0.8656, F1 Micro: 0.6861, F1 Macro: 0.6778\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.8666, F1 Micro: 0.6952, F1 Macro: 0.6909\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.8698, F1 Micro: 0.6737, F1 Macro: 0.6641\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.8691, F1 Micro: 0.6816, F1 Macro: 0.6753\n",
      "Best result for 971 samples: F1 Micro: 0.6952\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.83      0.85       369\n",
      "                sara       0.51      0.63      0.56       262\n",
      "         radikalisme       0.62      0.77      0.68       234\n",
      "pencemaran_nama_baik       0.65      0.68      0.66       478\n",
      "\n",
      "           micro avg       0.67      0.73      0.70      1343\n",
      "           macro avg       0.66      0.73      0.69      1343\n",
      "        weighted avg       0.68      0.73      0.70      1343\n",
      "         samples avg       0.39      0.41      0.39      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9324980050325393\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 31.24502658843994 seconds\n",
      "\n",
      "Fold 1 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3456, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.266, Accuracy: 0.7941, F1 Micro: 0.0365, F1 Macro: 0.0317\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.8459, F1 Micro: 0.5754, F1 Macro: 0.5479\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.8572, F1 Micro: 0.6786, F1 Macro: 0.676\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.8658, F1 Micro: 0.6935, F1 Macro: 0.6838\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.865, F1 Micro: 0.6851, F1 Macro: 0.6833\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.8681, F1 Micro: 0.6731, F1 Macro: 0.653\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.8705, F1 Micro: 0.6244, F1 Macro: 0.5988\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.8692, F1 Micro: 0.6922, F1 Macro: 0.6872\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.8758, F1 Micro: 0.6821, F1 Macro: 0.6689\n",
      "Best result for 1496 samples: F1 Micro: 0.6935\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.82      0.86       369\n",
      "                sara       0.55      0.57      0.56       262\n",
      "         radikalisme       0.67      0.61      0.64       234\n",
      "pencemaran_nama_baik       0.59      0.79      0.67       478\n",
      "\n",
      "           micro avg       0.67      0.72      0.69      1343\n",
      "           macro avg       0.68      0.70      0.68      1343\n",
      "        weighted avg       0.68      0.72      0.70      1343\n",
      "         samples avg       0.40      0.41      0.39      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8010759428143507\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 27.8158597946167 seconds\n",
      "\n",
      "Fold 1 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3145, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.8184, F1 Micro: 0.3165, F1 Macro: 0.2231\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.86, F1 Micro: 0.6334, F1 Macro: 0.6148\n",
      "Epoch 4/10, Train Loss: 0.1457, Accuracy: 0.8653, F1 Micro: 0.6085, F1 Macro: 0.5623\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.8666, F1 Micro: 0.6895, F1 Macro: 0.6852\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.857, F1 Micro: 0.6977, F1 Macro: 0.6978\n",
      "Epoch 7/10, Train Loss: 0.064, Accuracy: 0.8702, F1 Micro: 0.7023, F1 Macro: 0.6936\n",
      "Epoch 8/10, Train Loss: 0.054, Accuracy: 0.8694, F1 Micro: 0.6986, F1 Macro: 0.6965\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.8722, F1 Micro: 0.7072, F1 Macro: 0.7009\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.8814, F1 Micro: 0.7075, F1 Macro: 0.7018\n",
      "Best result for 1969 samples: F1 Micro: 0.7075\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.84      0.88       369\n",
      "                sara       0.57      0.55      0.56       262\n",
      "         radikalisme       0.68      0.75      0.71       234\n",
      "pencemaran_nama_baik       0.71      0.60      0.65       478\n",
      "\n",
      "           micro avg       0.73      0.68      0.71      1343\n",
      "           macro avg       0.72      0.69      0.70      1343\n",
      "        weighted avg       0.74      0.68      0.71      1343\n",
      "         samples avg       0.40      0.40      0.39      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8970272853970531\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 25.606788635253906 seconds\n",
      "\n",
      "Fold 1 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3293, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.8159, F1 Micro: 0.2199, F1 Macro: 0.1662\n",
      "Epoch 3/10, Train Loss: 0.2003, Accuracy: 0.8617, F1 Micro: 0.5953, F1 Macro: 0.5753\n",
      "Epoch 4/10, Train Loss: 0.145, Accuracy: 0.875, F1 Micro: 0.694, F1 Macro: 0.6697\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.8702, F1 Micro: 0.704, F1 Macro: 0.6987\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.8698, F1 Micro: 0.7162, F1 Macro: 0.7144\n",
      "Epoch 7/10, Train Loss: 0.0665, Accuracy: 0.8836, F1 Micro: 0.7288, F1 Macro: 0.7224\n",
      "Epoch 8/10, Train Loss: 0.0465, Accuracy: 0.8769, F1 Micro: 0.7301, F1 Macro: 0.7296\n",
      "Epoch 9/10, Train Loss: 0.0426, Accuracy: 0.878, F1 Micro: 0.7274, F1 Macro: 0.7239\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.8819, F1 Micro: 0.7306, F1 Macro: 0.7294\n",
      "Best result for 2394 samples: F1 Micro: 0.7306\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       369\n",
      "                sara       0.59      0.66      0.63       262\n",
      "         radikalisme       0.69      0.76      0.72       234\n",
      "pencemaran_nama_baik       0.63      0.74      0.68       478\n",
      "\n",
      "           micro avg       0.70      0.76      0.73      1343\n",
      "           macro avg       0.71      0.76      0.73      1343\n",
      "        weighted avg       0.71      0.76      0.73      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.42927443981170726\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 23.04999804496765 seconds\n",
      "\n",
      "Fold 1 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3392, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2506, Accuracy: 0.8297, F1 Micro: 0.3677, F1 Macro: 0.2786\n",
      "Epoch 3/10, Train Loss: 0.198, Accuracy: 0.8773, F1 Micro: 0.694, F1 Macro: 0.6904\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.8805, F1 Micro: 0.7106, F1 Macro: 0.7014\n",
      "Epoch 5/10, Train Loss: 0.111, Accuracy: 0.8759, F1 Micro: 0.7243, F1 Macro: 0.7227\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.8845, F1 Micro: 0.7288, F1 Macro: 0.7251\n",
      "Epoch 7/10, Train Loss: 0.0628, Accuracy: 0.8773, F1 Micro: 0.7329, F1 Macro: 0.7332\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.89, F1 Micro: 0.7373, F1 Macro: 0.7349\n",
      "Epoch 9/10, Train Loss: 0.032, Accuracy: 0.8878, F1 Micro: 0.7421, F1 Macro: 0.7392\n",
      "Epoch 10/10, Train Loss: 0.0307, Accuracy: 0.8889, F1 Micro: 0.7456, F1 Macro: 0.7427\n",
      "Best result for 2777 samples: F1 Micro: 0.7456\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       369\n",
      "                sara       0.63      0.60      0.61       262\n",
      "         radikalisme       0.71      0.81      0.76       234\n",
      "pencemaran_nama_baik       0.63      0.78      0.70       478\n",
      "\n",
      "           micro avg       0.72      0.78      0.75      1343\n",
      "           macro avg       0.72      0.77      0.74      1343\n",
      "        weighted avg       0.73      0.78      0.75      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.15781325101852417\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 20.99130129814148 seconds\n",
      "\n",
      "Fold 1 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3476, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2701, Accuracy: 0.8567, F1 Micro: 0.6234, F1 Macro: 0.628\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.8781, F1 Micro: 0.7048, F1 Macro: 0.7032\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.8842, F1 Micro: 0.7184, F1 Macro: 0.7099\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.8853, F1 Micro: 0.7455, F1 Macro: 0.7431\n",
      "Epoch 6/10, Train Loss: 0.0875, Accuracy: 0.888, F1 Micro: 0.7449, F1 Macro: 0.7433\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.8859, F1 Micro: 0.7448, F1 Macro: 0.7436\n",
      "Epoch 8/10, Train Loss: 0.0533, Accuracy: 0.8931, F1 Micro: 0.744, F1 Macro: 0.738\n",
      "Epoch 9/10, Train Loss: 0.0398, Accuracy: 0.8919, F1 Micro: 0.7496, F1 Macro: 0.7483\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.8922, F1 Micro: 0.7509, F1 Macro: 0.7494\n",
      "Best result for 3122 samples: F1 Micro: 0.7509\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       369\n",
      "                sara       0.59      0.66      0.62       262\n",
      "         radikalisme       0.71      0.82      0.76       234\n",
      "pencemaran_nama_baik       0.68      0.74      0.71       478\n",
      "\n",
      "           micro avg       0.73      0.77      0.75      1343\n",
      "           macro avg       0.73      0.77      0.75      1343\n",
      "        weighted avg       0.74      0.77      0.75      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.07768765091896057\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.009710788726807 seconds\n",
      "\n",
      "Fold 1 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.358, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2798, Accuracy: 0.8581, F1 Micro: 0.5899, F1 Macro: 0.5457\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.8822, F1 Micro: 0.7075, F1 Macro: 0.7075\n",
      "Epoch 4/10, Train Loss: 0.1673, Accuracy: 0.8881, F1 Micro: 0.7334, F1 Macro: 0.7271\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.8891, F1 Micro: 0.7459, F1 Macro: 0.7444\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.8755, F1 Micro: 0.7364, F1 Macro: 0.7414\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.8944, F1 Micro: 0.7462, F1 Macro: 0.7446\n",
      "Epoch 8/10, Train Loss: 0.0464, Accuracy: 0.8955, F1 Micro: 0.7523, F1 Macro: 0.7485\n",
      "Epoch 9/10, Train Loss: 0.0341, Accuracy: 0.8897, F1 Micro: 0.7502, F1 Macro: 0.7488\n",
      "Epoch 10/10, Train Loss: 0.0273, Accuracy: 0.8916, F1 Micro: 0.7445, F1 Macro: 0.7374\n",
      "Best result for 3432 samples: F1 Micro: 0.7523\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.91       369\n",
      "                sara       0.66      0.59      0.62       262\n",
      "         radikalisme       0.75      0.76      0.76       234\n",
      "pencemaran_nama_baik       0.66      0.76      0.71       478\n",
      "\n",
      "           micro avg       0.75      0.76      0.75      1343\n",
      "           macro avg       0.76      0.75      0.75      1343\n",
      "        weighted avg       0.76      0.76      0.75      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.06288054585456848\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 16.959205627441406 seconds\n",
      "\n",
      "Fold 1 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3783, Accuracy: 0.8002, F1 Micro: 0.0974, F1 Macro: 0.0895\n",
      "Epoch 2/10, Train Loss: 0.2682, Accuracy: 0.8714, F1 Micro: 0.6227, F1 Macro: 0.6091\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.8797, F1 Micro: 0.7399, F1 Macro: 0.7417\n",
      "Epoch 4/10, Train Loss: 0.1706, Accuracy: 0.8927, F1 Micro: 0.7506, F1 Macro: 0.748\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.8956, F1 Micro: 0.747, F1 Macro: 0.7373\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9003, F1 Micro: 0.7529, F1 Macro: 0.7459\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.8947, F1 Micro: 0.7577, F1 Macro: 0.754\n",
      "Epoch 8/10, Train Loss: 0.0454, Accuracy: 0.8967, F1 Micro: 0.756, F1 Macro: 0.7562\n",
      "Epoch 9/10, Train Loss: 0.036, Accuracy: 0.8959, F1 Micro: 0.7611, F1 Macro: 0.7601\n",
      "Epoch 10/10, Train Loss: 0.0269, Accuracy: 0.8964, F1 Micro: 0.759, F1 Macro: 0.7551\n",
      "Best result for 3711 samples: F1 Micro: 0.7611\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       369\n",
      "                sara       0.62      0.68      0.65       262\n",
      "         radikalisme       0.69      0.86      0.76       234\n",
      "pencemaran_nama_baik       0.68      0.76      0.71       478\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1343\n",
      "           macro avg       0.74      0.79      0.76      1343\n",
      "        weighted avg       0.75      0.79      0.76      1343\n",
      "         samples avg       0.44      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.015232050418853761\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 15.700502634048462 seconds\n",
      "\n",
      "Fold 1 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3717, Accuracy: 0.8255, F1 Micro: 0.2988, F1 Macro: 0.245\n",
      "Epoch 2/10, Train Loss: 0.2758, Accuracy: 0.8719, F1 Micro: 0.6717, F1 Macro: 0.6486\n",
      "Epoch 3/10, Train Loss: 0.2201, Accuracy: 0.8878, F1 Micro: 0.7438, F1 Macro: 0.7401\n",
      "Epoch 4/10, Train Loss: 0.1704, Accuracy: 0.892, F1 Micro: 0.7542, F1 Macro: 0.754\n",
      "Epoch 5/10, Train Loss: 0.1303, Accuracy: 0.8944, F1 Micro: 0.7432, F1 Macro: 0.7398\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.8898, F1 Micro: 0.7566, F1 Macro: 0.7589\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.8969, F1 Micro: 0.7584, F1 Macro: 0.7563\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.8966, F1 Micro: 0.7502, F1 Macro: 0.7472\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.8939, F1 Micro: 0.7584, F1 Macro: 0.7577\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.8948, F1 Micro: 0.7568, F1 Macro: 0.7549\n",
      "Best result for 3886 samples: F1 Micro: 0.7584\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.63      0.68      0.65       262\n",
      "         radikalisme       0.70      0.83      0.76       234\n",
      "pencemaran_nama_baik       0.66      0.77      0.71       478\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1343\n",
      "           macro avg       0.73      0.79      0.76      1343\n",
      "        weighted avg       0.74      0.79      0.76      1343\n",
      "         samples avg       0.44      0.45      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00823490619659424\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.57187795639038 seconds\n",
      "\n",
      "Fold 1 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3807, Accuracy: 0.8258, F1 Micro: 0.3027, F1 Macro: 0.2327\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.8809, F1 Micro: 0.6707, F1 Macro: 0.6463\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.8938, F1 Micro: 0.7428, F1 Macro: 0.737\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.8955, F1 Micro: 0.7655, F1 Macro: 0.7662\n",
      "Epoch 5/10, Train Loss: 0.1355, Accuracy: 0.897, F1 Micro: 0.7603, F1 Macro: 0.7612\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.8936, F1 Micro: 0.7555, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.0667, Accuracy: 0.8986, F1 Micro: 0.7572, F1 Macro: 0.7553\n",
      "Epoch 8/10, Train Loss: 0.0497, Accuracy: 0.8956, F1 Micro: 0.7623, F1 Macro: 0.7628\n",
      "Epoch 9/10, Train Loss: 0.0361, Accuracy: 0.898, F1 Micro: 0.7595, F1 Macro: 0.7592\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9003, F1 Micro: 0.766, F1 Macro: 0.7634\n",
      "Best result for 4120 samples: F1 Micro: 0.766\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.92       369\n",
      "                sara       0.67      0.65      0.66       262\n",
      "         radikalisme       0.73      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.69      0.74      0.71       478\n",
      "\n",
      "           micro avg       0.75      0.78      0.77      1343\n",
      "           macro avg       0.75      0.77      0.76      1343\n",
      "        weighted avg       0.76      0.78      0.77      1343\n",
      "         samples avg       0.45      0.44      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0056124448776245115\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.011930227279663 seconds\n",
      "\n",
      "Fold 1 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8575, F1 Micro: 0.5774, F1 Macro: 0.5261\n",
      "Epoch 2/10, Train Loss: 0.2748, Accuracy: 0.8928, F1 Micro: 0.7249, F1 Macro: 0.7146\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.8947, F1 Micro: 0.7342, F1 Macro: 0.7182\n",
      "Epoch 4/10, Train Loss: 0.1684, Accuracy: 0.8991, F1 Micro: 0.7549, F1 Macro: 0.753\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.8963, F1 Micro: 0.7629, F1 Macro: 0.7623\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9041, F1 Micro: 0.7697, F1 Macro: 0.7667\n",
      "Epoch 7/10, Train Loss: 0.0653, Accuracy: 0.8997, F1 Micro: 0.7672, F1 Macro: 0.766\n",
      "Epoch 8/10, Train Loss: 0.0512, Accuracy: 0.9006, F1 Micro: 0.7571, F1 Macro: 0.7512\n",
      "Epoch 9/10, Train Loss: 0.0424, Accuracy: 0.8969, F1 Micro: 0.7616, F1 Macro: 0.762\n",
      "Epoch 10/10, Train Loss: 0.0306, Accuracy: 0.9008, F1 Micro: 0.7692, F1 Macro: 0.7694\n",
      "Best result for 4330 samples: F1 Micro: 0.7697\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       369\n",
      "                sara       0.71      0.63      0.66       262\n",
      "         radikalisme       0.73      0.84      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.70      0.71       478\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1343\n",
      "           macro avg       0.77      0.77      0.77      1343\n",
      "        weighted avg       0.78      0.76      0.77      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.022747659683227537\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.787606239318848 seconds\n",
      "\n",
      "Fold 1 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3902, Accuracy: 0.8698, F1 Micro: 0.6765, F1 Macro: 0.6694\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.8903, F1 Micro: 0.7386, F1 Macro: 0.7258\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.8969, F1 Micro: 0.7498, F1 Macro: 0.7363\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.9042, F1 Micro: 0.7693, F1 Macro: 0.7644\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.9009, F1 Micro: 0.765, F1 Macro: 0.7619\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9041, F1 Micro: 0.7656, F1 Macro: 0.7621\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9025, F1 Micro: 0.7652, F1 Macro: 0.7636\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.8973, F1 Micro: 0.7691, F1 Macro: 0.7682\n",
      "Epoch 9/10, Train Loss: 0.0414, Accuracy: 0.9011, F1 Micro: 0.7625, F1 Macro: 0.7537\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9002, F1 Micro: 0.7634, F1 Macro: 0.7612\n",
      "Best result for 4530 samples: F1 Micro: 0.7693\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.66      0.66      0.66       262\n",
      "         radikalisme       0.74      0.78      0.76       234\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       478\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1343\n",
      "           macro avg       0.77      0.76      0.76      1343\n",
      "        weighted avg       0.78      0.76      0.77      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0805226445198059\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 10.660472869873047 seconds\n",
      "\n",
      "Fold 1 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8614, F1 Micro: 0.6453, F1 Macro: 0.6469\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.8923, F1 Micro: 0.7209, F1 Macro: 0.6936\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.8975, F1 Micro: 0.7659, F1 Macro: 0.7647\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9066, F1 Micro: 0.7684, F1 Macro: 0.7642\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9027, F1 Micro: 0.7705, F1 Macro: 0.7702\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9023, F1 Micro: 0.7735, F1 Macro: 0.7704\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.8967, F1 Micro: 0.7706, F1 Macro: 0.7723\n",
      "Epoch 8/10, Train Loss: 0.0533, Accuracy: 0.9038, F1 Micro: 0.7691, F1 Macro: 0.7692\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.9047, F1 Micro: 0.7757, F1 Macro: 0.7743\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.8989, F1 Micro: 0.7708, F1 Macro: 0.7726\n",
      "Best result for 4663 samples: F1 Micro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       369\n",
      "                sara       0.67      0.68      0.67       262\n",
      "         radikalisme       0.77      0.80      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       478\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1343\n",
      "           macro avg       0.77      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.79      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.006487786769866943\n",
      "Samples above threshold: 191\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.961878061294556 seconds\n",
      "\n",
      "Fold 1 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4037, Accuracy: 0.8653, F1 Micro: 0.6692, F1 Macro: 0.6672\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.8928, F1 Micro: 0.7236, F1 Macro: 0.7125\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.8988, F1 Micro: 0.7538, F1 Macro: 0.74\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9013, F1 Micro: 0.7626, F1 Macro: 0.7584\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9061, F1 Micro: 0.7661, F1 Macro: 0.7564\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.908, F1 Micro: 0.7721, F1 Macro: 0.7651\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9058, F1 Micro: 0.7687, F1 Macro: 0.7615\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9038, F1 Micro: 0.7708, F1 Macro: 0.7703\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9069, F1 Micro: 0.773, F1 Macro: 0.7676\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.8963, F1 Micro: 0.7682, F1 Macro: 0.7712\n",
      "Best result for 4863 samples: F1 Micro: 0.773\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       369\n",
      "                sara       0.71      0.58      0.64       262\n",
      "         radikalisme       0.76      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       478\n",
      "\n",
      "           micro avg       0.79      0.76      0.77      1343\n",
      "           macro avg       0.79      0.75      0.77      1343\n",
      "        weighted avg       0.79      0.76      0.77      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.008245038986206057\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.692120790481567 seconds\n",
      "\n",
      "Fold 1 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3879, Accuracy: 0.8739, F1 Micro: 0.6664, F1 Macro: 0.66\n",
      "Epoch 2/10, Train Loss: 0.2574, Accuracy: 0.8972, F1 Micro: 0.7418, F1 Macro: 0.7301\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9008, F1 Micro: 0.7666, F1 Macro: 0.764\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9008, F1 Micro: 0.7665, F1 Macro: 0.7613\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9022, F1 Micro: 0.7774, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.8963, F1 Micro: 0.7665, F1 Macro: 0.7667\n",
      "Epoch 7/10, Train Loss: 0.0721, Accuracy: 0.902, F1 Micro: 0.7736, F1 Macro: 0.7732\n",
      "Epoch 8/10, Train Loss: 0.0487, Accuracy: 0.9028, F1 Micro: 0.7763, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9013, F1 Micro: 0.7772, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0311, Accuracy: 0.8984, F1 Micro: 0.7697, F1 Macro: 0.7671\n",
      "Best result for 5063 samples: F1 Micro: 0.7774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       369\n",
      "                sara       0.67      0.68      0.67       262\n",
      "         radikalisme       0.73      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       478\n",
      "\n",
      "           micro avg       0.74      0.81      0.78      1343\n",
      "           macro avg       0.75      0.80      0.77      1343\n",
      "        weighted avg       0.75      0.81      0.78      1343\n",
      "         samples avg       0.45      0.46      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.01581689119338991\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.6899309158325195 seconds\n",
      "\n",
      "Fold 1 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8753, F1 Micro: 0.6885, F1 Macro: 0.6847\n",
      "Epoch 2/10, Train Loss: 0.2576, Accuracy: 0.8967, F1 Micro: 0.7445, F1 Macro: 0.7339\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.9033, F1 Micro: 0.7545, F1 Macro: 0.7451\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9038, F1 Micro: 0.7695, F1 Macro: 0.769\n",
      "Epoch 5/10, Train Loss: 0.1294, Accuracy: 0.8997, F1 Micro: 0.7648, F1 Macro: 0.7628\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.903, F1 Micro: 0.7714, F1 Macro: 0.7693\n",
      "Epoch 7/10, Train Loss: 0.0672, Accuracy: 0.9047, F1 Micro: 0.7677, F1 Macro: 0.7648\n",
      "Epoch 8/10, Train Loss: 0.054, Accuracy: 0.9047, F1 Micro: 0.7643, F1 Macro: 0.7566\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9025, F1 Micro: 0.7679, F1 Macro: 0.766\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9023, F1 Micro: 0.7575, F1 Macro: 0.7481\n",
      "Best result for 5263 samples: F1 Micro: 0.7714\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.66      0.66      0.66       262\n",
      "         radikalisme       0.74      0.82      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.73      0.72       478\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1343\n",
      "           macro avg       0.76      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.77      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.006924450397491457\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.441403388977051 seconds\n",
      "\n",
      "Fold 1 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3981, Accuracy: 0.8786, F1 Micro: 0.6845, F1 Macro: 0.6733\n",
      "Epoch 2/10, Train Loss: 0.2554, Accuracy: 0.8963, F1 Micro: 0.7444, F1 Macro: 0.7403\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9005, F1 Micro: 0.7631, F1 Macro: 0.7616\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9009, F1 Micro: 0.7631, F1 Macro: 0.7615\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.8995, F1 Micro: 0.7698, F1 Macro: 0.7683\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.903, F1 Micro: 0.7762, F1 Macro: 0.7724\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.9044, F1 Micro: 0.7752, F1 Macro: 0.7718\n",
      "Epoch 8/10, Train Loss: 0.0542, Accuracy: 0.9045, F1 Micro: 0.7638, F1 Macro: 0.758\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9062, F1 Micro: 0.7678, F1 Macro: 0.7588\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9061, F1 Micro: 0.7707, F1 Macro: 0.7676\n",
      "Best result for 5441 samples: F1 Micro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       369\n",
      "                sara       0.68      0.66      0.67       262\n",
      "         radikalisme       0.74      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1343\n",
      "           macro avg       0.76      0.79      0.77      1343\n",
      "        weighted avg       0.76      0.80      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.005385363101959228\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.485948801040649 seconds\n",
      "\n",
      "Fold 1 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3876, Accuracy: 0.8808, F1 Micro: 0.7082, F1 Macro: 0.6998\n",
      "Epoch 2/10, Train Loss: 0.2659, Accuracy: 0.8988, F1 Micro: 0.751, F1 Macro: 0.7462\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9022, F1 Micro: 0.7549, F1 Macro: 0.7506\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9066, F1 Micro: 0.7638, F1 Macro: 0.7601\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9016, F1 Micro: 0.7702, F1 Macro: 0.7658\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.8984, F1 Micro: 0.769, F1 Macro: 0.7674\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9025, F1 Micro: 0.7684, F1 Macro: 0.7652\n",
      "Epoch 8/10, Train Loss: 0.0548, Accuracy: 0.9055, F1 Micro: 0.7699, F1 Macro: 0.7666\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9044, F1 Micro: 0.7664, F1 Macro: 0.7607\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.902, F1 Micro: 0.7679, F1 Macro: 0.7642\n",
      "Best result for 5641 samples: F1 Micro: 0.7702\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       369\n",
      "                sara       0.64      0.69      0.66       262\n",
      "         radikalisme       0.73      0.79      0.76       234\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1343\n",
      "           macro avg       0.75      0.78      0.77      1343\n",
      "        weighted avg       0.76      0.79      0.77      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.00714656114578247\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.283633232116699 seconds\n",
      "\n",
      "Fold 1 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.39, Accuracy: 0.8872, F1 Micro: 0.7197, F1 Macro: 0.7087\n",
      "Epoch 2/10, Train Loss: 0.2464, Accuracy: 0.9011, F1 Micro: 0.7572, F1 Macro: 0.7472\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.907, F1 Micro: 0.7662, F1 Macro: 0.7636\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.9064, F1 Micro: 0.7808, F1 Macro: 0.7806\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9062, F1 Micro: 0.7799, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9048, F1 Micro: 0.766, F1 Macro: 0.7622\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9067, F1 Micro: 0.7777, F1 Macro: 0.7729\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.903, F1 Micro: 0.7726, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9005, F1 Micro: 0.7755, F1 Macro: 0.7786\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9036, F1 Micro: 0.7673, F1 Macro: 0.7642\n",
      "Best result for 5841 samples: F1 Micro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       369\n",
      "                sara       0.68      0.74      0.71       262\n",
      "         radikalisme       0.75      0.81      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       478\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1343\n",
      "           macro avg       0.77      0.79      0.78      1343\n",
      "        weighted avg       0.77      0.79      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.006149601936340333\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.1586363315582275 seconds\n",
      "\n",
      "Fold 1 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.8845, F1 Micro: 0.7135, F1 Macro: 0.709\n",
      "Epoch 2/10, Train Loss: 0.2415, Accuracy: 0.8997, F1 Micro: 0.7527, F1 Macro: 0.7379\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.9056, F1 Micro: 0.767, F1 Macro: 0.7571\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.9039, F1 Micro: 0.7671, F1 Macro: 0.76\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.9022, F1 Micro: 0.7681, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9055, F1 Micro: 0.7775, F1 Macro: 0.7747\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.8992, F1 Micro: 0.7757, F1 Macro: 0.779\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9058, F1 Micro: 0.7761, F1 Macro: 0.7731\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9036, F1 Micro: 0.7667, F1 Macro: 0.7623\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.9047, F1 Micro: 0.7757, F1 Macro: 0.7728\n",
      "Best result for 6041 samples: F1 Micro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       369\n",
      "                sara       0.68      0.68      0.68       262\n",
      "         radikalisme       0.79      0.74      0.77       234\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       478\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1343\n",
      "           macro avg       0.78      0.77      0.77      1343\n",
      "        weighted avg       0.78      0.79      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.0020189523696899415\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.0267765522003174 seconds\n",
      "\n",
      "Fold 1 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3675, Accuracy: 0.89, F1 Micro: 0.7186, F1 Macro: 0.7041\n",
      "Epoch 2/10, Train Loss: 0.2372, Accuracy: 0.9005, F1 Micro: 0.7592, F1 Macro: 0.7548\n",
      "Epoch 3/10, Train Loss: 0.1933, Accuracy: 0.9045, F1 Micro: 0.7631, F1 Macro: 0.7534\n",
      "Epoch 4/10, Train Loss: 0.1531, Accuracy: 0.9069, F1 Micro: 0.7603, F1 Macro: 0.7471\n",
      "Epoch 5/10, Train Loss: 0.1145, Accuracy: 0.9087, F1 Micro: 0.7801, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.081, Accuracy: 0.9066, F1 Micro: 0.7782, F1 Macro: 0.775\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9028, F1 Micro: 0.7656, F1 Macro: 0.7579\n",
      "Epoch 8/10, Train Loss: 0.0477, Accuracy: 0.9028, F1 Micro: 0.7732, F1 Macro: 0.7712\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.905, F1 Micro: 0.7738, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.9047, F1 Micro: 0.7666, F1 Macro: 0.7623\n",
      "Best result for 6218 samples: F1 Micro: 0.7801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       369\n",
      "                sara       0.72      0.64      0.68       262\n",
      "         radikalisme       0.77      0.81      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       478\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1343\n",
      "           macro avg       0.78      0.77      0.78      1343\n",
      "        weighted avg       0.79      0.77      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 3804.34 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4954, Accuracy: 0.7953, F1 Micro: 0.099, F1 Macro: 0.08\n",
      "Epoch 3/10, Train Loss: 0.448, Accuracy: 0.8238, F1 Micro: 0.3229, F1 Macro: 0.2108\n",
      "Epoch 4/10, Train Loss: 0.3886, Accuracy: 0.8323, F1 Micro: 0.3975, F1 Macro: 0.3047\n",
      "Epoch 5/10, Train Loss: 0.3358, Accuracy: 0.8384, F1 Micro: 0.4615, F1 Macro: 0.3692\n",
      "Epoch 6/10, Train Loss: 0.2851, Accuracy: 0.8464, F1 Micro: 0.5131, F1 Macro: 0.4513\n",
      "Epoch 7/10, Train Loss: 0.2509, Accuracy: 0.8661, F1 Micro: 0.6355, F1 Macro: 0.5946\n",
      "Epoch 8/10, Train Loss: 0.2376, Accuracy: 0.872, F1 Micro: 0.6667, F1 Macro: 0.6441\n",
      "Epoch 9/10, Train Loss: 0.188, Accuracy: 0.8628, F1 Micro: 0.6132, F1 Macro: 0.5838\n",
      "Epoch 10/10, Train Loss: 0.1613, Accuracy: 0.8709, F1 Micro: 0.6541, F1 Macro: 0.628\n",
      "Best result for 388 samples: F1 Micro: 0.6667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.80      0.86       378\n",
      "                sara       0.62      0.35      0.45       253\n",
      "         radikalisme       0.67      0.62      0.65       234\n",
      "pencemaran_nama_baik       0.72      0.54      0.62       517\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1382\n",
      "           macro avg       0.74      0.58      0.64      1382\n",
      "        weighted avg       0.75      0.59      0.66      1382\n",
      "         samples avg       0.38      0.35      0.35      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8907355055212974\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 35.10951232910156 seconds\n",
      "\n",
      "Fold 2 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2967, Accuracy: 0.8153, F1 Micro: 0.2538, F1 Macro: 0.1736\n",
      "Epoch 3/10, Train Loss: 0.2236, Accuracy: 0.8333, F1 Micro: 0.4352, F1 Macro: 0.2894\n",
      "Epoch 4/10, Train Loss: 0.2002, Accuracy: 0.8512, F1 Micro: 0.5673, F1 Macro: 0.5179\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.8705, F1 Micro: 0.6685, F1 Macro: 0.6323\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.8744, F1 Micro: 0.6768, F1 Macro: 0.6487\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.8723, F1 Micro: 0.6939, F1 Macro: 0.6839\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.8714, F1 Micro: 0.7049, F1 Macro: 0.6857\n",
      "Epoch 9/10, Train Loss: 0.0661, Accuracy: 0.8752, F1 Micro: 0.7006, F1 Macro: 0.6828\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.873, F1 Micro: 0.6741, F1 Macro: 0.6491\n",
      "Best result for 971 samples: F1 Micro: 0.7049\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.84      0.89      0.87       378\n",
      "                sara       0.55      0.51      0.53       253\n",
      "         radikalisme       0.65      0.67      0.66       234\n",
      "pencemaran_nama_baik       0.68      0.69      0.69       517\n",
      "\n",
      "           micro avg       0.70      0.71      0.70      1382\n",
      "           macro avg       0.68      0.69      0.69      1382\n",
      "        weighted avg       0.69      0.71      0.70      1382\n",
      "         samples avg       0.42      0.42      0.41      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9249873310327531\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 31.147512197494507 seconds\n",
      "\n",
      "Fold 2 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3173, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2208, Accuracy: 0.8123, F1 Micro: 0.2326, F1 Macro: 0.1629\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.8336, F1 Micro: 0.4221, F1 Macro: 0.2893\n",
      "Epoch 4/10, Train Loss: 0.1515, Accuracy: 0.8627, F1 Micro: 0.6457, F1 Macro: 0.5992\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.8609, F1 Micro: 0.6096, F1 Macro: 0.5966\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.873, F1 Micro: 0.7128, F1 Macro: 0.7103\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.8723, F1 Micro: 0.7191, F1 Macro: 0.7074\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.877, F1 Micro: 0.7186, F1 Macro: 0.7113\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.873, F1 Micro: 0.7124, F1 Macro: 0.6991\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.8753, F1 Micro: 0.7206, F1 Macro: 0.7095\n",
      "Best result for 1496 samples: F1 Micro: 0.7206\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       378\n",
      "                sara       0.55      0.64      0.59       253\n",
      "         radikalisme       0.63      0.71      0.67       234\n",
      "pencemaran_nama_baik       0.69      0.71      0.70       517\n",
      "\n",
      "           micro avg       0.70      0.74      0.72      1382\n",
      "           macro avg       0.69      0.74      0.71      1382\n",
      "        weighted avg       0.70      0.74      0.72      1382\n",
      "         samples avg       0.44      0.43      0.43      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9157791808247567\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 27.91181993484497 seconds\n",
      "\n",
      "Fold 2 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2976, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2185, Accuracy: 0.8186, F1 Micro: 0.2838, F1 Macro: 0.1888\n",
      "Epoch 3/10, Train Loss: 0.1788, Accuracy: 0.8634, F1 Micro: 0.6736, F1 Macro: 0.635\n",
      "Epoch 4/10, Train Loss: 0.1425, Accuracy: 0.867, F1 Micro: 0.6761, F1 Macro: 0.6684\n",
      "Epoch 5/10, Train Loss: 0.1081, Accuracy: 0.8748, F1 Micro: 0.7101, F1 Macro: 0.7063\n",
      "Epoch 6/10, Train Loss: 0.0802, Accuracy: 0.8781, F1 Micro: 0.725, F1 Macro: 0.7077\n",
      "Epoch 7/10, Train Loss: 0.0641, Accuracy: 0.8814, F1 Micro: 0.7338, F1 Macro: 0.7241\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.8789, F1 Micro: 0.7299, F1 Macro: 0.724\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.8798, F1 Micro: 0.7398, F1 Macro: 0.7325\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.8778, F1 Micro: 0.7142, F1 Macro: 0.7104\n",
      "Best result for 1969 samples: F1 Micro: 0.7398\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.85      0.89       378\n",
      "                sara       0.54      0.74      0.62       253\n",
      "         radikalisme       0.62      0.76      0.69       234\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       517\n",
      "\n",
      "           micro avg       0.69      0.79      0.74      1382\n",
      "           macro avg       0.70      0.78      0.73      1382\n",
      "        weighted avg       0.72      0.79      0.75      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.7218879878520966\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 25.833433628082275 seconds\n",
      "\n",
      "Fold 2 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2977, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2288, Accuracy: 0.8283, F1 Micro: 0.3584, F1 Macro: 0.2542\n",
      "Epoch 3/10, Train Loss: 0.1714, Accuracy: 0.8716, F1 Micro: 0.7081, F1 Macro: 0.6996\n",
      "Epoch 4/10, Train Loss: 0.1449, Accuracy: 0.8727, F1 Micro: 0.7367, F1 Macro: 0.7299\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.8823, F1 Micro: 0.7401, F1 Macro: 0.7327\n",
      "Epoch 6/10, Train Loss: 0.081, Accuracy: 0.8808, F1 Micro: 0.7474, F1 Macro: 0.7402\n",
      "Epoch 7/10, Train Loss: 0.0627, Accuracy: 0.8841, F1 Micro: 0.7352, F1 Macro: 0.7291\n",
      "Epoch 8/10, Train Loss: 0.0437, Accuracy: 0.8867, F1 Micro: 0.7533, F1 Macro: 0.7421\n",
      "Epoch 9/10, Train Loss: 0.0339, Accuracy: 0.8898, F1 Micro: 0.7463, F1 Macro: 0.7345\n",
      "Epoch 10/10, Train Loss: 0.0231, Accuracy: 0.8878, F1 Micro: 0.7526, F1 Macro: 0.7502\n",
      "Best result for 2394 samples: F1 Micro: 0.7533\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       378\n",
      "                sara       0.61      0.59      0.60       253\n",
      "         radikalisme       0.70      0.77      0.73       234\n",
      "pencemaran_nama_baik       0.64      0.85      0.73       517\n",
      "\n",
      "           micro avg       0.71      0.80      0.75      1382\n",
      "           macro avg       0.72      0.78      0.74      1382\n",
      "        weighted avg       0.72      0.80      0.75      1382\n",
      "         samples avg       0.48      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.3597006440162663\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 23.11076831817627 seconds\n",
      "\n",
      "Fold 2 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3298, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.8564, F1 Micro: 0.5659, F1 Macro: 0.5023\n",
      "Epoch 3/10, Train Loss: 0.1953, Accuracy: 0.8816, F1 Micro: 0.6948, F1 Macro: 0.6834\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.8758, F1 Micro: 0.6481, F1 Macro: 0.6039\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.8936, F1 Micro: 0.7613, F1 Macro: 0.7607\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.8928, F1 Micro: 0.7312, F1 Macro: 0.7088\n",
      "Epoch 7/10, Train Loss: 0.063, Accuracy: 0.8978, F1 Micro: 0.7599, F1 Macro: 0.7484\n",
      "Epoch 8/10, Train Loss: 0.0447, Accuracy: 0.8963, F1 Micro: 0.7494, F1 Macro: 0.743\n",
      "Epoch 9/10, Train Loss: 0.0333, Accuracy: 0.8955, F1 Micro: 0.7605, F1 Macro: 0.7521\n",
      "Epoch 10/10, Train Loss: 0.0254, Accuracy: 0.8908, F1 Micro: 0.7607, F1 Macro: 0.7553\n",
      "Best result for 2777 samples: F1 Micro: 0.7613\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       378\n",
      "                sara       0.58      0.77      0.66       253\n",
      "         radikalisme       0.67      0.90      0.77       234\n",
      "pencemaran_nama_baik       0.76      0.68      0.71       517\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1382\n",
      "           macro avg       0.73      0.81      0.76      1382\n",
      "        weighted avg       0.75      0.79      0.76      1382\n",
      "         samples avg       0.45      0.45      0.43      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.5021842122077942\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 20.60030770301819 seconds\n",
      "\n",
      "Fold 2 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3396, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2629, Accuracy: 0.8436, F1 Micro: 0.4575, F1 Macro: 0.393\n",
      "Epoch 3/10, Train Loss: 0.2031, Accuracy: 0.8792, F1 Micro: 0.6748, F1 Macro: 0.6266\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.8953, F1 Micro: 0.7688, F1 Macro: 0.764\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.8875, F1 Micro: 0.7659, F1 Macro: 0.7631\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9, F1 Micro: 0.7678, F1 Macro: 0.7623\n",
      "Epoch 7/10, Train Loss: 0.06, Accuracy: 0.9022, F1 Micro: 0.7814, F1 Macro: 0.778\n",
      "Epoch 8/10, Train Loss: 0.0531, Accuracy: 0.8966, F1 Micro: 0.7667, F1 Macro: 0.753\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.8978, F1 Micro: 0.7708, F1 Macro: 0.7677\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.8998, F1 Micro: 0.7656, F1 Macro: 0.7607\n",
      "Best result for 3122 samples: F1 Micro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       378\n",
      "                sara       0.65      0.68      0.66       253\n",
      "         radikalisme       0.73      0.86      0.79       234\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       517\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1382\n",
      "           macro avg       0.75      0.81      0.78      1382\n",
      "        weighted avg       0.76      0.81      0.78      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.11887922883033752\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 18.838172674179077 seconds\n",
      "\n",
      "Fold 2 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3586, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.8678, F1 Micro: 0.624, F1 Macro: 0.6074\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.8906, F1 Micro: 0.718, F1 Macro: 0.7176\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9011, F1 Micro: 0.7737, F1 Macro: 0.7678\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9013, F1 Micro: 0.784, F1 Macro: 0.7793\n",
      "Epoch 6/10, Train Loss: 0.0872, Accuracy: 0.8995, F1 Micro: 0.7688, F1 Macro: 0.7488\n",
      "Epoch 7/10, Train Loss: 0.0621, Accuracy: 0.9003, F1 Micro: 0.7789, F1 Macro: 0.7773\n",
      "Epoch 8/10, Train Loss: 0.0465, Accuracy: 0.9052, F1 Micro: 0.7912, F1 Macro: 0.7863\n",
      "Epoch 9/10, Train Loss: 0.0299, Accuracy: 0.9038, F1 Micro: 0.7776, F1 Macro: 0.773\n",
      "Epoch 10/10, Train Loss: 0.0251, Accuracy: 0.9006, F1 Micro: 0.7838, F1 Macro: 0.7788\n",
      "Best result for 3432 samples: F1 Micro: 0.7912\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       378\n",
      "                sara       0.67      0.67      0.67       253\n",
      "         radikalisme       0.73      0.88      0.80       234\n",
      "pencemaran_nama_baik       0.69      0.82      0.75       517\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1382\n",
      "           macro avg       0.75      0.82      0.79      1382\n",
      "        weighted avg       0.76      0.83      0.79      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.02729073166847229\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.07800269126892 seconds\n",
      "\n",
      "Fold 2 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3574, Accuracy: 0.8339, F1 Micro: 0.4058, F1 Macro: 0.3233\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.8875, F1 Micro: 0.7176, F1 Macro: 0.7171\n",
      "Epoch 3/10, Train Loss: 0.1971, Accuracy: 0.903, F1 Micro: 0.7708, F1 Macro: 0.76\n",
      "Epoch 4/10, Train Loss: 0.156, Accuracy: 0.9062, F1 Micro: 0.7828, F1 Macro: 0.7784\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9039, F1 Micro: 0.7716, F1 Macro: 0.7638\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.905, F1 Micro: 0.7779, F1 Macro: 0.7647\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.9033, F1 Micro: 0.7827, F1 Macro: 0.7749\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9069, F1 Micro: 0.7881, F1 Macro: 0.7817\n",
      "Epoch 9/10, Train Loss: 0.0326, Accuracy: 0.9086, F1 Micro: 0.7941, F1 Macro: 0.7866\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.9052, F1 Micro: 0.7766, F1 Macro: 0.7697\n",
      "Best result for 3711 samples: F1 Micro: 0.7941\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       378\n",
      "                sara       0.68      0.69      0.68       253\n",
      "         radikalisme       0.72      0.84      0.77       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.79      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.018613326549530032\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 15.484089851379395 seconds\n",
      "\n",
      "Fold 2 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3648, Accuracy: 0.8438, F1 Micro: 0.4759, F1 Macro: 0.411\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.8934, F1 Micro: 0.7468, F1 Macro: 0.7461\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9, F1 Micro: 0.7858, F1 Macro: 0.7842\n",
      "Epoch 4/10, Train Loss: 0.1729, Accuracy: 0.9041, F1 Micro: 0.7906, F1 Macro: 0.7888\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9086, F1 Micro: 0.7901, F1 Macro: 0.7817\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.9053, F1 Micro: 0.7942, F1 Macro: 0.7925\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9081, F1 Micro: 0.794, F1 Macro: 0.7865\n",
      "Epoch 8/10, Train Loss: 0.0489, Accuracy: 0.9077, F1 Micro: 0.7939, F1 Macro: 0.7888\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.9052, F1 Micro: 0.7883, F1 Macro: 0.7855\n",
      "Epoch 10/10, Train Loss: 0.0276, Accuracy: 0.9077, F1 Micro: 0.7966, F1 Macro: 0.789\n",
      "Best result for 3886 samples: F1 Micro: 0.7966\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.93       378\n",
      "                sara       0.66      0.67      0.67       253\n",
      "         radikalisme       0.72      0.87      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.83      0.79      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.006295812129974372\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.372623205184937 seconds\n",
      "\n",
      "Fold 2 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3661, Accuracy: 0.8569, F1 Micro: 0.5562, F1 Macro: 0.477\n",
      "Epoch 2/10, Train Loss: 0.2667, Accuracy: 0.8983, F1 Micro: 0.7628, F1 Macro: 0.7475\n",
      "Epoch 3/10, Train Loss: 0.2126, Accuracy: 0.8991, F1 Micro: 0.7653, F1 Macro: 0.7651\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.9105, F1 Micro: 0.7929, F1 Macro: 0.7814\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9116, F1 Micro: 0.801, F1 Macro: 0.7971\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9055, F1 Micro: 0.793, F1 Macro: 0.7859\n",
      "Epoch 7/10, Train Loss: 0.065, Accuracy: 0.9066, F1 Micro: 0.7951, F1 Macro: 0.7907\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.9087, F1 Micro: 0.7938, F1 Macro: 0.7878\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9061, F1 Micro: 0.7908, F1 Macro: 0.7848\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9073, F1 Micro: 0.7945, F1 Macro: 0.7898\n",
      "Best result for 4120 samples: F1 Micro: 0.801\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       378\n",
      "                sara       0.68      0.72      0.70       253\n",
      "         radikalisme       0.73      0.88      0.80       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.82      0.80      1382\n",
      "           macro avg       0.78      0.82      0.80      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.05946151614189147\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.010995149612427 seconds\n",
      "\n",
      "Fold 2 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3831, Accuracy: 0.8686, F1 Micro: 0.6364, F1 Macro: 0.6202\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.897, F1 Micro: 0.7464, F1 Macro: 0.739\n",
      "Epoch 3/10, Train Loss: 0.2165, Accuracy: 0.9028, F1 Micro: 0.7576, F1 Macro: 0.7375\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9098, F1 Micro: 0.7898, F1 Macro: 0.7832\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.9061, F1 Micro: 0.7703, F1 Macro: 0.7622\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9086, F1 Micro: 0.7995, F1 Macro: 0.7973\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.9045, F1 Micro: 0.7869, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9056, F1 Micro: 0.7907, F1 Macro: 0.7875\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.908, F1 Micro: 0.7778, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9056, F1 Micro: 0.7834, F1 Macro: 0.7807\n",
      "Best result for 4330 samples: F1 Micro: 0.7995\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       378\n",
      "                sara       0.69      0.75      0.72       253\n",
      "         radikalisme       0.71      0.89      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.85      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.84      0.80      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.01864876747131347\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.85274600982666 seconds\n",
      "\n",
      "Fold 2 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.8481, F1 Micro: 0.5193, F1 Macro: 0.4601\n",
      "Epoch 2/10, Train Loss: 0.2622, Accuracy: 0.9017, F1 Micro: 0.7707, F1 Macro: 0.7677\n",
      "Epoch 3/10, Train Loss: 0.2162, Accuracy: 0.9008, F1 Micro: 0.7799, F1 Macro: 0.7791\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9087, F1 Micro: 0.7958, F1 Macro: 0.7931\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9084, F1 Micro: 0.7846, F1 Macro: 0.7765\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9045, F1 Micro: 0.7736, F1 Macro: 0.7754\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9095, F1 Micro: 0.7966, F1 Macro: 0.7955\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9084, F1 Micro: 0.795, F1 Macro: 0.7925\n",
      "Epoch 9/10, Train Loss: 0.0372, Accuracy: 0.9073, F1 Micro: 0.788, F1 Macro: 0.7811\n",
      "Epoch 10/10, Train Loss: 0.03, Accuracy: 0.9061, F1 Micro: 0.7867, F1 Macro: 0.7835\n",
      "Best result for 4530 samples: F1 Micro: 0.7966\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.65      0.76      0.70       253\n",
      "         radikalisme       0.75      0.86      0.80       234\n",
      "pencemaran_nama_baik       0.74      0.78      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.80      1382\n",
      "           macro avg       0.77      0.82      0.80      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.011085927486419678\n",
      "Samples above threshold: 271\n",
      "Acquired samples: 133\n",
      "Sampling duration: 10.837773323059082 seconds\n",
      "\n",
      "Fold 2 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3894, Accuracy: 0.8828, F1 Micro: 0.714, F1 Macro: 0.6884\n",
      "Epoch 2/10, Train Loss: 0.2657, Accuracy: 0.9013, F1 Micro: 0.774, F1 Macro: 0.774\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9047, F1 Micro: 0.7845, F1 Macro: 0.7843\n",
      "Epoch 4/10, Train Loss: 0.1798, Accuracy: 0.9047, F1 Micro: 0.7828, F1 Macro: 0.7757\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9066, F1 Micro: 0.7873, F1 Macro: 0.7744\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.905, F1 Micro: 0.7954, F1 Macro: 0.7927\n",
      "Epoch 7/10, Train Loss: 0.0696, Accuracy: 0.9067, F1 Micro: 0.785, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0508, Accuracy: 0.9091, F1 Micro: 0.7908, F1 Macro: 0.7892\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9056, F1 Micro: 0.7838, F1 Macro: 0.7785\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9056, F1 Micro: 0.7901, F1 Macro: 0.7844\n",
      "Best result for 4663 samples: F1 Micro: 0.7954\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       378\n",
      "                sara       0.63      0.80      0.71       253\n",
      "         radikalisme       0.67      0.92      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.74      0.86      0.80      1382\n",
      "           macro avg       0.74      0.86      0.79      1382\n",
      "        weighted avg       0.76      0.86      0.80      1382\n",
      "         samples avg       0.47      0.49      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.011271071434020997\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.89431118965149 seconds\n",
      "\n",
      "Fold 2 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.8766, F1 Micro: 0.7023, F1 Macro: 0.7015\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9047, F1 Micro: 0.7815, F1 Macro: 0.7766\n",
      "Epoch 3/10, Train Loss: 0.2181, Accuracy: 0.9053, F1 Micro: 0.768, F1 Macro: 0.754\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9083, F1 Micro: 0.7825, F1 Macro: 0.7809\n",
      "Epoch 5/10, Train Loss: 0.1336, Accuracy: 0.9078, F1 Micro: 0.7873, F1 Macro: 0.7837\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9081, F1 Micro: 0.7894, F1 Macro: 0.7895\n",
      "Epoch 7/10, Train Loss: 0.0666, Accuracy: 0.9077, F1 Micro: 0.7892, F1 Macro: 0.7832\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9119, F1 Micro: 0.7958, F1 Macro: 0.7853\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9089, F1 Micro: 0.7902, F1 Macro: 0.7909\n",
      "Epoch 10/10, Train Loss: 0.0355, Accuracy: 0.9072, F1 Micro: 0.7816, F1 Macro: 0.7773\n",
      "Best result for 4863 samples: F1 Micro: 0.7958\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.93       378\n",
      "                sara       0.73      0.59      0.66       253\n",
      "         radikalisme       0.75      0.84      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.80      0.80      0.80      1382\n",
      "           macro avg       0.79      0.78      0.79      1382\n",
      "        weighted avg       0.80      0.80      0.79      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.005325400829315186\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.843029260635376 seconds\n",
      "\n",
      "Fold 2 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.386, Accuracy: 0.8819, F1 Micro: 0.7271, F1 Macro: 0.7246\n",
      "Epoch 2/10, Train Loss: 0.2656, Accuracy: 0.9016, F1 Micro: 0.7716, F1 Macro: 0.7722\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.8984, F1 Micro: 0.7343, F1 Macro: 0.7044\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9095, F1 Micro: 0.7867, F1 Macro: 0.7781\n",
      "Epoch 5/10, Train Loss: 0.1298, Accuracy: 0.9072, F1 Micro: 0.7888, F1 Macro: 0.7803\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.91, F1 Micro: 0.7944, F1 Macro: 0.7906\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.9111, F1 Micro: 0.7957, F1 Macro: 0.7935\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.907, F1 Micro: 0.781, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.9112, F1 Micro: 0.7964, F1 Macro: 0.7913\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.905, F1 Micro: 0.7921, F1 Macro: 0.7917\n",
      "Best result for 5063 samples: F1 Micro: 0.7964\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.69      0.68      0.69       253\n",
      "         radikalisme       0.74      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       517\n",
      "\n",
      "           micro avg       0.79      0.80      0.80      1382\n",
      "           macro avg       0.78      0.80      0.79      1382\n",
      "        weighted avg       0.79      0.80      0.80      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.0033828139305114757\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.695819139480591 seconds\n",
      "\n",
      "Fold 2 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.371, Accuracy: 0.8848, F1 Micro: 0.702, F1 Macro: 0.6912\n",
      "Epoch 2/10, Train Loss: 0.2499, Accuracy: 0.9027, F1 Micro: 0.7612, F1 Macro: 0.7328\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.9089, F1 Micro: 0.7959, F1 Macro: 0.7925\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9109, F1 Micro: 0.7901, F1 Macro: 0.7774\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.908, F1 Micro: 0.7933, F1 Macro: 0.7908\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9052, F1 Micro: 0.7691, F1 Macro: 0.7545\n",
      "Epoch 7/10, Train Loss: 0.0668, Accuracy: 0.9098, F1 Micro: 0.7996, F1 Macro: 0.7958\n",
      "Epoch 8/10, Train Loss: 0.0474, Accuracy: 0.91, F1 Micro: 0.7931, F1 Macro: 0.7879\n",
      "Epoch 9/10, Train Loss: 0.0361, Accuracy: 0.9098, F1 Micro: 0.7851, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.0329, Accuracy: 0.9053, F1 Micro: 0.7699, F1 Macro: 0.7596\n",
      "Best result for 5263 samples: F1 Micro: 0.7996\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       378\n",
      "                sara       0.70      0.73      0.71       253\n",
      "         radikalisme       0.71      0.87      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.79      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.83      0.80      1382\n",
      "           macro avg       0.77      0.83      0.80      1382\n",
      "        weighted avg       0.77      0.83      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.005705416202545166\n",
      "Samples above threshold: 140\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.52406907081604 seconds\n",
      "\n",
      "Fold 2 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3746, Accuracy: 0.8936, F1 Micro: 0.7548, F1 Macro: 0.7493\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9055, F1 Micro: 0.7799, F1 Macro: 0.7789\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9078, F1 Micro: 0.7794, F1 Macro: 0.7732\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.912, F1 Micro: 0.7886, F1 Macro: 0.7764\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9097, F1 Micro: 0.7782, F1 Macro: 0.7633\n",
      "Epoch 6/10, Train Loss: 0.0827, Accuracy: 0.9106, F1 Micro: 0.7885, F1 Macro: 0.7776\n",
      "Epoch 7/10, Train Loss: 0.0634, Accuracy: 0.9041, F1 Micro: 0.7975, F1 Macro: 0.7956\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.907, F1 Micro: 0.7885, F1 Macro: 0.7783\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.91, F1 Micro: 0.8, F1 Macro: 0.7965\n",
      "Epoch 10/10, Train Loss: 0.0314, Accuracy: 0.9084, F1 Micro: 0.795, F1 Macro: 0.7926\n",
      "Best result for 5441 samples: F1 Micro: 0.8\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       378\n",
      "                sara       0.64      0.77      0.70       253\n",
      "         radikalisme       0.72      0.88      0.79       234\n",
      "pencemaran_nama_baik       0.75      0.80      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.83      0.80      1382\n",
      "           macro avg       0.76      0.84      0.80      1382\n",
      "        weighted avg       0.78      0.83      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.0018857717514038086\n",
      "Samples above threshold: 79\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.588743209838867 seconds\n",
      "\n",
      "Fold 2 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3653, Accuracy: 0.8947, F1 Micro: 0.7576, F1 Macro: 0.7454\n",
      "Epoch 2/10, Train Loss: 0.2468, Accuracy: 0.9052, F1 Micro: 0.7795, F1 Macro: 0.7754\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.912, F1 Micro: 0.8001, F1 Macro: 0.7914\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.9098, F1 Micro: 0.7939, F1 Macro: 0.7908\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9062, F1 Micro: 0.7887, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.0838, Accuracy: 0.9077, F1 Micro: 0.7954, F1 Macro: 0.7936\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.905, F1 Micro: 0.8, F1 Macro: 0.797\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.9087, F1 Micro: 0.7842, F1 Macro: 0.7813\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.9075, F1 Micro: 0.7901, F1 Macro: 0.7848\n",
      "Epoch 10/10, Train Loss: 0.0274, Accuracy: 0.9111, F1 Micro: 0.7913, F1 Macro: 0.7837\n",
      "Best result for 5641 samples: F1 Micro: 0.8001\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       378\n",
      "                sara       0.71      0.67      0.69       253\n",
      "         radikalisme       0.72      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.75      0.80      0.78       517\n",
      "\n",
      "           micro avg       0.79      0.82      0.80      1382\n",
      "           macro avg       0.78      0.81      0.79      1382\n",
      "        weighted avg       0.79      0.82      0.80      1382\n",
      "         samples avg       0.47      0.46      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.07417775392532346\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.332147121429443 seconds\n",
      "\n",
      "Fold 2 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3714, Accuracy: 0.8925, F1 Micro: 0.765, F1 Macro: 0.7634\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9073, F1 Micro: 0.7921, F1 Macro: 0.7879\n",
      "Epoch 3/10, Train Loss: 0.2017, Accuracy: 0.9052, F1 Micro: 0.7928, F1 Macro: 0.7894\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.912, F1 Micro: 0.8069, F1 Macro: 0.8029\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9105, F1 Micro: 0.7899, F1 Macro: 0.7803\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9122, F1 Micro: 0.8039, F1 Macro: 0.8005\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9125, F1 Micro: 0.7949, F1 Macro: 0.7856\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.9091, F1 Micro: 0.8014, F1 Macro: 0.797\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9102, F1 Micro: 0.7914, F1 Macro: 0.7842\n",
      "Epoch 10/10, Train Loss: 0.0292, Accuracy: 0.9078, F1 Micro: 0.7785, F1 Macro: 0.7674\n",
      "Best result for 5841 samples: F1 Micro: 0.8069\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       378\n",
      "                sara       0.68      0.79      0.73       253\n",
      "         radikalisme       0.69      0.91      0.78       234\n",
      "pencemaran_nama_baik       0.75      0.82      0.78       517\n",
      "\n",
      "           micro avg       0.77      0.85      0.81      1382\n",
      "           macro avg       0.76      0.85      0.80      1382\n",
      "        weighted avg       0.78      0.85      0.81      1382\n",
      "         samples avg       0.47      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.007215380668640137\n",
      "Samples above threshold: 39\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.1413590908050537 seconds\n",
      "\n",
      "Fold 2 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3639, Accuracy: 0.8967, F1 Micro: 0.7654, F1 Macro: 0.7608\n",
      "Epoch 2/10, Train Loss: 0.2402, Accuracy: 0.9067, F1 Micro: 0.7786, F1 Macro: 0.7705\n",
      "Epoch 3/10, Train Loss: 0.199, Accuracy: 0.9084, F1 Micro: 0.7842, F1 Macro: 0.7733\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.9092, F1 Micro: 0.7954, F1 Macro: 0.7918\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9077, F1 Micro: 0.7961, F1 Macro: 0.7942\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.9045, F1 Micro: 0.7649, F1 Macro: 0.7482\n",
      "Epoch 7/10, Train Loss: 0.0645, Accuracy: 0.9095, F1 Micro: 0.7901, F1 Macro: 0.7824\n",
      "Epoch 8/10, Train Loss: 0.0455, Accuracy: 0.9106, F1 Micro: 0.7953, F1 Macro: 0.793\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9116, F1 Micro: 0.7945, F1 Macro: 0.7887\n",
      "Epoch 10/10, Train Loss: 0.03, Accuracy: 0.9086, F1 Micro: 0.7937, F1 Macro: 0.7839\n",
      "Best result for 6041 samples: F1 Micro: 0.7961\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       378\n",
      "                sara       0.65      0.81      0.72       253\n",
      "         radikalisme       0.69      0.88      0.78       234\n",
      "pencemaran_nama_baik       0.74      0.77      0.76       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.84      0.79      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.0029504418373107913\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.0558459758758545 seconds\n",
      "\n",
      "Fold 2 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3602, Accuracy: 0.8948, F1 Micro: 0.7575, F1 Macro: 0.7564\n",
      "Epoch 2/10, Train Loss: 0.2377, Accuracy: 0.9048, F1 Micro: 0.7727, F1 Macro: 0.762\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9077, F1 Micro: 0.7936, F1 Macro: 0.7875\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9119, F1 Micro: 0.8011, F1 Macro: 0.7969\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9019, F1 Micro: 0.7556, F1 Macro: 0.7386\n",
      "Epoch 6/10, Train Loss: 0.089, Accuracy: 0.9078, F1 Micro: 0.8016, F1 Macro: 0.7972\n",
      "Epoch 7/10, Train Loss: 0.0619, Accuracy: 0.9087, F1 Micro: 0.7892, F1 Macro: 0.7832\n",
      "Epoch 8/10, Train Loss: 0.0463, Accuracy: 0.9075, F1 Micro: 0.7759, F1 Macro: 0.7674\n",
      "Epoch 9/10, Train Loss: 0.0349, Accuracy: 0.9062, F1 Micro: 0.7887, F1 Macro: 0.7833\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9083, F1 Micro: 0.7945, F1 Macro: 0.788\n",
      "Best result for 6218 samples: F1 Micro: 0.8016\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       378\n",
      "                sara       0.66      0.78      0.72       253\n",
      "         radikalisme       0.72      0.82      0.77       234\n",
      "pencemaran_nama_baik       0.69      0.88      0.78       517\n",
      "\n",
      "           micro avg       0.75      0.86      0.80      1382\n",
      "           macro avg       0.75      0.85      0.80      1382\n",
      "        weighted avg       0.76      0.86      0.81      1382\n",
      "         samples avg       0.48      0.49      0.48      1382\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 3749.57 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6513, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4812, Accuracy: 0.7903, F1 Micro: 0.1183, F1 Macro: 0.098\n",
      "Epoch 4/10, Train Loss: 0.4323, Accuracy: 0.8141, F1 Micro: 0.2942, F1 Macro: 0.2249\n",
      "Epoch 5/10, Train Loss: 0.3936, Accuracy: 0.8339, F1 Micro: 0.4907, F1 Macro: 0.409\n",
      "Epoch 6/10, Train Loss: 0.3469, Accuracy: 0.8489, F1 Micro: 0.565, F1 Macro: 0.4941\n",
      "Epoch 7/10, Train Loss: 0.2903, Accuracy: 0.8502, F1 Micro: 0.5889, F1 Macro: 0.5297\n",
      "Epoch 8/10, Train Loss: 0.227, Accuracy: 0.8589, F1 Micro: 0.6596, F1 Macro: 0.6437\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.8583, F1 Micro: 0.6931, F1 Macro: 0.6907\n",
      "Epoch 10/10, Train Loss: 0.1624, Accuracy: 0.8569, F1 Micro: 0.6041, F1 Macro: 0.5588\n",
      "Best result for 388 samples: F1 Micro: 0.6931\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.85      0.85       355\n",
      "                sara       0.58      0.58      0.58       273\n",
      "         radikalisme       0.64      0.70      0.67       281\n",
      "pencemaran_nama_baik       0.62      0.71      0.66       521\n",
      "\n",
      "           micro avg       0.67      0.72      0.69      1430\n",
      "           macro avg       0.67      0.71      0.69      1430\n",
      "        weighted avg       0.68      0.72      0.69      1430\n",
      "         samples avg       0.40      0.40      0.39      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8778910867869855\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 34.94585347175598 seconds\n",
      "\n",
      "Fold 3 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4669, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.295, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.253, Accuracy: 0.8109, F1 Micro: 0.3481, F1 Macro: 0.2645\n",
      "Epoch 4/10, Train Loss: 0.203, Accuracy: 0.8448, F1 Micro: 0.5537, F1 Macro: 0.491\n",
      "Epoch 5/10, Train Loss: 0.1614, Accuracy: 0.8578, F1 Micro: 0.6676, F1 Macro: 0.6507\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.8603, F1 Micro: 0.6173, F1 Macro: 0.6054\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.8681, F1 Micro: 0.681, F1 Macro: 0.664\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.8692, F1 Micro: 0.7083, F1 Macro: 0.7022\n",
      "Epoch 9/10, Train Loss: 0.071, Accuracy: 0.8616, F1 Micro: 0.6439, F1 Macro: 0.6264\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.8644, F1 Micro: 0.727, F1 Macro: 0.7287\n",
      "Best result for 971 samples: F1 Micro: 0.727\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.87      0.87       355\n",
      "                sara       0.58      0.70      0.63       273\n",
      "         radikalisme       0.62      0.83      0.71       281\n",
      "pencemaran_nama_baik       0.61      0.82      0.70       521\n",
      "\n",
      "           micro avg       0.66      0.81      0.73      1430\n",
      "           macro avg       0.67      0.80      0.73      1430\n",
      "        weighted avg       0.67      0.81      0.73      1430\n",
      "         samples avg       0.45      0.46      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9146101862192154\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 31.222636938095093 seconds\n",
      "\n",
      "Fold 3 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3727, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2351, Accuracy: 0.7767, F1 Micro: 0.0014, F1 Macro: 0.001\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.8338, F1 Micro: 0.4712, F1 Macro: 0.3872\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.8511, F1 Micro: 0.685, F1 Macro: 0.6832\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.8712, F1 Micro: 0.6973, F1 Macro: 0.6902\n",
      "Epoch 6/10, Train Loss: 0.0984, Accuracy: 0.8698, F1 Micro: 0.7239, F1 Macro: 0.7246\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.8723, F1 Micro: 0.6696, F1 Macro: 0.6606\n",
      "Epoch 8/10, Train Loss: 0.0549, Accuracy: 0.8705, F1 Micro: 0.7198, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.8719, F1 Micro: 0.7155, F1 Macro: 0.7132\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.8666, F1 Micro: 0.7211, F1 Macro: 0.724\n",
      "Best result for 1496 samples: F1 Micro: 0.7239\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.85      0.85       355\n",
      "                sara       0.61      0.68      0.64       273\n",
      "         radikalisme       0.65      0.78      0.71       281\n",
      "pencemaran_nama_baik       0.65      0.74      0.69       521\n",
      "\n",
      "           micro avg       0.69      0.76      0.72      1430\n",
      "           macro avg       0.69      0.76      0.72      1430\n",
      "        weighted avg       0.69      0.76      0.73      1430\n",
      "         samples avg       0.42      0.43      0.41      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8485497802495956\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 28.0929696559906 seconds\n",
      "\n",
      "Fold 3 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3371, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2307, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.1894, Accuracy: 0.8295, F1 Micro: 0.4618, F1 Macro: 0.3543\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.8569, F1 Micro: 0.7055, F1 Macro: 0.7068\n",
      "Epoch 5/10, Train Loss: 0.1115, Accuracy: 0.8747, F1 Micro: 0.7236, F1 Macro: 0.7191\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.8614, F1 Micro: 0.7275, F1 Macro: 0.7308\n",
      "Epoch 7/10, Train Loss: 0.0636, Accuracy: 0.8789, F1 Micro: 0.7379, F1 Macro: 0.7394\n",
      "Epoch 8/10, Train Loss: 0.0461, Accuracy: 0.8802, F1 Micro: 0.7051, F1 Macro: 0.6891\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.8766, F1 Micro: 0.6919, F1 Macro: 0.6527\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.8761, F1 Micro: 0.693, F1 Macro: 0.6627\n",
      "Best result for 1969 samples: F1 Micro: 0.7379\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.87       355\n",
      "                sara       0.63      0.68      0.65       273\n",
      "         radikalisme       0.68      0.79      0.73       281\n",
      "pencemaran_nama_baik       0.68      0.72      0.70       521\n",
      "\n",
      "           micro avg       0.71      0.76      0.74      1430\n",
      "           macro avg       0.72      0.77      0.74      1430\n",
      "        weighted avg       0.72      0.76      0.74      1430\n",
      "         samples avg       0.42      0.43      0.42      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8324899166822436\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 25.456176042556763 seconds\n",
      "\n",
      "Fold 3 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3126, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2403, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.196, Accuracy: 0.8583, F1 Micro: 0.6547, F1 Macro: 0.6518\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.8798, F1 Micro: 0.7121, F1 Macro: 0.7079\n",
      "Epoch 5/10, Train Loss: 0.1187, Accuracy: 0.8719, F1 Micro: 0.7194, F1 Macro: 0.7114\n",
      "Epoch 6/10, Train Loss: 0.0824, Accuracy: 0.8794, F1 Micro: 0.7056, F1 Macro: 0.705\n",
      "Epoch 7/10, Train Loss: 0.0638, Accuracy: 0.8788, F1 Micro: 0.7391, F1 Macro: 0.7397\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.8752, F1 Micro: 0.7346, F1 Macro: 0.7317\n",
      "Epoch 9/10, Train Loss: 0.036, Accuracy: 0.8817, F1 Micro: 0.7416, F1 Macro: 0.7406\n",
      "Epoch 10/10, Train Loss: 0.0257, Accuracy: 0.878, F1 Micro: 0.7123, F1 Macro: 0.7149\n",
      "Best result for 2394 samples: F1 Micro: 0.7416\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.89      0.87       355\n",
      "                sara       0.65      0.65      0.65       273\n",
      "         radikalisme       0.74      0.73      0.74       281\n",
      "pencemaran_nama_baik       0.67      0.74      0.70       521\n",
      "\n",
      "           micro avg       0.72      0.76      0.74      1430\n",
      "           macro avg       0.73      0.75      0.74      1430\n",
      "        weighted avg       0.73      0.76      0.74      1430\n",
      "         samples avg       0.45      0.44      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.5966779589653015\n",
      "Samples above threshold: 384\n",
      "Acquired samples: 383\n",
      "Sampling duration: 22.998448610305786 seconds\n",
      "\n",
      "Fold 3 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.333, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2489, Accuracy: 0.7912, F1 Micro: 0.1555, F1 Macro: 0.1303\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.8634, F1 Micro: 0.6942, F1 Macro: 0.6996\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.8842, F1 Micro: 0.7318, F1 Macro: 0.723\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.8769, F1 Micro: 0.7126, F1 Macro: 0.6997\n",
      "Epoch 6/10, Train Loss: 0.0848, Accuracy: 0.8806, F1 Micro: 0.741, F1 Macro: 0.7432\n",
      "Epoch 7/10, Train Loss: 0.0642, Accuracy: 0.8853, F1 Micro: 0.7435, F1 Macro: 0.7487\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.8855, F1 Micro: 0.74, F1 Macro: 0.7354\n",
      "Epoch 9/10, Train Loss: 0.0343, Accuracy: 0.8861, F1 Micro: 0.7394, F1 Macro: 0.7367\n",
      "Epoch 10/10, Train Loss: 0.0272, Accuracy: 0.8764, F1 Micro: 0.7416, F1 Macro: 0.7421\n",
      "Best result for 2777 samples: F1 Micro: 0.7435\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.82      0.87       355\n",
      "                sara       0.61      0.74      0.67       273\n",
      "         radikalisme       0.73      0.79      0.76       281\n",
      "pencemaran_nama_baik       0.72      0.67      0.69       521\n",
      "\n",
      "           micro avg       0.74      0.74      0.74      1430\n",
      "           macro avg       0.75      0.75      0.75      1430\n",
      "        weighted avg       0.75      0.74      0.75      1430\n",
      "         samples avg       0.43      0.43      0.42      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.5122766196727753\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 20.712650060653687 seconds\n",
      "\n",
      "Fold 3 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3334, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2689, Accuracy: 0.8205, F1 Micro: 0.4658, F1 Macro: 0.4116\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.8641, F1 Micro: 0.7268, F1 Macro: 0.7343\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.8859, F1 Micro: 0.7592, F1 Macro: 0.7608\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.8858, F1 Micro: 0.7583, F1 Macro: 0.7604\n",
      "Epoch 6/10, Train Loss: 0.0843, Accuracy: 0.8898, F1 Micro: 0.7534, F1 Macro: 0.7517\n",
      "Epoch 7/10, Train Loss: 0.0569, Accuracy: 0.8878, F1 Micro: 0.7615, F1 Macro: 0.761\n",
      "Epoch 8/10, Train Loss: 0.0424, Accuracy: 0.8895, F1 Micro: 0.7583, F1 Macro: 0.7599\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.8902, F1 Micro: 0.7431, F1 Macro: 0.734\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.8897, F1 Micro: 0.7523, F1 Macro: 0.7437\n",
      "Best result for 3122 samples: F1 Micro: 0.7615\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.91      0.88       355\n",
      "                sara       0.66      0.67      0.66       273\n",
      "         radikalisme       0.73      0.83      0.77       281\n",
      "pencemaran_nama_baik       0.67      0.78      0.72       521\n",
      "\n",
      "           micro avg       0.73      0.80      0.76      1430\n",
      "           macro avg       0.73      0.80      0.76      1430\n",
      "        weighted avg       0.73      0.80      0.76      1430\n",
      "         samples avg       0.47      0.47      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.09829112887382507\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 18.907580375671387 seconds\n",
      "\n",
      "Fold 3 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3619, Accuracy: 0.7767, F1 Micro: 0.0014, F1 Macro: 0.001\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.8459, F1 Micro: 0.5542, F1 Macro: 0.5396\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.8803, F1 Micro: 0.7299, F1 Macro: 0.7236\n",
      "Epoch 4/10, Train Loss: 0.1697, Accuracy: 0.8892, F1 Micro: 0.7655, F1 Macro: 0.7653\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.8828, F1 Micro: 0.7488, F1 Macro: 0.7477\n",
      "Epoch 6/10, Train Loss: 0.0916, Accuracy: 0.8875, F1 Micro: 0.7561, F1 Macro: 0.759\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.8847, F1 Micro: 0.7572, F1 Macro: 0.7579\n",
      "Epoch 8/10, Train Loss: 0.0489, Accuracy: 0.888, F1 Micro: 0.7547, F1 Macro: 0.7529\n",
      "Epoch 9/10, Train Loss: 0.035, Accuracy: 0.8914, F1 Micro: 0.7589, F1 Macro: 0.7574\n",
      "Epoch 10/10, Train Loss: 0.0278, Accuracy: 0.8919, F1 Micro: 0.7518, F1 Macro: 0.7492\n",
      "Best result for 3432 samples: F1 Micro: 0.7655\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.89      0.88       355\n",
      "                sara       0.67      0.67      0.67       273\n",
      "         radikalisme       0.70      0.87      0.78       281\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       521\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1430\n",
      "           macro avg       0.73      0.81      0.77      1430\n",
      "        weighted avg       0.73      0.81      0.77      1430\n",
      "         samples avg       0.46      0.47      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.16649827361106873\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.006380081176758 seconds\n",
      "\n",
      "Fold 3 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3859, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.8641, F1 Micro: 0.6695, F1 Macro: 0.6592\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.8836, F1 Micro: 0.7198, F1 Macro: 0.7097\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.8933, F1 Micro: 0.7506, F1 Macro: 0.7511\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.8905, F1 Micro: 0.7557, F1 Macro: 0.7537\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.8956, F1 Micro: 0.7692, F1 Macro: 0.7708\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.8925, F1 Micro: 0.7603, F1 Macro: 0.7569\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.8923, F1 Micro: 0.7648, F1 Macro: 0.7641\n",
      "Epoch 9/10, Train Loss: 0.0368, Accuracy: 0.882, F1 Micro: 0.7595, F1 Macro: 0.7629\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.8911, F1 Micro: 0.7635, F1 Macro: 0.7623\n",
      "Best result for 3711 samples: F1 Micro: 0.7692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       355\n",
      "                sara       0.70      0.66      0.68       273\n",
      "         radikalisme       0.74      0.85      0.79       281\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       521\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1430\n",
      "           macro avg       0.77      0.78      0.77      1430\n",
      "        weighted avg       0.76      0.78      0.77      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.07666834592819216\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 15.448535919189453 seconds\n",
      "\n",
      "Fold 3 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3811, Accuracy: 0.7797, F1 Micro: 0.0303, F1 Macro: 0.0312\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.8641, F1 Micro: 0.6244, F1 Macro: 0.6155\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.8909, F1 Micro: 0.7553, F1 Macro: 0.7547\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.8888, F1 Micro: 0.7604, F1 Macro: 0.764\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.8895, F1 Micro: 0.7517, F1 Macro: 0.7503\n",
      "Epoch 6/10, Train Loss: 0.0912, Accuracy: 0.8914, F1 Micro: 0.764, F1 Macro: 0.7653\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.8889, F1 Micro: 0.7586, F1 Macro: 0.7613\n",
      "Epoch 8/10, Train Loss: 0.0505, Accuracy: 0.8888, F1 Micro: 0.7577, F1 Macro: 0.7574\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.8906, F1 Micro: 0.7609, F1 Macro: 0.764\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.8911, F1 Micro: 0.7519, F1 Macro: 0.7507\n",
      "Best result for 3886 samples: F1 Micro: 0.764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       355\n",
      "                sara       0.67      0.63      0.65       273\n",
      "         radikalisme       0.76      0.83      0.79       281\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       521\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1430\n",
      "           macro avg       0.75      0.78      0.77      1430\n",
      "        weighted avg       0.75      0.79      0.77      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0405291259288788\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.356275796890259 seconds\n",
      "\n",
      "Fold 3 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.388, Accuracy: 0.8181, F1 Micro: 0.3326, F1 Macro: 0.2957\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.8811, F1 Micro: 0.7416, F1 Macro: 0.7465\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.8909, F1 Micro: 0.7482, F1 Macro: 0.7503\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.8917, F1 Micro: 0.7536, F1 Macro: 0.7498\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.8852, F1 Micro: 0.737, F1 Macro: 0.7309\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.8892, F1 Micro: 0.7554, F1 Macro: 0.7552\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.8891, F1 Micro: 0.745, F1 Macro: 0.7448\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.892, F1 Micro: 0.759, F1 Macro: 0.7587\n",
      "Epoch 9/10, Train Loss: 0.0365, Accuracy: 0.8866, F1 Micro: 0.7599, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.8891, F1 Micro: 0.7613, F1 Macro: 0.7615\n",
      "Best result for 4120 samples: F1 Micro: 0.7613\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       355\n",
      "                sara       0.64      0.65      0.65       273\n",
      "         radikalisme       0.73      0.83      0.78       281\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       521\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1430\n",
      "           macro avg       0.74      0.79      0.76      1430\n",
      "        weighted avg       0.74      0.79      0.76      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.007768809795379639\n",
      "Samples above threshold: 218\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.060470581054688 seconds\n",
      "\n",
      "Fold 3 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4011, Accuracy: 0.8398, F1 Micro: 0.4821, F1 Macro: 0.4411\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.883, F1 Micro: 0.7349, F1 Macro: 0.7269\n",
      "Epoch 3/10, Train Loss: 0.2143, Accuracy: 0.8905, F1 Micro: 0.7618, F1 Macro: 0.7667\n",
      "Epoch 4/10, Train Loss: 0.1789, Accuracy: 0.8911, F1 Micro: 0.7665, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1306, Accuracy: 0.8916, F1 Micro: 0.7577, F1 Macro: 0.7601\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.8908, F1 Micro: 0.7584, F1 Macro: 0.759\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.8931, F1 Micro: 0.7691, F1 Macro: 0.7702\n",
      "Epoch 8/10, Train Loss: 0.0532, Accuracy: 0.8934, F1 Micro: 0.761, F1 Macro: 0.764\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.8961, F1 Micro: 0.7616, F1 Macro: 0.7639\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.8941, F1 Micro: 0.7675, F1 Macro: 0.7649\n",
      "Best result for 4330 samples: F1 Micro: 0.7691\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.90      0.89       355\n",
      "                sara       0.65      0.70      0.67       273\n",
      "         radikalisme       0.76      0.81      0.79       281\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1430\n",
      "           macro avg       0.75      0.80      0.77      1430\n",
      "        weighted avg       0.75      0.80      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.009017860889434808\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.716323852539062 seconds\n",
      "\n",
      "Fold 3 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8687, F1 Micro: 0.6526, F1 Macro: 0.64\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.8853, F1 Micro: 0.7181, F1 Macro: 0.7177\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.8822, F1 Micro: 0.76, F1 Macro: 0.7661\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.8905, F1 Micro: 0.7541, F1 Macro: 0.7534\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.89, F1 Micro: 0.7545, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.8881, F1 Micro: 0.7594, F1 Macro: 0.7614\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.892, F1 Micro: 0.7669, F1 Macro: 0.7706\n",
      "Epoch 8/10, Train Loss: 0.0503, Accuracy: 0.8945, F1 Micro: 0.7719, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.8933, F1 Micro: 0.7644, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0271, Accuracy: 0.893, F1 Micro: 0.7708, F1 Macro: 0.7734\n",
      "Best result for 4530 samples: F1 Micro: 0.7719\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.86      0.91       355\n",
      "                sara       0.62      0.78      0.69       273\n",
      "         radikalisme       0.72      0.84      0.78       281\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       521\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1430\n",
      "           macro avg       0.75      0.81      0.78      1430\n",
      "        weighted avg       0.76      0.80      0.78      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.012147784233093262\n",
      "Samples above threshold: 223\n",
      "Acquired samples: 133\n",
      "Sampling duration: 10.859674215316772 seconds\n",
      "\n",
      "Fold 3 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3965, Accuracy: 0.87, F1 Micro: 0.7151, F1 Macro: 0.7208\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.8888, F1 Micro: 0.7365, F1 Macro: 0.7364\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.8942, F1 Micro: 0.7595, F1 Macro: 0.7561\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.8922, F1 Micro: 0.767, F1 Macro: 0.7699\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.8897, F1 Micro: 0.7634, F1 Macro: 0.7654\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.8916, F1 Micro: 0.7651, F1 Macro: 0.7668\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.8919, F1 Micro: 0.7534, F1 Macro: 0.7513\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.8917, F1 Micro: 0.7606, F1 Macro: 0.7596\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.8878, F1 Micro: 0.7644, F1 Macro: 0.7683\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8925, F1 Micro: 0.7606, F1 Macro: 0.7631\n",
      "Best result for 4663 samples: F1 Micro: 0.767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       355\n",
      "                sara       0.63      0.72      0.67       273\n",
      "         radikalisme       0.74      0.82      0.78       281\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1430\n",
      "           macro avg       0.75      0.79      0.77      1430\n",
      "        weighted avg       0.75      0.79      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.05004991292953492\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.066603183746338 seconds\n",
      "\n",
      "Fold 3 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.412, Accuracy: 0.8642, F1 Micro: 0.7033, F1 Macro: 0.7071\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.8892, F1 Micro: 0.7561, F1 Macro: 0.7564\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.8961, F1 Micro: 0.7572, F1 Macro: 0.7521\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.8923, F1 Micro: 0.7535, F1 Macro: 0.7515\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.8938, F1 Micro: 0.7693, F1 Macro: 0.7741\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.892, F1 Micro: 0.7682, F1 Macro: 0.7708\n",
      "Epoch 7/10, Train Loss: 0.0691, Accuracy: 0.8903, F1 Micro: 0.7636, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.887, F1 Micro: 0.7624, F1 Macro: 0.7682\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.8936, F1 Micro: 0.7601, F1 Macro: 0.7597\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.8941, F1 Micro: 0.7587, F1 Macro: 0.7593\n",
      "Best result for 4863 samples: F1 Micro: 0.7693\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       355\n",
      "                sara       0.62      0.80      0.70       273\n",
      "         radikalisme       0.71      0.87      0.78       281\n",
      "pencemaran_nama_baik       0.74      0.68      0.71       521\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1430\n",
      "           macro avg       0.75      0.81      0.77      1430\n",
      "        weighted avg       0.76      0.79      0.77      1430\n",
      "         samples avg       0.44      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.01716597080230714\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.806044816970825 seconds\n",
      "\n",
      "Fold 3 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4029, Accuracy: 0.8758, F1 Micro: 0.6983, F1 Macro: 0.6969\n",
      "Epoch 2/10, Train Loss: 0.256, Accuracy: 0.8941, F1 Micro: 0.7478, F1 Macro: 0.7468\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.8925, F1 Micro: 0.7553, F1 Macro: 0.7532\n",
      "Epoch 4/10, Train Loss: 0.1666, Accuracy: 0.8914, F1 Micro: 0.7681, F1 Macro: 0.772\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.8898, F1 Micro: 0.7645, F1 Macro: 0.7671\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.8933, F1 Micro: 0.7577, F1 Macro: 0.7577\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.8908, F1 Micro: 0.7437, F1 Macro: 0.7382\n",
      "Epoch 8/10, Train Loss: 0.048, Accuracy: 0.8914, F1 Micro: 0.7611, F1 Macro: 0.7605\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.8891, F1 Micro: 0.7536, F1 Macro: 0.7523\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.8919, F1 Micro: 0.758, F1 Macro: 0.7619\n",
      "Best result for 5063 samples: F1 Micro: 0.7681\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       355\n",
      "                sara       0.62      0.74      0.67       273\n",
      "         radikalisme       0.75      0.84      0.79       281\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       521\n",
      "\n",
      "           micro avg       0.73      0.80      0.77      1430\n",
      "           macro avg       0.74      0.81      0.77      1430\n",
      "        weighted avg       0.74      0.80      0.77      1430\n",
      "         samples avg       0.45      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.023741567134857193\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.58967399597168 seconds\n",
      "\n",
      "Fold 3 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4076, Accuracy: 0.8705, F1 Micro: 0.668, F1 Macro: 0.6717\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.897, F1 Micro: 0.7634, F1 Macro: 0.7561\n",
      "Epoch 3/10, Train Loss: 0.2119, Accuracy: 0.8992, F1 Micro: 0.777, F1 Macro: 0.7775\n",
      "Epoch 4/10, Train Loss: 0.176, Accuracy: 0.8961, F1 Micro: 0.7653, F1 Macro: 0.763\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.8898, F1 Micro: 0.7598, F1 Macro: 0.7645\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.892, F1 Micro: 0.7638, F1 Macro: 0.7691\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.8923, F1 Micro: 0.7561, F1 Macro: 0.7527\n",
      "Epoch 8/10, Train Loss: 0.0534, Accuracy: 0.895, F1 Micro: 0.7588, F1 Macro: 0.7585\n",
      "Epoch 9/10, Train Loss: 0.0385, Accuracy: 0.8925, F1 Micro: 0.7557, F1 Macro: 0.754\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.8952, F1 Micro: 0.7575, F1 Macro: 0.7542\n",
      "Best result for 5263 samples: F1 Micro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.86      0.88       355\n",
      "                sara       0.66      0.69      0.68       273\n",
      "         radikalisme       0.78      0.83      0.80       281\n",
      "pencemaran_nama_baik       0.73      0.76      0.75       521\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1430\n",
      "           macro avg       0.77      0.79      0.78      1430\n",
      "        weighted avg       0.77      0.79      0.78      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.046434581279754646\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.525216817855835 seconds\n",
      "\n",
      "Fold 3 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4123, Accuracy: 0.8752, F1 Micro: 0.7246, F1 Macro: 0.7314\n",
      "Epoch 2/10, Train Loss: 0.2627, Accuracy: 0.8934, F1 Micro: 0.745, F1 Macro: 0.744\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.8977, F1 Micro: 0.761, F1 Macro: 0.7516\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.8984, F1 Micro: 0.7737, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.8917, F1 Micro: 0.7586, F1 Macro: 0.7571\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.8969, F1 Micro: 0.7624, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.8884, F1 Micro: 0.7654, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.8944, F1 Micro: 0.7696, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.8933, F1 Micro: 0.7537, F1 Macro: 0.7557\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.8916, F1 Micro: 0.7523, F1 Macro: 0.7523\n",
      "Best result for 5441 samples: F1 Micro: 0.7737\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       355\n",
      "                sara       0.67      0.70      0.68       273\n",
      "         radikalisme       0.72      0.86      0.79       281\n",
      "pencemaran_nama_baik       0.76      0.69      0.73       521\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1430\n",
      "           macro avg       0.77      0.79      0.77      1430\n",
      "        weighted avg       0.77      0.78      0.77      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.011220693588256836\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.35029411315918 seconds\n",
      "\n",
      "Fold 3 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.406, Accuracy: 0.8813, F1 Micro: 0.726, F1 Macro: 0.7285\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.8969, F1 Micro: 0.7498, F1 Macro: 0.7461\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.8956, F1 Micro: 0.769, F1 Macro: 0.7618\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.8942, F1 Micro: 0.7737, F1 Macro: 0.779\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.8947, F1 Micro: 0.7709, F1 Macro: 0.7753\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.8911, F1 Micro: 0.7504, F1 Macro: 0.7529\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.8931, F1 Micro: 0.7638, F1 Macro: 0.7673\n",
      "Epoch 8/10, Train Loss: 0.0535, Accuracy: 0.893, F1 Micro: 0.7591, F1 Macro: 0.7594\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.8938, F1 Micro: 0.7597, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8913, F1 Micro: 0.7566, F1 Macro: 0.758\n",
      "Best result for 5641 samples: F1 Micro: 0.7737\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.67      0.74      0.70       273\n",
      "         radikalisme       0.78      0.79      0.78       281\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1430\n",
      "           macro avg       0.76      0.80      0.78      1430\n",
      "        weighted avg       0.75      0.81      0.78      1430\n",
      "         samples avg       0.47      0.47      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.023084521293640137\n",
      "Samples above threshold: 90\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.350708723068237 seconds\n",
      "\n",
      "Fold 3 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4023, Accuracy: 0.8822, F1 Micro: 0.7428, F1 Macro: 0.7462\n",
      "Epoch 2/10, Train Loss: 0.2557, Accuracy: 0.8894, F1 Micro: 0.7707, F1 Macro: 0.7743\n",
      "Epoch 3/10, Train Loss: 0.2036, Accuracy: 0.8975, F1 Micro: 0.7775, F1 Macro: 0.7797\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.8939, F1 Micro: 0.7698, F1 Macro: 0.7726\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.8913, F1 Micro: 0.7695, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.8934, F1 Micro: 0.754, F1 Macro: 0.7519\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.893, F1 Micro: 0.7621, F1 Macro: 0.7654\n",
      "Epoch 8/10, Train Loss: 0.0463, Accuracy: 0.8902, F1 Micro: 0.7616, F1 Macro: 0.7651\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.8892, F1 Micro: 0.7458, F1 Macro: 0.7406\n",
      "Epoch 10/10, Train Loss: 0.0327, Accuracy: 0.8909, F1 Micro: 0.7482, F1 Macro: 0.7439\n",
      "Best result for 5841 samples: F1 Micro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       355\n",
      "                sara       0.68      0.71      0.69       273\n",
      "         radikalisme       0.79      0.78      0.79       281\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       521\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1430\n",
      "           macro avg       0.77      0.79      0.78      1430\n",
      "        weighted avg       0.76      0.80      0.78      1430\n",
      "         samples avg       0.47      0.46      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.008100247383117676\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.1818113327026367 seconds\n",
      "\n",
      "Fold 3 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.383, Accuracy: 0.8838, F1 Micro: 0.7399, F1 Macro: 0.7437\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.8959, F1 Micro: 0.7539, F1 Macro: 0.7482\n",
      "Epoch 3/10, Train Loss: 0.1983, Accuracy: 0.8986, F1 Micro: 0.7796, F1 Macro: 0.7794\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.8939, F1 Micro: 0.7569, F1 Macro: 0.7541\n",
      "Epoch 5/10, Train Loss: 0.1172, Accuracy: 0.8956, F1 Micro: 0.7704, F1 Macro: 0.7724\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.8908, F1 Micro: 0.7548, F1 Macro: 0.75\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.8956, F1 Micro: 0.7664, F1 Macro: 0.7677\n",
      "Epoch 8/10, Train Loss: 0.0458, Accuracy: 0.8914, F1 Micro: 0.7519, F1 Macro: 0.75\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.8964, F1 Micro: 0.7544, F1 Macro: 0.7539\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.8892, F1 Micro: 0.7609, F1 Macro: 0.765\n",
      "Best result for 6041 samples: F1 Micro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       355\n",
      "                sara       0.65      0.69      0.67       273\n",
      "         radikalisme       0.78      0.83      0.80       281\n",
      "pencemaran_nama_baik       0.71      0.78      0.75       521\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1430\n",
      "           macro avg       0.76      0.80      0.78      1430\n",
      "        weighted avg       0.76      0.80      0.78      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 0.005536389350891113\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 2.0167486667633057 seconds\n",
      "\n",
      "Fold 3 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3743, Accuracy: 0.89, F1 Micro: 0.7446, F1 Macro: 0.7439\n",
      "Epoch 2/10, Train Loss: 0.2289, Accuracy: 0.8925, F1 Micro: 0.7634, F1 Macro: 0.7621\n",
      "Epoch 3/10, Train Loss: 0.1931, Accuracy: 0.8931, F1 Micro: 0.7788, F1 Macro: 0.782\n",
      "Epoch 4/10, Train Loss: 0.1464, Accuracy: 0.8977, F1 Micro: 0.7602, F1 Macro: 0.7549\n",
      "Epoch 5/10, Train Loss: 0.112, Accuracy: 0.892, F1 Micro: 0.7663, F1 Macro: 0.7681\n",
      "Epoch 6/10, Train Loss: 0.0829, Accuracy: 0.8938, F1 Micro: 0.7645, F1 Macro: 0.7667\n",
      "Epoch 7/10, Train Loss: 0.0637, Accuracy: 0.8938, F1 Micro: 0.7422, F1 Macro: 0.7392\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8922, F1 Micro: 0.7607, F1 Macro: 0.7606\n",
      "Epoch 9/10, Train Loss: 0.0336, Accuracy: 0.8928, F1 Micro: 0.7581, F1 Macro: 0.7607\n",
      "Epoch 10/10, Train Loss: 0.0294, Accuracy: 0.8931, F1 Micro: 0.7545, F1 Macro: 0.7544\n",
      "Best result for 6218 samples: F1 Micro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       355\n",
      "                sara       0.64      0.77      0.70       273\n",
      "         radikalisme       0.73      0.86      0.79       281\n",
      "pencemaran_nama_baik       0.67      0.83      0.74       521\n",
      "\n",
      "           micro avg       0.72      0.84      0.78      1430\n",
      "           macro avg       0.73      0.84      0.78      1430\n",
      "        weighted avg       0.73      0.84      0.78      1430\n",
      "         samples avg       0.48      0.49      0.47      1430\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 3777.69 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5795, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4106, Accuracy: 0.8267, F1 Micro: 0.3666, F1 Macro: 0.3071\n",
      "Epoch 4/10, Train Loss: 0.3659, Accuracy: 0.8505, F1 Micro: 0.5812, F1 Macro: 0.5497\n",
      "Epoch 5/10, Train Loss: 0.3165, Accuracy: 0.8614, F1 Micro: 0.6272, F1 Macro: 0.608\n",
      "Epoch 6/10, Train Loss: 0.2503, Accuracy: 0.8612, F1 Micro: 0.655, F1 Macro: 0.6302\n",
      "Epoch 7/10, Train Loss: 0.2107, Accuracy: 0.8628, F1 Micro: 0.6613, F1 Macro: 0.6477\n",
      "Epoch 8/10, Train Loss: 0.1713, Accuracy: 0.8617, F1 Micro: 0.6757, F1 Macro: 0.6675\n",
      "Epoch 9/10, Train Loss: 0.1484, Accuracy: 0.8644, F1 Micro: 0.672, F1 Macro: 0.662\n",
      "Epoch 10/10, Train Loss: 0.1116, Accuracy: 0.8662, F1 Micro: 0.686, F1 Macro: 0.6766\n",
      "Best result for 388 samples: F1 Micro: 0.686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.81      0.86       342\n",
      "                sara       0.54      0.47      0.51       249\n",
      "         radikalisme       0.72      0.63      0.67       302\n",
      "pencemaran_nama_baik       0.64      0.69      0.66       508\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1401\n",
      "           macro avg       0.71      0.65      0.68      1401\n",
      "        weighted avg       0.71      0.67      0.69      1401\n",
      "         samples avg       0.38      0.39      0.37      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9334616512060165\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 35.17131543159485 seconds\n",
      "\n",
      "Fold 4 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4161, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2973, Accuracy: 0.7814, F1 Micro: 0.0029, F1 Macro: 0.0029\n",
      "Epoch 3/10, Train Loss: 0.24, Accuracy: 0.8523, F1 Micro: 0.6054, F1 Macro: 0.5782\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.8517, F1 Micro: 0.6713, F1 Macro: 0.6664\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.8641, F1 Micro: 0.6575, F1 Macro: 0.6415\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.8698, F1 Micro: 0.68, F1 Macro: 0.6682\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.8722, F1 Micro: 0.7036, F1 Macro: 0.7012\n",
      "Epoch 8/10, Train Loss: 0.0792, Accuracy: 0.8655, F1 Micro: 0.714, F1 Macro: 0.7142\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.8759, F1 Micro: 0.7013, F1 Macro: 0.6958\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.8755, F1 Micro: 0.7249, F1 Macro: 0.7202\n",
      "Best result for 972 samples: F1 Micro: 0.7249\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.88      0.88       342\n",
      "                sara       0.55      0.61      0.58       249\n",
      "         radikalisme       0.72      0.73      0.73       302\n",
      "pencemaran_nama_baik       0.66      0.74      0.70       508\n",
      "\n",
      "           micro avg       0.70      0.75      0.72      1401\n",
      "           macro avg       0.70      0.74      0.72      1401\n",
      "        weighted avg       0.71      0.75      0.73      1401\n",
      "         samples avg       0.42      0.43      0.41      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9620809011161329\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 31.25131344795227 seconds\n",
      "\n",
      "Fold 4 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3461, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2146, Accuracy: 0.7833, F1 Micro: 0.0198, F1 Macro: 0.0197\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.8572, F1 Micro: 0.6617, F1 Macro: 0.6461\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.863, F1 Micro: 0.632, F1 Macro: 0.6062\n",
      "Epoch 5/10, Train Loss: 0.1069, Accuracy: 0.8677, F1 Micro: 0.713, F1 Macro: 0.7084\n",
      "Epoch 6/10, Train Loss: 0.0756, Accuracy: 0.8534, F1 Micro: 0.7011, F1 Macro: 0.7053\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.8712, F1 Micro: 0.7161, F1 Macro: 0.7134\n",
      "Epoch 8/10, Train Loss: 0.0428, Accuracy: 0.8744, F1 Micro: 0.6998, F1 Macro: 0.6937\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.8745, F1 Micro: 0.7096, F1 Macro: 0.7081\n",
      "Epoch 10/10, Train Loss: 0.0257, Accuracy: 0.8734, F1 Micro: 0.6996, F1 Macro: 0.6952\n",
      "Best result for 1497 samples: F1 Micro: 0.7161\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.83      0.87       342\n",
      "                sara       0.54      0.64      0.59       249\n",
      "         radikalisme       0.73      0.68      0.70       302\n",
      "pencemaran_nama_baik       0.64      0.77      0.70       508\n",
      "\n",
      "           micro avg       0.69      0.74      0.72      1401\n",
      "           macro avg       0.70      0.73      0.71      1401\n",
      "        weighted avg       0.71      0.74      0.72      1401\n",
      "         samples avg       0.41      0.43      0.41      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9017807684838774\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 28.04719638824463 seconds\n",
      "\n",
      "Fold 4 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3062, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2085, Accuracy: 0.7827, F1 Micro: 0.0142, F1 Macro: 0.0142\n",
      "Epoch 3/10, Train Loss: 0.1733, Accuracy: 0.858, F1 Micro: 0.6053, F1 Macro: 0.5829\n",
      "Epoch 4/10, Train Loss: 0.1171, Accuracy: 0.8605, F1 Micro: 0.6821, F1 Macro: 0.6765\n",
      "Epoch 5/10, Train Loss: 0.0943, Accuracy: 0.8753, F1 Micro: 0.6954, F1 Macro: 0.6863\n",
      "Epoch 6/10, Train Loss: 0.0759, Accuracy: 0.8659, F1 Micro: 0.72, F1 Macro: 0.7222\n",
      "Epoch 7/10, Train Loss: 0.06, Accuracy: 0.8697, F1 Micro: 0.7057, F1 Macro: 0.6999\n",
      "Epoch 8/10, Train Loss: 0.0435, Accuracy: 0.8753, F1 Micro: 0.7174, F1 Macro: 0.7187\n",
      "Epoch 9/10, Train Loss: 0.0295, Accuracy: 0.8689, F1 Micro: 0.7234, F1 Macro: 0.7263\n",
      "Epoch 10/10, Train Loss: 0.0232, Accuracy: 0.8723, F1 Micro: 0.7218, F1 Macro: 0.7254\n",
      "Best result for 1970 samples: F1 Micro: 0.7234\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.87      0.88       342\n",
      "                sara       0.53      0.71      0.61       249\n",
      "         radikalisme       0.74      0.72      0.73       302\n",
      "pencemaran_nama_baik       0.61      0.79      0.69       508\n",
      "\n",
      "           micro avg       0.67      0.78      0.72      1401\n",
      "           macro avg       0.69      0.78      0.73      1401\n",
      "        weighted avg       0.69      0.78      0.73      1401\n",
      "         samples avg       0.44      0.46      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.6812044560909278\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 25.297968864440918 seconds\n",
      "\n",
      "Fold 4 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2973, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2201, Accuracy: 0.8092, F1 Micro: 0.2326, F1 Macro: 0.195\n",
      "Epoch 3/10, Train Loss: 0.1693, Accuracy: 0.87, F1 Micro: 0.6717, F1 Macro: 0.6543\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.8748, F1 Micro: 0.7256, F1 Macro: 0.7253\n",
      "Epoch 5/10, Train Loss: 0.1071, Accuracy: 0.8648, F1 Micro: 0.7308, F1 Macro: 0.7351\n",
      "Epoch 6/10, Train Loss: 0.0749, Accuracy: 0.8639, F1 Micro: 0.7321, F1 Macro: 0.7374\n",
      "Epoch 7/10, Train Loss: 0.0579, Accuracy: 0.8736, F1 Micro: 0.7331, F1 Macro: 0.7375\n",
      "Epoch 8/10, Train Loss: 0.0386, Accuracy: 0.8817, F1 Micro: 0.7317, F1 Macro: 0.7357\n",
      "Epoch 9/10, Train Loss: 0.0301, Accuracy: 0.8778, F1 Micro: 0.7379, F1 Macro: 0.742\n",
      "Epoch 10/10, Train Loss: 0.0211, Accuracy: 0.8859, F1 Micro: 0.742, F1 Macro: 0.742\n",
      "Best result for 2395 samples: F1 Micro: 0.742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       342\n",
      "                sara       0.61      0.63      0.62       249\n",
      "         radikalisme       0.78      0.75      0.77       302\n",
      "pencemaran_nama_baik       0.67      0.73      0.70       508\n",
      "\n",
      "           micro avg       0.73      0.75      0.74      1401\n",
      "           macro avg       0.74      0.74      0.74      1401\n",
      "        weighted avg       0.74      0.75      0.74      1401\n",
      "         samples avg       0.44      0.44      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.6925312459468843\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 23.146737337112427 seconds\n",
      "\n",
      "Fold 4 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3068, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2298, Accuracy: 0.8567, F1 Micro: 0.6328, F1 Macro: 0.6113\n",
      "Epoch 3/10, Train Loss: 0.1703, Accuracy: 0.8698, F1 Micro: 0.7129, F1 Macro: 0.7042\n",
      "Epoch 4/10, Train Loss: 0.1402, Accuracy: 0.8803, F1 Micro: 0.7307, F1 Macro: 0.7283\n",
      "Epoch 5/10, Train Loss: 0.0964, Accuracy: 0.8748, F1 Micro: 0.741, F1 Macro: 0.7429\n",
      "Epoch 6/10, Train Loss: 0.0733, Accuracy: 0.8822, F1 Micro: 0.7456, F1 Macro: 0.7474\n",
      "Epoch 7/10, Train Loss: 0.0519, Accuracy: 0.8838, F1 Micro: 0.7461, F1 Macro: 0.7436\n",
      "Epoch 8/10, Train Loss: 0.041, Accuracy: 0.8823, F1 Micro: 0.7436, F1 Macro: 0.7429\n",
      "Epoch 9/10, Train Loss: 0.0271, Accuracy: 0.8827, F1 Micro: 0.7422, F1 Macro: 0.7419\n",
      "Epoch 10/10, Train Loss: 0.0238, Accuracy: 0.8852, F1 Micro: 0.7484, F1 Macro: 0.7459\n",
      "Best result for 2778 samples: F1 Micro: 0.7484\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.88       342\n",
      "                sara       0.59      0.68      0.63       249\n",
      "         radikalisme       0.77      0.72      0.74       302\n",
      "pencemaran_nama_baik       0.66      0.80      0.73       508\n",
      "\n",
      "           micro avg       0.72      0.78      0.75      1401\n",
      "           macro avg       0.73      0.77      0.75      1401\n",
      "        weighted avg       0.73      0.78      0.75      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.2028186321258545\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 20.858704566955566 seconds\n",
      "\n",
      "Fold 4 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3304, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2426, Accuracy: 0.8492, F1 Micro: 0.5134, F1 Macro: 0.4608\n",
      "Epoch 3/10, Train Loss: 0.1904, Accuracy: 0.8873, F1 Micro: 0.7437, F1 Macro: 0.7357\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.8917, F1 Micro: 0.7556, F1 Macro: 0.7472\n",
      "Epoch 5/10, Train Loss: 0.1061, Accuracy: 0.8756, F1 Micro: 0.7468, F1 Macro: 0.7503\n",
      "Epoch 6/10, Train Loss: 0.0775, Accuracy: 0.8848, F1 Micro: 0.758, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.8847, F1 Micro: 0.7551, F1 Macro: 0.7558\n",
      "Epoch 8/10, Train Loss: 0.0393, Accuracy: 0.8842, F1 Micro: 0.7536, F1 Macro: 0.7537\n",
      "Epoch 9/10, Train Loss: 0.0268, Accuracy: 0.892, F1 Micro: 0.7628, F1 Macro: 0.7633\n",
      "Epoch 10/10, Train Loss: 0.0229, Accuracy: 0.888, F1 Micro: 0.7592, F1 Macro: 0.7606\n",
      "Best result for 3123 samples: F1 Micro: 0.7628\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       342\n",
      "                sara       0.59      0.71      0.65       249\n",
      "         radikalisme       0.75      0.79      0.77       302\n",
      "pencemaran_nama_baik       0.69      0.78      0.73       508\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1401\n",
      "           macro avg       0.74      0.79      0.76      1401\n",
      "        weighted avg       0.75      0.79      0.77      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.08930927515029907\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 18.651398181915283 seconds\n",
      "\n",
      "Fold 4 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3501, Accuracy: 0.7895, F1 Micro: 0.0868, F1 Macro: 0.0655\n",
      "Epoch 2/10, Train Loss: 0.2458, Accuracy: 0.8702, F1 Micro: 0.7083, F1 Macro: 0.7057\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.8911, F1 Micro: 0.7604, F1 Macro: 0.7589\n",
      "Epoch 4/10, Train Loss: 0.1428, Accuracy: 0.8797, F1 Micro: 0.7565, F1 Macro: 0.7625\n",
      "Epoch 5/10, Train Loss: 0.1, Accuracy: 0.8777, F1 Micro: 0.7531, F1 Macro: 0.7571\n",
      "Epoch 6/10, Train Loss: 0.0763, Accuracy: 0.8923, F1 Micro: 0.7678, F1 Macro: 0.7687\n",
      "Epoch 7/10, Train Loss: 0.059, Accuracy: 0.8889, F1 Micro: 0.7618, F1 Macro: 0.7648\n",
      "Epoch 8/10, Train Loss: 0.0374, Accuracy: 0.883, F1 Micro: 0.7594, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.0324, Accuracy: 0.8881, F1 Micro: 0.7621, F1 Macro: 0.7604\n",
      "Epoch 10/10, Train Loss: 0.0226, Accuracy: 0.8938, F1 Micro: 0.7684, F1 Macro: 0.7705\n",
      "Best result for 3433 samples: F1 Micro: 0.7684\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       342\n",
      "                sara       0.61      0.75      0.67       249\n",
      "         radikalisme       0.75      0.81      0.78       302\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       508\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1401\n",
      "           macro avg       0.75      0.80      0.77      1401\n",
      "        weighted avg       0.75      0.81      0.77      1401\n",
      "         samples avg       0.46      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.02610146999359131\n",
      "Samples above threshold: 281\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.3044753074646 seconds\n",
      "\n",
      "Fold 4 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.353, Accuracy: 0.8236, F1 Micro: 0.3867, F1 Macro: 0.3606\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.8828, F1 Micro: 0.7433, F1 Macro: 0.7448\n",
      "Epoch 3/10, Train Loss: 0.2021, Accuracy: 0.8873, F1 Micro: 0.754, F1 Macro: 0.7559\n",
      "Epoch 4/10, Train Loss: 0.1526, Accuracy: 0.8953, F1 Micro: 0.7666, F1 Macro: 0.7635\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.8902, F1 Micro: 0.7595, F1 Macro: 0.7584\n",
      "Epoch 6/10, Train Loss: 0.0834, Accuracy: 0.8886, F1 Micro: 0.764, F1 Macro: 0.7653\n",
      "Epoch 7/10, Train Loss: 0.0588, Accuracy: 0.89, F1 Micro: 0.7638, F1 Macro: 0.7658\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.8928, F1 Micro: 0.7613, F1 Macro: 0.7628\n",
      "Epoch 9/10, Train Loss: 0.0324, Accuracy: 0.8959, F1 Micro: 0.7689, F1 Macro: 0.7703\n",
      "Epoch 10/10, Train Loss: 0.0271, Accuracy: 0.8922, F1 Micro: 0.7675, F1 Macro: 0.7693\n",
      "Best result for 3712 samples: F1 Micro: 0.7689\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       342\n",
      "                sara       0.63      0.71      0.67       249\n",
      "         radikalisme       0.77      0.78      0.78       302\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       508\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1401\n",
      "           macro avg       0.76      0.79      0.77      1401\n",
      "        weighted avg       0.76      0.79      0.77      1401\n",
      "         samples avg       0.45      0.46      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.02663809061050416\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 174\n",
      "Sampling duration: 15.553979396820068 seconds\n",
      "\n",
      "Fold 4 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3518, Accuracy: 0.8541, F1 Micro: 0.5981, F1 Macro: 0.571\n",
      "Epoch 2/10, Train Loss: 0.2475, Accuracy: 0.8766, F1 Micro: 0.7074, F1 Macro: 0.6996\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.8927, F1 Micro: 0.7617, F1 Macro: 0.7601\n",
      "Epoch 4/10, Train Loss: 0.1579, Accuracy: 0.8886, F1 Micro: 0.7654, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.8914, F1 Micro: 0.7656, F1 Macro: 0.7662\n",
      "Epoch 6/10, Train Loss: 0.0835, Accuracy: 0.8884, F1 Micro: 0.7601, F1 Macro: 0.7623\n",
      "Epoch 7/10, Train Loss: 0.0595, Accuracy: 0.8911, F1 Micro: 0.7685, F1 Macro: 0.7706\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8906, F1 Micro: 0.7708, F1 Macro: 0.7748\n",
      "Epoch 9/10, Train Loss: 0.0334, Accuracy: 0.8928, F1 Micro: 0.7657, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0235, Accuracy: 0.8908, F1 Micro: 0.7674, F1 Macro: 0.7708\n",
      "Best result for 3886 samples: F1 Micro: 0.7708\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       342\n",
      "                sara       0.61      0.77      0.68       249\n",
      "         radikalisme       0.75      0.82      0.78       302\n",
      "pencemaran_nama_baik       0.64      0.86      0.73       508\n",
      "\n",
      "           micro avg       0.71      0.84      0.77      1401\n",
      "           macro avg       0.73      0.83      0.77      1401\n",
      "        weighted avg       0.73      0.84      0.78      1401\n",
      "         samples avg       0.46      0.48      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.010961675643920902\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.453217029571533 seconds\n",
      "\n",
      "Fold 4 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3587, Accuracy: 0.8523, F1 Micro: 0.5277, F1 Macro: 0.4615\n",
      "Epoch 2/10, Train Loss: 0.2531, Accuracy: 0.8856, F1 Micro: 0.7444, F1 Macro: 0.7329\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.89, F1 Micro: 0.753, F1 Macro: 0.7537\n",
      "Epoch 4/10, Train Loss: 0.155, Accuracy: 0.8952, F1 Micro: 0.7658, F1 Macro: 0.7658\n",
      "Epoch 5/10, Train Loss: 0.1055, Accuracy: 0.8928, F1 Micro: 0.7742, F1 Macro: 0.7741\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.8988, F1 Micro: 0.7723, F1 Macro: 0.7695\n",
      "Epoch 7/10, Train Loss: 0.0618, Accuracy: 0.8983, F1 Micro: 0.7699, F1 Macro: 0.77\n",
      "Epoch 8/10, Train Loss: 0.0409, Accuracy: 0.8875, F1 Micro: 0.7646, F1 Macro: 0.7649\n",
      "Epoch 9/10, Train Loss: 0.0293, Accuracy: 0.8981, F1 Micro: 0.7739, F1 Macro: 0.7732\n",
      "Epoch 10/10, Train Loss: 0.025, Accuracy: 0.8992, F1 Micro: 0.7715, F1 Macro: 0.7703\n",
      "Best result for 4120 samples: F1 Micro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       342\n",
      "                sara       0.59      0.77      0.67       249\n",
      "         radikalisme       0.75      0.82      0.78       302\n",
      "pencemaran_nama_baik       0.68      0.84      0.75       508\n",
      "\n",
      "           micro avg       0.72      0.84      0.77      1401\n",
      "           macro avg       0.73      0.83      0.77      1401\n",
      "        weighted avg       0.73      0.84      0.78      1401\n",
      "         samples avg       0.45      0.48      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.03214991092681885\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.067571640014648 seconds\n",
      "\n",
      "Fold 4 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3584, Accuracy: 0.868, F1 Micro: 0.6533, F1 Macro: 0.6186\n",
      "Epoch 2/10, Train Loss: 0.2536, Accuracy: 0.8847, F1 Micro: 0.7076, F1 Macro: 0.6964\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.8947, F1 Micro: 0.7663, F1 Macro: 0.7679\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.8969, F1 Micro: 0.7679, F1 Macro: 0.7675\n",
      "Epoch 5/10, Train Loss: 0.1122, Accuracy: 0.8902, F1 Micro: 0.7662, F1 Macro: 0.7684\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.8959, F1 Micro: 0.775, F1 Macro: 0.776\n",
      "Epoch 7/10, Train Loss: 0.0608, Accuracy: 0.8994, F1 Micro: 0.7737, F1 Macro: 0.7718\n",
      "Epoch 8/10, Train Loss: 0.0439, Accuracy: 0.8973, F1 Micro: 0.7712, F1 Macro: 0.7704\n",
      "Epoch 9/10, Train Loss: 0.0329, Accuracy: 0.8988, F1 Micro: 0.7686, F1 Macro: 0.7686\n",
      "Epoch 10/10, Train Loss: 0.0253, Accuracy: 0.8956, F1 Micro: 0.7778, F1 Macro: 0.7766\n",
      "Best result for 4330 samples: F1 Micro: 0.7778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       342\n",
      "                sara       0.60      0.74      0.67       249\n",
      "         radikalisme       0.73      0.85      0.79       302\n",
      "pencemaran_nama_baik       0.69      0.83      0.76       508\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1401\n",
      "           macro avg       0.73      0.83      0.78      1401\n",
      "        weighted avg       0.74      0.83      0.78      1401\n",
      "         samples avg       0.47      0.48      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.003278422355651856\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.73294711112976 seconds\n",
      "\n",
      "Fold 4 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3688, Accuracy: 0.8677, F1 Micro: 0.7002, F1 Macro: 0.6972\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.8961, F1 Micro: 0.7337, F1 Macro: 0.7287\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.8934, F1 Micro: 0.7569, F1 Macro: 0.7604\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.8931, F1 Micro: 0.7683, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.8953, F1 Micro: 0.7779, F1 Macro: 0.7783\n",
      "Epoch 6/10, Train Loss: 0.0902, Accuracy: 0.8939, F1 Micro: 0.776, F1 Macro: 0.774\n",
      "Epoch 7/10, Train Loss: 0.0633, Accuracy: 0.9022, F1 Micro: 0.779, F1 Macro: 0.7786\n",
      "Epoch 8/10, Train Loss: 0.0443, Accuracy: 0.8989, F1 Micro: 0.7737, F1 Macro: 0.7755\n",
      "Epoch 9/10, Train Loss: 0.035, Accuracy: 0.8978, F1 Micro: 0.7757, F1 Macro: 0.7772\n",
      "Epoch 10/10, Train Loss: 0.0256, Accuracy: 0.8992, F1 Micro: 0.7784, F1 Macro: 0.7795\n",
      "Best result for 4530 samples: F1 Micro: 0.779\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       342\n",
      "                sara       0.63      0.71      0.67       249\n",
      "         radikalisme       0.78      0.80      0.79       302\n",
      "pencemaran_nama_baik       0.73      0.76      0.75       508\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1401\n",
      "           macro avg       0.77      0.79      0.78      1401\n",
      "        weighted avg       0.78      0.79      0.78      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.013792443275451677\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 10.681205749511719 seconds\n",
      "\n",
      "Fold 4 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3733, Accuracy: 0.8731, F1 Micro: 0.6672, F1 Macro: 0.6528\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.8947, F1 Micro: 0.7551, F1 Macro: 0.7507\n",
      "Epoch 3/10, Train Loss: 0.2076, Accuracy: 0.897, F1 Micro: 0.7765, F1 Macro: 0.7794\n",
      "Epoch 4/10, Train Loss: 0.1638, Accuracy: 0.9002, F1 Micro: 0.7796, F1 Macro: 0.7798\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9022, F1 Micro: 0.7761, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.8995, F1 Micro: 0.7823, F1 Macro: 0.7836\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.8983, F1 Micro: 0.7819, F1 Macro: 0.7837\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8977, F1 Micro: 0.7779, F1 Macro: 0.7807\n",
      "Epoch 9/10, Train Loss: 0.0348, Accuracy: 0.8973, F1 Micro: 0.7737, F1 Macro: 0.7759\n",
      "Epoch 10/10, Train Loss: 0.0282, Accuracy: 0.8953, F1 Micro: 0.7659, F1 Macro: 0.7684\n",
      "Best result for 4663 samples: F1 Micro: 0.7823\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       342\n",
      "                sara       0.63      0.75      0.69       249\n",
      "         radikalisme       0.76      0.83      0.80       302\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       508\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1401\n",
      "           macro avg       0.75      0.82      0.78      1401\n",
      "        weighted avg       0.75      0.82      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.01227426528930664\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.85798168182373 seconds\n",
      "\n",
      "Fold 4 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.8683, F1 Micro: 0.6818, F1 Macro: 0.6846\n",
      "Epoch 2/10, Train Loss: 0.2691, Accuracy: 0.8955, F1 Micro: 0.7612, F1 Macro: 0.7611\n",
      "Epoch 3/10, Train Loss: 0.2127, Accuracy: 0.8856, F1 Micro: 0.7648, F1 Macro: 0.7666\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.9056, F1 Micro: 0.7869, F1 Macro: 0.787\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9019, F1 Micro: 0.7738, F1 Macro: 0.7734\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9011, F1 Micro: 0.777, F1 Macro: 0.7754\n",
      "Epoch 7/10, Train Loss: 0.0653, Accuracy: 0.9013, F1 Micro: 0.7789, F1 Macro: 0.778\n",
      "Epoch 8/10, Train Loss: 0.0533, Accuracy: 0.8991, F1 Micro: 0.7744, F1 Macro: 0.7766\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.8964, F1 Micro: 0.7783, F1 Macro: 0.779\n",
      "Epoch 10/10, Train Loss: 0.0292, Accuracy: 0.9, F1 Micro: 0.7775, F1 Macro: 0.778\n",
      "Best result for 4863 samples: F1 Micro: 0.7869\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       342\n",
      "                sara       0.67      0.72      0.70       249\n",
      "         radikalisme       0.78      0.81      0.79       302\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       508\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1401\n",
      "           macro avg       0.78      0.80      0.79      1401\n",
      "        weighted avg       0.78      0.80      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.04972231388092041\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.77255630493164 seconds\n",
      "\n",
      "Fold 4 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3768, Accuracy: 0.8811, F1 Micro: 0.7297, F1 Macro: 0.7265\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.8939, F1 Micro: 0.7642, F1 Macro: 0.7647\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.8988, F1 Micro: 0.7764, F1 Macro: 0.7752\n",
      "Epoch 4/10, Train Loss: 0.1744, Accuracy: 0.9003, F1 Micro: 0.7768, F1 Macro: 0.7731\n",
      "Epoch 5/10, Train Loss: 0.1277, Accuracy: 0.9019, F1 Micro: 0.7734, F1 Macro: 0.7709\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.8889, F1 Micro: 0.7682, F1 Macro: 0.7726\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.8905, F1 Micro: 0.7726, F1 Macro: 0.774\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.8989, F1 Micro: 0.7713, F1 Macro: 0.7705\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.8959, F1 Micro: 0.7753, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9038, F1 Micro: 0.7778, F1 Macro: 0.7767\n",
      "Best result for 5063 samples: F1 Micro: 0.7778\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.65      0.67      0.66       249\n",
      "         radikalisme       0.79      0.84      0.81       302\n",
      "pencemaran_nama_baik       0.76      0.69      0.73       508\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1401\n",
      "           macro avg       0.78      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.77      0.78      1401\n",
      "         samples avg       0.45      0.44      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.005292057991027832\n",
      "Samples above threshold: 178\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.705599546432495 seconds\n",
      "\n",
      "Fold 4 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3689, Accuracy: 0.8759, F1 Micro: 0.7182, F1 Macro: 0.7138\n",
      "Epoch 2/10, Train Loss: 0.2517, Accuracy: 0.8991, F1 Micro: 0.7625, F1 Macro: 0.7595\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.8933, F1 Micro: 0.7758, F1 Macro: 0.7775\n",
      "Epoch 4/10, Train Loss: 0.1627, Accuracy: 0.9017, F1 Micro: 0.785, F1 Macro: 0.7841\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9005, F1 Micro: 0.7849, F1 Macro: 0.7839\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9067, F1 Micro: 0.7878, F1 Macro: 0.7855\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9003, F1 Micro: 0.7805, F1 Macro: 0.7783\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9041, F1 Micro: 0.7835, F1 Macro: 0.7812\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9062, F1 Micro: 0.7828, F1 Macro: 0.7815\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9031, F1 Micro: 0.7814, F1 Macro: 0.7789\n",
      "Best result for 5263 samples: F1 Micro: 0.7878\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       342\n",
      "                sara       0.71      0.65      0.68       249\n",
      "         radikalisme       0.80      0.81      0.80       302\n",
      "pencemaran_nama_baik       0.72      0.79      0.76       508\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1401\n",
      "           macro avg       0.79      0.78      0.79      1401\n",
      "        weighted avg       0.79      0.79      0.79      1401\n",
      "         samples avg       0.46      0.45      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.010777473449707031\n",
      "Samples above threshold: 101\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.568714618682861 seconds\n",
      "\n",
      "Fold 4 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3701, Accuracy: 0.8845, F1 Micro: 0.7321, F1 Macro: 0.7279\n",
      "Epoch 2/10, Train Loss: 0.2431, Accuracy: 0.9042, F1 Micro: 0.7744, F1 Macro: 0.771\n",
      "Epoch 3/10, Train Loss: 0.2, Accuracy: 0.9033, F1 Micro: 0.7868, F1 Macro: 0.7831\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9039, F1 Micro: 0.7837, F1 Macro: 0.7816\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9067, F1 Micro: 0.7816, F1 Macro: 0.7798\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9042, F1 Micro: 0.772, F1 Macro: 0.769\n",
      "Epoch 7/10, Train Loss: 0.0635, Accuracy: 0.9014, F1 Micro: 0.7703, F1 Macro: 0.7689\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.9011, F1 Micro: 0.7804, F1 Macro: 0.7801\n",
      "Epoch 9/10, Train Loss: 0.0333, Accuracy: 0.8989, F1 Micro: 0.7753, F1 Macro: 0.7776\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9009, F1 Micro: 0.7741, F1 Macro: 0.7722\n",
      "Best result for 5441 samples: F1 Micro: 0.7868\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       342\n",
      "                sara       0.65      0.69      0.67       249\n",
      "         radikalisme       0.74      0.84      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.81      0.77       508\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1401\n",
      "           macro avg       0.76      0.81      0.78      1401\n",
      "        weighted avg       0.77      0.82      0.79      1401\n",
      "         samples avg       0.46      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.05296906828880313\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.545005559921265 seconds\n",
      "\n",
      "Fold 4 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3638, Accuracy: 0.8736, F1 Micro: 0.7249, F1 Macro: 0.7299\n",
      "Epoch 2/10, Train Loss: 0.2576, Accuracy: 0.9031, F1 Micro: 0.7752, F1 Macro: 0.7726\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.9033, F1 Micro: 0.7856, F1 Macro: 0.7825\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.8928, F1 Micro: 0.7659, F1 Macro: 0.7666\n",
      "Epoch 5/10, Train Loss: 0.1212, Accuracy: 0.9045, F1 Micro: 0.782, F1 Macro: 0.7797\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9002, F1 Micro: 0.7655, F1 Macro: 0.7643\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.8961, F1 Micro: 0.7791, F1 Macro: 0.7784\n",
      "Epoch 8/10, Train Loss: 0.0503, Accuracy: 0.902, F1 Micro: 0.7753, F1 Macro: 0.7733\n",
      "Epoch 9/10, Train Loss: 0.0355, Accuracy: 0.9016, F1 Micro: 0.7811, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.7732\n",
      "Best result for 5641 samples: F1 Micro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       342\n",
      "                sara       0.71      0.65      0.68       249\n",
      "         radikalisme       0.75      0.84      0.79       302\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       508\n",
      "\n",
      "           micro avg       0.76      0.81      0.79      1401\n",
      "           macro avg       0.77      0.80      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.025520426034927386\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.413408994674683 seconds\n",
      "\n",
      "Fold 4 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3715, Accuracy: 0.8903, F1 Micro: 0.7165, F1 Macro: 0.6984\n",
      "Epoch 2/10, Train Loss: 0.2438, Accuracy: 0.9019, F1 Micro: 0.7778, F1 Macro: 0.7792\n",
      "Epoch 3/10, Train Loss: 0.2032, Accuracy: 0.8978, F1 Micro: 0.7804, F1 Macro: 0.7823\n",
      "Epoch 4/10, Train Loss: 0.1587, Accuracy: 0.9023, F1 Micro: 0.7815, F1 Macro: 0.7747\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9017, F1 Micro: 0.7675, F1 Macro: 0.7603\n",
      "Epoch 6/10, Train Loss: 0.0873, Accuracy: 0.9019, F1 Micro: 0.7662, F1 Macro: 0.7629\n",
      "Epoch 7/10, Train Loss: 0.0622, Accuracy: 0.9002, F1 Micro: 0.7746, F1 Macro: 0.772\n",
      "Epoch 8/10, Train Loss: 0.0455, Accuracy: 0.8966, F1 Micro: 0.7587, F1 Macro: 0.7542\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9019, F1 Micro: 0.7792, F1 Macro: 0.7765\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9003, F1 Micro: 0.7812, F1 Macro: 0.7784\n",
      "Best result for 5841 samples: F1 Micro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.91       342\n",
      "                sara       0.69      0.60      0.64       249\n",
      "         radikalisme       0.78      0.82      0.80       302\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       508\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1401\n",
      "           macro avg       0.77      0.78      0.77      1401\n",
      "        weighted avg       0.77      0.80      0.78      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.005506139993667602\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.2575395107269287 seconds\n",
      "\n",
      "Fold 4 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3538, Accuracy: 0.89, F1 Micro: 0.7371, F1 Macro: 0.735\n",
      "Epoch 2/10, Train Loss: 0.2368, Accuracy: 0.9027, F1 Micro: 0.7766, F1 Macro: 0.7763\n",
      "Epoch 3/10, Train Loss: 0.2, Accuracy: 0.9069, F1 Micro: 0.7856, F1 Macro: 0.7817\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.9006, F1 Micro: 0.7847, F1 Macro: 0.7839\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9009, F1 Micro: 0.7708, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.083, Accuracy: 0.9031, F1 Micro: 0.7758, F1 Macro: 0.7692\n",
      "Epoch 7/10, Train Loss: 0.0633, Accuracy: 0.8995, F1 Micro: 0.7743, F1 Macro: 0.7766\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.8956, F1 Micro: 0.7766, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0339, Accuracy: 0.895, F1 Micro: 0.7686, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9047, F1 Micro: 0.779, F1 Macro: 0.7764\n",
      "Best result for 6041 samples: F1 Micro: 0.7856\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       342\n",
      "                sara       0.73      0.62      0.67       249\n",
      "         radikalisme       0.80      0.79      0.80       302\n",
      "pencemaran_nama_baik       0.73      0.77      0.75       508\n",
      "\n",
      "           micro avg       0.79      0.78      0.79      1401\n",
      "           macro avg       0.79      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.78      0.78      1401\n",
      "         samples avg       0.46      0.45      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.010902047157287598\n",
      "Samples above threshold: 21\n",
      "Acquired samples: 178\n",
      "Sampling duration: 2.0135724544525146 seconds\n",
      "\n",
      "Fold 4 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3474, Accuracy: 0.8908, F1 Micro: 0.7205, F1 Macro: 0.7075\n",
      "Epoch 2/10, Train Loss: 0.2317, Accuracy: 0.8958, F1 Micro: 0.7746, F1 Macro: 0.7742\n",
      "Epoch 3/10, Train Loss: 0.1894, Accuracy: 0.8998, F1 Micro: 0.7806, F1 Macro: 0.7797\n",
      "Epoch 4/10, Train Loss: 0.1492, Accuracy: 0.9045, F1 Micro: 0.7822, F1 Macro: 0.7766\n",
      "Epoch 5/10, Train Loss: 0.1106, Accuracy: 0.8989, F1 Micro: 0.7818, F1 Macro: 0.7835\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.8978, F1 Micro: 0.7723, F1 Macro: 0.7702\n",
      "Epoch 7/10, Train Loss: 0.0602, Accuracy: 0.9048, F1 Micro: 0.7827, F1 Macro: 0.7798\n",
      "Epoch 8/10, Train Loss: 0.0417, Accuracy: 0.9039, F1 Micro: 0.7804, F1 Macro: 0.7784\n",
      "Epoch 9/10, Train Loss: 0.0328, Accuracy: 0.9023, F1 Micro: 0.7788, F1 Macro: 0.7763\n",
      "Epoch 10/10, Train Loss: 0.0254, Accuracy: 0.8989, F1 Micro: 0.7812, F1 Macro: 0.781\n",
      "Best result for 6219 samples: F1 Micro: 0.7827\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       342\n",
      "                sara       0.67      0.65      0.66       249\n",
      "         radikalisme       0.83      0.78      0.81       302\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       508\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1401\n",
      "           macro avg       0.79      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 3807.82 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4673, Accuracy: 0.7939, F1 Micro: 0.0449, F1 Macro: 0.0404\n",
      "Epoch 3/10, Train Loss: 0.421, Accuracy: 0.8194, F1 Micro: 0.2561, F1 Macro: 0.1786\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8313, F1 Micro: 0.3933, F1 Macro: 0.3032\n",
      "Epoch 5/10, Train Loss: 0.3375, Accuracy: 0.8416, F1 Micro: 0.4483, F1 Macro: 0.3709\n",
      "Epoch 6/10, Train Loss: 0.2901, Accuracy: 0.8577, F1 Micro: 0.5902, F1 Macro: 0.5609\n",
      "Epoch 7/10, Train Loss: 0.2547, Accuracy: 0.8578, F1 Micro: 0.5852, F1 Macro: 0.5546\n",
      "Epoch 8/10, Train Loss: 0.2265, Accuracy: 0.867, F1 Micro: 0.6411, F1 Macro: 0.6284\n",
      "Epoch 9/10, Train Loss: 0.174, Accuracy: 0.8655, F1 Micro: 0.6682, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.1497, Accuracy: 0.8669, F1 Micro: 0.6611, F1 Macro: 0.6529\n",
      "Best result for 388 samples: F1 Micro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.82      0.84       353\n",
      "                sara       0.54      0.56      0.55       239\n",
      "         radikalisme       0.69      0.58      0.63       273\n",
      "pencemaran_nama_baik       0.66      0.59      0.62       485\n",
      "\n",
      "           micro avg       0.70      0.64      0.67      1350\n",
      "           macro avg       0.69      0.64      0.66      1350\n",
      "        weighted avg       0.70      0.64      0.67      1350\n",
      "         samples avg       0.36      0.36      0.35      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9149346426129341\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 35.242043256759644 seconds\n",
      "\n",
      "Fold 5 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3997, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.3134, Accuracy: 0.7975, F1 Micro: 0.0769, F1 Macro: 0.0663\n",
      "Epoch 3/10, Train Loss: 0.2517, Accuracy: 0.8409, F1 Micro: 0.4357, F1 Macro: 0.3451\n",
      "Epoch 4/10, Train Loss: 0.2251, Accuracy: 0.862, F1 Micro: 0.6341, F1 Macro: 0.6227\n",
      "Epoch 5/10, Train Loss: 0.1647, Accuracy: 0.8625, F1 Micro: 0.5982, F1 Macro: 0.5802\n",
      "Epoch 6/10, Train Loss: 0.144, Accuracy: 0.8709, F1 Micro: 0.6852, F1 Macro: 0.6776\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.8669, F1 Micro: 0.6819, F1 Macro: 0.6779\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.8684, F1 Micro: 0.6287, F1 Macro: 0.6027\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.8672, F1 Micro: 0.6999, F1 Macro: 0.6942\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.8659, F1 Micro: 0.6815, F1 Macro: 0.6766\n",
      "Best result for 972 samples: F1 Micro: 0.6999\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.83      0.90      0.86       353\n",
      "                sara       0.52      0.62      0.56       239\n",
      "         radikalisme       0.65      0.76      0.70       273\n",
      "pencemaran_nama_baik       0.64      0.66      0.65       485\n",
      "\n",
      "           micro avg       0.67      0.73      0.70      1350\n",
      "           macro avg       0.66      0.73      0.69      1350\n",
      "        weighted avg       0.67      0.73      0.70      1350\n",
      "         samples avg       0.40      0.42      0.40      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.9340130925178528\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 31.38499641418457 seconds\n",
      "\n",
      "Fold 5 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3368, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.8155, F1 Micro: 0.2235, F1 Macro: 0.1622\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.8531, F1 Micro: 0.5927, F1 Macro: 0.5461\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.8666, F1 Micro: 0.6768, F1 Macro: 0.6688\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.8644, F1 Micro: 0.6866, F1 Macro: 0.6843\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.8669, F1 Micro: 0.6423, F1 Macro: 0.628\n",
      "Epoch 7/10, Train Loss: 0.0722, Accuracy: 0.8689, F1 Micro: 0.6981, F1 Macro: 0.6928\n",
      "Epoch 8/10, Train Loss: 0.0546, Accuracy: 0.8705, F1 Micro: 0.6808, F1 Macro: 0.6729\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.8736, F1 Micro: 0.6653, F1 Macro: 0.6414\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.8733, F1 Micro: 0.6788, F1 Macro: 0.6645\n",
      "Best result for 1497 samples: F1 Micro: 0.6981\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.88       353\n",
      "                sara       0.51      0.61      0.56       239\n",
      "         radikalisme       0.63      0.75      0.68       273\n",
      "pencemaran_nama_baik       0.66      0.64      0.65       485\n",
      "\n",
      "           micro avg       0.68      0.72      0.70      1350\n",
      "           macro avg       0.67      0.72      0.69      1350\n",
      "        weighted avg       0.69      0.72      0.70      1350\n",
      "         samples avg       0.39      0.40      0.38      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.8931724652647973\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 28.310474395751953 seconds\n",
      "\n",
      "Fold 5 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3078, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2259, Accuracy: 0.8236, F1 Micro: 0.2859, F1 Macro: 0.194\n",
      "Epoch 3/10, Train Loss: 0.1786, Accuracy: 0.8675, F1 Micro: 0.6672, F1 Macro: 0.6553\n",
      "Epoch 4/10, Train Loss: 0.1344, Accuracy: 0.8747, F1 Micro: 0.6737, F1 Macro: 0.6563\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.8641, F1 Micro: 0.7148, F1 Macro: 0.7129\n",
      "Epoch 6/10, Train Loss: 0.0807, Accuracy: 0.87, F1 Micro: 0.7135, F1 Macro: 0.7061\n",
      "Epoch 7/10, Train Loss: 0.06, Accuracy: 0.8781, F1 Micro: 0.7007, F1 Macro: 0.6878\n",
      "Epoch 8/10, Train Loss: 0.0469, Accuracy: 0.8767, F1 Micro: 0.7081, F1 Macro: 0.6997\n",
      "Epoch 9/10, Train Loss: 0.0307, Accuracy: 0.8731, F1 Micro: 0.7112, F1 Macro: 0.7075\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.8725, F1 Micro: 0.6987, F1 Macro: 0.6945\n",
      "Best result for 1970 samples: F1 Micro: 0.7148\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.90      0.88       353\n",
      "                sara       0.48      0.73      0.58       239\n",
      "         radikalisme       0.67      0.73      0.70       273\n",
      "pencemaran_nama_baik       0.59      0.82      0.69       485\n",
      "\n",
      "           micro avg       0.64      0.81      0.71      1350\n",
      "           macro avg       0.65      0.80      0.71      1350\n",
      "        weighted avg       0.66      0.81      0.72      1350\n",
      "         samples avg       0.41      0.45      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.5831565201282503\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 25.611511945724487 seconds\n",
      "\n",
      "Fold 5 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2988, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2477, Accuracy: 0.8361, F1 Micro: 0.3819, F1 Macro: 0.3101\n",
      "Epoch 3/10, Train Loss: 0.1817, Accuracy: 0.8562, F1 Micro: 0.5233, F1 Macro: 0.4745\n",
      "Epoch 4/10, Train Loss: 0.1522, Accuracy: 0.8786, F1 Micro: 0.6783, F1 Macro: 0.6648\n",
      "Epoch 5/10, Train Loss: 0.1023, Accuracy: 0.8802, F1 Micro: 0.7102, F1 Macro: 0.7023\n",
      "Epoch 6/10, Train Loss: 0.0798, Accuracy: 0.8652, F1 Micro: 0.7179, F1 Macro: 0.719\n",
      "Epoch 7/10, Train Loss: 0.06, Accuracy: 0.8652, F1 Micro: 0.719, F1 Macro: 0.7217\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.8788, F1 Micro: 0.7195, F1 Macro: 0.7136\n",
      "Epoch 9/10, Train Loss: 0.0349, Accuracy: 0.878, F1 Micro: 0.7192, F1 Macro: 0.7143\n",
      "Epoch 10/10, Train Loss: 0.0305, Accuracy: 0.8778, F1 Micro: 0.7148, F1 Macro: 0.7091\n",
      "Best result for 2395 samples: F1 Micro: 0.7195\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       353\n",
      "                sara       0.55      0.57      0.56       239\n",
      "         radikalisme       0.69      0.79      0.74       273\n",
      "pencemaran_nama_baik       0.65      0.69      0.67       485\n",
      "\n",
      "           micro avg       0.70      0.74      0.72      1350\n",
      "           macro avg       0.70      0.73      0.71      1350\n",
      "        weighted avg       0.71      0.74      0.72      1350\n",
      "         samples avg       0.43      0.42      0.41      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.5208918243646626\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 23.217766761779785 seconds\n",
      "\n",
      "Fold 5 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3185, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2472, Accuracy: 0.8625, F1 Micro: 0.6085, F1 Macro: 0.6078\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.8817, F1 Micro: 0.7033, F1 Macro: 0.7052\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.8881, F1 Micro: 0.7408, F1 Macro: 0.7368\n",
      "Epoch 5/10, Train Loss: 0.1064, Accuracy: 0.8847, F1 Micro: 0.7387, F1 Macro: 0.7386\n",
      "Epoch 6/10, Train Loss: 0.0781, Accuracy: 0.8853, F1 Micro: 0.7465, F1 Macro: 0.7463\n",
      "Epoch 7/10, Train Loss: 0.0541, Accuracy: 0.89, F1 Micro: 0.7235, F1 Macro: 0.719\n",
      "Epoch 8/10, Train Loss: 0.0413, Accuracy: 0.8877, F1 Micro: 0.7473, F1 Macro: 0.7462\n",
      "Epoch 9/10, Train Loss: 0.0329, Accuracy: 0.8923, F1 Micro: 0.7422, F1 Macro: 0.7391\n",
      "Epoch 10/10, Train Loss: 0.0251, Accuracy: 0.8834, F1 Micro: 0.7417, F1 Macro: 0.7437\n",
      "Best result for 2778 samples: F1 Micro: 0.7473\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       353\n",
      "                sara       0.55      0.71      0.62       239\n",
      "         radikalisme       0.69      0.84      0.76       273\n",
      "pencemaran_nama_baik       0.68      0.72      0.70       485\n",
      "\n",
      "           micro avg       0.71      0.79      0.75      1350\n",
      "           macro avg       0.71      0.79      0.75      1350\n",
      "        weighted avg       0.72      0.79      0.75      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.2086905837059021\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 21.22279644012451 seconds\n",
      "\n",
      "Fold 5 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3345, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.8612, F1 Micro: 0.5735, F1 Macro: 0.5449\n",
      "Epoch 3/10, Train Loss: 0.2012, Accuracy: 0.8797, F1 Micro: 0.6849, F1 Macro: 0.6836\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.892, F1 Micro: 0.7389, F1 Macro: 0.7353\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.8877, F1 Micro: 0.744, F1 Macro: 0.7427\n",
      "Epoch 6/10, Train Loss: 0.0816, Accuracy: 0.8823, F1 Micro: 0.7443, F1 Macro: 0.7398\n",
      "Epoch 7/10, Train Loss: 0.064, Accuracy: 0.8888, F1 Micro: 0.7493, F1 Macro: 0.7467\n",
      "Epoch 8/10, Train Loss: 0.0485, Accuracy: 0.8856, F1 Micro: 0.7522, F1 Macro: 0.7518\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.8875, F1 Micro: 0.7498, F1 Macro: 0.7456\n",
      "Epoch 10/10, Train Loss: 0.0289, Accuracy: 0.8884, F1 Micro: 0.7535, F1 Macro: 0.7521\n",
      "Best result for 3123 samples: F1 Micro: 0.7535\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.54      0.73      0.62       239\n",
      "         radikalisme       0.70      0.82      0.76       273\n",
      "pencemaran_nama_baik       0.67      0.76      0.71       485\n",
      "\n",
      "           micro avg       0.71      0.81      0.75      1350\n",
      "           macro avg       0.71      0.81      0.75      1350\n",
      "        weighted avg       0.72      0.81      0.76      1350\n",
      "         samples avg       0.44      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.03498968482017517\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 19.158056259155273 seconds\n",
      "\n",
      "Fold 5 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3431, Accuracy: 0.7961, F1 Micro: 0.0645, F1 Macro: 0.0565\n",
      "Epoch 2/10, Train Loss: 0.2517, Accuracy: 0.8681, F1 Micro: 0.6132, F1 Macro: 0.5973\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.8844, F1 Micro: 0.6987, F1 Macro: 0.696\n",
      "Epoch 4/10, Train Loss: 0.1578, Accuracy: 0.8897, F1 Micro: 0.7545, F1 Macro: 0.7561\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.8873, F1 Micro: 0.7532, F1 Macro: 0.7527\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.8873, F1 Micro: 0.758, F1 Macro: 0.759\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.8959, F1 Micro: 0.7637, F1 Macro: 0.7642\n",
      "Epoch 8/10, Train Loss: 0.0464, Accuracy: 0.8908, F1 Micro: 0.7552, F1 Macro: 0.7556\n",
      "Epoch 9/10, Train Loss: 0.033, Accuracy: 0.8936, F1 Micro: 0.7583, F1 Macro: 0.7563\n",
      "Epoch 10/10, Train Loss: 0.0249, Accuracy: 0.8964, F1 Micro: 0.7607, F1 Macro: 0.7572\n",
      "Best result for 3433 samples: F1 Micro: 0.7637\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.92       353\n",
      "                sara       0.59      0.72      0.65       239\n",
      "         radikalisme       0.75      0.79      0.77       273\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       485\n",
      "\n",
      "           micro avg       0.73      0.80      0.76      1350\n",
      "           macro avg       0.74      0.79      0.76      1350\n",
      "        weighted avg       0.75      0.80      0.77      1350\n",
      "         samples avg       0.43      0.45      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.06299769878387451\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 17.282893896102905 seconds\n",
      "\n",
      "Fold 5 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3557, Accuracy: 0.8241, F1 Micro: 0.3217, F1 Macro: 0.2928\n",
      "Epoch 2/10, Train Loss: 0.2704, Accuracy: 0.8741, F1 Micro: 0.6275, F1 Macro: 0.5538\n",
      "Epoch 3/10, Train Loss: 0.2106, Accuracy: 0.8933, F1 Micro: 0.7456, F1 Macro: 0.7398\n",
      "Epoch 4/10, Train Loss: 0.1588, Accuracy: 0.8911, F1 Micro: 0.7536, F1 Macro: 0.7551\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.8933, F1 Micro: 0.7614, F1 Macro: 0.761\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.8916, F1 Micro: 0.7587, F1 Macro: 0.7572\n",
      "Epoch 7/10, Train Loss: 0.0643, Accuracy: 0.8903, F1 Micro: 0.7586, F1 Macro: 0.7613\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.8955, F1 Micro: 0.7501, F1 Macro: 0.7455\n",
      "Epoch 9/10, Train Loss: 0.0347, Accuracy: 0.8969, F1 Micro: 0.7519, F1 Macro: 0.7464\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.8973, F1 Micro: 0.7569, F1 Macro: 0.7525\n",
      "Best result for 3712 samples: F1 Micro: 0.7614\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       353\n",
      "                sara       0.57      0.70      0.63       239\n",
      "         radikalisme       0.77      0.81      0.79       273\n",
      "pencemaran_nama_baik       0.66      0.79      0.71       485\n",
      "\n",
      "           micro avg       0.72      0.81      0.76      1350\n",
      "           macro avg       0.73      0.80      0.76      1350\n",
      "        weighted avg       0.73      0.81      0.77      1350\n",
      "         samples avg       0.44      0.45      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.06556026935577393\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 174\n",
      "Sampling duration: 15.748663902282715 seconds\n",
      "\n",
      "Fold 5 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3577, Accuracy: 0.8327, F1 Micro: 0.3651, F1 Macro: 0.3159\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.8889, F1 Micro: 0.7197, F1 Macro: 0.7095\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.8969, F1 Micro: 0.7534, F1 Macro: 0.7463\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.8913, F1 Micro: 0.7639, F1 Macro: 0.7638\n",
      "Epoch 5/10, Train Loss: 0.1212, Accuracy: 0.8872, F1 Micro: 0.7554, F1 Macro: 0.7573\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.8963, F1 Micro: 0.7596, F1 Macro: 0.758\n",
      "Epoch 7/10, Train Loss: 0.0643, Accuracy: 0.8927, F1 Micro: 0.7584, F1 Macro: 0.759\n",
      "Epoch 8/10, Train Loss: 0.0502, Accuracy: 0.8941, F1 Micro: 0.7624, F1 Macro: 0.7622\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.8978, F1 Micro: 0.7658, F1 Macro: 0.7612\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.8897, F1 Micro: 0.7602, F1 Macro: 0.7621\n",
      "Best result for 3886 samples: F1 Micro: 0.7658\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       353\n",
      "                sara       0.63      0.64      0.63       239\n",
      "         radikalisme       0.74      0.83      0.78       273\n",
      "pencemaran_nama_baik       0.68      0.75      0.71       485\n",
      "\n",
      "           micro avg       0.74      0.79      0.77      1350\n",
      "           macro avg       0.74      0.79      0.76      1350\n",
      "        weighted avg       0.74      0.79      0.77      1350\n",
      "         samples avg       0.44      0.45      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00966991186141969\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 14.639232635498047 seconds\n",
      "\n",
      "Fold 5 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3757, Accuracy: 0.8642, F1 Micro: 0.6341, F1 Macro: 0.6056\n",
      "Epoch 2/10, Train Loss: 0.2695, Accuracy: 0.8898, F1 Micro: 0.7212, F1 Macro: 0.7072\n",
      "Epoch 3/10, Train Loss: 0.2109, Accuracy: 0.9003, F1 Micro: 0.7592, F1 Macro: 0.7499\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.8981, F1 Micro: 0.7731, F1 Macro: 0.7755\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.897, F1 Micro: 0.7667, F1 Macro: 0.7665\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.8942, F1 Micro: 0.7666, F1 Macro: 0.7698\n",
      "Epoch 7/10, Train Loss: 0.0628, Accuracy: 0.8991, F1 Micro: 0.7706, F1 Macro: 0.7695\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.9005, F1 Micro: 0.7643, F1 Macro: 0.7583\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.8917, F1 Micro: 0.7626, F1 Macro: 0.7629\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.8986, F1 Micro: 0.7709, F1 Macro: 0.7682\n",
      "Best result for 4120 samples: F1 Micro: 0.7731\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       353\n",
      "                sara       0.61      0.74      0.67       239\n",
      "         radikalisme       0.80      0.80      0.80       273\n",
      "pencemaran_nama_baik       0.65      0.81      0.72       485\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1350\n",
      "           macro avg       0.74      0.82      0.78      1350\n",
      "        weighted avg       0.74      0.82      0.78      1350\n",
      "         samples avg       0.44      0.46      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.1030306100845337\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 13.232641696929932 seconds\n",
      "\n",
      "Fold 5 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3777, Accuracy: 0.873, F1 Micro: 0.6731, F1 Macro: 0.661\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.8878, F1 Micro: 0.7417, F1 Macro: 0.7456\n",
      "Epoch 3/10, Train Loss: 0.2225, Accuracy: 0.8906, F1 Micro: 0.7616, F1 Macro: 0.762\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.9047, F1 Micro: 0.7665, F1 Macro: 0.76\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.8811, F1 Micro: 0.7555, F1 Macro: 0.7565\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.9058, F1 Micro: 0.7724, F1 Macro: 0.7685\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9006, F1 Micro: 0.7697, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9058, F1 Micro: 0.7772, F1 Macro: 0.7754\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9044, F1 Micro: 0.7753, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.0308, Accuracy: 0.9006, F1 Micro: 0.7716, F1 Macro: 0.7646\n",
      "Best result for 4330 samples: F1 Micro: 0.7772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.94       353\n",
      "                sara       0.64      0.67      0.65       239\n",
      "         radikalisme       0.78      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.72      0.70      0.71       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.77      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.01174100637435913\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.129027843475342 seconds\n",
      "\n",
      "Fold 5 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3826, Accuracy: 0.8725, F1 Micro: 0.6726, F1 Macro: 0.6726\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.8902, F1 Micro: 0.7565, F1 Macro: 0.756\n",
      "Epoch 3/10, Train Loss: 0.2186, Accuracy: 0.9028, F1 Micro: 0.7653, F1 Macro: 0.764\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9017, F1 Micro: 0.7685, F1 Macro: 0.7654\n",
      "Epoch 5/10, Train Loss: 0.1336, Accuracy: 0.8945, F1 Micro: 0.7675, F1 Macro: 0.7675\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9011, F1 Micro: 0.7665, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9014, F1 Micro: 0.7715, F1 Macro: 0.7679\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9042, F1 Micro: 0.7789, F1 Macro: 0.7756\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9042, F1 Micro: 0.776, F1 Macro: 0.7709\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.9067, F1 Micro: 0.7795, F1 Macro: 0.7759\n",
      "Best result for 4530 samples: F1 Micro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       353\n",
      "                sara       0.66      0.65      0.66       239\n",
      "         radikalisme       0.80      0.79      0.79       273\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.004595649242401124\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 11.039088249206543 seconds\n",
      "\n",
      "Fold 5 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3897, Accuracy: 0.8755, F1 Micro: 0.6813, F1 Macro: 0.6768\n",
      "Epoch 2/10, Train Loss: 0.2717, Accuracy: 0.8959, F1 Micro: 0.7384, F1 Macro: 0.7249\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9, F1 Micro: 0.7438, F1 Macro: 0.7257\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.8978, F1 Micro: 0.772, F1 Macro: 0.773\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.9019, F1 Micro: 0.759, F1 Macro: 0.7508\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9038, F1 Micro: 0.7691, F1 Macro: 0.7647\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.8977, F1 Micro: 0.7698, F1 Macro: 0.7697\n",
      "Epoch 8/10, Train Loss: 0.0539, Accuracy: 0.9023, F1 Micro: 0.7726, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.903, F1 Micro: 0.7704, F1 Macro: 0.7676\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9031, F1 Micro: 0.7714, F1 Macro: 0.7635\n",
      "Best result for 4663 samples: F1 Micro: 0.7726\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       353\n",
      "                sara       0.63      0.69      0.66       239\n",
      "         radikalisme       0.80      0.80      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.73      0.71       485\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1350\n",
      "           macro avg       0.76      0.78      0.77      1350\n",
      "        weighted avg       0.76      0.79      0.77      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.006190091371536255\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.123321533203125 seconds\n",
      "\n",
      "Fold 5 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3893, Accuracy: 0.8834, F1 Micro: 0.7129, F1 Macro: 0.7122\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.8978, F1 Micro: 0.7525, F1 Macro: 0.749\n",
      "Epoch 3/10, Train Loss: 0.219, Accuracy: 0.8944, F1 Micro: 0.7683, F1 Macro: 0.7675\n",
      "Epoch 4/10, Train Loss: 0.1754, Accuracy: 0.9045, F1 Micro: 0.7662, F1 Macro: 0.7614\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9055, F1 Micro: 0.7699, F1 Macro: 0.7648\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9044, F1 Micro: 0.777, F1 Macro: 0.7752\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.9005, F1 Micro: 0.7719, F1 Macro: 0.7725\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9044, F1 Micro: 0.7732, F1 Macro: 0.7722\n",
      "Epoch 9/10, Train Loss: 0.0373, Accuracy: 0.9005, F1 Micro: 0.7695, F1 Macro: 0.7691\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9028, F1 Micro: 0.7717, F1 Macro: 0.7674\n",
      "Best result for 4863 samples: F1 Micro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.67      0.64      0.65       239\n",
      "         radikalisme       0.82      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       485\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1350\n",
      "           macro avg       0.77      0.78      0.78      1350\n",
      "        weighted avg       0.77      0.79      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.011758089065551758\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.902649641036987 seconds\n",
      "\n",
      "Fold 5 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8848, F1 Micro: 0.7361, F1 Macro: 0.7383\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.8998, F1 Micro: 0.7673, F1 Macro: 0.7629\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.9034, F1 Micro: 0.7754, F1 Macro: 0.7732\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9034, F1 Micro: 0.7761, F1 Macro: 0.7756\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.8934, F1 Micro: 0.7671, F1 Macro: 0.7674\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9045, F1 Micro: 0.7789, F1 Macro: 0.7757\n",
      "Epoch 7/10, Train Loss: 0.0701, Accuracy: 0.9064, F1 Micro: 0.7799, F1 Macro: 0.7755\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.9027, F1 Micro: 0.7781, F1 Macro: 0.7773\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9052, F1 Micro: 0.7774, F1 Macro: 0.7747\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9009, F1 Micro: 0.7777, F1 Macro: 0.7782\n",
      "Best result for 5063 samples: F1 Micro: 0.7799\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.65      0.66      0.66       239\n",
      "         radikalisme       0.83      0.76      0.79       273\n",
      "pencemaran_nama_baik       0.71      0.76      0.74       485\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1350\n",
      "           macro avg       0.78      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.79      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.004477143287658691\n",
      "Samples above threshold: 118\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.836426019668579 seconds\n",
      "\n",
      "Fold 5 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3867, Accuracy: 0.8858, F1 Micro: 0.724, F1 Macro: 0.724\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.8994, F1 Micro: 0.7687, F1 Macro: 0.7688\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9022, F1 Micro: 0.7707, F1 Macro: 0.768\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.8995, F1 Micro: 0.7649, F1 Macro: 0.7611\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9036, F1 Micro: 0.7791, F1 Macro: 0.7772\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9072, F1 Micro: 0.7676, F1 Macro: 0.7626\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9028, F1 Micro: 0.7701, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9047, F1 Micro: 0.7715, F1 Macro: 0.7679\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9005, F1 Micro: 0.7573, F1 Macro: 0.754\n",
      "Epoch 10/10, Train Loss: 0.0342, Accuracy: 0.9048, F1 Micro: 0.7685, F1 Macro: 0.7626\n",
      "Best result for 5263 samples: F1 Micro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.64      0.69      0.66       239\n",
      "         radikalisme       0.77      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.76      0.81      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.009352952241897583\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 6.859491586685181 seconds\n",
      "\n",
      "Fold 5 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.888, F1 Micro: 0.7245, F1 Macro: 0.7261\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.8986, F1 Micro: 0.7622, F1 Macro: 0.7618\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.9005, F1 Micro: 0.7789, F1 Macro: 0.7779\n",
      "Epoch 4/10, Train Loss: 0.1726, Accuracy: 0.9042, F1 Micro: 0.7724, F1 Macro: 0.7694\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9027, F1 Micro: 0.7758, F1 Macro: 0.7731\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9059, F1 Micro: 0.7777, F1 Macro: 0.7728\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9061, F1 Micro: 0.779, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9036, F1 Micro: 0.7765, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9091, F1 Micro: 0.7832, F1 Macro: 0.7789\n",
      "Epoch 10/10, Train Loss: 0.0352, Accuracy: 0.9092, F1 Micro: 0.7763, F1 Macro: 0.7692\n",
      "Best result for 5441 samples: F1 Micro: 0.7832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       353\n",
      "                sara       0.65      0.66      0.66       239\n",
      "         radikalisme       0.80      0.79      0.80       273\n",
      "pencemaran_nama_baik       0.74      0.73      0.73       485\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.78      0.78      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.003923845291137697\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.717350721359253 seconds\n",
      "\n",
      "Fold 5 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.372, Accuracy: 0.8898, F1 Micro: 0.7447, F1 Macro: 0.7489\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9044, F1 Micro: 0.7723, F1 Macro: 0.7715\n",
      "Epoch 3/10, Train Loss: 0.2021, Accuracy: 0.9067, F1 Micro: 0.7748, F1 Macro: 0.7701\n",
      "Epoch 4/10, Train Loss: 0.1589, Accuracy: 0.9047, F1 Micro: 0.7643, F1 Macro: 0.7545\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9075, F1 Micro: 0.7822, F1 Macro: 0.7831\n",
      "Epoch 6/10, Train Loss: 0.0892, Accuracy: 0.8977, F1 Micro: 0.7734, F1 Macro: 0.7748\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.8994, F1 Micro: 0.7692, F1 Macro: 0.7644\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9025, F1 Micro: 0.7747, F1 Macro: 0.7734\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.9019, F1 Micro: 0.7744, F1 Macro: 0.7716\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9006, F1 Micro: 0.7765, F1 Macro: 0.7758\n",
      "Best result for 5641 samples: F1 Micro: 0.7822\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.89      0.92       353\n",
      "                sara       0.63      0.74      0.68       239\n",
      "         radikalisme       0.78      0.84      0.81       273\n",
      "pencemaran_nama_baik       0.74      0.71      0.72       485\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1350\n",
      "           macro avg       0.78      0.79      0.78      1350\n",
      "        weighted avg       0.79      0.79      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.011790072917938248\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.550304889678955 seconds\n",
      "\n",
      "Fold 5 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3729, Accuracy: 0.8942, F1 Micro: 0.7435, F1 Macro: 0.7383\n",
      "Epoch 2/10, Train Loss: 0.2527, Accuracy: 0.9009, F1 Micro: 0.764, F1 Macro: 0.76\n",
      "Epoch 3/10, Train Loss: 0.1995, Accuracy: 0.9033, F1 Micro: 0.7545, F1 Macro: 0.7383\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9045, F1 Micro: 0.7746, F1 Macro: 0.776\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9053, F1 Micro: 0.7808, F1 Macro: 0.7802\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9058, F1 Micro: 0.7792, F1 Macro: 0.7756\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.9058, F1 Micro: 0.7758, F1 Macro: 0.7719\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.9042, F1 Micro: 0.776, F1 Macro: 0.7719\n",
      "Epoch 9/10, Train Loss: 0.0403, Accuracy: 0.9033, F1 Micro: 0.7672, F1 Macro: 0.7603\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.8997, F1 Micro: 0.7696, F1 Macro: 0.7667\n",
      "Best result for 5841 samples: F1 Micro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.93      0.92       353\n",
      "                sara       0.63      0.72      0.67       239\n",
      "         radikalisme       0.80      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.005043119192123414\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 3.3290371894836426 seconds\n",
      "\n",
      "Fold 5 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3553, Accuracy: 0.8919, F1 Micro: 0.7185, F1 Macro: 0.7026\n",
      "Epoch 2/10, Train Loss: 0.2444, Accuracy: 0.9038, F1 Micro: 0.7618, F1 Macro: 0.7538\n",
      "Epoch 3/10, Train Loss: 0.1924, Accuracy: 0.9045, F1 Micro: 0.7779, F1 Macro: 0.7763\n",
      "Epoch 4/10, Train Loss: 0.159, Accuracy: 0.9038, F1 Micro: 0.7794, F1 Macro: 0.7793\n",
      "Epoch 5/10, Train Loss: 0.1165, Accuracy: 0.9064, F1 Micro: 0.7787, F1 Macro: 0.7756\n",
      "Epoch 6/10, Train Loss: 0.0841, Accuracy: 0.9, F1 Micro: 0.7565, F1 Macro: 0.75\n",
      "Epoch 7/10, Train Loss: 0.0632, Accuracy: 0.9028, F1 Micro: 0.7706, F1 Macro: 0.7665\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.9061, F1 Micro: 0.77, F1 Macro: 0.7665\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9045, F1 Micro: 0.7828, F1 Macro: 0.7833\n",
      "Epoch 10/10, Train Loss: 0.0286, Accuracy: 0.9009, F1 Micro: 0.7705, F1 Macro: 0.7655\n",
      "Best result for 6041 samples: F1 Micro: 0.7828\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.93       353\n",
      "                sara       0.64      0.70      0.67       239\n",
      "         radikalisme       0.79      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1350\n",
      "           macro avg       0.76      0.81      0.78      1350\n",
      "        weighted avg       0.76      0.82      0.79      1350\n",
      "         samples avg       0.46      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 0.0017104923725128176\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 178\n",
      "Sampling duration: 2.233759641647339 seconds\n",
      "\n",
      "Fold 5 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3518, Accuracy: 0.8913, F1 Micro: 0.7313, F1 Macro: 0.7282\n",
      "Epoch 2/10, Train Loss: 0.2277, Accuracy: 0.9045, F1 Micro: 0.7695, F1 Macro: 0.7653\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9094, F1 Micro: 0.7871, F1 Macro: 0.7855\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9075, F1 Micro: 0.7766, F1 Macro: 0.7733\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9064, F1 Micro: 0.7819, F1 Macro: 0.7795\n",
      "Epoch 6/10, Train Loss: 0.0821, Accuracy: 0.9022, F1 Micro: 0.7766, F1 Macro: 0.7763\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9058, F1 Micro: 0.7763, F1 Macro: 0.7744\n",
      "Epoch 8/10, Train Loss: 0.0447, Accuracy: 0.9038, F1 Micro: 0.7696, F1 Macro: 0.7656\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9062, F1 Micro: 0.7875, F1 Macro: 0.7873\n",
      "Epoch 10/10, Train Loss: 0.0278, Accuracy: 0.903, F1 Micro: 0.7762, F1 Macro: 0.7724\n",
      "Best result for 6219 samples: F1 Micro: 0.7875\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       353\n",
      "                sara       0.64      0.73      0.68       239\n",
      "         radikalisme       0.76      0.86      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1350\n",
      "           macro avg       0.76      0.82      0.79      1350\n",
      "        weighted avg       0.76      0.82      0.79      1350\n",
      "         samples avg       0.45      0.46      0.44      1350\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 3971.86 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[2:6]\n",
    "X = data['processed_text'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Each element in these lists will be a list of metrics for one fold's learning curve\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    # Set seed for reproducibility within the fold\n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    # Adjust checkpoints based on the current fold's training size\n",
    "    checkpoints = [\n",
    "        # int(0.1 * total_train_fold_size)\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # 1. Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(f'{filename}-fold-{fold + 1}-model')\n",
    "        \n",
    "        # 3. Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        sampling_args = (model, X_pool, train_indices, remaining_indices, sampling_dur, new_samples_shared, fold, X_train_fold, y_train_fold)\n",
    "        notebook_launcher(least_confidence_sampling, sampling_args, num_processes=2)\n",
    "        \n",
    "        # 4. Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "997637da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T21:04:28.414539Z",
     "iopub.status.busy": "2025-06-27T21:04:28.414114Z",
     "iopub.status.idle": "2025-06-27T21:04:29.392618Z",
     "shell.execute_reply": "2025-06-27T21:04:29.391493Z"
    },
    "papermill": {
     "duration": 1.259288,
     "end_time": "2025-06-27T21:04:29.395535",
     "exception": false,
     "start_time": "2025-06-27T21:04:28.136247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6053344,
     "sourceId": 11762188,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19147.838005,
   "end_time": "2025-06-27T21:04:35.586595",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T15:45:27.748590",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "004c73e790c040be8d55a46ae07fbe3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_79d76225bb28432798d540e5adf6b1fc",
        "IPY_MODEL_0ad7cdf2ed9d41be96c3c53e4f96f8e0",
        "IPY_MODEL_31ca1a502b404a34a7d4334a383d7904"
       ],
       "layout": "IPY_MODEL_f45c8ed580fb40db8eca72618d134b64"
      }
     },
     "0ad7cdf2ed9d41be96c3c53e4f96f8e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_91fccec381f14291b1d3b05a6802b780",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_43877b264c7c4b82a7a407bdc3eb1eec",
       "value": 2.0
      }
     },
     "0d71718bcb4049938c678f18f9b87d69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16d3d7daae3e43f9b06324fc55aea833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b154308623ce43ddb687bcd49534f47f",
       "placeholder": "​",
       "style": "IPY_MODEL_a60efb532fbe49768f101d721fa7797b",
       "value": " 112/112 [00:00&lt;00:00, 9.23kB/s]"
      }
     },
     "182c7556bdff4a22b70869706121c691": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "1d3a1c018f8148c98f06f07de8e76990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "21ec50190c124321a18f6f8ed76277d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_55cd9b112eb448bbb3b87d18673594af",
        "IPY_MODEL_3b7b346148da4bfea368e07407493143",
        "IPY_MODEL_59f87b99324641ff868b65138f2196d2"
       ],
       "layout": "IPY_MODEL_d19a3dda47cb4a79ae3ebaa91084f3f4"
      }
     },
     "245aecb679ff4d4092f28fbcc0761ddd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "30801523115d4d5babf74f95d88cd311": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d2c5adb32a914824bae986e52af9c157",
       "placeholder": "​",
       "style": "IPY_MODEL_433e6b783d204332af042463415791a6",
       "value": " 1.53k/? [00:00&lt;00:00, 119kB/s]"
      }
     },
     "31ca1a502b404a34a7d4334a383d7904": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_424c3e9ab6eb495d80e438edfab2a895",
       "placeholder": "​",
       "style": "IPY_MODEL_75c8a232766d43ada52a79b526104fcc",
       "value": " 2.00/2.00 [00:00&lt;00:00, 159B/s]"
      }
     },
     "3705e591c0f541d1bf4d57bca109bacb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b7b346148da4bfea368e07407493143": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e5306f236c5a4c12ae87df7f549e5fe8",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fc2c1944fa454d5d9aa6d4bf6ba5bb98",
       "value": 497810400.0
      }
     },
     "3d3d9dfb882d455e937e31912d7c3376": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d5d85448128e4b3b95a6afe94662b115",
        "IPY_MODEL_80093b72de4d4954adff4c13eb15b749",
        "IPY_MODEL_30801523115d4d5babf74f95d88cd311"
       ],
       "layout": "IPY_MODEL_8547481a55b44e04858311f700ad9708"
      }
     },
     "424c3e9ab6eb495d80e438edfab2a895": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "433e6b783d204332af042463415791a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "43877b264c7c4b82a7a407bdc3eb1eec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4402d0fabbd04ba1b7999ad1595ebe85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46a7a2bf5b9b4d90830ef759098573b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c8e06f99b11443d9c96639369fd16fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "55cd9b112eb448bbb3b87d18673594af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0d71718bcb4049938c678f18f9b87d69",
       "placeholder": "​",
       "style": "IPY_MODEL_f73044b2863c406db38354695b23284c",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "570b41cdaeda405e9a8c8263afc421c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e80ac846581f47c0bb8855df7b28e47d",
        "IPY_MODEL_ad048f56012e412a8b7fadffd7392622",
        "IPY_MODEL_16d3d7daae3e43f9b06324fc55aea833"
       ],
       "layout": "IPY_MODEL_87810556faf64351be560913bf5d590f"
      }
     },
     "59f87b99324641ff868b65138f2196d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f2664c7be14645f7beedba8ac15800cd",
       "placeholder": "​",
       "style": "IPY_MODEL_bcceceb39b3d4c7f8f48b97d244bff12",
       "value": " 498M/498M [00:02&lt;00:00, 221MB/s]"
      }
     },
     "613747e9e6484801a4e1b6f53812cc6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f9fe6065f24b4bafb6e92ae726991917",
        "IPY_MODEL_ca4ae8cf81ec4d56ae1c6099f62c8123",
        "IPY_MODEL_8822ac46bbba4dafa4c0d7da127eb955"
       ],
       "layout": "IPY_MODEL_d4097c55d42f4ffabed83ea3af124f73"
      }
     },
     "75c8a232766d43ada52a79b526104fcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "79d76225bb28432798d540e5adf6b1fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d3e9a01750f04bba98841437a89d3198",
       "placeholder": "​",
       "style": "IPY_MODEL_c6b868ff82604779ac9a0b375965f043",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "7db831dc8f604f90b7482b0260c64760": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80093b72de4d4954adff4c13eb15b749": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_182c7556bdff4a22b70869706121c691",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e43594475c74889904468d3809cc4f9",
       "value": 1.0
      }
     },
     "8547481a55b44e04858311f700ad9708": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87810556faf64351be560913bf5d590f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8822ac46bbba4dafa4c0d7da127eb955": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e61fe17d1e9943af8588eedf4c961828",
       "placeholder": "​",
       "style": "IPY_MODEL_4402d0fabbd04ba1b7999ad1595ebe85",
       "value": " 229k/? [00:00&lt;00:00, 7.25MB/s]"
      }
     },
     "8e43594475c74889904468d3809cc4f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "91fccec381f14291b1d3b05a6802b780": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a10b18189e1a444cb22a7bea8d953def": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "a60efb532fbe49768f101d721fa7797b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ad048f56012e412a8b7fadffd7392622": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_de666130c9ba4b08a26caa0ae74345ec",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fbc6b44760b0431ca180dfe86913ba04",
       "value": 112.0
      }
     },
     "b154308623ce43ddb687bcd49534f47f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bcceceb39b3d4c7f8f48b97d244bff12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c6b868ff82604779ac9a0b375965f043": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ca4ae8cf81ec4d56ae1c6099f62c8123": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a10b18189e1a444cb22a7bea8d953def",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1d3a1c018f8148c98f06f07de8e76990",
       "value": 1.0
      }
     },
     "d19a3dda47cb4a79ae3ebaa91084f3f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2c5adb32a914824bae986e52af9c157": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3e9a01750f04bba98841437a89d3198": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4097c55d42f4ffabed83ea3af124f73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5d85448128e4b3b95a6afe94662b115": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3705e591c0f541d1bf4d57bca109bacb",
       "placeholder": "​",
       "style": "IPY_MODEL_4c8e06f99b11443d9c96639369fd16fc",
       "value": "config.json: "
      }
     },
     "de666130c9ba4b08a26caa0ae74345ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e39cc9f1ae2345deaedcf15d3918dcc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5306f236c5a4c12ae87df7f549e5fe8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e61fe17d1e9943af8588eedf4c961828": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e80ac846581f47c0bb8855df7b28e47d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e39cc9f1ae2345deaedcf15d3918dcc8",
       "placeholder": "​",
       "style": "IPY_MODEL_7db831dc8f604f90b7482b0260c64760",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "f2664c7be14645f7beedba8ac15800cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f45c8ed580fb40db8eca72618d134b64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f73044b2863c406db38354695b23284c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f9fe6065f24b4bafb6e92ae726991917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46a7a2bf5b9b4d90830ef759098573b8",
       "placeholder": "​",
       "style": "IPY_MODEL_245aecb679ff4d4092f28fbcc0761ddd",
       "value": "vocab.txt: "
      }
     },
     "fbc6b44760b0431ca180dfe86913ba04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc2c1944fa454d5d9aa6d4bf6ba5bb98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
