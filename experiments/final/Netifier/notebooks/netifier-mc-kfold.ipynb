{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459c40ac",
   "metadata": {
    "papermill": {
     "duration": 0.011555,
     "end_time": "2025-06-26T14:10:26.937598",
     "exception": false,
     "start_time": "2025-06-26T14:10:26.926043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8027efb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:10:26.959548Z",
     "iopub.status.busy": "2025-06-26T14:10:26.959292Z",
     "iopub.status.idle": "2025-06-26T14:11:01.511138Z",
     "shell.execute_reply": "2025-06-26T14:11:01.510390Z"
    },
    "papermill": {
     "duration": 34.564311,
     "end_time": "2025-06-26T14:11:01.512733",
     "exception": false,
     "start_time": "2025-06-26T14:10:26.948422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f24b96",
   "metadata": {
    "papermill": {
     "duration": 0.01019,
     "end_time": "2025-06-26T14:11:01.533796",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.523606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daad6fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.555274Z",
     "iopub.status.busy": "2025-06-26T14:11:01.554831Z",
     "iopub.status.idle": "2025-06-26T14:11:01.558137Z",
     "shell.execute_reply": "2025-06-26T14:11:01.557547Z"
    },
    "papermill": {
     "duration": 0.015271,
     "end_time": "2025-06-26T14:11:01.559216",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.543945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52575841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.580788Z",
     "iopub.status.busy": "2025-06-26T14:11:01.580534Z",
     "iopub.status.idle": "2025-06-26T14:11:01.583991Z",
     "shell.execute_reply": "2025-06-26T14:11:01.583414Z"
    },
    "papermill": {
     "duration": 0.015336,
     "end_time": "2025-06-26T14:11:01.585050",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.569714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576f58cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.606246Z",
     "iopub.status.busy": "2025-06-26T14:11:01.606053Z",
     "iopub.status.idle": "2025-06-26T14:11:01.618412Z",
     "shell.execute_reply": "2025-06-26T14:11:01.617861Z"
    },
    "papermill": {
     "duration": 0.024188,
     "end_time": "2025-06-26T14:11:01.619640",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.595452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b187362",
   "metadata": {
    "papermill": {
     "duration": 0.010104,
     "end_time": "2025-06-26T14:11:01.640898",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.630794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98817de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.663017Z",
     "iopub.status.busy": "2025-06-26T14:11:01.662821Z",
     "iopub.status.idle": "2025-06-26T14:11:01.718960Z",
     "shell.execute_reply": "2025-06-26T14:11:01.717437Z"
    },
    "papermill": {
     "duration": 0.06928,
     "end_time": "2025-06-26T14:11:01.720813",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.651533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-mc-kfold'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391ef5e",
   "metadata": {
    "papermill": {
     "duration": 0.009944,
     "end_time": "2025-06-26T14:11:01.741554",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.731610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42ca27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.762994Z",
     "iopub.status.busy": "2025-06-26T14:11:01.762748Z",
     "iopub.status.idle": "2025-06-26T14:11:01.929160Z",
     "shell.execute_reply": "2025-06-26T14:11:01.928208Z"
    },
    "papermill": {
     "duration": 0.17903,
     "end_time": "2025-06-26T14:11:01.930854",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.751824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-3/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-3/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a301a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:01.966946Z",
     "iopub.status.busy": "2025-06-26T14:11:01.966661Z",
     "iopub.status.idle": "2025-06-26T14:11:02.000756Z",
     "shell.execute_reply": "2025-06-26T14:11:01.999908Z"
    },
    "papermill": {
     "duration": 0.048253,
     "end_time": "2025-06-26T14:11:02.002270",
     "exception": false,
     "start_time": "2025-06-26T14:11:01.954017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5953acb",
   "metadata": {
    "papermill": {
     "duration": 0.010316,
     "end_time": "2025-06-26T14:11:02.024500",
     "exception": false,
     "start_time": "2025-06-26T14:11:02.014184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:02.046431Z",
     "iopub.status.busy": "2025-06-26T14:11:02.046166Z",
     "iopub.status.idle": "2025-06-26T14:11:05.840767Z",
     "shell.execute_reply": "2025-06-26T14:11:05.839906Z"
    },
    "papermill": {
     "duration": 3.807162,
     "end_time": "2025-06-26T14:11:05.842280",
     "exception": false,
     "start_time": "2025-06-26T14:11:02.035118",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4daf3b02b44ec4ad863f462613159f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1f01b5948d46b2ad5a7b52d61c67f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687d38e0ecac4eb0b27ddbba18772726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a04ef342774662887358d36276de64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define custom Dataset class\n",
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567c1599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:05.866252Z",
     "iopub.status.busy": "2025-06-26T14:11:05.865730Z",
     "iopub.status.idle": "2025-06-26T14:11:05.870203Z",
     "shell.execute_reply": "2025-06-26T14:11:05.869429Z"
    },
    "papermill": {
     "duration": 0.017673,
     "end_time": "2025-06-26T14:11:05.871670",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.853997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228a703",
   "metadata": {
    "papermill": {
     "duration": 0.010741,
     "end_time": "2025-06-26T14:11:05.893572",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.882831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e36c464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:05.916092Z",
     "iopub.status.busy": "2025-06-26T14:11:05.915865Z",
     "iopub.status.idle": "2025-06-26T14:11:05.920478Z",
     "shell.execute_reply": "2025-06-26T14:11:05.919685Z"
    },
    "papermill": {
     "duration": 0.017366,
     "end_time": "2025-06-26T14:11:05.921783",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.904417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0954bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:05.944231Z",
     "iopub.status.busy": "2025-06-26T14:11:05.944034Z",
     "iopub.status.idle": "2025-06-26T14:11:05.955785Z",
     "shell.execute_reply": "2025-06-26T14:11:05.955129Z"
    },
    "papermill": {
     "duration": 0.024256,
     "end_time": "2025-06-26T14:11:05.956975",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.932719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples...\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa941f",
   "metadata": {
    "papermill": {
     "duration": 0.010787,
     "end_time": "2025-06-26T14:11:05.979099",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.968312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d06ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:06.002199Z",
     "iopub.status.busy": "2025-06-26T14:11:06.001951Z",
     "iopub.status.idle": "2025-06-26T14:11:06.006898Z",
     "shell.execute_reply": "2025-06-26T14:11:06.006286Z"
    },
    "papermill": {
     "duration": 0.01752,
     "end_time": "2025-06-26T14:11:06.008079",
     "exception": false,
     "start_time": "2025-06-26T14:11:05.990559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73912ed3",
   "metadata": {
    "papermill": {
     "duration": 0.010804,
     "end_time": "2025-06-26T14:11:06.029851",
     "exception": false,
     "start_time": "2025-06-26T14:11:06.019047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5b95c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:06.052781Z",
     "iopub.status.busy": "2025-06-26T14:11:06.052579Z",
     "iopub.status.idle": "2025-06-26T14:11:06.062160Z",
     "shell.execute_reply": "2025-06-26T14:11:06.061467Z"
    },
    "papermill": {
     "duration": 0.022556,
     "end_time": "2025-06-26T14:11:06.063311",
     "exception": false,
     "start_time": "2025-06-26T14:11:06.040755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def monte_carlo_dropout_sampling(model, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, mc_passes=3, n_samples=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    dataset = NetifierDataset(X_pool, np.zeros((len(X_pool), 4)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    confidences = []\n",
    "    for data in dataloader:\n",
    "        # Collect multiple predictions to calculate uncertainty\n",
    "        batch_probs = []\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "        for _ in range(mc_passes):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()  # Shape: (batch_size, num_classes)\n",
    "            batch_probs.append(probs)\n",
    "\n",
    "        # Stack the probabilities from multiple MC passes\n",
    "        batch_probs = np.stack(batch_probs, axis=0)  # Shape: (mc_passes, batch_size, num_classes)\n",
    "\n",
    "        # Calculate mean probability and uncertainty for each sample in the batch\n",
    "        mean_probs = np.mean(batch_probs, axis=0)  # Shape: (batch_size, num_classes)\n",
    "        uncertainties = np.mean(np.var(batch_probs, axis=0), axis=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Append the uncertainties to the confidences list\n",
    "        confidences.extend(uncertainties)\n",
    "    \n",
    "    uncertainties = np.array(confidences)\n",
    "    sorted_unc = np.argsort(confidences)\n",
    "    sorted_unc = sorted_unc[::-1]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        threshold = np.percentile(confidences, 90)\n",
    "        items_greater_than_average = uncertainties[confidences >= threshold]\n",
    "        num_of_candidates = len(items_greater_than_average)\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "        \n",
    "        if num_of_candidates <= n_samples and n_samples < nearest_cp - current_train_size:\n",
    "            most_uncertain_indices = sorted_unc[:n_samples]\n",
    "        elif num_of_candidates > n_samples and num_of_candidates < nearest_cp - current_train_size:\n",
    "            most_uncertain_indices = sorted_unc[:max(n_samples, min(math.ceil(0.1*len(sorted_unc)), num_of_candidates))]\n",
    "        else:\n",
    "            most_uncertain_indices = sorted_unc[:nearest_cp - current_train_size]\n",
    "    \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend([remaining_indices[i] for i in most_uncertain_indices])\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train_fold[i] for i in temp],\n",
    "                'pornografi': [y_train_fold[i][0] for i in temp],\n",
    "                'sara': [y_train_fold[i][1] for i in temp],\n",
    "                'radikalisme': [y_train_fold[i][2] for i in temp],\n",
    "                'pencemaran_nama_baik': [y_train_fold[i][3] for i in temp],\n",
    "            })\n",
    "    \n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "        end_time = time.time() \n",
    "        duration = end_time - start_time\n",
    "    \n",
    "        sampling_dur.append(duration)\n",
    "        for i in most_uncertain_indices:\n",
    "            new_samples.append(remaining_indices[i])\n",
    "            \n",
    "        print(\"Nearest checkpoint:\", nearest_cp)\n",
    "        print(\"Threshold:\", threshold)\n",
    "        print(\"Samples above threshold:\", num_of_candidates)\n",
    "        print(\"Acquired samples:\", len(most_uncertain_indices))\n",
    "        print(f\"Sampling duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e5ba7",
   "metadata": {
    "papermill": {
     "duration": 0.010609,
     "end_time": "2025-06-26T14:11:06.085185",
     "exception": false,
     "start_time": "2025-06-26T14:11:06.074576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebfddee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T14:11:06.107574Z",
     "iopub.status.busy": "2025-06-26T14:11:06.107329Z",
     "iopub.status.idle": "2025-06-26T20:05:22.752379Z",
     "shell.execute_reply": "2025-06-26T20:05:22.751351Z"
    },
    "papermill": {
     "duration": 21256.657899,
     "end_time": "2025-06-26T20:05:22.753900",
     "exception": false,
     "start_time": "2025-06-26T14:11:06.096001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 388 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48289651e354d10bee0cc03be199cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5233, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4851, Accuracy: 0.8166, F1 Micro: 0.2367, F1 Macro: 0.1613\n",
      "Epoch 4/10, Train Loss: 0.4274, Accuracy: 0.8252, F1 Micro: 0.3327, F1 Macro: 0.227\n",
      "Epoch 5/10, Train Loss: 0.3978, Accuracy: 0.8386, F1 Micro: 0.4661, F1 Macro: 0.3619\n",
      "Epoch 6/10, Train Loss: 0.349, Accuracy: 0.8506, F1 Micro: 0.5631, F1 Macro: 0.5253\n",
      "Epoch 7/10, Train Loss: 0.3106, Accuracy: 0.862, F1 Micro: 0.652, F1 Macro: 0.6434\n",
      "Epoch 8/10, Train Loss: 0.2486, Accuracy: 0.8617, F1 Micro: 0.6641, F1 Macro: 0.6497\n",
      "Epoch 9/10, Train Loss: 0.2302, Accuracy: 0.8678, F1 Micro: 0.6874, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.8681, F1 Micro: 0.6544, F1 Macro: 0.6387\n",
      "Best result for 388 samples: F1 Micro: 0.6874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.79      0.85       369\n",
      "                sara       0.55      0.57      0.56       262\n",
      "         radikalisme       0.65      0.72      0.68       234\n",
      "pencemaran_nama_baik       0.63      0.67      0.65       478\n",
      "\n",
      "           micro avg       0.68      0.69      0.69      1343\n",
      "           macro avg       0.68      0.69      0.68      1343\n",
      "        weighted avg       0.69      0.69      0.69      1343\n",
      "         samples avg       0.37      0.39      0.37      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0033680403139442223\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 110.1017575263977 seconds\n",
      "\n",
      "Fold 1 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5788, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5324, Accuracy: 0.8116, F1 Micro: 0.3285, F1 Macro: 0.3368\n",
      "Epoch 3/10, Train Loss: 0.4424, Accuracy: 0.8583, F1 Micro: 0.5967, F1 Macro: 0.5939\n",
      "Epoch 4/10, Train Loss: 0.3789, Accuracy: 0.8711, F1 Micro: 0.6845, F1 Macro: 0.6756\n",
      "Epoch 5/10, Train Loss: 0.3261, Accuracy: 0.8806, F1 Micro: 0.7086, F1 Macro: 0.7006\n",
      "Epoch 6/10, Train Loss: 0.2608, Accuracy: 0.8828, F1 Micro: 0.7029, F1 Macro: 0.6915\n",
      "Epoch 7/10, Train Loss: 0.2274, Accuracy: 0.8856, F1 Micro: 0.693, F1 Macro: 0.6838\n",
      "Epoch 8/10, Train Loss: 0.1699, Accuracy: 0.8808, F1 Micro: 0.71, F1 Macro: 0.7096\n",
      "Epoch 9/10, Train Loss: 0.1311, Accuracy: 0.8828, F1 Micro: 0.7084, F1 Macro: 0.7059\n",
      "Epoch 10/10, Train Loss: 0.0935, Accuracy: 0.8773, F1 Micro: 0.7163, F1 Macro: 0.7201\n",
      "Best result for 971 samples: F1 Micro: 0.7163\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.79      0.86       369\n",
      "                sara       0.58      0.66      0.62       262\n",
      "         radikalisme       0.72      0.75      0.74       234\n",
      "pencemaran_nama_baik       0.61      0.73      0.67       478\n",
      "\n",
      "           micro avg       0.70      0.74      0.72      1343\n",
      "           macro avg       0.71      0.73      0.72      1343\n",
      "        weighted avg       0.72      0.74      0.72      1343\n",
      "         samples avg       0.40      0.41      0.39      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.005992312449961904\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 101.53451824188232 seconds\n",
      "\n",
      "Fold 1 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5764, Accuracy: 0.79, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5058, Accuracy: 0.8522, F1 Micro: 0.5469, F1 Macro: 0.5022\n",
      "Epoch 3/10, Train Loss: 0.4204, Accuracy: 0.8797, F1 Micro: 0.7016, F1 Macro: 0.6916\n",
      "Epoch 4/10, Train Loss: 0.355, Accuracy: 0.8903, F1 Micro: 0.7205, F1 Macro: 0.7106\n",
      "Epoch 5/10, Train Loss: 0.2828, Accuracy: 0.8884, F1 Micro: 0.7304, F1 Macro: 0.723\n",
      "Epoch 6/10, Train Loss: 0.2319, Accuracy: 0.8919, F1 Micro: 0.7395, F1 Macro: 0.7354\n",
      "Epoch 7/10, Train Loss: 0.1867, Accuracy: 0.8864, F1 Micro: 0.7388, F1 Macro: 0.7417\n",
      "Epoch 8/10, Train Loss: 0.1375, Accuracy: 0.8944, F1 Micro: 0.7347, F1 Macro: 0.727\n",
      "Epoch 9/10, Train Loss: 0.1135, Accuracy: 0.8891, F1 Micro: 0.7337, F1 Macro: 0.7283\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.892, F1 Micro: 0.7351, F1 Macro: 0.7323\n",
      "Best result for 1496 samples: F1 Micro: 0.7395\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.84      0.88       369\n",
      "                sara       0.67      0.57      0.61       262\n",
      "         radikalisme       0.74      0.78      0.76       234\n",
      "pencemaran_nama_baik       0.68      0.71      0.69       478\n",
      "\n",
      "           micro avg       0.75      0.73      0.74      1343\n",
      "           macro avg       0.75      0.73      0.74      1343\n",
      "        weighted avg       0.75      0.73      0.74      1343\n",
      "         samples avg       0.41      0.41      0.40      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.002922911779023709\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 90.749027967453 seconds\n",
      "\n",
      "Fold 1 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5603, Accuracy: 0.7975, F1 Micro: 0.2174, F1 Macro: 0.212\n",
      "Epoch 2/10, Train Loss: 0.4536, Accuracy: 0.8531, F1 Micro: 0.5668, F1 Macro: 0.5329\n",
      "Epoch 3/10, Train Loss: 0.3728, Accuracy: 0.8883, F1 Micro: 0.738, F1 Macro: 0.7369\n",
      "Epoch 4/10, Train Loss: 0.3088, Accuracy: 0.8956, F1 Micro: 0.7433, F1 Macro: 0.7445\n",
      "Epoch 5/10, Train Loss: 0.2453, Accuracy: 0.8995, F1 Micro: 0.741, F1 Macro: 0.7351\n",
      "Epoch 6/10, Train Loss: 0.21, Accuracy: 0.8948, F1 Micro: 0.7539, F1 Macro: 0.7531\n",
      "Epoch 7/10, Train Loss: 0.1431, Accuracy: 0.897, F1 Micro: 0.7417, F1 Macro: 0.7341\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.895, F1 Micro: 0.7441, F1 Macro: 0.7384\n",
      "Epoch 9/10, Train Loss: 0.0927, Accuracy: 0.8944, F1 Micro: 0.7572, F1 Macro: 0.7577\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.8933, F1 Micro: 0.7429, F1 Macro: 0.7355\n",
      "Best result for 1969 samples: F1 Micro: 0.7572\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       369\n",
      "                sara       0.64      0.68      0.66       262\n",
      "         radikalisme       0.75      0.78      0.77       234\n",
      "pencemaran_nama_baik       0.65      0.77      0.70       478\n",
      "\n",
      "           micro avg       0.73      0.78      0.76      1343\n",
      "           macro avg       0.74      0.78      0.76      1343\n",
      "        weighted avg       0.74      0.78      0.76      1343\n",
      "         samples avg       0.42      0.44      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004929054714739336\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 82.31550979614258 seconds\n",
      "\n",
      "Fold 1 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5512, Accuracy: 0.8291, F1 Micro: 0.47, F1 Macro: 0.439\n",
      "Epoch 2/10, Train Loss: 0.4319, Accuracy: 0.8755, F1 Micro: 0.6389, F1 Macro: 0.618\n",
      "Epoch 3/10, Train Loss: 0.3753, Accuracy: 0.8855, F1 Micro: 0.7436, F1 Macro: 0.7437\n",
      "Epoch 4/10, Train Loss: 0.3057, Accuracy: 0.9013, F1 Micro: 0.7552, F1 Macro: 0.7484\n",
      "Epoch 5/10, Train Loss: 0.2429, Accuracy: 0.8936, F1 Micro: 0.7605, F1 Macro: 0.7589\n",
      "Epoch 6/10, Train Loss: 0.1984, Accuracy: 0.8994, F1 Micro: 0.767, F1 Macro: 0.7647\n",
      "Epoch 7/10, Train Loss: 0.1365, Accuracy: 0.8944, F1 Micro: 0.7565, F1 Macro: 0.7576\n",
      "Epoch 8/10, Train Loss: 0.1129, Accuracy: 0.8986, F1 Micro: 0.7554, F1 Macro: 0.752\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.8917, F1 Micro: 0.7526, F1 Macro: 0.7569\n",
      "Epoch 10/10, Train Loss: 0.0699, Accuracy: 0.8989, F1 Micro: 0.7559, F1 Macro: 0.7501\n",
      "Best result for 2394 samples: F1 Micro: 0.767\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       369\n",
      "                sara       0.64      0.66      0.65       262\n",
      "         radikalisme       0.75      0.81      0.78       234\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1343\n",
      "           macro avg       0.75      0.78      0.76      1343\n",
      "        weighted avg       0.75      0.79      0.77      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0026146111311391\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 74.07221794128418 seconds\n",
      "\n",
      "Fold 1 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.833, F1 Micro: 0.4625, F1 Macro: 0.4299\n",
      "Epoch 2/10, Train Loss: 0.4153, Accuracy: 0.8827, F1 Micro: 0.7137, F1 Macro: 0.7004\n",
      "Epoch 3/10, Train Loss: 0.3494, Accuracy: 0.9027, F1 Micro: 0.7531, F1 Macro: 0.7488\n",
      "Epoch 4/10, Train Loss: 0.2858, Accuracy: 0.9031, F1 Micro: 0.7726, F1 Macro: 0.7692\n",
      "Epoch 5/10, Train Loss: 0.2285, Accuracy: 0.9023, F1 Micro: 0.7448, F1 Macro: 0.7338\n",
      "Epoch 6/10, Train Loss: 0.18, Accuracy: 0.9041, F1 Micro: 0.7558, F1 Macro: 0.7497\n",
      "Epoch 7/10, Train Loss: 0.1225, Accuracy: 0.902, F1 Micro: 0.7454, F1 Macro: 0.7351\n",
      "Epoch 8/10, Train Loss: 0.1025, Accuracy: 0.9033, F1 Micro: 0.7683, F1 Macro: 0.7664\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.9011, F1 Micro: 0.767, F1 Macro: 0.7678\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.897, F1 Micro: 0.7672, F1 Macro: 0.7708\n",
      "Best result for 2777 samples: F1 Micro: 0.7726\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.73      0.60      0.66       262\n",
      "         radikalisme       0.72      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       478\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1343\n",
      "           macro avg       0.77      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.77      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.001260840450413525\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.08328366279602 seconds\n",
      "\n",
      "Fold 1 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5202, Accuracy: 0.8425, F1 Micro: 0.4563, F1 Macro: 0.4472\n",
      "Epoch 2/10, Train Loss: 0.397, Accuracy: 0.8917, F1 Micro: 0.7268, F1 Macro: 0.7174\n",
      "Epoch 3/10, Train Loss: 0.3257, Accuracy: 0.8889, F1 Micro: 0.7549, F1 Macro: 0.758\n",
      "Epoch 4/10, Train Loss: 0.2713, Accuracy: 0.908, F1 Micro: 0.7645, F1 Macro: 0.7609\n",
      "Epoch 5/10, Train Loss: 0.2212, Accuracy: 0.9017, F1 Micro: 0.768, F1 Macro: 0.7665\n",
      "Epoch 6/10, Train Loss: 0.1756, Accuracy: 0.9053, F1 Micro: 0.7739, F1 Macro: 0.772\n",
      "Epoch 7/10, Train Loss: 0.1199, Accuracy: 0.8941, F1 Micro: 0.7639, F1 Macro: 0.7666\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9005, F1 Micro: 0.7617, F1 Macro: 0.7592\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.8981, F1 Micro: 0.7675, F1 Macro: 0.7662\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.8983, F1 Micro: 0.763, F1 Macro: 0.7641\n",
      "Best result for 3122 samples: F1 Micro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.70      0.65      0.67       262\n",
      "         radikalisme       0.77      0.79      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.74      0.72       478\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1343\n",
      "           macro avg       0.78      0.77      0.77      1343\n",
      "        weighted avg       0.78      0.77      0.77      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.001028920989483595\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.31136441230774 seconds\n",
      "\n",
      "Fold 1 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5071, Accuracy: 0.8516, F1 Micro: 0.5197, F1 Macro: 0.4478\n",
      "Epoch 2/10, Train Loss: 0.3866, Accuracy: 0.8931, F1 Micro: 0.7192, F1 Macro: 0.7038\n",
      "Epoch 3/10, Train Loss: 0.3256, Accuracy: 0.9023, F1 Micro: 0.7554, F1 Macro: 0.7507\n",
      "Epoch 4/10, Train Loss: 0.2608, Accuracy: 0.9092, F1 Micro: 0.7686, F1 Macro: 0.7614\n",
      "Epoch 5/10, Train Loss: 0.1953, Accuracy: 0.9009, F1 Micro: 0.7593, F1 Macro: 0.756\n",
      "Epoch 6/10, Train Loss: 0.1579, Accuracy: 0.9031, F1 Micro: 0.7653, F1 Macro: 0.7651\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.9005, F1 Micro: 0.7586, F1 Macro: 0.7576\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9011, F1 Micro: 0.7614, F1 Macro: 0.7565\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.8952, F1 Micro: 0.761, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9005, F1 Micro: 0.7588, F1 Macro: 0.7522\n",
      "Best result for 3432 samples: F1 Micro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.74      0.56      0.64       262\n",
      "         radikalisme       0.78      0.80      0.79       234\n",
      "pencemaran_nama_baik       0.80      0.64      0.71       478\n",
      "\n",
      "           micro avg       0.83      0.72      0.77      1343\n",
      "           macro avg       0.81      0.72      0.76      1343\n",
      "        weighted avg       0.82      0.72      0.76      1343\n",
      "         samples avg       0.42      0.40      0.40      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0007094581960700452\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.60748767852783 seconds\n",
      "\n",
      "Fold 1 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5001, Accuracy: 0.8559, F1 Micro: 0.6659, F1 Macro: 0.6657\n",
      "Epoch 2/10, Train Loss: 0.3649, Accuracy: 0.8916, F1 Micro: 0.7052, F1 Macro: 0.6927\n",
      "Epoch 3/10, Train Loss: 0.2929, Accuracy: 0.9031, F1 Micro: 0.7709, F1 Macro: 0.77\n",
      "Epoch 4/10, Train Loss: 0.2537, Accuracy: 0.9075, F1 Micro: 0.7688, F1 Macro: 0.7622\n",
      "Epoch 5/10, Train Loss: 0.2076, Accuracy: 0.9048, F1 Micro: 0.7438, F1 Macro: 0.7354\n",
      "Epoch 6/10, Train Loss: 0.1622, Accuracy: 0.9069, F1 Micro: 0.7709, F1 Macro: 0.7671\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9042, F1 Micro: 0.7696, F1 Macro: 0.7633\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.9044, F1 Micro: 0.7562, F1 Macro: 0.746\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9047, F1 Micro: 0.7606, F1 Macro: 0.7566\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9003, F1 Micro: 0.7697, F1 Macro: 0.7689\n",
      "Best result for 3711 samples: F1 Micro: 0.7709\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       369\n",
      "                sara       0.69      0.61      0.65       262\n",
      "         radikalisme       0.80      0.77      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.71      0.72       478\n",
      "\n",
      "           micro avg       0.80      0.75      0.77      1343\n",
      "           macro avg       0.79      0.74      0.77      1343\n",
      "        weighted avg       0.80      0.75      0.77      1343\n",
      "         samples avg       0.43      0.42      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.00035131651675328617\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 49.3495979309082 seconds\n",
      "\n",
      "Fold 1 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4835, Accuracy: 0.8737, F1 Micro: 0.6796, F1 Macro: 0.6697\n",
      "Epoch 2/10, Train Loss: 0.3533, Accuracy: 0.8956, F1 Micro: 0.7443, F1 Macro: 0.7323\n",
      "Epoch 3/10, Train Loss: 0.2893, Accuracy: 0.9033, F1 Micro: 0.7533, F1 Macro: 0.7429\n",
      "Epoch 4/10, Train Loss: 0.2509, Accuracy: 0.9041, F1 Micro: 0.7688, F1 Macro: 0.7647\n",
      "Epoch 5/10, Train Loss: 0.192, Accuracy: 0.9027, F1 Micro: 0.7491, F1 Macro: 0.7387\n",
      "Epoch 6/10, Train Loss: 0.1457, Accuracy: 0.905, F1 Micro: 0.7585, F1 Macro: 0.7535\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9073, F1 Micro: 0.7679, F1 Macro: 0.7635\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9042, F1 Micro: 0.7688, F1 Macro: 0.7644\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.9045, F1 Micro: 0.7685, F1 Macro: 0.7636\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9022, F1 Micro: 0.7738, F1 Macro: 0.7733\n",
      "Best result for 3886 samples: F1 Micro: 0.7738\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.65      0.68      0.67       262\n",
      "         radikalisme       0.75      0.83      0.79       234\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       478\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1343\n",
      "           macro avg       0.75      0.79      0.77      1343\n",
      "        weighted avg       0.76      0.80      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00016827270592330027\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 46.080894470214844 seconds\n",
      "\n",
      "Fold 1 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4823, Accuracy: 0.8716, F1 Micro: 0.6366, F1 Macro: 0.6289\n",
      "Epoch 2/10, Train Loss: 0.3368, Accuracy: 0.8988, F1 Micro: 0.7451, F1 Macro: 0.7401\n",
      "Epoch 3/10, Train Loss: 0.2886, Accuracy: 0.8992, F1 Micro: 0.7682, F1 Macro: 0.7638\n",
      "Epoch 4/10, Train Loss: 0.2414, Accuracy: 0.903, F1 Micro: 0.7704, F1 Macro: 0.771\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.9028, F1 Micro: 0.7651, F1 Macro: 0.7621\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.8995, F1 Micro: 0.7732, F1 Macro: 0.7748\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9052, F1 Micro: 0.77, F1 Macro: 0.7653\n",
      "Epoch 8/10, Train Loss: 0.0833, Accuracy: 0.9058, F1 Micro: 0.7554, F1 Macro: 0.7432\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9056, F1 Micro: 0.7686, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.907, F1 Micro: 0.7632, F1 Macro: 0.7529\n",
      "Best result for 4120 samples: F1 Micro: 0.7732\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       369\n",
      "                sara       0.60      0.77      0.68       262\n",
      "         radikalisme       0.79      0.78      0.79       234\n",
      "pencemaran_nama_baik       0.67      0.78      0.72       478\n",
      "\n",
      "           micro avg       0.73      0.82      0.77      1343\n",
      "           macro avg       0.74      0.81      0.77      1343\n",
      "        weighted avg       0.75      0.82      0.78      1343\n",
      "         samples avg       0.45      0.46      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0002431542219710536\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.40791416168213 seconds\n",
      "\n",
      "Fold 1 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4733, Accuracy: 0.8747, F1 Micro: 0.682, F1 Macro: 0.6807\n",
      "Epoch 2/10, Train Loss: 0.327, Accuracy: 0.8977, F1 Micro: 0.7507, F1 Macro: 0.7451\n",
      "Epoch 3/10, Train Loss: 0.2753, Accuracy: 0.9073, F1 Micro: 0.7686, F1 Macro: 0.7621\n",
      "Epoch 4/10, Train Loss: 0.2158, Accuracy: 0.9033, F1 Micro: 0.7758, F1 Macro: 0.7747\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.9053, F1 Micro: 0.7747, F1 Macro: 0.771\n",
      "Epoch 6/10, Train Loss: 0.133, Accuracy: 0.9056, F1 Micro: 0.7592, F1 Macro: 0.7517\n",
      "Epoch 7/10, Train Loss: 0.0943, Accuracy: 0.9052, F1 Micro: 0.7782, F1 Macro: 0.7739\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9041, F1 Micro: 0.7743, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9034, F1 Micro: 0.7593, F1 Macro: 0.7512\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.905, F1 Micro: 0.7763, F1 Macro: 0.7728\n",
      "Best result for 4330 samples: F1 Micro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       369\n",
      "                sara       0.67      0.66      0.67       262\n",
      "         radikalisme       0.78      0.77      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       478\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1343\n",
      "           macro avg       0.77      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.79      0.78      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 9.525343193672597e-05\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.485923528671265 seconds\n",
      "\n",
      "Fold 1 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4557, Accuracy: 0.8797, F1 Micro: 0.669, F1 Macro: 0.6562\n",
      "Epoch 2/10, Train Loss: 0.318, Accuracy: 0.8983, F1 Micro: 0.7426, F1 Macro: 0.7347\n",
      "Epoch 3/10, Train Loss: 0.2626, Accuracy: 0.9052, F1 Micro: 0.7705, F1 Macro: 0.766\n",
      "Epoch 4/10, Train Loss: 0.2242, Accuracy: 0.9066, F1 Micro: 0.7618, F1 Macro: 0.7599\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9058, F1 Micro: 0.783, F1 Macro: 0.7832\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9034, F1 Micro: 0.7645, F1 Macro: 0.7578\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9064, F1 Micro: 0.7646, F1 Macro: 0.7574\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9095, F1 Micro: 0.773, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.9058, F1 Micro: 0.7656, F1 Macro: 0.7608\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9048, F1 Micro: 0.7692, F1 Macro: 0.7665\n",
      "Best result for 4530 samples: F1 Micro: 0.783\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.65      0.73      0.69       262\n",
      "         radikalisme       0.77      0.81      0.79       234\n",
      "pencemaran_nama_baik       0.69      0.79      0.74       478\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1343\n",
      "           macro avg       0.76      0.81      0.78      1343\n",
      "        weighted avg       0.77      0.81      0.79      1343\n",
      "         samples avg       0.46      0.46      0.45      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00011332549911458038\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.49208068847656 seconds\n",
      "\n",
      "Fold 1 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4607, Accuracy: 0.8805, F1 Micro: 0.6869, F1 Macro: 0.6609\n",
      "Epoch 2/10, Train Loss: 0.3047, Accuracy: 0.9008, F1 Micro: 0.7631, F1 Macro: 0.7578\n",
      "Epoch 3/10, Train Loss: 0.2562, Accuracy: 0.902, F1 Micro: 0.7637, F1 Macro: 0.7565\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.9069, F1 Micro: 0.7618, F1 Macro: 0.7587\n",
      "Epoch 5/10, Train Loss: 0.174, Accuracy: 0.9087, F1 Micro: 0.7638, F1 Macro: 0.7556\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9077, F1 Micro: 0.7652, F1 Macro: 0.7616\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9047, F1 Micro: 0.7679, F1 Macro: 0.7621\n",
      "Epoch 8/10, Train Loss: 0.0784, Accuracy: 0.9045, F1 Micro: 0.7707, F1 Macro: 0.768\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9023, F1 Micro: 0.7696, F1 Macro: 0.7656\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9044, F1 Micro: 0.7733, F1 Macro: 0.7734\n",
      "Best result for 4663 samples: F1 Micro: 0.7733\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.87      0.91       369\n",
      "                sara       0.64      0.72      0.68       262\n",
      "         radikalisme       0.76      0.79      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.73      0.72       478\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1343\n",
      "           macro avg       0.77      0.78      0.77      1343\n",
      "        weighted avg       0.78      0.78      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 1.581098731548992e-05\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.899582862854004 seconds\n",
      "\n",
      "Fold 1 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4457, Accuracy: 0.878, F1 Micro: 0.7142, F1 Macro: 0.7097\n",
      "Epoch 2/10, Train Loss: 0.3039, Accuracy: 0.8936, F1 Micro: 0.7555, F1 Macro: 0.7528\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.9061, F1 Micro: 0.7743, F1 Macro: 0.7724\n",
      "Epoch 4/10, Train Loss: 0.2021, Accuracy: 0.9061, F1 Micro: 0.7666, F1 Macro: 0.7598\n",
      "Epoch 5/10, Train Loss: 0.1718, Accuracy: 0.9041, F1 Micro: 0.7646, F1 Macro: 0.7549\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.8944, F1 Micro: 0.7667, F1 Macro: 0.7682\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9042, F1 Micro: 0.772, F1 Macro: 0.7708\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.8955, F1 Micro: 0.7642, F1 Macro: 0.7619\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9005, F1 Micro: 0.7701, F1 Macro: 0.7682\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9014, F1 Micro: 0.7648, F1 Macro: 0.7627\n",
      "Best result for 4863 samples: F1 Micro: 0.7743\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       369\n",
      "                sara       0.68      0.69      0.68       262\n",
      "         radikalisme       0.74      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.74      0.71      0.72       478\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1343\n",
      "           macro avg       0.78      0.77      0.77      1343\n",
      "        weighted avg       0.78      0.77      0.78      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.00012638279004022502\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.050596714019775 seconds\n",
      "\n",
      "Fold 1 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4301, Accuracy: 0.8844, F1 Micro: 0.7184, F1 Macro: 0.7118\n",
      "Epoch 2/10, Train Loss: 0.2899, Accuracy: 0.9013, F1 Micro: 0.7579, F1 Macro: 0.7514\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.8988, F1 Micro: 0.7681, F1 Macro: 0.7639\n",
      "Epoch 4/10, Train Loss: 0.191, Accuracy: 0.9041, F1 Micro: 0.7759, F1 Macro: 0.7738\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.9031, F1 Micro: 0.7817, F1 Macro: 0.7787\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9048, F1 Micro: 0.7696, F1 Macro: 0.762\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.8955, F1 Micro: 0.7649, F1 Macro: 0.7644\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9003, F1 Micro: 0.7589, F1 Macro: 0.752\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9038, F1 Micro: 0.766, F1 Macro: 0.7591\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9005, F1 Micro: 0.7688, F1 Macro: 0.7672\n",
      "Best result for 5063 samples: F1 Micro: 0.7817\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       369\n",
      "                sara       0.64      0.72      0.68       262\n",
      "         radikalisme       0.76      0.79      0.77       234\n",
      "pencemaran_nama_baik       0.68      0.83      0.75       478\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1343\n",
      "           macro avg       0.75      0.82      0.78      1343\n",
      "        weighted avg       0.75      0.83      0.78      1343\n",
      "         samples avg       0.46      0.47      0.45      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 5.981592694297438e-05\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.330378532409668 seconds\n",
      "\n",
      "Fold 1 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4141, Accuracy: 0.8828, F1 Micro: 0.7066, F1 Macro: 0.6995\n",
      "Epoch 2/10, Train Loss: 0.2759, Accuracy: 0.8991, F1 Micro: 0.7622, F1 Macro: 0.7627\n",
      "Epoch 3/10, Train Loss: 0.2339, Accuracy: 0.9077, F1 Micro: 0.7771, F1 Macro: 0.7738\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9027, F1 Micro: 0.7761, F1 Macro: 0.7751\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.8945, F1 Micro: 0.7713, F1 Macro: 0.7729\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9061, F1 Micro: 0.7785, F1 Macro: 0.7785\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9022, F1 Micro: 0.7758, F1 Macro: 0.7745\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9048, F1 Micro: 0.7648, F1 Macro: 0.7618\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9036, F1 Micro: 0.7794, F1 Macro: 0.7783\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9056, F1 Micro: 0.7734, F1 Macro: 0.7663\n",
      "Best result for 5263 samples: F1 Micro: 0.7794\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       369\n",
      "                sara       0.66      0.71      0.68       262\n",
      "         radikalisme       0.77      0.81      0.79       234\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1343\n",
      "           macro avg       0.75      0.80      0.78      1343\n",
      "        weighted avg       0.76      0.81      0.78      1343\n",
      "         samples avg       0.46      0.46      0.45      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 5.6334431974391925e-06\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.592669010162354 seconds\n",
      "\n",
      "Fold 1 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4136, Accuracy: 0.8836, F1 Micro: 0.7144, F1 Macro: 0.699\n",
      "Epoch 2/10, Train Loss: 0.2721, Accuracy: 0.8986, F1 Micro: 0.7456, F1 Macro: 0.7186\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.908, F1 Micro: 0.762, F1 Macro: 0.7536\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.9044, F1 Micro: 0.7461, F1 Macro: 0.731\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.9006, F1 Micro: 0.7753, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9005, F1 Micro: 0.7716, F1 Macro: 0.7706\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9053, F1 Micro: 0.7754, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9072, F1 Micro: 0.7785, F1 Macro: 0.779\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.9042, F1 Micro: 0.7791, F1 Macro: 0.7789\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.9053, F1 Micro: 0.7757, F1 Macro: 0.7739\n",
      "Best result for 5441 samples: F1 Micro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       369\n",
      "                sara       0.65      0.71      0.68       262\n",
      "         radikalisme       0.76      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1343\n",
      "           macro avg       0.76      0.80      0.78      1343\n",
      "        weighted avg       0.76      0.80      0.78      1343\n",
      "         samples avg       0.46      0.45      0.45      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 4.433647154655773e-06\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.991999626159668 seconds\n",
      "\n",
      "Fold 1 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3927, Accuracy: 0.8861, F1 Micro: 0.7242, F1 Macro: 0.7191\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.8983, F1 Micro: 0.7387, F1 Macro: 0.7348\n",
      "Epoch 3/10, Train Loss: 0.2146, Accuracy: 0.9006, F1 Micro: 0.7743, F1 Macro: 0.7735\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9042, F1 Micro: 0.7752, F1 Macro: 0.7729\n",
      "Epoch 5/10, Train Loss: 0.1399, Accuracy: 0.9048, F1 Micro: 0.777, F1 Macro: 0.7692\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9022, F1 Micro: 0.7805, F1 Macro: 0.7808\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9022, F1 Micro: 0.7687, F1 Macro: 0.7617\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9014, F1 Micro: 0.7742, F1 Macro: 0.7765\n",
      "Epoch 9/10, Train Loss: 0.0452, Accuracy: 0.9056, F1 Micro: 0.7703, F1 Macro: 0.7638\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9067, F1 Micro: 0.7721, F1 Macro: 0.7633\n",
      "Best result for 5641 samples: F1 Micro: 0.7805\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       369\n",
      "                sara       0.63      0.73      0.68       262\n",
      "         radikalisme       0.74      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.67      0.82      0.74       478\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1343\n",
      "           macro avg       0.74      0.82      0.78      1343\n",
      "        weighted avg       0.75      0.83      0.78      1343\n",
      "         samples avg       0.46      0.47      0.45      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 8.670989700476637e-06\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.030725002288818 seconds\n",
      "\n",
      "Fold 1 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.8788, F1 Micro: 0.7273, F1 Macro: 0.7213\n",
      "Epoch 2/10, Train Loss: 0.2533, Accuracy: 0.902, F1 Micro: 0.7532, F1 Macro: 0.7476\n",
      "Epoch 3/10, Train Loss: 0.2, Accuracy: 0.9053, F1 Micro: 0.7703, F1 Macro: 0.7688\n",
      "Epoch 4/10, Train Loss: 0.1643, Accuracy: 0.9075, F1 Micro: 0.7671, F1 Macro: 0.757\n",
      "Epoch 5/10, Train Loss: 0.1289, Accuracy: 0.9042, F1 Micro: 0.7547, F1 Macro: 0.7455\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.908, F1 Micro: 0.7749, F1 Macro: 0.7697\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9058, F1 Micro: 0.7741, F1 Macro: 0.7674\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.9061, F1 Micro: 0.7664, F1 Macro: 0.7597\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9056, F1 Micro: 0.768, F1 Macro: 0.7596\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9059, F1 Micro: 0.7742, F1 Macro: 0.7676\n",
      "Best result for 5841 samples: F1 Micro: 0.7749\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.92       369\n",
      "                sara       0.72      0.61      0.66       262\n",
      "         radikalisme       0.81      0.76      0.78       234\n",
      "pencemaran_nama_baik       0.73      0.72      0.72       478\n",
      "\n",
      "           micro avg       0.80      0.76      0.77      1343\n",
      "           macro avg       0.80      0.75      0.77      1343\n",
      "        weighted avg       0.79      0.76      0.77      1343\n",
      "         samples avg       0.44      0.43      0.42      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 4.29810679634102e-06\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.193889379501343 seconds\n",
      "\n",
      "Fold 1 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.8884, F1 Micro: 0.7069, F1 Macro: 0.6905\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.9022, F1 Micro: 0.7614, F1 Macro: 0.7528\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9036, F1 Micro: 0.769, F1 Macro: 0.7626\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.9055, F1 Micro: 0.7775, F1 Macro: 0.7764\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9038, F1 Micro: 0.7724, F1 Macro: 0.7698\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9059, F1 Micro: 0.7674, F1 Macro: 0.7626\n",
      "Epoch 7/10, Train Loss: 0.0658, Accuracy: 0.9027, F1 Micro: 0.765, F1 Macro: 0.7578\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9045, F1 Micro: 0.7699, F1 Macro: 0.7637\n",
      "Epoch 9/10, Train Loss: 0.0453, Accuracy: 0.9008, F1 Micro: 0.7628, F1 Macro: 0.7596\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.9067, F1 Micro: 0.7624, F1 Macro: 0.7539\n",
      "Best result for 6041 samples: F1 Micro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       369\n",
      "                sara       0.65      0.71      0.68       262\n",
      "         radikalisme       0.75      0.83      0.79       234\n",
      "pencemaran_nama_baik       0.73      0.72      0.72       478\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1343\n",
      "           macro avg       0.76      0.79      0.78      1343\n",
      "        weighted avg       0.77      0.79      0.78      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 7.440328408847564e-06\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.369537115097046 seconds\n",
      "\n",
      "Fold 1 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3699, Accuracy: 0.8875, F1 Micro: 0.6992, F1 Macro: 0.675\n",
      "Epoch 2/10, Train Loss: 0.2327, Accuracy: 0.8984, F1 Micro: 0.7481, F1 Macro: 0.7454\n",
      "Epoch 3/10, Train Loss: 0.1866, Accuracy: 0.9048, F1 Micro: 0.7739, F1 Macro: 0.7731\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9008, F1 Micro: 0.7768, F1 Macro: 0.776\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9053, F1 Micro: 0.7568, F1 Macro: 0.7474\n",
      "Epoch 6/10, Train Loss: 0.0873, Accuracy: 0.9072, F1 Micro: 0.7731, F1 Macro: 0.7636\n",
      "Epoch 7/10, Train Loss: 0.061, Accuracy: 0.9038, F1 Micro: 0.7594, F1 Macro: 0.7521\n",
      "Epoch 8/10, Train Loss: 0.0462, Accuracy: 0.903, F1 Micro: 0.7687, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9044, F1 Micro: 0.772, F1 Macro: 0.7673\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9053, F1 Micro: 0.7729, F1 Macro: 0.7686\n",
      "Best result for 6218 samples: F1 Micro: 0.7768\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       369\n",
      "                sara       0.65      0.72      0.68       262\n",
      "         radikalisme       0.75      0.82      0.78       234\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       478\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1343\n",
      "           macro avg       0.74      0.82      0.78      1343\n",
      "        weighted avg       0.74      0.82      0.78      1343\n",
      "         samples avg       0.46      0.47      0.45      1343\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 4298.84 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4954, Accuracy: 0.7953, F1 Micro: 0.099, F1 Macro: 0.08\n",
      "Epoch 3/10, Train Loss: 0.448, Accuracy: 0.8238, F1 Micro: 0.3229, F1 Macro: 0.2108\n",
      "Epoch 4/10, Train Loss: 0.3886, Accuracy: 0.8323, F1 Micro: 0.3975, F1 Macro: 0.3047\n",
      "Epoch 5/10, Train Loss: 0.3358, Accuracy: 0.8384, F1 Micro: 0.4615, F1 Macro: 0.3692\n",
      "Epoch 6/10, Train Loss: 0.2851, Accuracy: 0.8464, F1 Micro: 0.5131, F1 Macro: 0.4513\n",
      "Epoch 7/10, Train Loss: 0.2509, Accuracy: 0.8661, F1 Micro: 0.6355, F1 Macro: 0.5946\n",
      "Epoch 8/10, Train Loss: 0.2376, Accuracy: 0.872, F1 Micro: 0.6667, F1 Macro: 0.6441\n",
      "Epoch 9/10, Train Loss: 0.188, Accuracy: 0.8628, F1 Micro: 0.6132, F1 Macro: 0.5838\n",
      "Epoch 10/10, Train Loss: 0.1613, Accuracy: 0.8709, F1 Micro: 0.6541, F1 Macro: 0.628\n",
      "Best result for 388 samples: F1 Micro: 0.6667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.80      0.86       378\n",
      "                sara       0.62      0.35      0.45       253\n",
      "         radikalisme       0.67      0.62      0.65       234\n",
      "pencemaran_nama_baik       0.72      0.54      0.62       517\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1382\n",
      "           macro avg       0.74      0.58      0.64      1382\n",
      "        weighted avg       0.75      0.59      0.66      1382\n",
      "         samples avg       0.38      0.35      0.35      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0028885551728308205\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 112.97424507141113 seconds\n",
      "\n",
      "Fold 2 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5731, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5005, Accuracy: 0.8295, F1 Micro: 0.4717, F1 Macro: 0.3684\n",
      "Epoch 3/10, Train Loss: 0.4062, Accuracy: 0.8552, F1 Micro: 0.567, F1 Macro: 0.4912\n",
      "Epoch 4/10, Train Loss: 0.3563, Accuracy: 0.8898, F1 Micro: 0.7281, F1 Macro: 0.7239\n",
      "Epoch 5/10, Train Loss: 0.2796, Accuracy: 0.8945, F1 Micro: 0.7322, F1 Macro: 0.7141\n",
      "Epoch 6/10, Train Loss: 0.2196, Accuracy: 0.8956, F1 Micro: 0.7548, F1 Macro: 0.749\n",
      "Epoch 7/10, Train Loss: 0.1641, Accuracy: 0.8956, F1 Micro: 0.7513, F1 Macro: 0.7459\n",
      "Epoch 8/10, Train Loss: 0.135, Accuracy: 0.8952, F1 Micro: 0.7543, F1 Macro: 0.7472\n",
      "Epoch 9/10, Train Loss: 0.1002, Accuracy: 0.8961, F1 Micro: 0.7447, F1 Macro: 0.7389\n",
      "Epoch 10/10, Train Loss: 0.0865, Accuracy: 0.8931, F1 Micro: 0.7438, F1 Macro: 0.7411\n",
      "Best result for 971 samples: F1 Micro: 0.7548\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       378\n",
      "                sara       0.64      0.64      0.64       253\n",
      "         radikalisme       0.70      0.79      0.74       234\n",
      "pencemaran_nama_baik       0.74      0.69      0.71       517\n",
      "\n",
      "           micro avg       0.77      0.74      0.75      1382\n",
      "           macro avg       0.76      0.74      0.75      1382\n",
      "        weighted avg       0.77      0.74      0.76      1382\n",
      "         samples avg       0.42      0.42      0.41      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004062265995889914\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 101.22400689125061 seconds\n",
      "\n",
      "Fold 2 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5703, Accuracy: 0.7906, F1 Micro: 0.3864, F1 Macro: 0.1712\n",
      "Epoch 2/10, Train Loss: 0.4853, Accuracy: 0.8477, F1 Micro: 0.4972, F1 Macro: 0.4164\n",
      "Epoch 3/10, Train Loss: 0.4004, Accuracy: 0.883, F1 Micro: 0.6857, F1 Macro: 0.6472\n",
      "Epoch 4/10, Train Loss: 0.324, Accuracy: 0.9014, F1 Micro: 0.763, F1 Macro: 0.7508\n",
      "Epoch 5/10, Train Loss: 0.2729, Accuracy: 0.8998, F1 Micro: 0.7553, F1 Macro: 0.7496\n",
      "Epoch 6/10, Train Loss: 0.2023, Accuracy: 0.8967, F1 Micro: 0.7457, F1 Macro: 0.7344\n",
      "Epoch 7/10, Train Loss: 0.1542, Accuracy: 0.9008, F1 Micro: 0.7649, F1 Macro: 0.755\n",
      "Epoch 8/10, Train Loss: 0.1165, Accuracy: 0.9014, F1 Micro: 0.7546, F1 Macro: 0.7437\n",
      "Epoch 9/10, Train Loss: 0.0924, Accuracy: 0.9019, F1 Micro: 0.7781, F1 Macro: 0.7726\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.8994, F1 Micro: 0.7593, F1 Macro: 0.7558\n",
      "Best result for 1496 samples: F1 Micro: 0.7781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       378\n",
      "                sara       0.65      0.69      0.67       253\n",
      "         radikalisme       0.71      0.82      0.76       234\n",
      "pencemaran_nama_baik       0.73      0.76      0.74       517\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1382\n",
      "           macro avg       0.75      0.79      0.77      1382\n",
      "        weighted avg       0.76      0.80      0.78      1382\n",
      "         samples avg       0.45      0.45      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0061687381472438595\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 91.35010051727295 seconds\n",
      "\n",
      "Fold 2 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5487, Accuracy: 0.8347, F1 Micro: 0.433, F1 Macro: 0.3575\n",
      "Epoch 2/10, Train Loss: 0.4422, Accuracy: 0.8655, F1 Micro: 0.6041, F1 Macro: 0.5707\n",
      "Epoch 3/10, Train Loss: 0.3724, Accuracy: 0.8955, F1 Micro: 0.7353, F1 Macro: 0.7151\n",
      "Epoch 4/10, Train Loss: 0.2937, Accuracy: 0.8988, F1 Micro: 0.7414, F1 Macro: 0.7324\n",
      "Epoch 5/10, Train Loss: 0.2517, Accuracy: 0.9039, F1 Micro: 0.7615, F1 Macro: 0.7516\n",
      "Epoch 6/10, Train Loss: 0.1893, Accuracy: 0.9045, F1 Micro: 0.77, F1 Macro: 0.7634\n",
      "Epoch 7/10, Train Loss: 0.1467, Accuracy: 0.9048, F1 Micro: 0.7873, F1 Macro: 0.7835\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.9053, F1 Micro: 0.7887, F1 Macro: 0.7836\n",
      "Epoch 9/10, Train Loss: 0.0827, Accuracy: 0.9075, F1 Micro: 0.7766, F1 Macro: 0.7679\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9042, F1 Micro: 0.7693, F1 Macro: 0.7608\n",
      "Best result for 1969 samples: F1 Micro: 0.7887\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       378\n",
      "                sara       0.67      0.72      0.69       253\n",
      "         radikalisme       0.73      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.71      0.81      0.76       517\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1382\n",
      "           macro avg       0.76      0.81      0.78      1382\n",
      "        weighted avg       0.77      0.82      0.79      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004635968059301377\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 82.18301582336426 seconds\n",
      "\n",
      "Fold 2 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5288, Accuracy: 0.8364, F1 Micro: 0.46, F1 Macro: 0.3716\n",
      "Epoch 2/10, Train Loss: 0.4214, Accuracy: 0.8941, F1 Micro: 0.7349, F1 Macro: 0.7092\n",
      "Epoch 3/10, Train Loss: 0.3493, Accuracy: 0.9013, F1 Micro: 0.7659, F1 Macro: 0.7594\n",
      "Epoch 4/10, Train Loss: 0.2793, Accuracy: 0.9067, F1 Micro: 0.7701, F1 Macro: 0.7659\n",
      "Epoch 5/10, Train Loss: 0.2282, Accuracy: 0.9027, F1 Micro: 0.7582, F1 Macro: 0.7505\n",
      "Epoch 6/10, Train Loss: 0.1614, Accuracy: 0.9073, F1 Micro: 0.7874, F1 Macro: 0.7836\n",
      "Epoch 7/10, Train Loss: 0.1308, Accuracy: 0.9061, F1 Micro: 0.7844, F1 Macro: 0.7742\n",
      "Epoch 8/10, Train Loss: 0.0969, Accuracy: 0.9075, F1 Micro: 0.7863, F1 Macro: 0.7819\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9045, F1 Micro: 0.7623, F1 Macro: 0.7532\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9069, F1 Micro: 0.7842, F1 Macro: 0.7756\n",
      "Best result for 2394 samples: F1 Micro: 0.7874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       378\n",
      "                sara       0.68      0.71      0.69       253\n",
      "         radikalisme       0.72      0.84      0.78       234\n",
      "pencemaran_nama_baik       0.75      0.74      0.75       517\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1382\n",
      "           macro avg       0.77      0.80      0.78      1382\n",
      "        weighted avg       0.78      0.79      0.79      1382\n",
      "         samples avg       0.45      0.45      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0024945105193182847\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 74.28183102607727 seconds\n",
      "\n",
      "Fold 2 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5187, Accuracy: 0.8461, F1 Micro: 0.5647, F1 Macro: 0.4687\n",
      "Epoch 2/10, Train Loss: 0.406, Accuracy: 0.8908, F1 Micro: 0.7212, F1 Macro: 0.6935\n",
      "Epoch 3/10, Train Loss: 0.3463, Accuracy: 0.8952, F1 Micro: 0.7323, F1 Macro: 0.7353\n",
      "Epoch 4/10, Train Loss: 0.2769, Accuracy: 0.9042, F1 Micro: 0.7595, F1 Macro: 0.7473\n",
      "Epoch 5/10, Train Loss: 0.2162, Accuracy: 0.9072, F1 Micro: 0.7722, F1 Macro: 0.7612\n",
      "Epoch 6/10, Train Loss: 0.1693, Accuracy: 0.9056, F1 Micro: 0.7709, F1 Macro: 0.757\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.907, F1 Micro: 0.7784, F1 Macro: 0.7644\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9044, F1 Micro: 0.7678, F1 Macro: 0.7557\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.9025, F1 Micro: 0.7635, F1 Macro: 0.7561\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.91, F1 Micro: 0.7916, F1 Macro: 0.7868\n",
      "Best result for 2777 samples: F1 Micro: 0.7916\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.91       378\n",
      "                sara       0.69      0.70      0.70       253\n",
      "         radikalisme       0.75      0.82      0.78       234\n",
      "pencemaran_nama_baik       0.76      0.76      0.76       517\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1382\n",
      "           macro avg       0.79      0.79      0.79      1382\n",
      "        weighted avg       0.80      0.79      0.79      1382\n",
      "         samples avg       0.46      0.45      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0020221127197146416\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.07941222190857 seconds\n",
      "\n",
      "Fold 2 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.506, Accuracy: 0.8553, F1 Micro: 0.5648, F1 Macro: 0.497\n",
      "Epoch 2/10, Train Loss: 0.3843, Accuracy: 0.8948, F1 Micro: 0.7362, F1 Macro: 0.7068\n",
      "Epoch 3/10, Train Loss: 0.321, Accuracy: 0.9042, F1 Micro: 0.757, F1 Macro: 0.7374\n",
      "Epoch 4/10, Train Loss: 0.2621, Accuracy: 0.9112, F1 Micro: 0.7932, F1 Macro: 0.7893\n",
      "Epoch 5/10, Train Loss: 0.2212, Accuracy: 0.9067, F1 Micro: 0.768, F1 Macro: 0.756\n",
      "Epoch 6/10, Train Loss: 0.1649, Accuracy: 0.913, F1 Micro: 0.7993, F1 Macro: 0.7894\n",
      "Epoch 7/10, Train Loss: 0.1171, Accuracy: 0.9105, F1 Micro: 0.7962, F1 Macro: 0.7885\n",
      "Epoch 8/10, Train Loss: 0.1, Accuracy: 0.9112, F1 Micro: 0.796, F1 Macro: 0.7884\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9056, F1 Micro: 0.7672, F1 Macro: 0.7606\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9092, F1 Micro: 0.7817, F1 Macro: 0.774\n",
      "Best result for 3122 samples: F1 Micro: 0.7993\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       378\n",
      "                sara       0.70      0.64      0.67       253\n",
      "         radikalisme       0.75      0.83      0.79       234\n",
      "pencemaran_nama_baik       0.76      0.79      0.78       517\n",
      "\n",
      "           micro avg       0.80      0.80      0.80      1382\n",
      "           macro avg       0.79      0.79      0.79      1382\n",
      "        weighted avg       0.80      0.80      0.80      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0009099163289647549\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.289772510528564 seconds\n",
      "\n",
      "Fold 2 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4911, Accuracy: 0.8617, F1 Micro: 0.5927, F1 Macro: 0.561\n",
      "Epoch 2/10, Train Loss: 0.375, Accuracy: 0.8975, F1 Micro: 0.777, F1 Macro: 0.7728\n",
      "Epoch 3/10, Train Loss: 0.3068, Accuracy: 0.9081, F1 Micro: 0.7766, F1 Macro: 0.769\n",
      "Epoch 4/10, Train Loss: 0.2638, Accuracy: 0.9116, F1 Micro: 0.7915, F1 Macro: 0.7812\n",
      "Epoch 5/10, Train Loss: 0.2115, Accuracy: 0.9067, F1 Micro: 0.7733, F1 Macro: 0.7679\n",
      "Epoch 6/10, Train Loss: 0.1568, Accuracy: 0.9067, F1 Micro: 0.7678, F1 Macro: 0.7467\n",
      "Epoch 7/10, Train Loss: 0.1127, Accuracy: 0.9095, F1 Micro: 0.7798, F1 Macro: 0.7679\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9105, F1 Micro: 0.795, F1 Macro: 0.7889\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9102, F1 Micro: 0.7847, F1 Macro: 0.7762\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9094, F1 Micro: 0.7833, F1 Macro: 0.7718\n",
      "Best result for 3432 samples: F1 Micro: 0.795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.89      0.92       378\n",
      "                sara       0.71      0.68      0.70       253\n",
      "         radikalisme       0.72      0.84      0.77       234\n",
      "pencemaran_nama_baik       0.75      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1382\n",
      "           macro avg       0.78      0.80      0.79      1382\n",
      "        weighted avg       0.79      0.80      0.80      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0004786364734172821\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.48707938194275 seconds\n",
      "\n",
      "Fold 2 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4856, Accuracy: 0.8731, F1 Micro: 0.6721, F1 Macro: 0.654\n",
      "Epoch 2/10, Train Loss: 0.353, Accuracy: 0.9062, F1 Micro: 0.7781, F1 Macro: 0.7664\n",
      "Epoch 3/10, Train Loss: 0.2917, Accuracy: 0.9077, F1 Micro: 0.7764, F1 Macro: 0.7691\n",
      "Epoch 4/10, Train Loss: 0.2467, Accuracy: 0.9044, F1 Micro: 0.7581, F1 Macro: 0.7402\n",
      "Epoch 5/10, Train Loss: 0.1924, Accuracy: 0.9123, F1 Micro: 0.7932, F1 Macro: 0.7877\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.9119, F1 Micro: 0.7948, F1 Macro: 0.7829\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9145, F1 Micro: 0.8058, F1 Macro: 0.801\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9111, F1 Micro: 0.7979, F1 Macro: 0.7915\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9141, F1 Micro: 0.8014, F1 Macro: 0.7954\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9109, F1 Micro: 0.7945, F1 Macro: 0.7875\n",
      "Best result for 3711 samples: F1 Micro: 0.8058\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       378\n",
      "                sara       0.71      0.73      0.72       253\n",
      "         radikalisme       0.73      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.76      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.79      0.82      0.81      1382\n",
      "           macro avg       0.79      0.82      0.80      1382\n",
      "        weighted avg       0.80      0.82      0.81      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0002997658040840182\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 49.474366664886475 seconds\n",
      "\n",
      "Fold 2 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4754, Accuracy: 0.882, F1 Micro: 0.707, F1 Macro: 0.6621\n",
      "Epoch 2/10, Train Loss: 0.3482, Accuracy: 0.8966, F1 Micro: 0.7404, F1 Macro: 0.7292\n",
      "Epoch 3/10, Train Loss: 0.2963, Accuracy: 0.9114, F1 Micro: 0.7984, F1 Macro: 0.7938\n",
      "Epoch 4/10, Train Loss: 0.2417, Accuracy: 0.9148, F1 Micro: 0.7988, F1 Macro: 0.7945\n",
      "Epoch 5/10, Train Loss: 0.1788, Accuracy: 0.9117, F1 Micro: 0.7946, F1 Macro: 0.7849\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9092, F1 Micro: 0.7839, F1 Macro: 0.7789\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9073, F1 Micro: 0.7855, F1 Macro: 0.7772\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9092, F1 Micro: 0.7828, F1 Macro: 0.7799\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9086, F1 Micro: 0.7895, F1 Macro: 0.7849\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9094, F1 Micro: 0.7866, F1 Macro: 0.7783\n",
      "Best result for 3886 samples: F1 Micro: 0.7988\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.97      0.87      0.92       378\n",
      "                sara       0.72      0.70      0.71       253\n",
      "         radikalisme       0.74      0.84      0.79       234\n",
      "pencemaran_nama_baik       0.79      0.74      0.76       517\n",
      "\n",
      "           micro avg       0.82      0.78      0.80      1382\n",
      "           macro avg       0.81      0.79      0.79      1382\n",
      "        weighted avg       0.82      0.78      0.80      1382\n",
      "         samples avg       0.46      0.44      0.44      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00032114204077515757\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 46.02102756500244 seconds\n",
      "\n",
      "Fold 2 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4692, Accuracy: 0.8838, F1 Micro: 0.7192, F1 Macro: 0.7105\n",
      "Epoch 2/10, Train Loss: 0.3277, Accuracy: 0.9072, F1 Micro: 0.7714, F1 Macro: 0.7569\n",
      "Epoch 3/10, Train Loss: 0.2718, Accuracy: 0.9098, F1 Micro: 0.7893, F1 Macro: 0.7875\n",
      "Epoch 4/10, Train Loss: 0.2239, Accuracy: 0.9078, F1 Micro: 0.7752, F1 Macro: 0.7703\n",
      "Epoch 5/10, Train Loss: 0.1744, Accuracy: 0.913, F1 Micro: 0.7963, F1 Macro: 0.7908\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9139, F1 Micro: 0.8087, F1 Macro: 0.8055\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.913, F1 Micro: 0.7996, F1 Macro: 0.7972\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.9091, F1 Micro: 0.7951, F1 Macro: 0.7904\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9103, F1 Micro: 0.7951, F1 Macro: 0.7944\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9114, F1 Micro: 0.7937, F1 Macro: 0.7908\n",
      "Best result for 4120 samples: F1 Micro: 0.8087\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.94      0.93       378\n",
      "                sara       0.74      0.73      0.73       253\n",
      "         radikalisme       0.75      0.83      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.83      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.84      0.81      1382\n",
      "           macro avg       0.78      0.83      0.81      1382\n",
      "        weighted avg       0.78      0.84      0.81      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0002503433934180065\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.35119652748108 seconds\n",
      "\n",
      "Fold 2 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4616, Accuracy: 0.8819, F1 Micro: 0.685, F1 Macro: 0.6704\n",
      "Epoch 2/10, Train Loss: 0.3237, Accuracy: 0.9047, F1 Micro: 0.7843, F1 Macro: 0.7752\n",
      "Epoch 3/10, Train Loss: 0.2725, Accuracy: 0.9092, F1 Micro: 0.7954, F1 Macro: 0.7911\n",
      "Epoch 4/10, Train Loss: 0.2232, Accuracy: 0.9089, F1 Micro: 0.7978, F1 Macro: 0.7957\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9148, F1 Micro: 0.8042, F1 Macro: 0.7985\n",
      "Epoch 6/10, Train Loss: 0.1288, Accuracy: 0.9108, F1 Micro: 0.7939, F1 Macro: 0.7912\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.9114, F1 Micro: 0.7943, F1 Macro: 0.786\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9105, F1 Micro: 0.7947, F1 Macro: 0.7856\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.912, F1 Micro: 0.7967, F1 Macro: 0.7919\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9106, F1 Micro: 0.7848, F1 Macro: 0.7807\n",
      "Best result for 4330 samples: F1 Micro: 0.8042\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.93       378\n",
      "                sara       0.73      0.69      0.71       253\n",
      "         radikalisme       0.71      0.88      0.79       234\n",
      "pencemaran_nama_baik       0.78      0.76      0.77       517\n",
      "\n",
      "           micro avg       0.80      0.81      0.80      1382\n",
      "           macro avg       0.79      0.81      0.80      1382\n",
      "        weighted avg       0.80      0.81      0.80      1382\n",
      "         samples avg       0.46      0.46      0.45      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00012650764838326723\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.340599060058594 seconds\n",
      "\n",
      "Fold 2 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4436, Accuracy: 0.8936, F1 Micro: 0.7591, F1 Macro: 0.7556\n",
      "Epoch 2/10, Train Loss: 0.3226, Accuracy: 0.9033, F1 Micro: 0.7842, F1 Macro: 0.779\n",
      "Epoch 3/10, Train Loss: 0.2651, Accuracy: 0.9081, F1 Micro: 0.7866, F1 Macro: 0.7727\n",
      "Epoch 4/10, Train Loss: 0.217, Accuracy: 0.9111, F1 Micro: 0.7999, F1 Macro: 0.7924\n",
      "Epoch 5/10, Train Loss: 0.1631, Accuracy: 0.9109, F1 Micro: 0.8037, F1 Macro: 0.7979\n",
      "Epoch 6/10, Train Loss: 0.1207, Accuracy: 0.9098, F1 Micro: 0.7815, F1 Macro: 0.7638\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.9131, F1 Micro: 0.8039, F1 Macro: 0.801\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9116, F1 Micro: 0.7936, F1 Macro: 0.7906\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9092, F1 Micro: 0.7897, F1 Macro: 0.7841\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9077, F1 Micro: 0.8008, F1 Macro: 0.7988\n",
      "Best result for 4530 samples: F1 Micro: 0.8039\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       378\n",
      "                sara       0.66      0.77      0.71       253\n",
      "         radikalisme       0.74      0.85      0.80       234\n",
      "pencemaran_nama_baik       0.76      0.78      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.82      0.80      1382\n",
      "           macro avg       0.78      0.83      0.80      1382\n",
      "        weighted avg       0.79      0.82      0.81      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 5.817344099341426e-05\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.68134021759033 seconds\n",
      "\n",
      "Fold 2 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4438, Accuracy: 0.8886, F1 Micro: 0.7172, F1 Macro: 0.7106\n",
      "Epoch 2/10, Train Loss: 0.3046, Accuracy: 0.9055, F1 Micro: 0.7679, F1 Macro: 0.7623\n",
      "Epoch 3/10, Train Loss: 0.2612, Accuracy: 0.9047, F1 Micro: 0.7621, F1 Macro: 0.7439\n",
      "Epoch 4/10, Train Loss: 0.211, Accuracy: 0.9128, F1 Micro: 0.8041, F1 Macro: 0.7985\n",
      "Epoch 5/10, Train Loss: 0.1679, Accuracy: 0.9116, F1 Micro: 0.7891, F1 Macro: 0.7766\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9105, F1 Micro: 0.7994, F1 Macro: 0.7961\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9103, F1 Micro: 0.7953, F1 Macro: 0.7922\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9136, F1 Micro: 0.8023, F1 Macro: 0.7982\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9133, F1 Micro: 0.8079, F1 Macro: 0.8045\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9086, F1 Micro: 0.7823, F1 Macro: 0.7808\n",
      "Best result for 4663 samples: F1 Micro: 0.8079\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.93       378\n",
      "                sara       0.70      0.76      0.73       253\n",
      "         radikalisme       0.74      0.84      0.79       234\n",
      "pencemaran_nama_baik       0.72      0.83      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.84      0.81      1382\n",
      "           macro avg       0.77      0.84      0.80      1382\n",
      "        weighted avg       0.78      0.84      0.81      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 2.3353407232207254e-05\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 31.0566668510437 seconds\n",
      "\n",
      "Fold 2 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.43, Accuracy: 0.8959, F1 Micro: 0.7528, F1 Macro: 0.7447\n",
      "Epoch 2/10, Train Loss: 0.3055, Accuracy: 0.9058, F1 Micro: 0.7737, F1 Macro: 0.7622\n",
      "Epoch 3/10, Train Loss: 0.2447, Accuracy: 0.9089, F1 Micro: 0.7861, F1 Macro: 0.7783\n",
      "Epoch 4/10, Train Loss: 0.2027, Accuracy: 0.9098, F1 Micro: 0.7953, F1 Macro: 0.7815\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9117, F1 Micro: 0.7946, F1 Macro: 0.7922\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9131, F1 Micro: 0.8056, F1 Macro: 0.801\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9139, F1 Micro: 0.8014, F1 Macro: 0.7961\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9095, F1 Micro: 0.7898, F1 Macro: 0.7847\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.908, F1 Micro: 0.7918, F1 Macro: 0.7903\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9094, F1 Micro: 0.7869, F1 Macro: 0.7798\n",
      "Best result for 4863 samples: F1 Micro: 0.8056\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.92      0.93       378\n",
      "                sara       0.70      0.74      0.72       253\n",
      "         radikalisme       0.70      0.89      0.78       234\n",
      "pencemaran_nama_baik       0.75      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.83      0.81      1382\n",
      "           macro avg       0.77      0.83      0.80      1382\n",
      "        weighted avg       0.79      0.83      0.81      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 7.256330573000021e-05\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.09159016609192 seconds\n",
      "\n",
      "Fold 2 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4296, Accuracy: 0.8898, F1 Micro: 0.7208, F1 Macro: 0.7132\n",
      "Epoch 2/10, Train Loss: 0.2927, Accuracy: 0.9044, F1 Micro: 0.7866, F1 Macro: 0.7818\n",
      "Epoch 3/10, Train Loss: 0.2446, Accuracy: 0.9095, F1 Micro: 0.8015, F1 Macro: 0.7987\n",
      "Epoch 4/10, Train Loss: 0.2011, Accuracy: 0.908, F1 Micro: 0.7687, F1 Macro: 0.7525\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9106, F1 Micro: 0.7822, F1 Macro: 0.778\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9103, F1 Micro: 0.7826, F1 Macro: 0.7765\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9139, F1 Micro: 0.8055, F1 Macro: 0.8012\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9109, F1 Micro: 0.7839, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.9108, F1 Micro: 0.7908, F1 Macro: 0.7874\n",
      "Epoch 10/10, Train Loss: 0.0388, Accuracy: 0.9111, F1 Micro: 0.7923, F1 Macro: 0.7873\n",
      "Best result for 5063 samples: F1 Micro: 0.8055\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       378\n",
      "                sara       0.70      0.75      0.72       253\n",
      "         radikalisme       0.76      0.81      0.78       234\n",
      "pencemaran_nama_baik       0.74      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.79      0.83      0.81      1382\n",
      "           macro avg       0.78      0.82      0.80      1382\n",
      "        weighted avg       0.79      0.83      0.81      1382\n",
      "         samples avg       0.48      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 1.690348071861081e-05\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.29389762878418 seconds\n",
      "\n",
      "Fold 2 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4061, Accuracy: 0.8944, F1 Micro: 0.7575, F1 Macro: 0.7429\n",
      "Epoch 2/10, Train Loss: 0.2775, Accuracy: 0.9047, F1 Micro: 0.7766, F1 Macro: 0.7649\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.9048, F1 Micro: 0.7933, F1 Macro: 0.791\n",
      "Epoch 4/10, Train Loss: 0.1898, Accuracy: 0.9144, F1 Micro: 0.8039, F1 Macro: 0.7926\n",
      "Epoch 5/10, Train Loss: 0.1516, Accuracy: 0.9127, F1 Micro: 0.8046, F1 Macro: 0.8013\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9103, F1 Micro: 0.7926, F1 Macro: 0.7806\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9102, F1 Micro: 0.7989, F1 Macro: 0.7911\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9053, F1 Micro: 0.7777, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0441, Accuracy: 0.9108, F1 Micro: 0.7929, F1 Macro: 0.787\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9077, F1 Micro: 0.7774, F1 Macro: 0.7648\n",
      "Best result for 5263 samples: F1 Micro: 0.8046\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.93       378\n",
      "                sara       0.67      0.80      0.73       253\n",
      "         radikalisme       0.71      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.76      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.83      0.80      1382\n",
      "           macro avg       0.77      0.84      0.80      1382\n",
      "        weighted avg       0.79      0.83      0.81      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 2.7644462170428605e-05\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.515787839889526 seconds\n",
      "\n",
      "Fold 2 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3941, Accuracy: 0.8956, F1 Micro: 0.7564, F1 Macro: 0.7437\n",
      "Epoch 2/10, Train Loss: 0.2704, Accuracy: 0.9069, F1 Micro: 0.7941, F1 Macro: 0.7908\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9106, F1 Micro: 0.7875, F1 Macro: 0.7794\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9061, F1 Micro: 0.7693, F1 Macro: 0.7571\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9047, F1 Micro: 0.7945, F1 Macro: 0.7903\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9091, F1 Micro: 0.7846, F1 Macro: 0.7716\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9083, F1 Micro: 0.7791, F1 Macro: 0.7744\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9059, F1 Micro: 0.7693, F1 Macro: 0.754\n",
      "Epoch 9/10, Train Loss: 0.0418, Accuracy: 0.9092, F1 Micro: 0.7942, F1 Macro: 0.7876\n",
      "Epoch 10/10, Train Loss: 0.0339, Accuracy: 0.9102, F1 Micro: 0.7969, F1 Macro: 0.7928\n",
      "Best result for 5441 samples: F1 Micro: 0.7969\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.90      0.93       378\n",
      "                sara       0.67      0.74      0.70       253\n",
      "         radikalisme       0.73      0.84      0.78       234\n",
      "pencemaran_nama_baik       0.74      0.78      0.76       517\n",
      "\n",
      "           micro avg       0.78      0.82      0.80      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 2.6858797355089332e-06\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.949198484420776 seconds\n",
      "\n",
      "Fold 2 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.385, Accuracy: 0.8986, F1 Micro: 0.7646, F1 Macro: 0.756\n",
      "Epoch 2/10, Train Loss: 0.2526, Accuracy: 0.9044, F1 Micro: 0.756, F1 Macro: 0.7395\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.908, F1 Micro: 0.7753, F1 Macro: 0.7632\n",
      "Epoch 4/10, Train Loss: 0.1712, Accuracy: 0.9064, F1 Micro: 0.7643, F1 Macro: 0.7544\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.912, F1 Micro: 0.7946, F1 Macro: 0.7889\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9102, F1 Micro: 0.7884, F1 Macro: 0.7834\n",
      "Epoch 7/10, Train Loss: 0.0682, Accuracy: 0.908, F1 Micro: 0.7816, F1 Macro: 0.7783\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9108, F1 Micro: 0.7921, F1 Macro: 0.7872\n",
      "Epoch 9/10, Train Loss: 0.0415, Accuracy: 0.9075, F1 Micro: 0.7949, F1 Macro: 0.7915\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9102, F1 Micro: 0.7829, F1 Macro: 0.7762\n",
      "Best result for 5641 samples: F1 Micro: 0.7949\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       378\n",
      "                sara       0.63      0.83      0.71       253\n",
      "         radikalisme       0.73      0.80      0.76       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1382\n",
      "           macro avg       0.76      0.83      0.79      1382\n",
      "        weighted avg       0.77      0.83      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 3.381710848771034e-06\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.175241708755493 seconds\n",
      "\n",
      "Fold 2 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3762, Accuracy: 0.8969, F1 Micro: 0.766, F1 Macro: 0.7607\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.9044, F1 Micro: 0.7745, F1 Macro: 0.7706\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9067, F1 Micro: 0.7717, F1 Macro: 0.7672\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9119, F1 Micro: 0.8058, F1 Macro: 0.7997\n",
      "Epoch 5/10, Train Loss: 0.124, Accuracy: 0.9102, F1 Micro: 0.7916, F1 Macro: 0.7908\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9105, F1 Micro: 0.788, F1 Macro: 0.7836\n",
      "Epoch 7/10, Train Loss: 0.0647, Accuracy: 0.9081, F1 Micro: 0.7932, F1 Macro: 0.7904\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9092, F1 Micro: 0.7893, F1 Macro: 0.7824\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.9117, F1 Micro: 0.7904, F1 Macro: 0.7844\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.9084, F1 Micro: 0.7957, F1 Macro: 0.7916\n",
      "Best result for 5841 samples: F1 Micro: 0.8058\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.93       378\n",
      "                sara       0.69      0.72      0.71       253\n",
      "         radikalisme       0.71      0.86      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.86      0.78       517\n",
      "\n",
      "           micro avg       0.77      0.85      0.81      1382\n",
      "           macro avg       0.77      0.84      0.80      1382\n",
      "        weighted avg       0.78      0.85      0.81      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 2.1164027930353796e-05\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.283124208450317 seconds\n",
      "\n",
      "Fold 2 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3784, Accuracy: 0.8964, F1 Micro: 0.7425, F1 Macro: 0.7296\n",
      "Epoch 2/10, Train Loss: 0.2444, Accuracy: 0.9086, F1 Micro: 0.7831, F1 Macro: 0.7731\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9114, F1 Micro: 0.7899, F1 Macro: 0.7828\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9078, F1 Micro: 0.7981, F1 Macro: 0.7934\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9077, F1 Micro: 0.7764, F1 Macro: 0.77\n",
      "Epoch 6/10, Train Loss: 0.0851, Accuracy: 0.9091, F1 Micro: 0.7765, F1 Macro: 0.766\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.9116, F1 Micro: 0.7922, F1 Macro: 0.7849\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.9106, F1 Micro: 0.7915, F1 Macro: 0.781\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.908, F1 Micro: 0.7956, F1 Macro: 0.7892\n",
      "Epoch 10/10, Train Loss: 0.0281, Accuracy: 0.9073, F1 Micro: 0.7932, F1 Macro: 0.7887\n",
      "Best result for 6041 samples: F1 Micro: 0.7981\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       378\n",
      "                sara       0.67      0.77      0.72       253\n",
      "         radikalisme       0.74      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.71      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.83      0.79      1382\n",
      "        weighted avg       0.76      0.84      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 1.6178997975657696e-05\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.469923973083496 seconds\n",
      "\n",
      "Fold 2 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3558, Accuracy: 0.8972, F1 Micro: 0.7753, F1 Macro: 0.7685\n",
      "Epoch 2/10, Train Loss: 0.2289, Accuracy: 0.9091, F1 Micro: 0.7992, F1 Macro: 0.7928\n",
      "Epoch 3/10, Train Loss: 0.2005, Accuracy: 0.912, F1 Micro: 0.7999, F1 Macro: 0.7908\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9041, F1 Micro: 0.7573, F1 Macro: 0.7435\n",
      "Epoch 5/10, Train Loss: 0.1178, Accuracy: 0.9094, F1 Micro: 0.7943, F1 Macro: 0.7887\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.9106, F1 Micro: 0.788, F1 Macro: 0.7774\n",
      "Epoch 7/10, Train Loss: 0.0661, Accuracy: 0.9034, F1 Micro: 0.768, F1 Macro: 0.7526\n",
      "Epoch 8/10, Train Loss: 0.0495, Accuracy: 0.9092, F1 Micro: 0.7983, F1 Macro: 0.794\n",
      "Epoch 9/10, Train Loss: 0.0346, Accuracy: 0.912, F1 Micro: 0.795, F1 Macro: 0.7855\n",
      "Epoch 10/10, Train Loss: 0.0313, Accuracy: 0.9094, F1 Micro: 0.7921, F1 Macro: 0.7845\n",
      "Best result for 6218 samples: F1 Micro: 0.7999\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       378\n",
      "                sara       0.73      0.63      0.68       253\n",
      "         radikalisme       0.74      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.79      0.81      0.80      1382\n",
      "           macro avg       0.78      0.80      0.79      1382\n",
      "        weighted avg       0.79      0.81      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 4229.19 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6513, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4812, Accuracy: 0.7903, F1 Micro: 0.1183, F1 Macro: 0.098\n",
      "Epoch 4/10, Train Loss: 0.4323, Accuracy: 0.8141, F1 Micro: 0.2942, F1 Macro: 0.2249\n",
      "Epoch 5/10, Train Loss: 0.3936, Accuracy: 0.8339, F1 Micro: 0.4907, F1 Macro: 0.409\n",
      "Epoch 6/10, Train Loss: 0.3469, Accuracy: 0.8489, F1 Micro: 0.565, F1 Macro: 0.4941\n",
      "Epoch 7/10, Train Loss: 0.2903, Accuracy: 0.8502, F1 Micro: 0.5889, F1 Macro: 0.5297\n",
      "Epoch 8/10, Train Loss: 0.227, Accuracy: 0.8589, F1 Micro: 0.6596, F1 Macro: 0.6437\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.8583, F1 Micro: 0.6931, F1 Macro: 0.6907\n",
      "Epoch 10/10, Train Loss: 0.1624, Accuracy: 0.8569, F1 Micro: 0.6041, F1 Macro: 0.5588\n",
      "Best result for 388 samples: F1 Micro: 0.6931\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.85      0.85       355\n",
      "                sara       0.58      0.58      0.58       273\n",
      "         radikalisme       0.64      0.70      0.67       281\n",
      "pencemaran_nama_baik       0.62      0.71      0.66       521\n",
      "\n",
      "           micro avg       0.67      0.72      0.69      1430\n",
      "           macro avg       0.67      0.71      0.69      1430\n",
      "        weighted avg       0.68      0.72      0.69      1430\n",
      "         samples avg       0.40      0.40      0.39      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0033147081965580584\n",
      "Samples above threshold: 583\n",
      "Acquired samples: 583\n",
      "Sampling duration: 112.9546263217926 seconds\n",
      "\n",
      "Fold 3 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6069, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5188, Accuracy: 0.8192, F1 Micro: 0.3474, F1 Macro: 0.2906\n",
      "Epoch 3/10, Train Loss: 0.4512, Accuracy: 0.8339, F1 Micro: 0.4776, F1 Macro: 0.4054\n",
      "Epoch 4/10, Train Loss: 0.3763, Accuracy: 0.8702, F1 Micro: 0.6677, F1 Macro: 0.6384\n",
      "Epoch 5/10, Train Loss: 0.3114, Accuracy: 0.8772, F1 Micro: 0.6848, F1 Macro: 0.6706\n",
      "Epoch 6/10, Train Loss: 0.2549, Accuracy: 0.8869, F1 Micro: 0.7412, F1 Macro: 0.7414\n",
      "Epoch 7/10, Train Loss: 0.1902, Accuracy: 0.8892, F1 Micro: 0.7408, F1 Macro: 0.7391\n",
      "Epoch 8/10, Train Loss: 0.1401, Accuracy: 0.8863, F1 Micro: 0.7555, F1 Macro: 0.7551\n",
      "Epoch 9/10, Train Loss: 0.109, Accuracy: 0.885, F1 Micro: 0.7532, F1 Macro: 0.7538\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.8897, F1 Micro: 0.7526, F1 Macro: 0.7534\n",
      "Best result for 971 samples: F1 Micro: 0.7555\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.85      0.89      0.87       355\n",
      "                sara       0.63      0.70      0.66       273\n",
      "         radikalisme       0.75      0.78      0.76       281\n",
      "pencemaran_nama_baik       0.69      0.77      0.72       521\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1430\n",
      "           macro avg       0.73      0.78      0.76      1430\n",
      "        weighted avg       0.73      0.79      0.76      1430\n",
      "         samples avg       0.44      0.45      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.005587540101259948\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 101.37861156463623 seconds\n",
      "\n",
      "Fold 3 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5737, Accuracy: 0.7434, F1 Micro: 0.3416, F1 Macro: 0.1344\n",
      "Epoch 2/10, Train Loss: 0.4877, Accuracy: 0.843, F1 Micro: 0.5396, F1 Macro: 0.4843\n",
      "Epoch 3/10, Train Loss: 0.4134, Accuracy: 0.8628, F1 Micro: 0.6105, F1 Macro: 0.5951\n",
      "Epoch 4/10, Train Loss: 0.3506, Accuracy: 0.8884, F1 Micro: 0.7419, F1 Macro: 0.7389\n",
      "Epoch 5/10, Train Loss: 0.2797, Accuracy: 0.8863, F1 Micro: 0.7251, F1 Macro: 0.7159\n",
      "Epoch 6/10, Train Loss: 0.2252, Accuracy: 0.8903, F1 Micro: 0.7458, F1 Macro: 0.7437\n",
      "Epoch 7/10, Train Loss: 0.1691, Accuracy: 0.8866, F1 Micro: 0.7429, F1 Macro: 0.7399\n",
      "Epoch 8/10, Train Loss: 0.1266, Accuracy: 0.8861, F1 Micro: 0.7197, F1 Macro: 0.7136\n",
      "Epoch 9/10, Train Loss: 0.1004, Accuracy: 0.8833, F1 Micro: 0.7519, F1 Macro: 0.753\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.8883, F1 Micro: 0.737, F1 Macro: 0.7351\n",
      "Best result for 1496 samples: F1 Micro: 0.7519\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       355\n",
      "                sara       0.61      0.70      0.65       273\n",
      "         radikalisme       0.75      0.76      0.75       281\n",
      "pencemaran_nama_baik       0.65      0.80      0.72       521\n",
      "\n",
      "           micro avg       0.72      0.79      0.75      1430\n",
      "           macro avg       0.73      0.78      0.75      1430\n",
      "        weighted avg       0.73      0.79      0.76      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.006034968793392182\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 91.19493770599365 seconds\n",
      "\n",
      "Fold 3 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5733, Accuracy: 0.7802, F1 Micro: 0.0551, F1 Macro: 0.0353\n",
      "Epoch 2/10, Train Loss: 0.467, Accuracy: 0.8447, F1 Micro: 0.5351, F1 Macro: 0.4771\n",
      "Epoch 3/10, Train Loss: 0.3868, Accuracy: 0.8866, F1 Micro: 0.7385, F1 Macro: 0.7322\n",
      "Epoch 4/10, Train Loss: 0.3191, Accuracy: 0.892, F1 Micro: 0.7535, F1 Macro: 0.7516\n",
      "Epoch 5/10, Train Loss: 0.2513, Accuracy: 0.8878, F1 Micro: 0.7531, F1 Macro: 0.75\n",
      "Epoch 6/10, Train Loss: 0.1963, Accuracy: 0.888, F1 Micro: 0.7535, F1 Macro: 0.751\n",
      "Epoch 7/10, Train Loss: 0.141, Accuracy: 0.8864, F1 Micro: 0.7466, F1 Macro: 0.7431\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.8897, F1 Micro: 0.7448, F1 Macro: 0.7447\n",
      "Epoch 9/10, Train Loss: 0.0865, Accuracy: 0.8883, F1 Micro: 0.754, F1 Macro: 0.7584\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.8892, F1 Micro: 0.7348, F1 Macro: 0.7282\n",
      "Best result for 1969 samples: F1 Micro: 0.754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       355\n",
      "                sara       0.62      0.73      0.67       273\n",
      "         radikalisme       0.77      0.78      0.77       281\n",
      "pencemaran_nama_baik       0.69      0.71      0.70       521\n",
      "\n",
      "           micro avg       0.74      0.77      0.75      1430\n",
      "           macro avg       0.75      0.77      0.76      1430\n",
      "        weighted avg       0.75      0.77      0.76      1430\n",
      "         samples avg       0.45      0.44      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.005704963114112618\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 82.4118869304657 seconds\n",
      "\n",
      "Fold 3 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7973, F1 Micro: 0.2275, F1 Macro: 0.2225\n",
      "Epoch 2/10, Train Loss: 0.4417, Accuracy: 0.872, F1 Micro: 0.6617, F1 Macro: 0.645\n",
      "Epoch 3/10, Train Loss: 0.361, Accuracy: 0.8856, F1 Micro: 0.7129, F1 Macro: 0.7109\n",
      "Epoch 4/10, Train Loss: 0.3015, Accuracy: 0.8933, F1 Micro: 0.7497, F1 Macro: 0.752\n",
      "Epoch 5/10, Train Loss: 0.2356, Accuracy: 0.8919, F1 Micro: 0.7587, F1 Macro: 0.7539\n",
      "Epoch 6/10, Train Loss: 0.1854, Accuracy: 0.8917, F1 Micro: 0.7492, F1 Macro: 0.748\n",
      "Epoch 7/10, Train Loss: 0.1329, Accuracy: 0.89, F1 Micro: 0.7535, F1 Macro: 0.7534\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.8925, F1 Micro: 0.7564, F1 Macro: 0.7559\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.8925, F1 Micro: 0.7461, F1 Macro: 0.7459\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.8919, F1 Micro: 0.7555, F1 Macro: 0.7533\n",
      "Best result for 2394 samples: F1 Micro: 0.7587\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.91      0.89       355\n",
      "                sara       0.69      0.64      0.66       273\n",
      "         radikalisme       0.79      0.68      0.73       281\n",
      "pencemaran_nama_baik       0.70      0.77      0.73       521\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1430\n",
      "           macro avg       0.76      0.75      0.75      1430\n",
      "        weighted avg       0.76      0.76      0.76      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0022576000075787317\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 74.2983660697937 seconds\n",
      "\n",
      "Fold 3 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5313, Accuracy: 0.8173, F1 Micro: 0.3678, F1 Macro: 0.3534\n",
      "Epoch 2/10, Train Loss: 0.4111, Accuracy: 0.8839, F1 Micro: 0.7191, F1 Macro: 0.699\n",
      "Epoch 3/10, Train Loss: 0.3438, Accuracy: 0.8889, F1 Micro: 0.7522, F1 Macro: 0.7455\n",
      "Epoch 4/10, Train Loss: 0.2814, Accuracy: 0.8947, F1 Micro: 0.7483, F1 Macro: 0.7405\n",
      "Epoch 5/10, Train Loss: 0.2119, Accuracy: 0.8983, F1 Micro: 0.7644, F1 Macro: 0.7638\n",
      "Epoch 6/10, Train Loss: 0.1679, Accuracy: 0.897, F1 Micro: 0.7706, F1 Macro: 0.7738\n",
      "Epoch 7/10, Train Loss: 0.1352, Accuracy: 0.8902, F1 Micro: 0.7332, F1 Macro: 0.7269\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.8958, F1 Micro: 0.76, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.8938, F1 Micro: 0.7542, F1 Macro: 0.7501\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.8927, F1 Micro: 0.7445, F1 Macro: 0.7401\n",
      "Best result for 2777 samples: F1 Micro: 0.7706\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       355\n",
      "                sara       0.64      0.75      0.69       273\n",
      "         radikalisme       0.75      0.82      0.78       281\n",
      "pencemaran_nama_baik       0.75      0.69      0.72       521\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1430\n",
      "           macro avg       0.77      0.79      0.77      1430\n",
      "        weighted avg       0.77      0.77      0.77      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0021326991263777018\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.04841828346252 seconds\n",
      "\n",
      "Fold 3 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5316, Accuracy: 0.8439, F1 Micro: 0.6156, F1 Macro: 0.5533\n",
      "Epoch 2/10, Train Loss: 0.396, Accuracy: 0.8883, F1 Micro: 0.7323, F1 Macro: 0.7278\n",
      "Epoch 3/10, Train Loss: 0.3251, Accuracy: 0.8981, F1 Micro: 0.7694, F1 Macro: 0.7694\n",
      "Epoch 4/10, Train Loss: 0.2725, Accuracy: 0.8911, F1 Micro: 0.7259, F1 Macro: 0.7117\n",
      "Epoch 5/10, Train Loss: 0.2229, Accuracy: 0.8905, F1 Micro: 0.7305, F1 Macro: 0.7146\n",
      "Epoch 6/10, Train Loss: 0.169, Accuracy: 0.8959, F1 Micro: 0.7496, F1 Macro: 0.744\n",
      "Epoch 7/10, Train Loss: 0.1268, Accuracy: 0.8983, F1 Micro: 0.7723, F1 Macro: 0.7745\n",
      "Epoch 8/10, Train Loss: 0.0877, Accuracy: 0.8973, F1 Micro: 0.7718, F1 Macro: 0.7746\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.8986, F1 Micro: 0.769, F1 Macro: 0.7694\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.8909, F1 Micro: 0.7554, F1 Macro: 0.7558\n",
      "Best result for 3122 samples: F1 Micro: 0.7723\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       355\n",
      "                sara       0.68      0.71      0.70       273\n",
      "         radikalisme       0.77      0.78      0.78       281\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       521\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1430\n",
      "           macro avg       0.77      0.77      0.77      1430\n",
      "        weighted avg       0.77      0.77      0.77      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.001557092764414847\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.28564190864563 seconds\n",
      "\n",
      "Fold 3 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5236, Accuracy: 0.8491, F1 Micro: 0.5722, F1 Macro: 0.5022\n",
      "Epoch 2/10, Train Loss: 0.3907, Accuracy: 0.8817, F1 Micro: 0.6845, F1 Macro: 0.6589\n",
      "Epoch 3/10, Train Loss: 0.3214, Accuracy: 0.897, F1 Micro: 0.7637, F1 Macro: 0.7629\n",
      "Epoch 4/10, Train Loss: 0.268, Accuracy: 0.8973, F1 Micro: 0.7573, F1 Macro: 0.7573\n",
      "Epoch 5/10, Train Loss: 0.2141, Accuracy: 0.8938, F1 Micro: 0.748, F1 Macro: 0.7403\n",
      "Epoch 6/10, Train Loss: 0.1561, Accuracy: 0.8977, F1 Micro: 0.7742, F1 Macro: 0.7782\n",
      "Epoch 7/10, Train Loss: 0.1117, Accuracy: 0.8933, F1 Micro: 0.7671, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.8942, F1 Micro: 0.7546, F1 Macro: 0.7502\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.89, F1 Micro: 0.7596, F1 Macro: 0.7628\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.8936, F1 Micro: 0.757, F1 Macro: 0.7561\n",
      "Best result for 3432 samples: F1 Micro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       355\n",
      "                sara       0.68      0.76      0.72       273\n",
      "         radikalisme       0.77      0.77      0.77       281\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       521\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1430\n",
      "           macro avg       0.77      0.79      0.78      1430\n",
      "        weighted avg       0.77      0.79      0.78      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0006924062909092754\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.550846099853516 seconds\n",
      "\n",
      "Fold 3 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5131, Accuracy: 0.8403, F1 Micro: 0.5082, F1 Macro: 0.4273\n",
      "Epoch 2/10, Train Loss: 0.3723, Accuracy: 0.8941, F1 Micro: 0.7634, F1 Macro: 0.763\n",
      "Epoch 3/10, Train Loss: 0.3067, Accuracy: 0.8984, F1 Micro: 0.7647, F1 Macro: 0.761\n",
      "Epoch 4/10, Train Loss: 0.2501, Accuracy: 0.8972, F1 Micro: 0.7523, F1 Macro: 0.7522\n",
      "Epoch 5/10, Train Loss: 0.1934, Accuracy: 0.8973, F1 Micro: 0.7704, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.1472, Accuracy: 0.8939, F1 Micro: 0.7473, F1 Macro: 0.7391\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.8978, F1 Micro: 0.7754, F1 Macro: 0.7775\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.8905, F1 Micro: 0.7546, F1 Macro: 0.7517\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.8959, F1 Micro: 0.7724, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.8955, F1 Micro: 0.7695, F1 Macro: 0.7726\n",
      "Best result for 3711 samples: F1 Micro: 0.7754\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       355\n",
      "                sara       0.67      0.72      0.69       273\n",
      "         radikalisme       0.76      0.80      0.78       281\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       521\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1430\n",
      "           macro avg       0.77      0.79      0.78      1430\n",
      "        weighted avg       0.77      0.79      0.78      1430\n",
      "         samples avg       0.46      0.45      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.000539739523082972\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 175\n",
      "Sampling duration: 49.1868052482605 seconds\n",
      "\n",
      "Fold 3 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4952, Accuracy: 0.8655, F1 Micro: 0.6952, F1 Macro: 0.6882\n",
      "Epoch 2/10, Train Loss: 0.3542, Accuracy: 0.8922, F1 Micro: 0.7553, F1 Macro: 0.7562\n",
      "Epoch 3/10, Train Loss: 0.293, Accuracy: 0.8964, F1 Micro: 0.7572, F1 Macro: 0.7491\n",
      "Epoch 4/10, Train Loss: 0.2387, Accuracy: 0.897, F1 Micro: 0.7576, F1 Macro: 0.76\n",
      "Epoch 5/10, Train Loss: 0.1981, Accuracy: 0.8983, F1 Micro: 0.7615, F1 Macro: 0.76\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.8945, F1 Micro: 0.7717, F1 Macro: 0.7761\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.8931, F1 Micro: 0.7625, F1 Macro: 0.766\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.8941, F1 Micro: 0.7568, F1 Macro: 0.7586\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.8934, F1 Micro: 0.7622, F1 Macro: 0.7645\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.8989, F1 Micro: 0.7636, F1 Macro: 0.7631\n",
      "Best result for 3886 samples: F1 Micro: 0.7717\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       355\n",
      "                sara       0.66      0.74      0.70       273\n",
      "         radikalisme       0.77      0.81      0.79       281\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       521\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1430\n",
      "           macro avg       0.75      0.80      0.78      1430\n",
      "        weighted avg       0.75      0.80      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00042841921094805035\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 45.73143148422241 seconds\n",
      "\n",
      "Fold 3 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4862, Accuracy: 0.8592, F1 Micro: 0.6094, F1 Macro: 0.5743\n",
      "Epoch 2/10, Train Loss: 0.3454, Accuracy: 0.8898, F1 Micro: 0.7228, F1 Macro: 0.7113\n",
      "Epoch 3/10, Train Loss: 0.2886, Accuracy: 0.8966, F1 Micro: 0.7555, F1 Macro: 0.7504\n",
      "Epoch 4/10, Train Loss: 0.2377, Accuracy: 0.8994, F1 Micro: 0.777, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.19, Accuracy: 0.8956, F1 Micro: 0.7628, F1 Macro: 0.7644\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.895, F1 Micro: 0.7705, F1 Macro: 0.7761\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.8984, F1 Micro: 0.7692, F1 Macro: 0.7727\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.8941, F1 Micro: 0.7485, F1 Macro: 0.7471\n",
      "Epoch 9/10, Train Loss: 0.0581, Accuracy: 0.8944, F1 Micro: 0.7626, F1 Macro: 0.7601\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.8928, F1 Micro: 0.7495, F1 Macro: 0.7442\n",
      "Best result for 4120 samples: F1 Micro: 0.777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.66      0.65      0.66       273\n",
      "         radikalisme       0.77      0.80      0.78       281\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       521\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1430\n",
      "           macro avg       0.77      0.78      0.77      1430\n",
      "        weighted avg       0.77      0.78      0.78      1430\n",
      "         samples avg       0.48      0.46      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00036571746168192473\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.44366955757141 seconds\n",
      "\n",
      "Fold 3 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4801, Accuracy: 0.8561, F1 Micro: 0.573, F1 Macro: 0.5472\n",
      "Epoch 2/10, Train Loss: 0.3358, Accuracy: 0.8944, F1 Micro: 0.7615, F1 Macro: 0.7556\n",
      "Epoch 3/10, Train Loss: 0.2711, Accuracy: 0.9008, F1 Micro: 0.7784, F1 Macro: 0.7796\n",
      "Epoch 4/10, Train Loss: 0.2239, Accuracy: 0.8978, F1 Micro: 0.7502, F1 Macro: 0.7515\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.8928, F1 Micro: 0.7446, F1 Macro: 0.7377\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.893, F1 Micro: 0.7594, F1 Macro: 0.7571\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.8914, F1 Micro: 0.7675, F1 Macro: 0.7699\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.8952, F1 Micro: 0.7521, F1 Macro: 0.7526\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.8955, F1 Micro: 0.7728, F1 Macro: 0.7764\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.8911, F1 Micro: 0.7565, F1 Macro: 0.7567\n",
      "Best result for 4330 samples: F1 Micro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       355\n",
      "                sara       0.67      0.72      0.69       273\n",
      "         radikalisme       0.75      0.82      0.78       281\n",
      "pencemaran_nama_baik       0.75      0.72      0.74       521\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1430\n",
      "           macro avg       0.78      0.79      0.78      1430\n",
      "        weighted avg       0.78      0.78      0.78      1430\n",
      "         samples avg       0.46      0.45      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0002998137118993327\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.41789984703064 seconds\n",
      "\n",
      "Fold 3 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4648, Accuracy: 0.8794, F1 Micro: 0.716, F1 Macro: 0.7164\n",
      "Epoch 2/10, Train Loss: 0.3146, Accuracy: 0.8969, F1 Micro: 0.7517, F1 Macro: 0.7453\n",
      "Epoch 3/10, Train Loss: 0.2618, Accuracy: 0.8889, F1 Micro: 0.7109, F1 Macro: 0.69\n",
      "Epoch 4/10, Train Loss: 0.2162, Accuracy: 0.8952, F1 Micro: 0.7592, F1 Macro: 0.7592\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.8978, F1 Micro: 0.7666, F1 Macro: 0.769\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.895, F1 Micro: 0.7649, F1 Macro: 0.7646\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.8855, F1 Micro: 0.7533, F1 Macro: 0.7543\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.893, F1 Micro: 0.7523, F1 Macro: 0.748\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.8917, F1 Micro: 0.7552, F1 Macro: 0.7544\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.8913, F1 Micro: 0.7603, F1 Macro: 0.762\n",
      "Best result for 4530 samples: F1 Micro: 0.7666\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       355\n",
      "                sara       0.67      0.68      0.68       273\n",
      "         radikalisme       0.78      0.78      0.78       281\n",
      "pencemaran_nama_baik       0.74      0.68      0.71       521\n",
      "\n",
      "           micro avg       0.78      0.75      0.77      1430\n",
      "           macro avg       0.78      0.76      0.77      1430\n",
      "        weighted avg       0.78      0.75      0.77      1430\n",
      "         samples avg       0.44      0.43      0.43      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 9.356506925541904e-05\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.42652177810669 seconds\n",
      "\n",
      "Fold 3 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.467, Accuracy: 0.8755, F1 Micro: 0.6878, F1 Macro: 0.6873\n",
      "Epoch 2/10, Train Loss: 0.3189, Accuracy: 0.8969, F1 Micro: 0.7572, F1 Macro: 0.753\n",
      "Epoch 3/10, Train Loss: 0.2512, Accuracy: 0.8967, F1 Micro: 0.7555, F1 Macro: 0.753\n",
      "Epoch 4/10, Train Loss: 0.2125, Accuracy: 0.8941, F1 Micro: 0.7735, F1 Macro: 0.7759\n",
      "Epoch 5/10, Train Loss: 0.1667, Accuracy: 0.8914, F1 Micro: 0.771, F1 Macro: 0.773\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.8953, F1 Micro: 0.7643, F1 Macro: 0.7635\n",
      "Epoch 7/10, Train Loss: 0.0907, Accuracy: 0.8931, F1 Micro: 0.7463, F1 Macro: 0.7369\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.8936, F1 Micro: 0.7699, F1 Macro: 0.772\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.8919, F1 Micro: 0.7579, F1 Macro: 0.7587\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.8909, F1 Micro: 0.7603, F1 Macro: 0.7624\n",
      "Best result for 4663 samples: F1 Micro: 0.7735\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       355\n",
      "                sara       0.65      0.75      0.69       273\n",
      "         radikalisme       0.73      0.83      0.78       281\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       521\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1430\n",
      "           macro avg       0.75      0.81      0.78      1430\n",
      "        weighted avg       0.75      0.81      0.78      1430\n",
      "         samples avg       0.47      0.46      0.46      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.00015796314983163034\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.940062046051025 seconds\n",
      "\n",
      "Fold 3 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4441, Accuracy: 0.883, F1 Micro: 0.7251, F1 Macro: 0.7247\n",
      "Epoch 2/10, Train Loss: 0.3121, Accuracy: 0.8963, F1 Micro: 0.7452, F1 Macro: 0.7409\n",
      "Epoch 3/10, Train Loss: 0.2439, Accuracy: 0.8917, F1 Micro: 0.7691, F1 Macro: 0.7702\n",
      "Epoch 4/10, Train Loss: 0.2029, Accuracy: 0.8956, F1 Micro: 0.7456, F1 Macro: 0.7419\n",
      "Epoch 5/10, Train Loss: 0.1681, Accuracy: 0.8939, F1 Micro: 0.7593, F1 Macro: 0.7612\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.8948, F1 Micro: 0.7683, F1 Macro: 0.7682\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.8914, F1 Micro: 0.7596, F1 Macro: 0.7609\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.8956, F1 Micro: 0.7551, F1 Macro: 0.7572\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.8905, F1 Micro: 0.7653, F1 Macro: 0.7652\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.8927, F1 Micro: 0.7587, F1 Macro: 0.7599\n",
      "Best result for 4863 samples: F1 Micro: 0.7691\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.63      0.68      0.66       273\n",
      "         radikalisme       0.73      0.86      0.79       281\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       521\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1430\n",
      "           macro avg       0.74      0.80      0.77      1430\n",
      "        weighted avg       0.74      0.81      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.0001325985591392964\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.317556619644165 seconds\n",
      "\n",
      "Fold 3 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4387, Accuracy: 0.8845, F1 Micro: 0.7438, F1 Macro: 0.7435\n",
      "Epoch 2/10, Train Loss: 0.2843, Accuracy: 0.8967, F1 Micro: 0.767, F1 Macro: 0.7627\n",
      "Epoch 3/10, Train Loss: 0.2297, Accuracy: 0.8978, F1 Micro: 0.7617, F1 Macro: 0.7634\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.8953, F1 Micro: 0.7699, F1 Macro: 0.7709\n",
      "Epoch 5/10, Train Loss: 0.1516, Accuracy: 0.8953, F1 Micro: 0.772, F1 Macro: 0.7732\n",
      "Epoch 6/10, Train Loss: 0.1089, Accuracy: 0.8925, F1 Micro: 0.7571, F1 Macro: 0.7592\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.8959, F1 Micro: 0.7665, F1 Macro: 0.7674\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.8923, F1 Micro: 0.7585, F1 Macro: 0.7598\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.8944, F1 Micro: 0.7568, F1 Macro: 0.7572\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.8922, F1 Micro: 0.7582, F1 Macro: 0.7579\n",
      "Best result for 5063 samples: F1 Micro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       355\n",
      "                sara       0.69      0.68      0.69       273\n",
      "         radikalisme       0.77      0.79      0.78       281\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       521\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1430\n",
      "           macro avg       0.76      0.79      0.77      1430\n",
      "        weighted avg       0.76      0.79      0.77      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 5.399592118919832e-05\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.361568212509155 seconds\n",
      "\n",
      "Fold 3 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4316, Accuracy: 0.8822, F1 Micro: 0.7066, F1 Macro: 0.6999\n",
      "Epoch 2/10, Train Loss: 0.2818, Accuracy: 0.8973, F1 Micro: 0.7632, F1 Macro: 0.7603\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.8911, F1 Micro: 0.7224, F1 Macro: 0.7062\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.8978, F1 Micro: 0.7686, F1 Macro: 0.7629\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.8919, F1 Micro: 0.7467, F1 Macro: 0.7455\n",
      "Epoch 6/10, Train Loss: 0.1078, Accuracy: 0.8928, F1 Micro: 0.7678, F1 Macro: 0.7685\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.8956, F1 Micro: 0.7672, F1 Macro: 0.7691\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.8942, F1 Micro: 0.7674, F1 Macro: 0.7701\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.8922, F1 Micro: 0.7448, F1 Macro: 0.737\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.8959, F1 Micro: 0.7683, F1 Macro: 0.7709\n",
      "Best result for 5263 samples: F1 Micro: 0.7686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       355\n",
      "                sara       0.69      0.56      0.62       273\n",
      "         radikalisme       0.78      0.82      0.80       281\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       521\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1430\n",
      "           macro avg       0.77      0.75      0.76      1430\n",
      "        weighted avg       0.77      0.76      0.77      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 4.055270037497395e-05\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.51644253730774 seconds\n",
      "\n",
      "Fold 3 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4222, Accuracy: 0.8903, F1 Micro: 0.7371, F1 Macro: 0.725\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.888, F1 Micro: 0.7098, F1 Macro: 0.6915\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.8967, F1 Micro: 0.7545, F1 Macro: 0.7547\n",
      "Epoch 4/10, Train Loss: 0.1743, Accuracy: 0.8956, F1 Micro: 0.7528, F1 Macro: 0.7427\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.8919, F1 Micro: 0.7393, F1 Macro: 0.7348\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.8927, F1 Micro: 0.7438, F1 Macro: 0.7385\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.8894, F1 Micro: 0.741, F1 Macro: 0.7377\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.8892, F1 Micro: 0.7338, F1 Macro: 0.7316\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.8867, F1 Micro: 0.748, F1 Macro: 0.7483\n",
      "Epoch 10/10, Train Loss: 0.0331, Accuracy: 0.8944, F1 Micro: 0.764, F1 Macro: 0.7654\n",
      "Best result for 5441 samples: F1 Micro: 0.764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       355\n",
      "                sara       0.66      0.67      0.66       273\n",
      "         radikalisme       0.76      0.81      0.78       281\n",
      "pencemaran_nama_baik       0.72      0.69      0.71       521\n",
      "\n",
      "           micro avg       0.76      0.77      0.76      1430\n",
      "           macro avg       0.76      0.77      0.77      1430\n",
      "        weighted avg       0.76      0.77      0.76      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 3.764978418985265e-06\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.916619777679443 seconds\n",
      "\n",
      "Fold 3 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4016, Accuracy: 0.8913, F1 Micro: 0.7464, F1 Macro: 0.7417\n",
      "Epoch 2/10, Train Loss: 0.2545, Accuracy: 0.8991, F1 Micro: 0.7688, F1 Macro: 0.766\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.8958, F1 Micro: 0.7525, F1 Macro: 0.743\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.8991, F1 Micro: 0.7627, F1 Macro: 0.7542\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.8953, F1 Micro: 0.7581, F1 Macro: 0.7565\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.893, F1 Micro: 0.7616, F1 Macro: 0.7643\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.8972, F1 Micro: 0.7611, F1 Macro: 0.7639\n",
      "Epoch 8/10, Train Loss: 0.0498, Accuracy: 0.8956, F1 Micro: 0.7682, F1 Macro: 0.7689\n",
      "Epoch 9/10, Train Loss: 0.042, Accuracy: 0.8892, F1 Micro: 0.7645, F1 Macro: 0.766\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.8969, F1 Micro: 0.7638, F1 Macro: 0.7643\n",
      "Best result for 5641 samples: F1 Micro: 0.7688\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       355\n",
      "                sara       0.67      0.63      0.65       273\n",
      "         radikalisme       0.78      0.79      0.78       281\n",
      "pencemaran_nama_baik       0.76      0.73      0.74       521\n",
      "\n",
      "           micro avg       0.79      0.75      0.77      1430\n",
      "           macro avg       0.78      0.75      0.77      1430\n",
      "        weighted avg       0.79      0.75      0.77      1430\n",
      "         samples avg       0.46      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 7.438254542648788e-05\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.212852954864502 seconds\n",
      "\n",
      "Fold 3 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.397, Accuracy: 0.89, F1 Micro: 0.7367, F1 Macro: 0.7312\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.8997, F1 Micro: 0.772, F1 Macro: 0.7728\n",
      "Epoch 3/10, Train Loss: 0.1967, Accuracy: 0.8984, F1 Micro: 0.7713, F1 Macro: 0.7657\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.8959, F1 Micro: 0.7623, F1 Macro: 0.767\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.892, F1 Micro: 0.7782, F1 Macro: 0.7813\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.8938, F1 Micro: 0.7698, F1 Macro: 0.7731\n",
      "Epoch 7/10, Train Loss: 0.0683, Accuracy: 0.8933, F1 Micro: 0.7429, F1 Macro: 0.7411\n",
      "Epoch 8/10, Train Loss: 0.0515, Accuracy: 0.8925, F1 Micro: 0.7439, F1 Macro: 0.74\n",
      "Epoch 9/10, Train Loss: 0.0425, Accuracy: 0.8947, F1 Micro: 0.7645, F1 Macro: 0.7672\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.8902, F1 Micro: 0.7572, F1 Macro: 0.7599\n",
      "Best result for 5841 samples: F1 Micro: 0.7782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.91      0.90       355\n",
      "                sara       0.64      0.78      0.70       273\n",
      "         radikalisme       0.73      0.85      0.79       281\n",
      "pencemaran_nama_baik       0.67      0.84      0.74       521\n",
      "\n",
      "           micro avg       0.72      0.85      0.78      1430\n",
      "           macro avg       0.73      0.85      0.78      1430\n",
      "        weighted avg       0.73      0.85      0.78      1430\n",
      "         samples avg       0.47      0.48      0.47      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 9.502880493528211e-06\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.235939502716064 seconds\n",
      "\n",
      "Fold 3 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3801, Accuracy: 0.8878, F1 Micro: 0.7169, F1 Macro: 0.7042\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.8966, F1 Micro: 0.7695, F1 Macro: 0.7677\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.898, F1 Micro: 0.7557, F1 Macro: 0.746\n",
      "Epoch 4/10, Train Loss: 0.1513, Accuracy: 0.8934, F1 Micro: 0.769, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.8967, F1 Micro: 0.7625, F1 Macro: 0.7593\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.8947, F1 Micro: 0.7622, F1 Macro: 0.7624\n",
      "Epoch 7/10, Train Loss: 0.0596, Accuracy: 0.8923, F1 Micro: 0.7486, F1 Macro: 0.7417\n",
      "Epoch 8/10, Train Loss: 0.0422, Accuracy: 0.8931, F1 Micro: 0.7625, F1 Macro: 0.7658\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.8903, F1 Micro: 0.7584, F1 Macro: 0.7568\n",
      "Epoch 10/10, Train Loss: 0.0315, Accuracy: 0.892, F1 Micro: 0.7495, F1 Macro: 0.7519\n",
      "Best result for 6041 samples: F1 Micro: 0.7695\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       355\n",
      "                sara       0.67      0.66      0.66       273\n",
      "         radikalisme       0.76      0.79      0.78       281\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       521\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1430\n",
      "           macro avg       0.77      0.77      0.77      1430\n",
      "        weighted avg       0.77      0.77      0.77      1430\n",
      "         samples avg       0.45      0.44      0.44      1430\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6218\n",
      "Threshold: 2.608091526781209e-05\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 177\n",
      "Sampling duration: 4.408201694488525 seconds\n",
      "\n",
      "Fold 3 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3695, Accuracy: 0.8706, F1 Micro: 0.6462, F1 Macro: 0.6208\n",
      "Epoch 2/10, Train Loss: 0.2257, Accuracy: 0.8952, F1 Micro: 0.7664, F1 Macro: 0.765\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.8956, F1 Micro: 0.7784, F1 Macro: 0.7802\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.8942, F1 Micro: 0.7653, F1 Macro: 0.7597\n",
      "Epoch 5/10, Train Loss: 0.1125, Accuracy: 0.8964, F1 Micro: 0.76, F1 Macro: 0.7618\n",
      "Epoch 6/10, Train Loss: 0.0852, Accuracy: 0.8928, F1 Micro: 0.7613, F1 Macro: 0.7592\n",
      "Epoch 7/10, Train Loss: 0.0666, Accuracy: 0.8934, F1 Micro: 0.7505, F1 Macro: 0.7487\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.897, F1 Micro: 0.7599, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.0344, Accuracy: 0.8939, F1 Micro: 0.7593, F1 Macro: 0.7619\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.8877, F1 Micro: 0.7513, F1 Macro: 0.7496\n",
      "Best result for 6218 samples: F1 Micro: 0.7784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       355\n",
      "                sara       0.65      0.73      0.69       273\n",
      "         radikalisme       0.75      0.84      0.79       281\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       521\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1430\n",
      "           macro avg       0.75      0.82      0.78      1430\n",
      "        weighted avg       0.75      0.82      0.78      1430\n",
      "         samples avg       0.47      0.47      0.46      1430\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 4224.16 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5795, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4106, Accuracy: 0.8267, F1 Micro: 0.3666, F1 Macro: 0.3071\n",
      "Epoch 4/10, Train Loss: 0.3659, Accuracy: 0.8505, F1 Micro: 0.5812, F1 Macro: 0.5497\n",
      "Epoch 5/10, Train Loss: 0.3165, Accuracy: 0.8614, F1 Micro: 0.6272, F1 Macro: 0.608\n",
      "Epoch 6/10, Train Loss: 0.2503, Accuracy: 0.8612, F1 Micro: 0.655, F1 Macro: 0.6302\n",
      "Epoch 7/10, Train Loss: 0.2107, Accuracy: 0.8628, F1 Micro: 0.6613, F1 Macro: 0.6477\n",
      "Epoch 8/10, Train Loss: 0.1713, Accuracy: 0.8617, F1 Micro: 0.6757, F1 Macro: 0.6675\n",
      "Epoch 9/10, Train Loss: 0.1484, Accuracy: 0.8644, F1 Micro: 0.672, F1 Macro: 0.662\n",
      "Epoch 10/10, Train Loss: 0.1116, Accuracy: 0.8662, F1 Micro: 0.686, F1 Macro: 0.6766\n",
      "Best result for 388 samples: F1 Micro: 0.686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.81      0.86       342\n",
      "                sara       0.54      0.47      0.51       249\n",
      "         radikalisme       0.72      0.63      0.67       302\n",
      "pencemaran_nama_baik       0.64      0.69      0.66       508\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1401\n",
      "           macro avg       0.71      0.65      0.68      1401\n",
      "        weighted avg       0.71      0.67      0.69      1401\n",
      "         samples avg       0.38      0.39      0.37      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.005371608305722475\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 112.63897895812988 seconds\n",
      "\n",
      "Fold 4 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5764, Accuracy: 0.7906, F1 Micro: 0.0958, F1 Macro: 0.0656\n",
      "Epoch 2/10, Train Loss: 0.4718, Accuracy: 0.8367, F1 Micro: 0.6196, F1 Macro: 0.6191\n",
      "Epoch 3/10, Train Loss: 0.3865, Accuracy: 0.8544, F1 Micro: 0.5916, F1 Macro: 0.5912\n",
      "Epoch 4/10, Train Loss: 0.3443, Accuracy: 0.8778, F1 Micro: 0.7045, F1 Macro: 0.7047\n",
      "Epoch 5/10, Train Loss: 0.2996, Accuracy: 0.8844, F1 Micro: 0.7226, F1 Macro: 0.7143\n",
      "Epoch 6/10, Train Loss: 0.2439, Accuracy: 0.8841, F1 Micro: 0.7312, F1 Macro: 0.7228\n",
      "Epoch 7/10, Train Loss: 0.1879, Accuracy: 0.8845, F1 Micro: 0.727, F1 Macro: 0.7243\n",
      "Epoch 8/10, Train Loss: 0.1472, Accuracy: 0.8844, F1 Micro: 0.7205, F1 Macro: 0.7163\n",
      "Epoch 9/10, Train Loss: 0.1116, Accuracy: 0.8855, F1 Micro: 0.7422, F1 Macro: 0.7372\n",
      "Epoch 10/10, Train Loss: 0.107, Accuracy: 0.8706, F1 Micro: 0.7348, F1 Macro: 0.7349\n",
      "Best result for 972 samples: F1 Micro: 0.7422\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.88      0.88       342\n",
      "                sara       0.62      0.63      0.63       249\n",
      "         radikalisme       0.75      0.71      0.73       302\n",
      "pencemaran_nama_baik       0.68      0.76      0.72       508\n",
      "\n",
      "           micro avg       0.73      0.75      0.74      1401\n",
      "           macro avg       0.73      0.74      0.74      1401\n",
      "        weighted avg       0.73      0.75      0.74      1401\n",
      "         samples avg       0.41      0.43      0.41      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0056358430534601215\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 101.42970871925354 seconds\n",
      "\n",
      "Fold 4 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5662, Accuracy: 0.8058, F1 Micro: 0.3413, F1 Macro: 0.2243\n",
      "Epoch 2/10, Train Loss: 0.4511, Accuracy: 0.8592, F1 Micro: 0.6203, F1 Macro: 0.6237\n",
      "Epoch 3/10, Train Loss: 0.3863, Accuracy: 0.8833, F1 Micro: 0.7121, F1 Macro: 0.7081\n",
      "Epoch 4/10, Train Loss: 0.3301, Accuracy: 0.8939, F1 Micro: 0.7433, F1 Macro: 0.7403\n",
      "Epoch 5/10, Train Loss: 0.2644, Accuracy: 0.8934, F1 Micro: 0.7395, F1 Macro: 0.7337\n",
      "Epoch 6/10, Train Loss: 0.1921, Accuracy: 0.8909, F1 Micro: 0.7484, F1 Macro: 0.7454\n",
      "Epoch 7/10, Train Loss: 0.1669, Accuracy: 0.8923, F1 Micro: 0.7613, F1 Macro: 0.7579\n",
      "Epoch 8/10, Train Loss: 0.1236, Accuracy: 0.8925, F1 Micro: 0.7529, F1 Macro: 0.7508\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.8959, F1 Micro: 0.7535, F1 Macro: 0.7506\n",
      "Epoch 10/10, Train Loss: 0.0838, Accuracy: 0.8902, F1 Micro: 0.76, F1 Macro: 0.7601\n",
      "Best result for 1497 samples: F1 Micro: 0.7613\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       342\n",
      "                sara       0.62      0.69      0.65       249\n",
      "         radikalisme       0.75      0.75      0.75       302\n",
      "pencemaran_nama_baik       0.70      0.80      0.74       508\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1401\n",
      "           macro avg       0.74      0.78      0.76      1401\n",
      "        weighted avg       0.75      0.78      0.76      1401\n",
      "         samples avg       0.43      0.45      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004219269379973413\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 91.77152752876282 seconds\n",
      "\n",
      "Fold 4 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5252, Accuracy: 0.8359, F1 Micro: 0.5621, F1 Macro: 0.537\n",
      "Epoch 2/10, Train Loss: 0.4196, Accuracy: 0.877, F1 Micro: 0.7131, F1 Macro: 0.7052\n",
      "Epoch 3/10, Train Loss: 0.3611, Accuracy: 0.8834, F1 Micro: 0.7456, F1 Macro: 0.7488\n",
      "Epoch 4/10, Train Loss: 0.3121, Accuracy: 0.8983, F1 Micro: 0.7606, F1 Macro: 0.7559\n",
      "Epoch 5/10, Train Loss: 0.2696, Accuracy: 0.8992, F1 Micro: 0.7576, F1 Macro: 0.754\n",
      "Epoch 6/10, Train Loss: 0.1994, Accuracy: 0.8983, F1 Micro: 0.7664, F1 Macro: 0.7641\n",
      "Epoch 7/10, Train Loss: 0.1464, Accuracy: 0.8963, F1 Micro: 0.7587, F1 Macro: 0.7554\n",
      "Epoch 8/10, Train Loss: 0.1141, Accuracy: 0.8938, F1 Micro: 0.7592, F1 Macro: 0.7572\n",
      "Epoch 9/10, Train Loss: 0.0911, Accuracy: 0.8939, F1 Micro: 0.7557, F1 Macro: 0.7496\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.8961, F1 Micro: 0.756, F1 Macro: 0.7538\n",
      "Best result for 1970 samples: F1 Micro: 0.7664\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.87      0.89       342\n",
      "                sara       0.65      0.66      0.65       249\n",
      "         radikalisme       0.77      0.78      0.78       302\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       508\n",
      "\n",
      "           micro avg       0.77      0.76      0.77      1401\n",
      "           macro avg       0.77      0.76      0.76      1401\n",
      "        weighted avg       0.77      0.76      0.77      1401\n",
      "         samples avg       0.43      0.44      0.42      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0024730264674872175\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 82.5128378868103 seconds\n",
      "\n",
      "Fold 4 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5254, Accuracy: 0.8637, F1 Micro: 0.6382, F1 Macro: 0.6285\n",
      "Epoch 2/10, Train Loss: 0.4089, Accuracy: 0.888, F1 Micro: 0.7458, F1 Macro: 0.7454\n",
      "Epoch 3/10, Train Loss: 0.3481, Accuracy: 0.8972, F1 Micro: 0.7678, F1 Macro: 0.7659\n",
      "Epoch 4/10, Train Loss: 0.2898, Accuracy: 0.89, F1 Micro: 0.7664, F1 Macro: 0.767\n",
      "Epoch 5/10, Train Loss: 0.2357, Accuracy: 0.9036, F1 Micro: 0.7775, F1 Macro: 0.7745\n",
      "Epoch 6/10, Train Loss: 0.1821, Accuracy: 0.8883, F1 Micro: 0.7649, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.155, Accuracy: 0.898, F1 Micro: 0.766, F1 Macro: 0.7622\n",
      "Epoch 8/10, Train Loss: 0.11, Accuracy: 0.903, F1 Micro: 0.7723, F1 Macro: 0.7686\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9005, F1 Micro: 0.7723, F1 Macro: 0.7713\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.8992, F1 Micro: 0.7709, F1 Macro: 0.7694\n",
      "Best result for 2395 samples: F1 Micro: 0.7775\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       342\n",
      "                sara       0.69      0.65      0.67       249\n",
      "         radikalisme       0.78      0.78      0.78       302\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       508\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1401\n",
      "           macro avg       0.79      0.76      0.77      1401\n",
      "        weighted avg       0.79      0.77      0.78      1401\n",
      "         samples avg       0.44      0.44      0.43      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0015825466951355338\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 74.41467618942261 seconds\n",
      "\n",
      "Fold 4 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5094, Accuracy: 0.8456, F1 Micro: 0.5546, F1 Macro: 0.5068\n",
      "Epoch 2/10, Train Loss: 0.3952, Accuracy: 0.8886, F1 Micro: 0.7487, F1 Macro: 0.7507\n",
      "Epoch 3/10, Train Loss: 0.3293, Accuracy: 0.9014, F1 Micro: 0.7602, F1 Macro: 0.756\n",
      "Epoch 4/10, Train Loss: 0.2741, Accuracy: 0.8998, F1 Micro: 0.7781, F1 Macro: 0.7755\n",
      "Epoch 5/10, Train Loss: 0.2175, Accuracy: 0.9002, F1 Micro: 0.7768, F1 Macro: 0.7752\n",
      "Epoch 6/10, Train Loss: 0.1666, Accuracy: 0.9048, F1 Micro: 0.7768, F1 Macro: 0.7688\n",
      "Epoch 7/10, Train Loss: 0.1213, Accuracy: 0.9019, F1 Micro: 0.7744, F1 Macro: 0.7715\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.9025, F1 Micro: 0.7793, F1 Macro: 0.7763\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9016, F1 Micro: 0.7779, F1 Macro: 0.7746\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9002, F1 Micro: 0.7724, F1 Macro: 0.7687\n",
      "Best result for 2778 samples: F1 Micro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       342\n",
      "                sara       0.67      0.65      0.66       249\n",
      "         radikalisme       0.77      0.82      0.79       302\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1401\n",
      "           macro avg       0.77      0.78      0.78      1401\n",
      "        weighted avg       0.77      0.79      0.78      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.002267024014145136\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.24899935722351 seconds\n",
      "\n",
      "Fold 4 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4991, Accuracy: 0.8609, F1 Micro: 0.6723, F1 Macro: 0.6407\n",
      "Epoch 2/10, Train Loss: 0.3712, Accuracy: 0.8966, F1 Micro: 0.7566, F1 Macro: 0.7532\n",
      "Epoch 3/10, Train Loss: 0.3081, Accuracy: 0.9033, F1 Micro: 0.7613, F1 Macro: 0.7492\n",
      "Epoch 4/10, Train Loss: 0.2637, Accuracy: 0.9025, F1 Micro: 0.7719, F1 Macro: 0.7632\n",
      "Epoch 5/10, Train Loss: 0.2045, Accuracy: 0.9052, F1 Micro: 0.7792, F1 Macro: 0.778\n",
      "Epoch 6/10, Train Loss: 0.1544, Accuracy: 0.9045, F1 Micro: 0.7766, F1 Macro: 0.7716\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9062, F1 Micro: 0.776, F1 Macro: 0.7716\n",
      "Epoch 8/10, Train Loss: 0.083, Accuracy: 0.905, F1 Micro: 0.7813, F1 Macro: 0.777\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.903, F1 Micro: 0.7797, F1 Macro: 0.7785\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9052, F1 Micro: 0.7717, F1 Macro: 0.7718\n",
      "Best result for 3123 samples: F1 Micro: 0.7813\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       342\n",
      "                sara       0.66      0.64      0.65       249\n",
      "         radikalisme       0.81      0.80      0.81       302\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       508\n",
      "\n",
      "           micro avg       0.79      0.78      0.78      1401\n",
      "           macro avg       0.78      0.77      0.78      1401\n",
      "        weighted avg       0.79      0.78      0.78      1401\n",
      "         samples avg       0.46      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.001059187634382397\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.39300298690796 seconds\n",
      "\n",
      "Fold 4 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4844, Accuracy: 0.8706, F1 Micro: 0.6428, F1 Macro: 0.5978\n",
      "Epoch 2/10, Train Loss: 0.3561, Accuracy: 0.8995, F1 Micro: 0.7612, F1 Macro: 0.7554\n",
      "Epoch 3/10, Train Loss: 0.3065, Accuracy: 0.903, F1 Micro: 0.7714, F1 Macro: 0.7713\n",
      "Epoch 4/10, Train Loss: 0.2412, Accuracy: 0.9019, F1 Micro: 0.7779, F1 Macro: 0.7743\n",
      "Epoch 5/10, Train Loss: 0.1835, Accuracy: 0.9023, F1 Micro: 0.7718, F1 Macro: 0.7651\n",
      "Epoch 6/10, Train Loss: 0.1376, Accuracy: 0.9045, F1 Micro: 0.7814, F1 Macro: 0.7801\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9006, F1 Micro: 0.7738, F1 Macro: 0.7723\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9041, F1 Micro: 0.7769, F1 Macro: 0.7758\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9044, F1 Micro: 0.7766, F1 Macro: 0.7729\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9052, F1 Micro: 0.7803, F1 Macro: 0.7795\n",
      "Best result for 3433 samples: F1 Micro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.92      0.90       342\n",
      "                sara       0.71      0.65      0.68       249\n",
      "         radikalisme       0.78      0.84      0.81       302\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       508\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1401\n",
      "           macro avg       0.78      0.78      0.78      1401\n",
      "        weighted avg       0.78      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0005768696137238294\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.56592059135437 seconds\n",
      "\n",
      "Fold 4 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4712, Accuracy: 0.8803, F1 Micro: 0.7022, F1 Macro: 0.6904\n",
      "Epoch 2/10, Train Loss: 0.3441, Accuracy: 0.8995, F1 Micro: 0.7721, F1 Macro: 0.7716\n",
      "Epoch 3/10, Train Loss: 0.2958, Accuracy: 0.8997, F1 Micro: 0.7795, F1 Macro: 0.7783\n",
      "Epoch 4/10, Train Loss: 0.2506, Accuracy: 0.9042, F1 Micro: 0.7887, F1 Macro: 0.7882\n",
      "Epoch 5/10, Train Loss: 0.1929, Accuracy: 0.9034, F1 Micro: 0.7728, F1 Macro: 0.7737\n",
      "Epoch 6/10, Train Loss: 0.144, Accuracy: 0.9045, F1 Micro: 0.7779, F1 Macro: 0.7763\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9014, F1 Micro: 0.7805, F1 Macro: 0.7784\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9036, F1 Micro: 0.7807, F1 Macro: 0.7776\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9041, F1 Micro: 0.7804, F1 Macro: 0.7776\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9041, F1 Micro: 0.7813, F1 Macro: 0.7798\n",
      "Best result for 3712 samples: F1 Micro: 0.7887\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.65      0.74      0.69       249\n",
      "         radikalisme       0.76      0.84      0.80       302\n",
      "pencemaran_nama_baik       0.73      0.79      0.76       508\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1401\n",
      "           macro avg       0.76      0.81      0.79      1401\n",
      "        weighted avg       0.77      0.82      0.79      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0004618804261554033\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 174\n",
      "Sampling duration: 49.31966519355774 seconds\n",
      "\n",
      "Fold 4 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4705, Accuracy: 0.8791, F1 Micro: 0.7121, F1 Macro: 0.7081\n",
      "Epoch 2/10, Train Loss: 0.346, Accuracy: 0.9005, F1 Micro: 0.7451, F1 Macro: 0.7399\n",
      "Epoch 3/10, Train Loss: 0.2915, Accuracy: 0.8972, F1 Micro: 0.7745, F1 Macro: 0.7742\n",
      "Epoch 4/10, Train Loss: 0.233, Accuracy: 0.9081, F1 Micro: 0.7907, F1 Macro: 0.7858\n",
      "Epoch 5/10, Train Loss: 0.1904, Accuracy: 0.9023, F1 Micro: 0.7718, F1 Macro: 0.7643\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9067, F1 Micro: 0.7827, F1 Macro: 0.7778\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.8986, F1 Micro: 0.7777, F1 Macro: 0.7762\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.8997, F1 Micro: 0.7751, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9044, F1 Micro: 0.7814, F1 Macro: 0.7807\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.8984, F1 Micro: 0.776, F1 Macro: 0.7755\n",
      "Best result for 3886 samples: F1 Micro: 0.7907\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       342\n",
      "                sara       0.69      0.65      0.67       249\n",
      "         radikalisme       0.78      0.83      0.81       302\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       508\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1401\n",
      "           macro avg       0.79      0.79      0.79      1401\n",
      "        weighted avg       0.79      0.79      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0003281050943769516\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 45.83793783187866 seconds\n",
      "\n",
      "Fold 4 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4559, Accuracy: 0.8808, F1 Micro: 0.7117, F1 Macro: 0.7101\n",
      "Epoch 2/10, Train Loss: 0.3353, Accuracy: 0.8938, F1 Micro: 0.7724, F1 Macro: 0.7735\n",
      "Epoch 3/10, Train Loss: 0.2701, Accuracy: 0.8977, F1 Micro: 0.7783, F1 Macro: 0.7781\n",
      "Epoch 4/10, Train Loss: 0.2219, Accuracy: 0.9044, F1 Micro: 0.7787, F1 Macro: 0.7713\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.9022, F1 Micro: 0.7709, F1 Macro: 0.7653\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9011, F1 Micro: 0.768, F1 Macro: 0.761\n",
      "Epoch 7/10, Train Loss: 0.0974, Accuracy: 0.9025, F1 Micro: 0.7718, F1 Macro: 0.7637\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.9019, F1 Micro: 0.78, F1 Macro: 0.7778\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9038, F1 Micro: 0.7778, F1 Macro: 0.7718\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.8969, F1 Micro: 0.7766, F1 Macro: 0.7758\n",
      "Best result for 4120 samples: F1 Micro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.90       342\n",
      "                sara       0.65      0.67      0.66       249\n",
      "         radikalisme       0.80      0.82      0.81       302\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1401\n",
      "           macro avg       0.77      0.79      0.78      1401\n",
      "        weighted avg       0.77      0.79      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00013461518392432484\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.49073839187622 seconds\n",
      "\n",
      "Fold 4 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4472, Accuracy: 0.8828, F1 Micro: 0.7111, F1 Macro: 0.6994\n",
      "Epoch 2/10, Train Loss: 0.3232, Accuracy: 0.8984, F1 Micro: 0.7775, F1 Macro: 0.778\n",
      "Epoch 3/10, Train Loss: 0.2667, Accuracy: 0.9017, F1 Micro: 0.7855, F1 Macro: 0.7841\n",
      "Epoch 4/10, Train Loss: 0.2231, Accuracy: 0.9097, F1 Micro: 0.7875, F1 Macro: 0.7765\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9053, F1 Micro: 0.7856, F1 Macro: 0.7824\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9094, F1 Micro: 0.7883, F1 Macro: 0.7819\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9067, F1 Micro: 0.7882, F1 Macro: 0.7865\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.903, F1 Micro: 0.7834, F1 Macro: 0.7803\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9086, F1 Micro: 0.792, F1 Macro: 0.7904\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9056, F1 Micro: 0.784, F1 Macro: 0.7822\n",
      "Best result for 4330 samples: F1 Micro: 0.792\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       342\n",
      "                sara       0.68      0.68      0.68       249\n",
      "         radikalisme       0.80      0.82      0.81       302\n",
      "pencemaran_nama_baik       0.75      0.75      0.75       508\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1401\n",
      "           macro avg       0.79      0.79      0.79      1401\n",
      "        weighted avg       0.79      0.80      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 3.476296333246866e-05\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.51007652282715 seconds\n",
      "\n",
      "Fold 4 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4307, Accuracy: 0.8856, F1 Micro: 0.7287, F1 Macro: 0.7266\n",
      "Epoch 2/10, Train Loss: 0.3055, Accuracy: 0.8916, F1 Micro: 0.7646, F1 Macro: 0.7665\n",
      "Epoch 3/10, Train Loss: 0.2618, Accuracy: 0.9052, F1 Micro: 0.7774, F1 Macro: 0.7763\n",
      "Epoch 4/10, Train Loss: 0.2144, Accuracy: 0.905, F1 Micro: 0.7883, F1 Macro: 0.7849\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.9005, F1 Micro: 0.7756, F1 Macro: 0.7754\n",
      "Epoch 6/10, Train Loss: 0.1252, Accuracy: 0.9053, F1 Micro: 0.7804, F1 Macro: 0.7774\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.8933, F1 Micro: 0.7772, F1 Macro: 0.7783\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9034, F1 Micro: 0.7813, F1 Macro: 0.7781\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9019, F1 Micro: 0.783, F1 Macro: 0.7836\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9042, F1 Micro: 0.7845, F1 Macro: 0.78\n",
      "Best result for 4530 samples: F1 Micro: 0.7883\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       342\n",
      "                sara       0.69      0.67      0.68       249\n",
      "         radikalisme       0.79      0.80      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.81      0.76       508\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1401\n",
      "           macro avg       0.77      0.80      0.78      1401\n",
      "        weighted avg       0.77      0.81      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00018014410161413253\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.68444895744324 seconds\n",
      "\n",
      "Fold 4 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4249, Accuracy: 0.8861, F1 Micro: 0.7401, F1 Macro: 0.7359\n",
      "Epoch 2/10, Train Loss: 0.3028, Accuracy: 0.9033, F1 Micro: 0.7761, F1 Macro: 0.7751\n",
      "Epoch 3/10, Train Loss: 0.2499, Accuracy: 0.9028, F1 Micro: 0.7794, F1 Macro: 0.7765\n",
      "Epoch 4/10, Train Loss: 0.2086, Accuracy: 0.9006, F1 Micro: 0.7825, F1 Macro: 0.7826\n",
      "Epoch 5/10, Train Loss: 0.164, Accuracy: 0.9042, F1 Micro: 0.7674, F1 Macro: 0.7631\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9027, F1 Micro: 0.7774, F1 Macro: 0.7773\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.8977, F1 Micro: 0.7734, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9014, F1 Micro: 0.776, F1 Macro: 0.7748\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.9023, F1 Micro: 0.7822, F1 Macro: 0.7795\n",
      "Epoch 10/10, Train Loss: 0.0391, Accuracy: 0.9005, F1 Micro: 0.7706, F1 Macro: 0.7677\n",
      "Best result for 4663 samples: F1 Micro: 0.7825\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       342\n",
      "                sara       0.61      0.75      0.67       249\n",
      "         radikalisme       0.76      0.84      0.80       302\n",
      "pencemaran_nama_baik       0.72      0.79      0.75       508\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1401\n",
      "           macro avg       0.76      0.82      0.78      1401\n",
      "        weighted avg       0.76      0.82      0.79      1401\n",
      "         samples avg       0.46      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 0.0001231664718943648\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.9691481590271 seconds\n",
      "\n",
      "Fold 4 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4152, Accuracy: 0.8827, F1 Micro: 0.7321, F1 Macro: 0.7301\n",
      "Epoch 2/10, Train Loss: 0.2914, Accuracy: 0.9031, F1 Micro: 0.7747, F1 Macro: 0.7694\n",
      "Epoch 3/10, Train Loss: 0.2436, Accuracy: 0.9045, F1 Micro: 0.7879, F1 Macro: 0.7862\n",
      "Epoch 4/10, Train Loss: 0.1971, Accuracy: 0.9031, F1 Micro: 0.7803, F1 Macro: 0.7777\n",
      "Epoch 5/10, Train Loss: 0.149, Accuracy: 0.8966, F1 Micro: 0.7748, F1 Macro: 0.7728\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9055, F1 Micro: 0.7732, F1 Macro: 0.7675\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.8956, F1 Micro: 0.7752, F1 Macro: 0.7745\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.8983, F1 Micro: 0.7737, F1 Macro: 0.7714\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.8986, F1 Micro: 0.7701, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.8969, F1 Micro: 0.7729, F1 Macro: 0.7736\n",
      "Best result for 4863 samples: F1 Micro: 0.7879\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       342\n",
      "                sara       0.64      0.72      0.68       249\n",
      "         radikalisme       0.76      0.83      0.79       302\n",
      "pencemaran_nama_baik       0.74      0.80      0.76       508\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1401\n",
      "           macro avg       0.77      0.81      0.79      1401\n",
      "        weighted avg       0.77      0.81      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 6.955300705158152e-05\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.212259769439697 seconds\n",
      "\n",
      "Fold 4 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4042, Accuracy: 0.8867, F1 Micro: 0.7162, F1 Macro: 0.7143\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.8948, F1 Micro: 0.7719, F1 Macro: 0.7718\n",
      "Epoch 3/10, Train Loss: 0.2289, Accuracy: 0.9062, F1 Micro: 0.7809, F1 Macro: 0.7747\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.9053, F1 Micro: 0.7808, F1 Macro: 0.7732\n",
      "Epoch 5/10, Train Loss: 0.1523, Accuracy: 0.9008, F1 Micro: 0.7763, F1 Macro: 0.7762\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.898, F1 Micro: 0.7787, F1 Macro: 0.7805\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.8911, F1 Micro: 0.7708, F1 Macro: 0.7722\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.8981, F1 Micro: 0.7725, F1 Macro: 0.7732\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.9002, F1 Micro: 0.7812, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.8992, F1 Micro: 0.7792, F1 Macro: 0.7793\n",
      "Best result for 5063 samples: F1 Micro: 0.7812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.90       342\n",
      "                sara       0.66      0.69      0.67       249\n",
      "         radikalisme       0.80      0.80      0.80       302\n",
      "pencemaran_nama_baik       0.69      0.81      0.74       508\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1401\n",
      "           macro avg       0.76      0.81      0.78      1401\n",
      "        weighted avg       0.75      0.81      0.78      1401\n",
      "         samples avg       0.47      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 8.653918939671712e-06\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.343612670898438 seconds\n",
      "\n",
      "Fold 4 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.387, Accuracy: 0.8861, F1 Micro: 0.7471, F1 Macro: 0.7459\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.9042, F1 Micro: 0.7767, F1 Macro: 0.7767\n",
      "Epoch 3/10, Train Loss: 0.2278, Accuracy: 0.9023, F1 Micro: 0.7851, F1 Macro: 0.7842\n",
      "Epoch 4/10, Train Loss: 0.1787, Accuracy: 0.9036, F1 Micro: 0.7849, F1 Macro: 0.7837\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.8984, F1 Micro: 0.7653, F1 Macro: 0.7645\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.897, F1 Micro: 0.7773, F1 Macro: 0.7773\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.8983, F1 Micro: 0.775, F1 Macro: 0.7751\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9053, F1 Micro: 0.7814, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0423, Accuracy: 0.9023, F1 Micro: 0.7772, F1 Macro: 0.776\n",
      "Epoch 10/10, Train Loss: 0.034, Accuracy: 0.9031, F1 Micro: 0.7811, F1 Macro: 0.7806\n",
      "Best result for 5263 samples: F1 Micro: 0.7851\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       342\n",
      "                sara       0.64      0.73      0.69       249\n",
      "         radikalisme       0.78      0.80      0.79       302\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       508\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1401\n",
      "           macro avg       0.76      0.81      0.78      1401\n",
      "        weighted avg       0.76      0.82      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 6.620540443691425e-05\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.54559350013733 seconds\n",
      "\n",
      "Fold 4 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.8866, F1 Micro: 0.7358, F1 Macro: 0.7257\n",
      "Epoch 2/10, Train Loss: 0.2674, Accuracy: 0.9022, F1 Micro: 0.7607, F1 Macro: 0.747\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9045, F1 Micro: 0.7763, F1 Macro: 0.7753\n",
      "Epoch 4/10, Train Loss: 0.1815, Accuracy: 0.903, F1 Micro: 0.7744, F1 Macro: 0.7689\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.9061, F1 Micro: 0.7712, F1 Macro: 0.7659\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9011, F1 Micro: 0.7786, F1 Macro: 0.777\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9017, F1 Micro: 0.7725, F1 Macro: 0.7694\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.9028, F1 Micro: 0.7745, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9027, F1 Micro: 0.7699, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.9017, F1 Micro: 0.7788, F1 Macro: 0.7782\n",
      "Best result for 5441 samples: F1 Micro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       342\n",
      "                sara       0.66      0.68      0.67       249\n",
      "         radikalisme       0.81      0.80      0.80       302\n",
      "pencemaran_nama_baik       0.71      0.76      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1401\n",
      "           macro avg       0.77      0.79      0.78      1401\n",
      "        weighted avg       0.77      0.79      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 1.8665276911633562e-06\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.934928894042969 seconds\n",
      "\n",
      "Fold 4 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3724, Accuracy: 0.8845, F1 Micro: 0.7354, F1 Macro: 0.7286\n",
      "Epoch 2/10, Train Loss: 0.2464, Accuracy: 0.9008, F1 Micro: 0.778, F1 Macro: 0.7783\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9002, F1 Micro: 0.7539, F1 Macro: 0.7473\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9017, F1 Micro: 0.7864, F1 Macro: 0.7851\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9044, F1 Micro: 0.7794, F1 Macro: 0.779\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9028, F1 Micro: 0.7737, F1 Macro: 0.7714\n",
      "Epoch 7/10, Train Loss: 0.0619, Accuracy: 0.8992, F1 Micro: 0.7633, F1 Macro: 0.7585\n",
      "Epoch 8/10, Train Loss: 0.0497, Accuracy: 0.89, F1 Micro: 0.7689, F1 Macro: 0.7694\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.9017, F1 Micro: 0.7801, F1 Macro: 0.778\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9022, F1 Micro: 0.768, F1 Macro: 0.7677\n",
      "Best result for 5641 samples: F1 Micro: 0.7864\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       342\n",
      "                sara       0.62      0.73      0.67       249\n",
      "         radikalisme       0.79      0.82      0.81       302\n",
      "pencemaran_nama_baik       0.70      0.82      0.76       508\n",
      "\n",
      "           micro avg       0.75      0.83      0.79      1401\n",
      "           macro avg       0.75      0.82      0.79      1401\n",
      "        weighted avg       0.76      0.83      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 2.3546161537524318e-05\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.204758882522583 seconds\n",
      "\n",
      "Fold 4 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3598, Accuracy: 0.8795, F1 Micro: 0.7355, F1 Macro: 0.738\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.9039, F1 Micro: 0.7758, F1 Macro: 0.7768\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.9006, F1 Micro: 0.7828, F1 Macro: 0.7814\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9034, F1 Micro: 0.7818, F1 Macro: 0.7804\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.8992, F1 Micro: 0.782, F1 Macro: 0.782\n",
      "Epoch 6/10, Train Loss: 0.0895, Accuracy: 0.9016, F1 Micro: 0.7835, F1 Macro: 0.7809\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.8969, F1 Micro: 0.7619, F1 Macro: 0.7584\n",
      "Epoch 8/10, Train Loss: 0.0475, Accuracy: 0.9009, F1 Micro: 0.7796, F1 Macro: 0.7791\n",
      "Epoch 9/10, Train Loss: 0.036, Accuracy: 0.9003, F1 Micro: 0.7744, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0282, Accuracy: 0.898, F1 Micro: 0.7761, F1 Macro: 0.7782\n",
      "Best result for 5841 samples: F1 Micro: 0.7835\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.93      0.91       342\n",
      "                sara       0.64      0.70      0.67       249\n",
      "         radikalisme       0.80      0.80      0.80       302\n",
      "pencemaran_nama_baik       0.71      0.80      0.75       508\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1401\n",
      "           macro avg       0.76      0.81      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.78      1401\n",
      "         samples avg       0.46      0.47      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 3.4448504720785426e-06\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.445658445358276 seconds\n",
      "\n",
      "Fold 4 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3575, Accuracy: 0.8858, F1 Micro: 0.744, F1 Macro: 0.7419\n",
      "Epoch 2/10, Train Loss: 0.238, Accuracy: 0.893, F1 Micro: 0.7682, F1 Macro: 0.7662\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9083, F1 Micro: 0.7925, F1 Macro: 0.7938\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9072, F1 Micro: 0.7874, F1 Macro: 0.7845\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9013, F1 Micro: 0.7727, F1 Macro: 0.7737\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9034, F1 Micro: 0.7801, F1 Macro: 0.7798\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9042, F1 Micro: 0.7796, F1 Macro: 0.7761\n",
      "Epoch 8/10, Train Loss: 0.0458, Accuracy: 0.8941, F1 Micro: 0.7695, F1 Macro: 0.7696\n",
      "Epoch 9/10, Train Loss: 0.0384, Accuracy: 0.8952, F1 Micro: 0.7729, F1 Macro: 0.7715\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.9005, F1 Micro: 0.7711, F1 Macro: 0.7689\n",
      "Best result for 6041 samples: F1 Micro: 0.7925\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       342\n",
      "                sara       0.70      0.73      0.71       249\n",
      "         radikalisme       0.79      0.83      0.81       302\n",
      "pencemaran_nama_baik       0.74      0.76      0.75       508\n",
      "\n",
      "           micro avg       0.79      0.80      0.79      1401\n",
      "           macro avg       0.79      0.80      0.79      1401\n",
      "        weighted avg       0.79      0.80      0.79      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 2.1485645811480944e-05\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 178\n",
      "Sampling duration: 4.479750394821167 seconds\n",
      "\n",
      "Fold 4 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3517, Accuracy: 0.8794, F1 Micro: 0.7297, F1 Macro: 0.7277\n",
      "Epoch 2/10, Train Loss: 0.2318, Accuracy: 0.8981, F1 Micro: 0.7675, F1 Macro: 0.7627\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.9045, F1 Micro: 0.7789, F1 Macro: 0.7746\n",
      "Epoch 4/10, Train Loss: 0.1493, Accuracy: 0.8966, F1 Micro: 0.7783, F1 Macro: 0.7761\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.9041, F1 Micro: 0.7664, F1 Macro: 0.7621\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.8977, F1 Micro: 0.768, F1 Macro: 0.7639\n",
      "Epoch 7/10, Train Loss: 0.0564, Accuracy: 0.903, F1 Micro: 0.7792, F1 Macro: 0.7756\n",
      "Epoch 8/10, Train Loss: 0.0424, Accuracy: 0.9023, F1 Micro: 0.7815, F1 Macro: 0.7807\n",
      "Epoch 9/10, Train Loss: 0.033, Accuracy: 0.8988, F1 Micro: 0.7739, F1 Macro: 0.7704\n",
      "Epoch 10/10, Train Loss: 0.0275, Accuracy: 0.8984, F1 Micro: 0.7737, F1 Macro: 0.7734\n",
      "Best result for 6219 samples: F1 Micro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       342\n",
      "                sara       0.68      0.68      0.68       249\n",
      "         radikalisme       0.79      0.81      0.80       302\n",
      "pencemaran_nama_baik       0.71      0.78      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1401\n",
      "           macro avg       0.77      0.79      0.78      1401\n",
      "        weighted avg       0.77      0.80      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 4245.09 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4673, Accuracy: 0.7939, F1 Micro: 0.0449, F1 Macro: 0.0404\n",
      "Epoch 3/10, Train Loss: 0.421, Accuracy: 0.8194, F1 Micro: 0.2561, F1 Macro: 0.1786\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8313, F1 Micro: 0.3933, F1 Macro: 0.3032\n",
      "Epoch 5/10, Train Loss: 0.3375, Accuracy: 0.8416, F1 Micro: 0.4483, F1 Macro: 0.3709\n",
      "Epoch 6/10, Train Loss: 0.2901, Accuracy: 0.8577, F1 Micro: 0.5902, F1 Macro: 0.5609\n",
      "Epoch 7/10, Train Loss: 0.2547, Accuracy: 0.8578, F1 Micro: 0.5852, F1 Macro: 0.5546\n",
      "Epoch 8/10, Train Loss: 0.2265, Accuracy: 0.867, F1 Micro: 0.6411, F1 Macro: 0.6284\n",
      "Epoch 9/10, Train Loss: 0.174, Accuracy: 0.8655, F1 Micro: 0.6682, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.1497, Accuracy: 0.8669, F1 Micro: 0.6611, F1 Macro: 0.6529\n",
      "Best result for 388 samples: F1 Micro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.82      0.84       353\n",
      "                sara       0.54      0.56      0.55       239\n",
      "         radikalisme       0.69      0.58      0.63       273\n",
      "pencemaran_nama_baik       0.66      0.59      0.62       485\n",
      "\n",
      "           micro avg       0.70      0.64      0.67      1350\n",
      "           macro avg       0.69      0.64      0.66      1350\n",
      "        weighted avg       0.70      0.64      0.67      1350\n",
      "         samples avg       0.36      0.36      0.35      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.003724553855136037\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 113.17982602119446 seconds\n",
      "\n",
      "Fold 5 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5693, Accuracy: 0.7897, F1 Micro: 0.0059, F1 Macro: 0.0072\n",
      "Epoch 2/10, Train Loss: 0.4776, Accuracy: 0.8475, F1 Micro: 0.513, F1 Macro: 0.4582\n",
      "Epoch 3/10, Train Loss: 0.3939, Accuracy: 0.8695, F1 Micro: 0.6508, F1 Macro: 0.6544\n",
      "Epoch 4/10, Train Loss: 0.3348, Accuracy: 0.8817, F1 Micro: 0.7023, F1 Macro: 0.6973\n",
      "Epoch 5/10, Train Loss: 0.2592, Accuracy: 0.8816, F1 Micro: 0.7023, F1 Macro: 0.7011\n",
      "Epoch 6/10, Train Loss: 0.1949, Accuracy: 0.8792, F1 Micro: 0.7258, F1 Macro: 0.7256\n",
      "Epoch 7/10, Train Loss: 0.1664, Accuracy: 0.8834, F1 Micro: 0.7247, F1 Macro: 0.7229\n",
      "Epoch 8/10, Train Loss: 0.1232, Accuracy: 0.8845, F1 Micro: 0.7242, F1 Macro: 0.7232\n",
      "Epoch 9/10, Train Loss: 0.1034, Accuracy: 0.8808, F1 Micro: 0.7133, F1 Macro: 0.713\n",
      "Epoch 10/10, Train Loss: 0.079, Accuracy: 0.8833, F1 Micro: 0.7106, F1 Macro: 0.7057\n",
      "Best result for 972 samples: F1 Micro: 0.7258\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.83      0.87       353\n",
      "                sara       0.53      0.66      0.59       239\n",
      "         radikalisme       0.73      0.80      0.76       273\n",
      "pencemaran_nama_baik       0.64      0.73      0.68       485\n",
      "\n",
      "           micro avg       0.70      0.76      0.73      1350\n",
      "           macro avg       0.70      0.75      0.73      1350\n",
      "        weighted avg       0.71      0.76      0.73      1350\n",
      "         samples avg       0.40      0.42      0.40      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004540824517607692\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 101.47651314735413 seconds\n",
      "\n",
      "Fold 5 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5572, Accuracy: 0.8037, F1 Micro: 0.1759, F1 Macro: 0.1596\n",
      "Epoch 2/10, Train Loss: 0.45, Accuracy: 0.867, F1 Micro: 0.6311, F1 Macro: 0.6337\n",
      "Epoch 3/10, Train Loss: 0.3772, Accuracy: 0.8861, F1 Micro: 0.7078, F1 Macro: 0.7094\n",
      "Epoch 4/10, Train Loss: 0.3117, Accuracy: 0.8908, F1 Micro: 0.73, F1 Macro: 0.7292\n",
      "Epoch 5/10, Train Loss: 0.2502, Accuracy: 0.8928, F1 Micro: 0.72, F1 Macro: 0.7204\n",
      "Epoch 6/10, Train Loss: 0.1826, Accuracy: 0.8958, F1 Micro: 0.7322, F1 Macro: 0.7275\n",
      "Epoch 7/10, Train Loss: 0.154, Accuracy: 0.895, F1 Micro: 0.7423, F1 Macro: 0.7374\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.8958, F1 Micro: 0.7497, F1 Macro: 0.7466\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.8909, F1 Micro: 0.7473, F1 Macro: 0.7479\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.8886, F1 Micro: 0.7438, F1 Macro: 0.7446\n",
      "Best result for 1497 samples: F1 Micro: 0.7497\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.87      0.89       353\n",
      "                sara       0.67      0.60      0.63       239\n",
      "         radikalisme       0.80      0.75      0.78       273\n",
      "pencemaran_nama_baik       0.68      0.71      0.70       485\n",
      "\n",
      "           micro avg       0.76      0.74      0.75      1350\n",
      "           macro avg       0.76      0.73      0.75      1350\n",
      "        weighted avg       0.76      0.74      0.75      1350\n",
      "         samples avg       0.41      0.41      0.40      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.005465259635820992\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 91.41640377044678 seconds\n",
      "\n",
      "Fold 5 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5324, Accuracy: 0.8459, F1 Micro: 0.5349, F1 Macro: 0.4756\n",
      "Epoch 2/10, Train Loss: 0.4335, Accuracy: 0.8808, F1 Micro: 0.6935, F1 Macro: 0.6813\n",
      "Epoch 3/10, Train Loss: 0.3497, Accuracy: 0.8952, F1 Micro: 0.7372, F1 Macro: 0.7364\n",
      "Epoch 4/10, Train Loss: 0.2987, Accuracy: 0.8894, F1 Micro: 0.7548, F1 Macro: 0.7591\n",
      "Epoch 5/10, Train Loss: 0.2385, Accuracy: 0.8977, F1 Micro: 0.74, F1 Macro: 0.7358\n",
      "Epoch 6/10, Train Loss: 0.1885, Accuracy: 0.8994, F1 Micro: 0.744, F1 Macro: 0.7364\n",
      "Epoch 7/10, Train Loss: 0.1442, Accuracy: 0.8958, F1 Micro: 0.7586, F1 Macro: 0.7547\n",
      "Epoch 8/10, Train Loss: 0.1119, Accuracy: 0.8964, F1 Micro: 0.7645, F1 Macro: 0.7624\n",
      "Epoch 9/10, Train Loss: 0.085, Accuracy: 0.8986, F1 Micro: 0.7533, F1 Macro: 0.7461\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.897, F1 Micro: 0.754, F1 Macro: 0.749\n",
      "Best result for 1970 samples: F1 Micro: 0.7645\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.91      0.89       353\n",
      "                sara       0.62      0.67      0.65       239\n",
      "         radikalisme       0.79      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.67      0.77      0.72       485\n",
      "\n",
      "           micro avg       0.73      0.80      0.76      1350\n",
      "           macro avg       0.74      0.79      0.76      1350\n",
      "        weighted avg       0.74      0.80      0.77      1350\n",
      "         samples avg       0.43      0.44      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.004257969651371245\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 425\n",
      "Sampling duration: 82.3944890499115 seconds\n",
      "\n",
      "Fold 5 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5261, Accuracy: 0.8481, F1 Micro: 0.4895, F1 Macro: 0.4527\n",
      "Epoch 2/10, Train Loss: 0.4226, Accuracy: 0.888, F1 Micro: 0.7043, F1 Macro: 0.6884\n",
      "Epoch 3/10, Train Loss: 0.3431, Accuracy: 0.8958, F1 Micro: 0.7346, F1 Macro: 0.7301\n",
      "Epoch 4/10, Train Loss: 0.2874, Accuracy: 0.8992, F1 Micro: 0.7493, F1 Macro: 0.7437\n",
      "Epoch 5/10, Train Loss: 0.2287, Accuracy: 0.9022, F1 Micro: 0.7494, F1 Macro: 0.7458\n",
      "Epoch 6/10, Train Loss: 0.1768, Accuracy: 0.8997, F1 Micro: 0.7636, F1 Macro: 0.7631\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9016, F1 Micro: 0.7539, F1 Macro: 0.7508\n",
      "Epoch 8/10, Train Loss: 0.1088, Accuracy: 0.9002, F1 Micro: 0.7624, F1 Macro: 0.7622\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9005, F1 Micro: 0.754, F1 Macro: 0.7532\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9039, F1 Micro: 0.7648, F1 Macro: 0.7625\n",
      "Best result for 2395 samples: F1 Micro: 0.7648\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       353\n",
      "                sara       0.69      0.61      0.65       239\n",
      "         radikalisme       0.83      0.75      0.79       273\n",
      "pencemaran_nama_baik       0.71      0.70      0.70       485\n",
      "\n",
      "           micro avg       0.79      0.74      0.76      1350\n",
      "           macro avg       0.79      0.73      0.76      1350\n",
      "        weighted avg       0.79      0.74      0.77      1350\n",
      "         samples avg       0.42      0.41      0.41      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0042727147694677165\n",
      "Samples above threshold: 383\n",
      "Acquired samples: 383\n",
      "Sampling duration: 74.34170532226562 seconds\n",
      "\n",
      "Fold 5 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5101, Accuracy: 0.8439, F1 Micro: 0.4632, F1 Macro: 0.4163\n",
      "Epoch 2/10, Train Loss: 0.3896, Accuracy: 0.8913, F1 Micro: 0.7264, F1 Macro: 0.7215\n",
      "Epoch 3/10, Train Loss: 0.3234, Accuracy: 0.9009, F1 Micro: 0.7578, F1 Macro: 0.7565\n",
      "Epoch 4/10, Train Loss: 0.2799, Accuracy: 0.9016, F1 Micro: 0.7508, F1 Macro: 0.7451\n",
      "Epoch 5/10, Train Loss: 0.2152, Accuracy: 0.9016, F1 Micro: 0.7554, F1 Macro: 0.7469\n",
      "Epoch 6/10, Train Loss: 0.1605, Accuracy: 0.9027, F1 Micro: 0.7577, F1 Macro: 0.7534\n",
      "Epoch 7/10, Train Loss: 0.1188, Accuracy: 0.902, F1 Micro: 0.7721, F1 Macro: 0.7706\n",
      "Epoch 8/10, Train Loss: 0.0975, Accuracy: 0.8991, F1 Micro: 0.7698, F1 Macro: 0.769\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9016, F1 Micro: 0.7656, F1 Macro: 0.7586\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.9023, F1 Micro: 0.7684, F1 Macro: 0.7649\n",
      "Best result for 2778 samples: F1 Micro: 0.7721\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       353\n",
      "                sara       0.69      0.63      0.66       239\n",
      "         radikalisme       0.79      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.67      0.76      0.71       485\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1350\n",
      "           macro avg       0.77      0.78      0.77      1350\n",
      "        weighted avg       0.76      0.79      0.77      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.002312145894393325\n",
      "Samples above threshold: 345\n",
      "Acquired samples: 345\n",
      "Sampling duration: 67.17980813980103 seconds\n",
      "\n",
      "Fold 5 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5063, Accuracy: 0.8542, F1 Micro: 0.5188, F1 Macro: 0.4872\n",
      "Epoch 2/10, Train Loss: 0.3885, Accuracy: 0.8948, F1 Micro: 0.7501, F1 Macro: 0.7483\n",
      "Epoch 3/10, Train Loss: 0.3219, Accuracy: 0.8984, F1 Micro: 0.7748, F1 Macro: 0.7746\n",
      "Epoch 4/10, Train Loss: 0.2741, Accuracy: 0.9064, F1 Micro: 0.7723, F1 Macro: 0.7678\n",
      "Epoch 5/10, Train Loss: 0.2176, Accuracy: 0.9039, F1 Micro: 0.7763, F1 Macro: 0.7736\n",
      "Epoch 6/10, Train Loss: 0.162, Accuracy: 0.9047, F1 Micro: 0.765, F1 Macro: 0.7605\n",
      "Epoch 7/10, Train Loss: 0.1234, Accuracy: 0.9028, F1 Micro: 0.7602, F1 Macro: 0.7521\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9039, F1 Micro: 0.7771, F1 Macro: 0.7748\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9006, F1 Micro: 0.7757, F1 Macro: 0.7728\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.908, F1 Micro: 0.78, F1 Macro: 0.7766\n",
      "Best result for 3123 samples: F1 Micro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.67      0.65      0.66       239\n",
      "         radikalisme       0.81      0.79      0.80       273\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       485\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.77      0.78      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0012684870744124055\n",
      "Samples above threshold: 310\n",
      "Acquired samples: 310\n",
      "Sampling duration: 60.70333194732666 seconds\n",
      "\n",
      "Fold 5 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4918, Accuracy: 0.8745, F1 Micro: 0.6686, F1 Macro: 0.6552\n",
      "Epoch 2/10, Train Loss: 0.3719, Accuracy: 0.8969, F1 Micro: 0.7566, F1 Macro: 0.7551\n",
      "Epoch 3/10, Train Loss: 0.3092, Accuracy: 0.9002, F1 Micro: 0.7358, F1 Macro: 0.7211\n",
      "Epoch 4/10, Train Loss: 0.2603, Accuracy: 0.9055, F1 Micro: 0.7725, F1 Macro: 0.7687\n",
      "Epoch 5/10, Train Loss: 0.2014, Accuracy: 0.9055, F1 Micro: 0.7658, F1 Macro: 0.7602\n",
      "Epoch 6/10, Train Loss: 0.1541, Accuracy: 0.9067, F1 Micro: 0.7691, F1 Macro: 0.7687\n",
      "Epoch 7/10, Train Loss: 0.1198, Accuracy: 0.9045, F1 Micro: 0.7761, F1 Macro: 0.7721\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9053, F1 Micro: 0.7757, F1 Macro: 0.7713\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9058, F1 Micro: 0.7725, F1 Macro: 0.7707\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9061, F1 Micro: 0.7721, F1 Macro: 0.768\n",
      "Best result for 3433 samples: F1 Micro: 0.7761\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       353\n",
      "                sara       0.69      0.62      0.65       239\n",
      "         radikalisme       0.79      0.82      0.81       273\n",
      "pencemaran_nama_baik       0.69      0.75      0.72       485\n",
      "\n",
      "           micro avg       0.77      0.78      0.78      1350\n",
      "           macro avg       0.77      0.78      0.77      1350\n",
      "        weighted avg       0.77      0.78      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.0005980833666399121\n",
      "Samples above threshold: 279\n",
      "Acquired samples: 279\n",
      "Sampling duration: 54.52871108055115 seconds\n",
      "\n",
      "Fold 5 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.485, Accuracy: 0.8819, F1 Micro: 0.6889, F1 Macro: 0.6816\n",
      "Epoch 2/10, Train Loss: 0.3624, Accuracy: 0.8964, F1 Micro: 0.7261, F1 Macro: 0.7086\n",
      "Epoch 3/10, Train Loss: 0.3095, Accuracy: 0.9041, F1 Micro: 0.7704, F1 Macro: 0.7727\n",
      "Epoch 4/10, Train Loss: 0.2506, Accuracy: 0.9062, F1 Micro: 0.7792, F1 Macro: 0.7781\n",
      "Epoch 5/10, Train Loss: 0.193, Accuracy: 0.8998, F1 Micro: 0.7742, F1 Macro: 0.774\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9047, F1 Micro: 0.7739, F1 Macro: 0.7699\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.903, F1 Micro: 0.7741, F1 Macro: 0.772\n",
      "Epoch 8/10, Train Loss: 0.0866, Accuracy: 0.9112, F1 Micro: 0.7892, F1 Macro: 0.7854\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9047, F1 Micro: 0.7779, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9045, F1 Micro: 0.7764, F1 Macro: 0.7739\n",
      "Best result for 3712 samples: F1 Micro: 0.7892\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       353\n",
      "                sara       0.72      0.64      0.68       239\n",
      "         radikalisme       0.82      0.80      0.81       273\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       485\n",
      "\n",
      "           micro avg       0.79      0.79      0.79      1350\n",
      "           macro avg       0.79      0.78      0.79      1350\n",
      "        weighted avg       0.79      0.79      0.79      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 3886\n",
      "Threshold: 0.00018451775831636048\n",
      "Samples above threshold: 251\n",
      "Acquired samples: 174\n",
      "Sampling duration: 49.34924864768982 seconds\n",
      "\n",
      "Fold 5 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.471, Accuracy: 0.8825, F1 Micro: 0.7154, F1 Macro: 0.7034\n",
      "Epoch 2/10, Train Loss: 0.3466, Accuracy: 0.898, F1 Micro: 0.7614, F1 Macro: 0.7621\n",
      "Epoch 3/10, Train Loss: 0.2845, Accuracy: 0.9039, F1 Micro: 0.7652, F1 Macro: 0.7579\n",
      "Epoch 4/10, Train Loss: 0.2232, Accuracy: 0.9019, F1 Micro: 0.7662, F1 Macro: 0.7649\n",
      "Epoch 5/10, Train Loss: 0.1789, Accuracy: 0.9075, F1 Micro: 0.7754, F1 Macro: 0.7701\n",
      "Epoch 6/10, Train Loss: 0.1366, Accuracy: 0.9064, F1 Micro: 0.767, F1 Macro: 0.7576\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.9045, F1 Micro: 0.784, F1 Macro: 0.7848\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7779\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.908, F1 Micro: 0.7792, F1 Macro: 0.7758\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9077, F1 Micro: 0.7828, F1 Macro: 0.7775\n",
      "Best result for 3886 samples: F1 Micro: 0.784\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       353\n",
      "                sara       0.62      0.73      0.67       239\n",
      "         radikalisme       0.80      0.83      0.81       273\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1350\n",
      "           macro avg       0.76      0.82      0.78      1350\n",
      "        weighted avg       0.76      0.82      0.79      1350\n",
      "         samples avg       0.45      0.46      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.0002665758540388197\n",
      "Samples above threshold: 234\n",
      "Acquired samples: 234\n",
      "Sampling duration: 45.436073541641235 seconds\n",
      "\n",
      "Fold 5 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4538, Accuracy: 0.8788, F1 Micro: 0.6856, F1 Macro: 0.6573\n",
      "Epoch 2/10, Train Loss: 0.3317, Accuracy: 0.9002, F1 Micro: 0.7632, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.2745, Accuracy: 0.908, F1 Micro: 0.781, F1 Macro: 0.7768\n",
      "Epoch 4/10, Train Loss: 0.2266, Accuracy: 0.9013, F1 Micro: 0.7831, F1 Macro: 0.7849\n",
      "Epoch 5/10, Train Loss: 0.1761, Accuracy: 0.9011, F1 Micro: 0.7789, F1 Macro: 0.7801\n",
      "Epoch 6/10, Train Loss: 0.126, Accuracy: 0.9067, F1 Micro: 0.7804, F1 Macro: 0.7789\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.905, F1 Micro: 0.781, F1 Macro: 0.7814\n",
      "Epoch 8/10, Train Loss: 0.0801, Accuracy: 0.9014, F1 Micro: 0.7776, F1 Macro: 0.778\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9084, F1 Micro: 0.779, F1 Macro: 0.7755\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9042, F1 Micro: 0.775, F1 Macro: 0.7739\n",
      "Best result for 4120 samples: F1 Micro: 0.7831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       353\n",
      "                sara       0.63      0.74      0.68       239\n",
      "         radikalisme       0.78      0.85      0.81       273\n",
      "pencemaran_nama_baik       0.65      0.84      0.73       485\n",
      "\n",
      "           micro avg       0.73      0.85      0.78      1350\n",
      "           macro avg       0.74      0.84      0.78      1350\n",
      "        weighted avg       0.74      0.85      0.79      1350\n",
      "         samples avg       0.46      0.47      0.46      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 0.00040172896697185937\n",
      "Samples above threshold: 210\n",
      "Acquired samples: 210\n",
      "Sampling duration: 41.413246393203735 seconds\n",
      "\n",
      "Fold 5 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4555, Accuracy: 0.8819, F1 Micro: 0.7019, F1 Macro: 0.6934\n",
      "Epoch 2/10, Train Loss: 0.3296, Accuracy: 0.8975, F1 Micro: 0.7305, F1 Macro: 0.7162\n",
      "Epoch 3/10, Train Loss: 0.277, Accuracy: 0.9022, F1 Micro: 0.7643, F1 Macro: 0.7542\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.9052, F1 Micro: 0.7738, F1 Macro: 0.7711\n",
      "Epoch 5/10, Train Loss: 0.168, Accuracy: 0.9078, F1 Micro: 0.7683, F1 Macro: 0.7656\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9067, F1 Micro: 0.7811, F1 Macro: 0.778\n",
      "Epoch 7/10, Train Loss: 0.0947, Accuracy: 0.9094, F1 Micro: 0.7829, F1 Macro: 0.7806\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.905, F1 Micro: 0.766, F1 Macro: 0.7585\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9053, F1 Micro: 0.7735, F1 Macro: 0.77\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9062, F1 Micro: 0.7815, F1 Macro: 0.7801\n",
      "Best result for 4330 samples: F1 Micro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       353\n",
      "                sara       0.70      0.65      0.68       239\n",
      "         radikalisme       0.80      0.78      0.79       273\n",
      "pencemaran_nama_baik       0.72      0.73      0.73       485\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1350\n",
      "           macro avg       0.79      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.77      0.78      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 8.90186056494713e-05\n",
      "Samples above threshold: 189\n",
      "Acquired samples: 200\n",
      "Sampling duration: 37.50732707977295 seconds\n",
      "\n",
      "Fold 5 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4434, Accuracy: 0.8808, F1 Micro: 0.6724, F1 Macro: 0.6661\n",
      "Epoch 2/10, Train Loss: 0.3162, Accuracy: 0.9003, F1 Micro: 0.7521, F1 Macro: 0.7489\n",
      "Epoch 3/10, Train Loss: 0.2552, Accuracy: 0.8998, F1 Micro: 0.776, F1 Macro: 0.7747\n",
      "Epoch 4/10, Train Loss: 0.2126, Accuracy: 0.9042, F1 Micro: 0.7802, F1 Macro: 0.7795\n",
      "Epoch 5/10, Train Loss: 0.167, Accuracy: 0.9106, F1 Micro: 0.779, F1 Macro: 0.7778\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9072, F1 Micro: 0.7829, F1 Macro: 0.7804\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9067, F1 Micro: 0.7727, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9084, F1 Micro: 0.7744, F1 Macro: 0.7683\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9078, F1 Micro: 0.7799, F1 Macro: 0.7737\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9087, F1 Micro: 0.7735, F1 Macro: 0.7655\n",
      "Best result for 4530 samples: F1 Micro: 0.7829\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.92      0.93       353\n",
      "                sara       0.65      0.67      0.66       239\n",
      "         radikalisme       0.78      0.84      0.81       273\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       485\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1350\n",
      "           macro avg       0.77      0.79      0.78      1350\n",
      "        weighted avg       0.77      0.79      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 4663\n",
      "Threshold: 5.744014051742853e-05\n",
      "Samples above threshold: 169\n",
      "Acquired samples: 133\n",
      "Sampling duration: 33.54995322227478 seconds\n",
      "\n",
      "Fold 5 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.431, Accuracy: 0.8813, F1 Micro: 0.7196, F1 Macro: 0.702\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.9013, F1 Micro: 0.749, F1 Macro: 0.7401\n",
      "Epoch 3/10, Train Loss: 0.2422, Accuracy: 0.9053, F1 Micro: 0.7735, F1 Macro: 0.7691\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.9052, F1 Micro: 0.7722, F1 Macro: 0.7681\n",
      "Epoch 5/10, Train Loss: 0.1553, Accuracy: 0.9014, F1 Micro: 0.7777, F1 Macro: 0.778\n",
      "Epoch 6/10, Train Loss: 0.121, Accuracy: 0.9078, F1 Micro: 0.7821, F1 Macro: 0.7788\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9047, F1 Micro: 0.7679, F1 Macro: 0.7643\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9072, F1 Micro: 0.7789, F1 Macro: 0.7754\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9058, F1 Micro: 0.7803, F1 Macro: 0.7814\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9036, F1 Micro: 0.7839, F1 Macro: 0.7835\n",
      "Best result for 4663 samples: F1 Micro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.95      0.91       353\n",
      "                sara       0.64      0.74      0.69       239\n",
      "         radikalisme       0.78      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       485\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1350\n",
      "           macro avg       0.75      0.82      0.78      1350\n",
      "        weighted avg       0.75      0.83      0.79      1350\n",
      "         samples avg       0.46      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 2.8479597858677153e-05\n",
      "Samples above threshold: 156\n",
      "Acquired samples: 200\n",
      "Sampling duration: 31.029383659362793 seconds\n",
      "\n",
      "Fold 5 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4315, Accuracy: 0.8867, F1 Micro: 0.7334, F1 Macro: 0.727\n",
      "Epoch 2/10, Train Loss: 0.2957, Accuracy: 0.9006, F1 Micro: 0.7561, F1 Macro: 0.752\n",
      "Epoch 3/10, Train Loss: 0.243, Accuracy: 0.9, F1 Micro: 0.7814, F1 Macro: 0.7825\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9045, F1 Micro: 0.7638, F1 Macro: 0.7592\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.9022, F1 Micro: 0.778, F1 Macro: 0.7798\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9072, F1 Micro: 0.7762, F1 Macro: 0.7731\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.9069, F1 Micro: 0.7831, F1 Macro: 0.7818\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9055, F1 Micro: 0.7781, F1 Macro: 0.7734\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9062, F1 Micro: 0.7799, F1 Macro: 0.7768\n",
      "Epoch 10/10, Train Loss: 0.0402, Accuracy: 0.9003, F1 Micro: 0.7726, F1 Macro: 0.7703\n",
      "Best result for 4863 samples: F1 Micro: 0.7831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       353\n",
      "                sara       0.63      0.71      0.67       239\n",
      "         radikalisme       0.80      0.82      0.81       273\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       485\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1350\n",
      "           macro avg       0.77      0.80      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.79      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 2.1233352526905946e-05\n",
      "Samples above threshold: 136\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.11148238182068 seconds\n",
      "\n",
      "Fold 5 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4096, Accuracy: 0.8883, F1 Micro: 0.7, F1 Macro: 0.6831\n",
      "Epoch 2/10, Train Loss: 0.2911, Accuracy: 0.9041, F1 Micro: 0.7613, F1 Macro: 0.7577\n",
      "Epoch 3/10, Train Loss: 0.2359, Accuracy: 0.9052, F1 Micro: 0.7763, F1 Macro: 0.7747\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9084, F1 Micro: 0.782, F1 Macro: 0.7822\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.903, F1 Micro: 0.7731, F1 Macro: 0.7719\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9073, F1 Micro: 0.7782, F1 Macro: 0.7736\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9044, F1 Micro: 0.7834, F1 Macro: 0.7828\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.8998, F1 Micro: 0.7712, F1 Macro: 0.7655\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9086, F1 Micro: 0.7788, F1 Macro: 0.7767\n",
      "Epoch 10/10, Train Loss: 0.0364, Accuracy: 0.9083, F1 Micro: 0.783, F1 Macro: 0.7787\n",
      "Best result for 5063 samples: F1 Micro: 0.7834\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       353\n",
      "                sara       0.62      0.72      0.67       239\n",
      "         radikalisme       0.80      0.82      0.81       273\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       485\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1350\n",
      "           macro avg       0.76      0.81      0.78      1350\n",
      "        weighted avg       0.76      0.82      0.79      1350\n",
      "         samples avg       0.45      0.46      0.45      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 1.3347732874535723e-05\n",
      "Samples above threshold: 116\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.31887698173523 seconds\n",
      "\n",
      "Fold 5 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3981, Accuracy: 0.8922, F1 Micro: 0.733, F1 Macro: 0.7294\n",
      "Epoch 2/10, Train Loss: 0.2777, Accuracy: 0.9027, F1 Micro: 0.771, F1 Macro: 0.7668\n",
      "Epoch 3/10, Train Loss: 0.2244, Accuracy: 0.9053, F1 Micro: 0.7694, F1 Macro: 0.7579\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9047, F1 Micro: 0.7769, F1 Macro: 0.7774\n",
      "Epoch 5/10, Train Loss: 0.1392, Accuracy: 0.9048, F1 Micro: 0.7773, F1 Macro: 0.7768\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9087, F1 Micro: 0.7793, F1 Macro: 0.7785\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9083, F1 Micro: 0.7817, F1 Macro: 0.7796\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9094, F1 Micro: 0.7778, F1 Macro: 0.7693\n",
      "Epoch 9/10, Train Loss: 0.0467, Accuracy: 0.9092, F1 Micro: 0.789, F1 Macro: 0.787\n",
      "Epoch 10/10, Train Loss: 0.0328, Accuracy: 0.9039, F1 Micro: 0.7755, F1 Macro: 0.7724\n",
      "Best result for 5263 samples: F1 Micro: 0.789\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.93      0.92       353\n",
      "                sara       0.68      0.70      0.69       239\n",
      "         radikalisme       0.77      0.84      0.80       273\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       485\n",
      "\n",
      "           micro avg       0.77      0.80      0.79      1350\n",
      "           macro avg       0.77      0.80      0.79      1350\n",
      "        weighted avg       0.77      0.80      0.79      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 5441\n",
      "Threshold: 4.362369281807332e-06\n",
      "Samples above threshold: 96\n",
      "Acquired samples: 178\n",
      "Sampling duration: 19.60939335823059 seconds\n",
      "\n",
      "Fold 5 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.391, Accuracy: 0.8856, F1 Micro: 0.7462, F1 Macro: 0.7431\n",
      "Epoch 2/10, Train Loss: 0.2686, Accuracy: 0.9038, F1 Micro: 0.777, F1 Macro: 0.7772\n",
      "Epoch 3/10, Train Loss: 0.2244, Accuracy: 0.9116, F1 Micro: 0.783, F1 Macro: 0.7781\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9122, F1 Micro: 0.7838, F1 Macro: 0.7808\n",
      "Epoch 5/10, Train Loss: 0.1401, Accuracy: 0.9039, F1 Micro: 0.7818, F1 Macro: 0.7787\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9066, F1 Micro: 0.7662, F1 Macro: 0.7594\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9058, F1 Micro: 0.7705, F1 Macro: 0.7639\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9053, F1 Micro: 0.7793, F1 Macro: 0.7767\n",
      "Epoch 9/10, Train Loss: 0.0406, Accuracy: 0.907, F1 Micro: 0.7732, F1 Macro: 0.7675\n",
      "Epoch 10/10, Train Loss: 0.0341, Accuracy: 0.9087, F1 Micro: 0.7798, F1 Macro: 0.7745\n",
      "Best result for 5441 samples: F1 Micro: 0.7838\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.71      0.65      0.68       239\n",
      "         radikalisme       0.81      0.80      0.80       273\n",
      "pencemaran_nama_baik       0.78      0.66      0.71       485\n",
      "\n",
      "           micro avg       0.82      0.75      0.78      1350\n",
      "           macro avg       0.81      0.76      0.78      1350\n",
      "        weighted avg       0.81      0.75      0.78      1350\n",
      "         samples avg       0.43      0.42      0.42      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 1.7051994109351658e-05\n",
      "Samples above threshold: 78\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.856112480163574 seconds\n",
      "\n",
      "Fold 5 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3831, Accuracy: 0.8934, F1 Micro: 0.7313, F1 Macro: 0.7221\n",
      "Epoch 2/10, Train Loss: 0.2524, Accuracy: 0.9036, F1 Micro: 0.7732, F1 Macro: 0.7728\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.9044, F1 Micro: 0.7758, F1 Macro: 0.7716\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9019, F1 Micro: 0.7765, F1 Macro: 0.774\n",
      "Epoch 5/10, Train Loss: 0.1303, Accuracy: 0.9062, F1 Micro: 0.7748, F1 Macro: 0.7701\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9066, F1 Micro: 0.7835, F1 Macro: 0.7797\n",
      "Epoch 7/10, Train Loss: 0.0702, Accuracy: 0.9022, F1 Micro: 0.7804, F1 Macro: 0.7799\n",
      "Epoch 8/10, Train Loss: 0.0514, Accuracy: 0.9059, F1 Micro: 0.778, F1 Macro: 0.77\n",
      "Epoch 9/10, Train Loss: 0.0412, Accuracy: 0.907, F1 Micro: 0.7836, F1 Macro: 0.7801\n",
      "Epoch 10/10, Train Loss: 0.0363, Accuracy: 0.9055, F1 Micro: 0.7753, F1 Macro: 0.7706\n",
      "Best result for 5641 samples: F1 Micro: 0.7836\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.93       353\n",
      "                sara       0.65      0.67      0.66       239\n",
      "         radikalisme       0.81      0.80      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.77      0.80      0.78      1350\n",
      "           macro avg       0.77      0.79      0.78      1350\n",
      "        weighted avg       0.77      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 2.1746839365732756e-06\n",
      "Samples above threshold: 58\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.178428888320923 seconds\n",
      "\n",
      "Fold 5 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.373, Accuracy: 0.8923, F1 Micro: 0.7291, F1 Macro: 0.7252\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.9009, F1 Micro: 0.7464, F1 Macro: 0.7386\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9058, F1 Micro: 0.7623, F1 Macro: 0.7483\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9078, F1 Micro: 0.782, F1 Macro: 0.7787\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.9028, F1 Micro: 0.778, F1 Macro: 0.7761\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9013, F1 Micro: 0.7787, F1 Macro: 0.7795\n",
      "Epoch 7/10, Train Loss: 0.0661, Accuracy: 0.8994, F1 Micro: 0.7755, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0488, Accuracy: 0.907, F1 Micro: 0.7771, F1 Macro: 0.7717\n",
      "Epoch 9/10, Train Loss: 0.0401, Accuracy: 0.9073, F1 Micro: 0.7806, F1 Macro: 0.7784\n",
      "Epoch 10/10, Train Loss: 0.033, Accuracy: 0.9047, F1 Micro: 0.7798, F1 Macro: 0.7788\n",
      "Best result for 5841 samples: F1 Micro: 0.782\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       353\n",
      "                sara       0.67      0.64      0.65       239\n",
      "         radikalisme       0.83      0.79      0.81       273\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.78      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.45      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 1.5911465561657684e-05\n",
      "Samples above threshold: 38\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.22574520111084 seconds\n",
      "\n",
      "Fold 5 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3534, Accuracy: 0.8931, F1 Micro: 0.7253, F1 Macro: 0.7143\n",
      "Epoch 2/10, Train Loss: 0.2353, Accuracy: 0.9056, F1 Micro: 0.7644, F1 Macro: 0.7528\n",
      "Epoch 3/10, Train Loss: 0.1968, Accuracy: 0.907, F1 Micro: 0.7662, F1 Macro: 0.7607\n",
      "Epoch 4/10, Train Loss: 0.1535, Accuracy: 0.9053, F1 Micro: 0.7775, F1 Macro: 0.775\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.9095, F1 Micro: 0.7863, F1 Macro: 0.7833\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.908, F1 Micro: 0.7785, F1 Macro: 0.7702\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9111, F1 Micro: 0.7806, F1 Macro: 0.7747\n",
      "Epoch 8/10, Train Loss: 0.0463, Accuracy: 0.908, F1 Micro: 0.7861, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.9067, F1 Micro: 0.7803, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9033, F1 Micro: 0.7768, F1 Macro: 0.776\n",
      "Best result for 6041 samples: F1 Micro: 0.7863\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.92       353\n",
      "                sara       0.67      0.67      0.67       239\n",
      "         radikalisme       0.83      0.78      0.81       273\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       485\n",
      "\n",
      "           micro avg       0.78      0.79      0.79      1350\n",
      "           macro avg       0.78      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.79      0.79      1350\n",
      "         samples avg       0.45      0.44      0.43      1350\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6219\n",
      "Threshold: 6.151711068014266e-06\n",
      "Samples above threshold: 18\n",
      "Acquired samples: 178\n",
      "Sampling duration: 4.602344036102295 seconds\n",
      "\n",
      "Fold 5 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3564, Accuracy: 0.8938, F1 Micro: 0.7375, F1 Macro: 0.7271\n",
      "Epoch 2/10, Train Loss: 0.2371, Accuracy: 0.9023, F1 Micro: 0.7626, F1 Macro: 0.7602\n",
      "Epoch 3/10, Train Loss: 0.1926, Accuracy: 0.9052, F1 Micro: 0.7814, F1 Macro: 0.7777\n",
      "Epoch 4/10, Train Loss: 0.1513, Accuracy: 0.9073, F1 Micro: 0.7756, F1 Macro: 0.7732\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9066, F1 Micro: 0.7774, F1 Macro: 0.7753\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9062, F1 Micro: 0.7691, F1 Macro: 0.7658\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.9055, F1 Micro: 0.7777, F1 Macro: 0.7743\n",
      "Epoch 8/10, Train Loss: 0.0464, Accuracy: 0.9062, F1 Micro: 0.7807, F1 Macro: 0.7777\n",
      "Epoch 9/10, Train Loss: 0.0367, Accuracy: 0.9064, F1 Micro: 0.7767, F1 Macro: 0.773\n",
      "Epoch 10/10, Train Loss: 0.0312, Accuracy: 0.905, F1 Micro: 0.7807, F1 Macro: 0.7813\n",
      "Best result for 6219 samples: F1 Micro: 0.7814\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.90       353\n",
      "                sara       0.65      0.66      0.65       239\n",
      "         radikalisme       0.79      0.86      0.82       273\n",
      "pencemaran_nama_baik       0.71      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.76      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 4258.90 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[2:6]\n",
    "X = data['processed_text'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    train_fold_df = pd.DataFrame(X_train_fold, columns=['processed_text'])\n",
    "    train_fold_df[label_columns] = y_train_fold\n",
    "\n",
    "    val_fold_df = pd.DataFrame(X_val_fold, columns=['processed_text'])\n",
    "    val_fold_df[label_columns] = y_val_fold\n",
    "\n",
    "    fold_data_dir = 'kfold_splits'\n",
    "    if not os.path.exists(fold_data_dir):\n",
    "        os.makedirs(fold_data_dir)\n",
    "\n",
    "    train_fold_df.to_csv(f'{fold_data_dir}/train_fold_{fold + 1}.csv', index=False)\n",
    "    val_fold_df.to_csv(f'{fold_data_dir}/val_fold_{fold + 1}.csv', index=False)\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    checkpoints = [\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(f'{filename}-fold-{fold + 1}-model')\n",
    "        \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        sampling_args = (\n",
    "            model, X_pool, train_indices, remaining_indices, sampling_dur, \n",
    "            new_samples_shared, fold, X_train_fold, y_train_fold\n",
    "        )\n",
    "        notebook_launcher(monte_carlo_dropout_sampling, sampling_args, num_processes=2)\n",
    "        \n",
    "        # Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a80a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T20:05:23.229405Z",
     "iopub.status.busy": "2025-06-26T20:05:23.229077Z",
     "iopub.status.idle": "2025-06-26T20:05:24.212947Z",
     "shell.execute_reply": "2025-06-26T20:05:24.212111Z"
    },
    "papermill": {
     "duration": 1.220994,
     "end_time": "2025-06-26T20:05:24.214999",
     "exception": false,
     "start_time": "2025-06-26T20:05:22.994005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7559686,
     "sourceId": 12015975,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21304.410318,
   "end_time": "2025-06-26T20:05:27.704526",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-26T14:10:23.294208",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05235d517b8045f483807a4560094f71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1586b19855a4446597a7f866458c2024": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8c6893607c24316ab5ef2b1e17f36a8",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8de8717f75c8417680a2b619e138af07",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "1c1f01b5948d46b2ad5a7b52d61c67f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_313f0fae229c412db547e580dafa14c2",
        "IPY_MODEL_bc25cb41f4a645fbba072dd2dae9f53b",
        "IPY_MODEL_f8029166856645339483549e6a482a46"
       ],
       "layout": "IPY_MODEL_3838490b5df44328b9d4e653adae62cd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "25145830f14e45db9bf361d3a2f6eca6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "293e44ada18b4926884361aebfae7e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "313f0fae229c412db547e580dafa14c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_53fcb13dbb1f43ee9a604e88f047aa67",
       "placeholder": "​",
       "style": "IPY_MODEL_d513f6e4185245848ec7330cdc5e8a56",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: "
      }
     },
     "334054335a7f43db84f6a7819a8fc7a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34311a7fdf0e4996afb6e5b38dffb18a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7a4534cf299f40efa878fc79ceb7bc56",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b7199d947014cf6a475a2c7e332df39",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3715e56656ee47f0b99f80d436e478fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3838490b5df44328b9d4e653adae62cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38d010488aae4aa28a565265114a0017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db42f578d45041dba4912a65752b8d4a",
       "placeholder": "​",
       "style": "IPY_MODEL_ef1508cb26be47e98343d166cb595de9",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "39250da05573454087e03ac04573a663": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77c712cc781f45f88798915be551a35b",
       "placeholder": "​",
       "style": "IPY_MODEL_d790742a9f9b42a5a36fca5ec14a0e81",
       "tabbable": null,
       "tooltip": null,
       "value": " 498M/498M [00:03&lt;00:00, 149MB/s]"
      }
     },
     "3af74dc4ff8d47f19f5134a62b6d1a9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0d03b7638ff4637bbd788d8ac8594ea",
       "placeholder": "​",
       "style": "IPY_MODEL_53bf2ec65f0a42f1ad51e0e0c95e0ebd",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 9.67kB/s]"
      }
     },
     "43a8506a2c514b71b501e2ace9c6bd3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b172893041d14e9e9a6f99903b1aa3b9",
       "placeholder": "​",
       "style": "IPY_MODEL_f1a84c0d344d470da2fe29279d41561f",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin: 100%"
      }
     },
     "4777a01997074c41bb37c4a11ce30b57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d4daf3b02b44ec4ad863f462613159f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_38d010488aae4aa28a565265114a0017",
        "IPY_MODEL_1586b19855a4446597a7f866458c2024",
        "IPY_MODEL_7f90eb359dc24581b000a1c7c4052f9e"
       ],
       "layout": "IPY_MODEL_a9b65af7a6cc4e7aa840db8ac614c5ae",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d816743925649a583908ef109afc5e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53bf2ec65f0a42f1ad51e0e0c95e0ebd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "53fcb13dbb1f43ee9a604e88f047aa67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "576eb876489d4994b580dd1c265cedea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d75b5b7ad0143cd9824596657ab6723": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9b69c345b1a44ad805b0640d8943e38",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3715e56656ee47f0b99f80d436e478fe",
       "tabbable": null,
       "tooltip": null,
       "value": 497810400.0
      }
     },
     "5da1ebe5d7504e92b182773cc163d113": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62ecf38751c94f27a5f38f958af3ba8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "65c30b03b7374263bb55b70edf7bf790": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67a04ef342774662887358d36276de64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_facb00b25b9c40af810c02c40d76b0e0",
        "IPY_MODEL_34311a7fdf0e4996afb6e5b38dffb18a",
        "IPY_MODEL_92b5b0fc05224d31b8897096c10fbee8"
       ],
       "layout": "IPY_MODEL_4d816743925649a583908ef109afc5e5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "687d38e0ecac4eb0b27ddbba18772726": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b97cc7c1506b47ee92ff8921d9eb9e42",
        "IPY_MODEL_df37e5f0523d41d58ec7e031a0a9f818",
        "IPY_MODEL_3af74dc4ff8d47f19f5134a62b6d1a9c"
       ],
       "layout": "IPY_MODEL_e4dd4a5573364655a1e38620137153b5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6b7199d947014cf6a475a2c7e332df39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "77c712cc781f45f88798915be551a35b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a4534cf299f40efa878fc79ceb7bc56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "7f90eb359dc24581b000a1c7c4052f9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0a0ae51bc6c42dfb82ac3036301a6b2",
       "placeholder": "​",
       "style": "IPY_MODEL_c8c4d54b964845b3ba90da1c7da746c7",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 159B/s]"
      }
     },
     "8de8717f75c8417680a2b619e138af07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92b5b0fc05224d31b8897096c10fbee8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05235d517b8045f483807a4560094f71",
       "placeholder": "​",
       "style": "IPY_MODEL_65c30b03b7374263bb55b70edf7bf790",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/? [00:00&lt;00:00, 137kB/s]"
      }
     },
     "a9b65af7a6cc4e7aa840db8ac614c5ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9b69c345b1a44ad805b0640d8943e38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad5b0c68ea78435aae3abdd3e04f8b0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b172893041d14e9e9a6f99903b1aa3b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8c6893607c24316ab5ef2b1e17f36a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b97cc7c1506b47ee92ff8921d9eb9e42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d4a969d482a649a29b93a1d42d8cbb97",
       "placeholder": "​",
       "style": "IPY_MODEL_4777a01997074c41bb37c4a11ce30b57",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "bc25cb41f4a645fbba072dd2dae9f53b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2f5aac918c247fda90f33faac93f710",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_25145830f14e45db9bf361d3a2f6eca6",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "c8c4d54b964845b3ba90da1c7da746c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0a0ae51bc6c42dfb82ac3036301a6b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0d03b7638ff4637bbd788d8ac8594ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4a969d482a649a29b93a1d42d8cbb97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d513f6e4185245848ec7330cdc5e8a56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d790742a9f9b42a5a36fca5ec14a0e81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db42f578d45041dba4912a65752b8d4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df37e5f0523d41d58ec7e031a0a9f818": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_576eb876489d4994b580dd1c265cedea",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_293e44ada18b4926884361aebfae7e2d",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "e4dd4a5573364655a1e38620137153b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5e07f8bd99b4f93ad1ceeec909086a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef1508cb26be47e98343d166cb595de9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1a84c0d344d470da2fe29279d41561f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2f5aac918c247fda90f33faac93f710": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f48289651e354d10bee0cc03be199cb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_43a8506a2c514b71b501e2ace9c6bd3b",
        "IPY_MODEL_5d75b5b7ad0143cd9824596657ab6723",
        "IPY_MODEL_39250da05573454087e03ac04573a663"
       ],
       "layout": "IPY_MODEL_ad5b0c68ea78435aae3abdd3e04f8b0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f8029166856645339483549e6a482a46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5da1ebe5d7504e92b182773cc163d113",
       "placeholder": "​",
       "style": "IPY_MODEL_62ecf38751c94f27a5f38f958af3ba8f",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/? [00:00&lt;00:00, 6.10MB/s]"
      }
     },
     "facb00b25b9c40af810c02c40d76b0e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_334054335a7f43db84f6a7819a8fc7a2",
       "placeholder": "​",
       "style": "IPY_MODEL_e5e07f8bd99b4f93ad1ceeec909086a7",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: "
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
