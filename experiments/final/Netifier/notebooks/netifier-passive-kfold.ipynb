{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10457689,"sourceType":"datasetVersion","datasetId":6473829}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport torch\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.multiprocessing import Manager\nfrom torch.utils.data import DataLoader, Dataset\nfrom accelerate import Accelerator, notebook_launcher\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\nfrom transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2025-06-27T07:28:07.094234Z","iopub.execute_input":"2025-06-27T07:28:07.094587Z","iopub.status.idle":"2025-06-27T07:28:29.368246Z","shell.execute_reply.started":"2025-06-27T07:28:07.094554Z","shell.execute_reply":"2025-06-27T07:28:29.367546Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:29.369363Z","iopub.execute_input":"2025-06-27T07:28:29.369928Z","iopub.status.idle":"2025-06-27T07:28:29.373426Z","shell.execute_reply.started":"2025-06-27T07:28:29.369902Z","shell.execute_reply":"2025-06-27T07:28:29.372631Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:29.374870Z","iopub.execute_input":"2025-06-27T07:28:29.375125Z","iopub.status.idle":"2025-06-27T07:28:29.401918Z","shell.execute_reply.started":"2025-06-27T07:28:29.375102Z","shell.execute_reply":"2025-06-27T07:28:29.401173Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/netifier-3/processed_train.csv', encoding='latin-1')\nval_data = pd.read_csv('/kaggle/input/netifier-3/processed_test.csv', encoding='latin-1')\n\ndata = pd.concat([train_data, val_data], ignore_index=True)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-06-27T07:28:29.403143Z","iopub.execute_input":"2025-06-27T07:28:29.403456Z","iopub.status.idle":"2025-06-27T07:28:29.555635Z","shell.execute_reply.started":"2025-06-27T07:28:29.403426Z","shell.execute_reply":"2025-06-27T07:28:29.554824Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                       original_text     source  pornografi  \\\n0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n\n   sara  radikalisme  pencemaran_nama_baik  \\\n0     0            0                     1   \n1     0            0                     0   \n2     1            1                     1   \n3     0            0                     0   \n4     0            0                     0   \n\n                                      processed_text  \n0  jabar memang provinsi barokah boleh juga dan n...  \n1  kita saja nitizen yang pada penasaran toh kelu...  \n2  sidangahok semoga sipenista agama dan ateknya ...  \n3  jakarta barusan baca undang ini tetap dibedaka...  \n4  buat anak melulu kamu nof nkaga mikir apa kasi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>source</th>\n      <th>pornografi</th>\n      <th>sara</th>\n      <th>radikalisme</th>\n      <th>pencemaran_nama_baik</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n      <td>kaskus</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n      <td>instagram</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n      <td>twitter</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n      <td>instagram</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n      <td>kaskus</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5","metadata":{"execution":{"iopub.status.busy":"2025-06-27T07:28:29.556513Z","iopub.execute_input":"2025-06-27T07:28:29.556826Z","iopub.status.idle":"2025-06-27T07:28:29.560496Z","shell.execute_reply.started":"2025-06-27T07:28:29.556795Z","shell.execute_reply":"2025-06-27T07:28:29.559639Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define custom Dataset class\nclass NetifierDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.use_float = use_float\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        labels = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n        return item\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')","metadata":{"execution":{"iopub.status.busy":"2025-06-27T07:28:29.561213Z","iopub.execute_input":"2025-06-27T07:28:29.561469Z","iopub.status.idle":"2025-06-27T07:28:30.508662Z","shell.execute_reply.started":"2025-06-27T07:28:29.561435Z","shell.execute_reply":"2025-06-27T07:28:30.508031Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf96ee87d48434d8d137af39e1e5931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39c545cdedd4eb4b151b8e1abef62b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7e082ec20447c2a7106fb6bf5ac030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cca35de316249b4800b0168b6f6cb3d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n    labels = torch.tensor(p.label_ids)\n\n    accuracy = (preds == labels).float().mean().item()\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n\n    report = classification_report(\n        labels, \n        preds, \n        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n        zero_division=0\n    ) \n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'report': report\n    }","metadata":{"execution":{"iopub.status.busy":"2025-06-27T07:28:30.509345Z","iopub.execute_input":"2025-06-27T07:28:30.509581Z","iopub.status.idle":"2025-06-27T07:28:30.514444Z","shell.execute_reply.started":"2025-06-27T07:28:30.509560Z","shell.execute_reply":"2025-06-27T07:28:30.513524Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = 42 + worker_id\n    np.random.seed(worker_seed)\n\ndef get_dataloaders(X_train, y_train, X_val, y_val, sequence_length, num_workers=4):\n    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers, worker_init_fn=seed_worker,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=num_workers, worker_init_fn=seed_worker,\n    )\n\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:30.516722Z","iopub.execute_input":"2025-06-27T07:28:30.516916Z","iopub.status.idle":"2025-06-27T07:28:30.534140Z","shell.execute_reply.started":"2025-06-27T07:28:30.516898Z","shell.execute_reply":"2025-06-27T07:28:30.533373Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"manager = Manager()\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:30.535155Z","iopub.execute_input":"2025-06-27T07:28:30.535450Z","iopub.status.idle":"2025-06-27T07:28:30.599809Z","shell.execute_reply.started":"2025-06-27T07:28:30.535401Z","shell.execute_reply":"2025-06-27T07:28:30.598136Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_model(sequence_length, model_name, metrics, X_train, y_train, X_val, y_val, fold, seed=42, layers_freezed=6, num_workers=4):\n    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n    device = accelerator.device\n\n    with accelerator.main_process_first():\n        model = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=4,\n            problem_type=\"multi_label_classification\"\n        )\n\n    # Freeze the first few layers of the encoder\n    for name, param in model.named_parameters():\n        if \"encoder.layer\" in name:\n            layer_num = name.split(\".\")[3]\n            try:\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                continue\n\n    # Define DataLoaders\n    train_loader, val_loader = get_dataloaders(X_train, y_train, X_val, y_val, sequence_length, num_workers)\n\n    # Define optimizer and loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    # Prepare everything with Accelerator\n    model, optimizer, train_loader, val_loader = accelerator.prepare(\n        model, optimizer, train_loader, val_loader\n    )\n\n    best_result = None\n    start_time = time.time()\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for batch in train_loader:\n            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels']\n\n            optimizer.zero_grad()\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels)\n            accelerator.backward(loss)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels']\n                \n                outputs = model(**inputs)\n                preds = torch.sigmoid(outputs.logits).round()\n\n                # Gather predictions and labels from all devices\n                all_preds.append(accelerator.gather(preds))\n                all_labels.append(accelerator.gather(labels))\n\n        all_preds = torch.cat(all_preds).cpu().numpy()\n        all_labels = torch.cat(all_labels).cpu().numpy()\n        \n        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n            best_result = result\n            \n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                f'model-{fold + 1}',\n                is_main_process=accelerator.is_main_process,\n                save_function=accelerator.save,\n            )\n\n        accelerator.print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    if accelerator.is_main_process:\n        metrics[0].append(best_result['accuracy'])\n        metrics[1].append(best_result['f1_micro'])\n        metrics[2].append(best_result['f1_macro'])\n        \n    accelerator.print(f\"\\nAccuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n    accelerator.print(best_result['report'])\n    accelerator.print(f\"Duration: {duration}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:30.601443Z","iopub.execute_input":"2025-06-27T07:28:30.601736Z","iopub.status.idle":"2025-06-27T07:28:30.615220Z","shell.execute_reply.started":"2025-06-27T07:28:30.601708Z","shell.execute_reply":"2025-06-27T07:28:30.614281Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nN_SPLITS = 5 \nRANDOM_SEED = 42\n\n# Prepare data for K-Fold\nlabel_columns = data.columns[2:6]\nX = data['processed_text'].values\ny = data[label_columns].values\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n\n# Shared resources for this fold's processes\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X)):\n    print(\"===============================================\")\n    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n    print(\"===============================================\")\n\n    X_train_fold, X_val_fold = X[train_index], X[val_index]\n    y_train_fold, y_val_fold = y[train_index], y[val_index]\n\n    seed = RANDOM_SEED + fold\n    set_seed(seed)\n    args = (96, 'indobenchmark/indobert-base-p1', (accuracies, f1_micros, f1_macros), X_train_fold, y_train_fold, X_val_fold, y_val_fold, fold, seed, 6)\n    notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:28:30.616361Z","iopub.execute_input":"2025-06-27T07:28:30.616709Z","iopub.status.idle":"2025-06-27T07:45:21.695690Z","shell.execute_reply.started":"2025-06-27T07:28:30.616681Z","shell.execute_reply":"2025-06-27T07:45:21.694777Z"}},"outputs":[{"name":"stdout","text":"===============================================\nSTARTING FOLD 1/5\n===============================================\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.3569, Accuracy: 0.8891, F1 Micro: 0.7227, F1 Macro: 0.716\nEpoch 2/10, Train Loss: 0.2266, Accuracy: 0.8991, F1 Micro: 0.7414, F1 Macro: 0.7351\nEpoch 3/10, Train Loss: 0.1944, Accuracy: 0.9075, F1 Micro: 0.7725, F1 Macro: 0.7698\nEpoch 4/10, Train Loss: 0.1505, Accuracy: 0.9064, F1 Micro: 0.7652, F1 Macro: 0.755\nEpoch 5/10, Train Loss: 0.1072, Accuracy: 0.9055, F1 Micro: 0.7794, F1 Macro: 0.7766\nEpoch 6/10, Train Loss: 0.0829, Accuracy: 0.9047, F1 Micro: 0.7804, F1 Macro: 0.7799\nEpoch 7/10, Train Loss: 0.0654, Accuracy: 0.9042, F1 Micro: 0.7744, F1 Macro: 0.7712\nEpoch 8/10, Train Loss: 0.0504, Accuracy: 0.905, F1 Micro: 0.7632, F1 Macro: 0.7577\nEpoch 9/10, Train Loss: 0.037, Accuracy: 0.9044, F1 Micro: 0.7723, F1 Macro: 0.7726\nEpoch 10/10, Train Loss: 0.0257, Accuracy: 0.902, F1 Micro: 0.7571, F1 Macro: 0.7489\n\nAccuracy: 0.9047, F1 Micro: 0.7804, F1 Macro: 0.7799\n                      precision    recall  f1-score   support\n\n          pornografi       0.94      0.89      0.92       369\n                sara       0.68      0.70      0.69       262\n         radikalisme       0.74      0.82      0.78       234\npencemaran_nama_baik       0.68      0.79      0.73       478\n\n           micro avg       0.76      0.81      0.78      1343\n           macro avg       0.76      0.80      0.78      1343\n        weighted avg       0.76      0.81      0.78      1343\n         samples avg       0.45      0.46      0.45      1343\n\nDuration: 197.05537605285645\n===============================================\nSTARTING FOLD 2/5\n===============================================\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.3567, Accuracy: 0.8881, F1 Micro: 0.7034, F1 Macro: 0.6723\nEpoch 2/10, Train Loss: 0.2304, Accuracy: 0.9039, F1 Micro: 0.7564, F1 Macro: 0.7495\nEpoch 3/10, Train Loss: 0.1879, Accuracy: 0.9111, F1 Micro: 0.7974, F1 Macro: 0.795\nEpoch 4/10, Train Loss: 0.1534, Accuracy: 0.9114, F1 Micro: 0.7936, F1 Macro: 0.7842\nEpoch 5/10, Train Loss: 0.1168, Accuracy: 0.9111, F1 Micro: 0.789, F1 Macro: 0.7838\nEpoch 6/10, Train Loss: 0.0789, Accuracy: 0.908, F1 Micro: 0.7787, F1 Macro: 0.7713\nEpoch 7/10, Train Loss: 0.0608, Accuracy: 0.9106, F1 Micro: 0.784, F1 Macro: 0.7723\nEpoch 8/10, Train Loss: 0.0422, Accuracy: 0.9086, F1 Micro: 0.7829, F1 Macro: 0.7782\nEpoch 9/10, Train Loss: 0.0355, Accuracy: 0.91, F1 Micro: 0.7947, F1 Macro: 0.7905\nEpoch 10/10, Train Loss: 0.0266, Accuracy: 0.9052, F1 Micro: 0.789, F1 Macro: 0.7822\n\nAccuracy: 0.9111, F1 Micro: 0.7974, F1 Macro: 0.795\n                      precision    recall  f1-score   support\n\n          pornografi       0.94      0.90      0.92       378\n                sara       0.69      0.76      0.73       253\n         radikalisme       0.69      0.90      0.78       234\npencemaran_nama_baik       0.78      0.73      0.75       517\n\n           micro avg       0.78      0.81      0.80      1382\n           macro avg       0.78      0.82      0.79      1382\n        weighted avg       0.79      0.81      0.80      1382\n         samples avg       0.46      0.45      0.45      1382\n\nDuration: 196.98376083374023\n===============================================\nSTARTING FOLD 3/5\n===============================================\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.3765, Accuracy: 0.8878, F1 Micro: 0.7341, F1 Macro: 0.7288\nEpoch 2/10, Train Loss: 0.2331, Accuracy: 0.8955, F1 Micro: 0.7678, F1 Macro: 0.7691\nEpoch 3/10, Train Loss: 0.1933, Accuracy: 0.8966, F1 Micro: 0.7573, F1 Macro: 0.7523\nEpoch 4/10, Train Loss: 0.1524, Accuracy: 0.8986, F1 Micro: 0.7685, F1 Macro: 0.7707\nEpoch 5/10, Train Loss: 0.1145, Accuracy: 0.8922, F1 Micro: 0.7637, F1 Macro: 0.7606\nEpoch 6/10, Train Loss: 0.0849, Accuracy: 0.8973, F1 Micro: 0.7638, F1 Macro: 0.7654\nEpoch 7/10, Train Loss: 0.0653, Accuracy: 0.8906, F1 Micro: 0.7583, F1 Macro: 0.7582\nEpoch 8/10, Train Loss: 0.0484, Accuracy: 0.8944, F1 Micro: 0.7674, F1 Macro: 0.7689\nEpoch 9/10, Train Loss: 0.0345, Accuracy: 0.8933, F1 Micro: 0.7514, F1 Macro: 0.75\nEpoch 10/10, Train Loss: 0.0302, Accuracy: 0.8911, F1 Micro: 0.7656, F1 Macro: 0.7671\n\nAccuracy: 0.8986, F1 Micro: 0.7685, F1 Macro: 0.7707\n                      precision    recall  f1-score   support\n\n          pornografi       0.91      0.88      0.90       355\n                sara       0.71      0.67      0.69       273\n         radikalisme       0.76      0.83      0.79       281\npencemaran_nama_baik       0.75      0.67      0.71       521\n\n           micro avg       0.78      0.75      0.77      1430\n           macro avg       0.78      0.76      0.77      1430\n        weighted avg       0.78      0.75      0.77      1430\n         samples avg       0.45      0.44      0.43      1430\n\nDuration: 198.02473759651184\n===============================================\nSTARTING FOLD 4/5\n===============================================\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.3457, Accuracy: 0.8856, F1 Micro: 0.7391, F1 Macro: 0.737\nEpoch 2/10, Train Loss: 0.2255, Accuracy: 0.8948, F1 Micro: 0.7759, F1 Macro: 0.7773\nEpoch 3/10, Train Loss: 0.1913, Accuracy: 0.9067, F1 Micro: 0.7831, F1 Macro: 0.7765\nEpoch 4/10, Train Loss: 0.1516, Accuracy: 0.903, F1 Micro: 0.7746, F1 Macro: 0.7699\nEpoch 5/10, Train Loss: 0.1132, Accuracy: 0.9056, F1 Micro: 0.7781, F1 Macro: 0.7732\nEpoch 6/10, Train Loss: 0.0838, Accuracy: 0.9011, F1 Micro: 0.7753, F1 Macro: 0.7752\nEpoch 7/10, Train Loss: 0.0598, Accuracy: 0.903, F1 Micro: 0.7744, F1 Macro: 0.7699\nEpoch 8/10, Train Loss: 0.0422, Accuracy: 0.9033, F1 Micro: 0.7771, F1 Macro: 0.7723\nEpoch 9/10, Train Loss: 0.0339, Accuracy: 0.9, F1 Micro: 0.769, F1 Macro: 0.7661\nEpoch 10/10, Train Loss: 0.0256, Accuracy: 0.8995, F1 Micro: 0.7803, F1 Macro: 0.7792\n\nAccuracy: 0.9067, F1 Micro: 0.7831, F1 Macro: 0.7765\n                      precision    recall  f1-score   support\n\n          pornografi       0.92      0.89      0.91       342\n                sara       0.72      0.61      0.66       249\n         radikalisme       0.79      0.78      0.79       302\npencemaran_nama_baik       0.76      0.76      0.76       508\n\n           micro avg       0.80      0.77      0.78      1401\n           macro avg       0.79      0.76      0.78      1401\n        weighted avg       0.80      0.77      0.78      1401\n         samples avg       0.46      0.45      0.45      1401\n\nDuration: 201.94380354881287\n===============================================\nSTARTING FOLD 5/5\n===============================================\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.3575, Accuracy: 0.8938, F1 Micro: 0.748, F1 Macro: 0.7457\nEpoch 2/10, Train Loss: 0.231, Accuracy: 0.9058, F1 Micro: 0.7736, F1 Macro: 0.7738\nEpoch 3/10, Train Loss: 0.1854, Accuracy: 0.8958, F1 Micro: 0.7731, F1 Macro: 0.7738\nEpoch 4/10, Train Loss: 0.1495, Accuracy: 0.91, F1 Micro: 0.7812, F1 Macro: 0.7753\nEpoch 5/10, Train Loss: 0.1145, Accuracy: 0.9089, F1 Micro: 0.7838, F1 Macro: 0.7836\nEpoch 6/10, Train Loss: 0.084, Accuracy: 0.9103, F1 Micro: 0.7816, F1 Macro: 0.7812\nEpoch 7/10, Train Loss: 0.0633, Accuracy: 0.9072, F1 Micro: 0.7793, F1 Macro: 0.7773\nEpoch 8/10, Train Loss: 0.0463, Accuracy: 0.9067, F1 Micro: 0.7745, F1 Macro: 0.7697\nEpoch 9/10, Train Loss: 0.0385, Accuracy: 0.9038, F1 Micro: 0.7819, F1 Macro: 0.7814\nEpoch 10/10, Train Loss: 0.0289, Accuracy: 0.9064, F1 Micro: 0.779, F1 Macro: 0.7777\n\nAccuracy: 0.9089, F1 Micro: 0.7838, F1 Macro: 0.7836\n                      precision    recall  f1-score   support\n\n          pornografi       0.95      0.91      0.93       353\n                sara       0.65      0.68      0.67       239\n         radikalisme       0.81      0.82      0.82       273\npencemaran_nama_baik       0.72      0.72      0.72       485\n\n           micro avg       0.78      0.78      0.78      1350\n           macro avg       0.78      0.78      0.78      1350\n        weighted avg       0.79      0.78      0.78      1350\n         samples avg       0.44      0.44      0.43      1350\n\nDuration: 200.57538294792175\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Trial': [1,2,3,4,5],\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'netifier-passive-kfold-result.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:45:21.696851Z","iopub.execute_input":"2025-06-27T07:45:21.697119Z","iopub.status.idle":"2025-06-27T07:45:21.720833Z","shell.execute_reply.started":"2025-06-27T07:45:21.697093Z","shell.execute_reply":"2025-06-27T07:45:21.720251Z"}},"outputs":[],"execution_count":12}]}