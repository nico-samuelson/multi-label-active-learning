{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9896348,"sourceType":"datasetVersion","datasetId":6078649}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.multiprocessing import Manager\nfrom torch.utils.data import DataLoader, Dataset\nfrom accelerate import Accelerator, notebook_launcher\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\nfrom transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2025-02-04T15:24:46.805338Z","iopub.execute_input":"2025-02-04T15:24:46.805621Z","iopub.status.idle":"2025-02-04T15:25:06.579924Z","shell.execute_reply.started":"2025-02-04T15:24:46.805599Z","shell.execute_reply":"2025-02-04T15:25:06.579218Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:06.580792Z","iopub.execute_input":"2025-02-04T15:25:06.581250Z","iopub.status.idle":"2025-02-04T15:25:06.584810Z","shell.execute_reply.started":"2025-02-04T15:25:06.581198Z","shell.execute_reply":"2025-02-04T15:25:06.583980Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:06.586480Z","iopub.execute_input":"2025-02-04T15:25:06.586766Z","iopub.status.idle":"2025-02-04T15:25:06.623026Z","shell.execute_reply.started":"2025-02-04T15:25:06.586739Z","shell.execute_reply":"2025-02-04T15:25:06.622358Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/netifier-2/processed_train.csv', encoding='latin-1')\nval_data = pd.read_csv('/kaggle/input/netifier-2/processed_test.csv', encoding='latin-1')\n\ndata = pd.concat([train_data, val_data], ignore_index=True)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-04T15:25:06.624283Z","iopub.execute_input":"2025-02-04T15:25:06.624500Z","iopub.status.idle":"2025-02-04T15:25:06.771682Z","shell.execute_reply.started":"2025-02-04T15:25:06.624482Z","shell.execute_reply":"2025-02-04T15:25:06.771013Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                       original_text     source  pornografi  \\\n0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n\n   sara  radikalisme  pencemaran_nama_baik  \\\n0     0            0                     1   \n1     0            0                     0   \n2     1            1                     1   \n3     0            0                     0   \n4     0            0                     0   \n\n                                      processed_text  \n0  jabar memang provinsi barokah boleh juga dan n...  \n1  kita saja nitizen yang pada penasaran toh kelu...  \n2  sidangahok semoga sipenista agama dan ateknya ...  \n3  jakarta barusan baca undang ini tetap dibedaka...  \n4  buat anak melulu kamu nof nkaga mikir apa kasi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>source</th>\n      <th>pornografi</th>\n      <th>sara</th>\n      <th>radikalisme</th>\n      <th>pencemaran_nama_baik</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n      <td>kaskus</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n      <td>instagram</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n      <td>twitter</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n      <td>instagram</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n      <td>kaskus</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:06.772392Z","iopub.execute_input":"2025-02-04T15:25:06.772667Z","iopub.status.idle":"2025-02-04T15:25:06.781149Z","shell.execute_reply.started":"2025-02-04T15:25:06.772645Z","shell.execute_reply":"2025-02-04T15:25:06.780304Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_labels = train_data.columns[2:6]\nval_labels = val_data.columns[2:6]\n\n# Extract features and labels for training and validation\nX_train = train_data['processed_text'].values\ny_train = train_data[train_labels].values\nX_val = val_data['processed_text'].values\ny_val = val_data[val_labels].values\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T15:25:06.781922Z","iopub.execute_input":"2025-02-04T15:25:06.782192Z","iopub.status.idle":"2025-02-04T15:25:06.793675Z","shell.execute_reply.started":"2025-02-04T15:25:06.782153Z","shell.execute_reply":"2025-02-04T15:25:06.793029Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(6218,) (6218, 4)\n(1555,) (1555, 4)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5","metadata":{"execution":{"iopub.status.busy":"2025-02-04T15:25:06.794363Z","iopub.execute_input":"2025-02-04T15:25:06.794574Z","iopub.status.idle":"2025-02-04T15:25:06.805677Z","shell.execute_reply.started":"2025-02-04T15:25:06.794556Z","shell.execute_reply":"2025-02-04T15:25:06.804990Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define custom Dataset class\nclass NetifierDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.use_float = use_float\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        labels = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n        return item\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:06.807831Z","iopub.execute_input":"2025-02-04T15:25:06.808027Z","iopub.status.idle":"2025-02-04T15:25:08.029515Z","shell.execute_reply.started":"2025-02-04T15:25:06.808010Z","shell.execute_reply":"2025-02-04T15:25:08.028671Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b03f40f8224999ba4aff5df6f8c637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6175cc8c0534459bac2ef07f1ab4f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd6df8ddfe2478d8c4af779c6c0921e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f980eb8fa34e4a6a847a48f4cb2748dc"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class BertForMultiLabelClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = [2,2,2,2]\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, i) for i in self.num_labels])\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        subword_to_word_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        sequence_output = self.dropout(outputs[1])\n        logits = []\n        for classifier in self.classifiers:\n            logit = classifier(sequence_output)\n            logits.append(logit)\n\n        \n        logits = [torch.sigmoid(logit) for logit in logits]\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        \n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            total_loss = 0\n            for i, (logit, num_label) in enumerate(zip(logits, self.num_labels)):\n                label = labels[:, i]\n                loss = loss_fct(logit.view(-1, num_label), label.view(-1))\n                total_loss += loss\n\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n# Define compute metrics for evaluation\ndef compute_metrics_multi(p):\n    logits = p.predictions # logits list<tensor(bs, num_label)> ~ list of batch prediction per class \n    label_batch = p.label_ids\n\n    # print(p.predictions)\n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = [torch.topk(torch.tensor(logit, dtype=torch.float), 1)[1] for logit in logits] # list<tensor(bs)>\n    batch_size = label_batch.shape[0]\n    num_label = len(hyp)\n    for i in range(batch_size):\n        hyps = []\n        labels = torch.tensor(label_batch[i,:], dtype=torch.float)\n        for j in range(num_label):\n            hyps.append(hyp[j][i].item())\n\n        hyps = torch.tensor(hyps, dtype=torch.float)\n        list_hyp.append(hyps)\n        list_label.append(labels)\n    \n    accuracy = accuracy_score(list_label, list_hyp)\n    # print(accuracy)\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(list_label, list_hyp, average='micro', zero_division=0)\n    f1_macro = f1_score(list_label, list_hyp, average='macro', zero_division=0)\n\n    # print(classification_report(list_label, list_hyp, zero_division=0, target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik']))\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.030589Z","iopub.execute_input":"2025-02-04T15:25:08.030828Z","iopub.status.idle":"2025-02-04T15:25:08.041139Z","shell.execute_reply.started":"2025-02-04T15:25:08.030807Z","shell.execute_reply":"2025-02-04T15:25:08.040300Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n    labels = torch.tensor(p.label_ids)\n\n    # Hamming accuracy: proportion of correctly predicted labels over total labels\n    hamming_accuracy = (preds == labels).float().mean().item()\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n\n    report = classification_report(\n        labels, \n        preds, \n        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n        zero_division=0\n    ) \n\n    return {\n        'accuracy': hamming_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'report': report\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.041971Z","iopub.execute_input":"2025-02-04T15:25:08.042289Z","iopub.status.idle":"2025-02-04T15:25:08.062169Z","shell.execute_reply.started":"2025-02-04T15:25:08.042258Z","shell.execute_reply":"2025-02-04T15:25:08.061514Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = 42 + worker_id\n    np.random.seed(worker_seed)\n\ndef get_dataloaders(sequence_length, num_workers=4):\n    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers, worker_init_fn=seed_worker,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=num_workers, worker_init_fn=seed_worker,\n    )\n\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.062961Z","iopub.execute_input":"2025-02-04T15:25:08.063154Z","iopub.status.idle":"2025-02-04T15:25:08.078456Z","shell.execute_reply.started":"2025-02-04T15:25:08.063128Z","shell.execute_reply":"2025-02-04T15:25:08.077670Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"manager = Manager()\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.079129Z","iopub.execute_input":"2025-02-04T15:25:08.079358Z","iopub.status.idle":"2025-02-04T15:25:08.133409Z","shell.execute_reply.started":"2025-02-04T15:25:08.079339Z","shell.execute_reply":"2025-02-04T15:25:08.132071Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(sequence_length, model_name, metrics, seed=42, layers_freezed=6, num_workers=4):\n    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n    device = accelerator.device\n\n    with accelerator.main_process_first():\n        model = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=len(train_labels),\n            problem_type=\"multi_label_classification\"\n        )\n\n    # Freeze the first few layers of the encoder\n    for name, param in model.named_parameters():\n        if \"encoder.layer\" in name:\n            layer_num = name.split(\".\")[3]\n            try:\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                continue\n\n    # Define DataLoaders\n    train_loader, val_loader = get_dataloaders(sequence_length, num_workers)\n\n    # Define optimizer and loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    # Prepare everything with Accelerator\n    model, optimizer, train_loader, val_loader = accelerator.prepare(\n        model, optimizer, train_loader, val_loader\n    )\n\n    best_result = None\n    start_time = time.time()\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for batch in train_loader:\n            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels']\n\n            optimizer.zero_grad()\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels)\n            accelerator.backward(loss)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels']\n                \n                outputs = model(**inputs)\n                preds = torch.sigmoid(outputs.logits).round()\n\n                # Gather predictions and labels from all devices\n                all_preds.append(accelerator.gather(preds))\n                all_labels.append(accelerator.gather(labels))\n\n        all_preds = torch.cat(all_preds).cpu().numpy()\n        all_labels = torch.cat(all_labels).cpu().numpy()\n        \n        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n            accelerator.print(\"Higher F1 achieved, saving model\")\n            best_result = result\n            \n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                f'model-{sequence_length}-{layers_freezed}-{seed}',\n                is_main_process=accelerator.is_main_process,\n                save_function=accelerator.save,\n            )\n\n        accelerator.print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    if accelerator.is_main_process:\n        metrics[0].append(best_result['accuracy'])\n        metrics[1].append(best_result['f1_micro'])\n        metrics[2].append(best_result['f1_macro'])\n        \n    accelerator.print(f\"\\nAccuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n    accelerator.print(best_result['report'])\n    accelerator.print(f\"Duration: {duration}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.134905Z","iopub.execute_input":"2025-02-04T15:25:08.135158Z","iopub.status.idle":"2025-02-04T15:25:08.148256Z","shell.execute_reply.started":"2025-02-04T15:25:08.135134Z","shell.execute_reply":"2025-02-04T15:25:08.147318Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Tokenize each text and calculate their lengths\ntoken_lengths = [len(tokenizer.tokenize(text)) for text in X_train]\n\n# Calculate the average length\naverage_length = sum(token_lengths) / len(token_lengths)\nmax_length = max(token_lengths)\n\nprint(\"Average length of tokenized text:\", average_length)\nprint(\"Max token length:\", max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:08.149201Z","iopub.execute_input":"2025-02-04T15:25:08.149458Z","iopub.status.idle":"2025-02-04T15:25:13.925078Z","shell.execute_reply.started":"2025-02-04T15:25:08.149428Z","shell.execute_reply":"2025-02-04T15:25:13.924287Z"}},"outputs":[{"name":"stdout","text":"Average length of tokenized text: 54.3126407204889\nMax token length: 2591\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"counts, bins = np.histogram(token_lengths, range=(0, 500))\nplt.stairs(counts, bins)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:13.925926Z","iopub.execute_input":"2025-02-04T15:25:13.926214Z","iopub.status.idle":"2025-02-04T15:25:14.135883Z","shell.execute_reply.started":"2025-02-04T15:25:13.926192Z","shell.execute_reply":"2025-02-04T15:25:14.135038Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# HYPERPARAMETER TUNING","metadata":{}},{"cell_type":"code","source":"sizes = [16, 32]\nlengths = [80, 96, 128]\n\nused_sizes = []\nused_lengths = []\n\nfor size in sizes:\n    BATCH_SIZE = size\n    for length in lengths:\n        print(\"=========================================================================================\")\n        print(f\"Batch size: {BATCH_SIZE}, sequence length: {length}\")\n        used_sizes.append(BATCH_SIZE)\n        used_lengths.append(length)\n        \n        args = (length, 'indobenchmark/indobert-base-p1', (accuracies, f1_micros, f1_macros), 42, 6)\n        notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:25:14.136819Z","iopub.execute_input":"2025-02-04T15:25:14.137105Z","iopub.status.idle":"2025-02-04T15:53:27.359593Z","shell.execute_reply.started":"2025-02-04T15:25:14.137081Z","shell.execute_reply":"2025-02-04T15:53:27.358496Z"}},"outputs":[{"name":"stdout","text":"=========================================================================================\nBatch size: 16, sequence length: 80\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3252, Accuracy: 0.8952, F1 Micro: 0.7482, F1 Macro: 0.7459\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2253, Accuracy: 0.9031, F1 Micro: 0.7662, F1 Macro: 0.7639\nEpoch 3/10, Train Loss: 0.17, Accuracy: 0.9008, F1 Micro: 0.7611, F1 Macro: 0.7558\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1225, Accuracy: 0.9015, F1 Micro: 0.7689, F1 Macro: 0.7651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0882, Accuracy: 0.9007, F1 Micro: 0.7737, F1 Macro: 0.7672\nEpoch 6/10, Train Loss: 0.0575, Accuracy: 0.905, F1 Micro: 0.7736, F1 Macro: 0.765\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0461, Accuracy: 0.9026, F1 Micro: 0.7771, F1 Macro: 0.7704\nEpoch 8/10, Train Loss: 0.0346, Accuracy: 0.9015, F1 Micro: 0.7673, F1 Macro: 0.7551\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.023, Accuracy: 0.9047, F1 Micro: 0.7792, F1 Macro: 0.7726\nEpoch 10/10, Train Loss: 0.0204, Accuracy: 0.9035, F1 Micro: 0.7693, F1 Macro: 0.7648\n\nAccuracy: 0.9047, F1 Micro: 0.7792, F1 Macro: 0.7726\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       366\n                sara       0.65      0.67      0.66       240\n         radikalisme       0.75      0.79      0.77       237\npencemaran_nama_baik       0.72      0.76      0.74       496\n\n           micro avg       0.77      0.79      0.78      1339\n           macro avg       0.77      0.78      0.77      1339\n        weighted avg       0.77      0.79      0.78      1339\n         samples avg       0.45      0.45      0.44      1339\n\nDuration: 316.16694927215576\n=========================================================================================\nBatch size: 16, sequence length: 96\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3244, Accuracy: 0.8933, F1 Micro: 0.7386, F1 Macro: 0.7328\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2244, Accuracy: 0.9021, F1 Micro: 0.7709, F1 Macro: 0.7666\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1684, Accuracy: 0.9056, F1 Micro: 0.7779, F1 Macro: 0.7715\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.121, Accuracy: 0.9023, F1 Micro: 0.783, F1 Macro: 0.7795\nEpoch 5/10, Train Loss: 0.0845, Accuracy: 0.8999, F1 Micro: 0.7639, F1 Macro: 0.7521\nEpoch 6/10, Train Loss: 0.0565, Accuracy: 0.9023, F1 Micro: 0.7755, F1 Macro: 0.7688\nEpoch 7/10, Train Loss: 0.04, Accuracy: 0.9029, F1 Micro: 0.7802, F1 Macro: 0.7744\nEpoch 8/10, Train Loss: 0.0304, Accuracy: 0.9059, F1 Micro: 0.781, F1 Macro: 0.7738\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0238, Accuracy: 0.9042, F1 Micro: 0.7836, F1 Macro: 0.7811\nEpoch 10/10, Train Loss: 0.0196, Accuracy: 0.9059, F1 Micro: 0.7821, F1 Macro: 0.7779\n\nAccuracy: 0.9042, F1 Micro: 0.7836, F1 Macro: 0.7811\n                      precision    recall  f1-score   support\n\n          pornografi       0.95      0.89      0.92       366\n                sara       0.64      0.72      0.68       240\n         radikalisme       0.75      0.83      0.79       237\npencemaran_nama_baik       0.70      0.80      0.75       496\n\n           micro avg       0.76      0.81      0.78      1339\n           macro avg       0.76      0.81      0.78      1339\n        weighted avg       0.77      0.81      0.79      1339\n         samples avg       0.47      0.47      0.46      1339\n\nDuration: 319.3316128253937\n=========================================================================================\nBatch size: 16, sequence length: 128\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3236, Accuracy: 0.894, F1 Micro: 0.736, F1 Macro: 0.7285\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2243, Accuracy: 0.9058, F1 Micro: 0.7794, F1 Macro: 0.7758\nEpoch 3/10, Train Loss: 0.1684, Accuracy: 0.9029, F1 Micro: 0.7727, F1 Macro: 0.7692\nEpoch 4/10, Train Loss: 0.1241, Accuracy: 0.9034, F1 Micro: 0.766, F1 Macro: 0.7594\nEpoch 5/10, Train Loss: 0.0805, Accuracy: 0.8989, F1 Micro: 0.7752, F1 Macro: 0.7686\nEpoch 6/10, Train Loss: 0.0518, Accuracy: 0.9047, F1 Micro: 0.7709, F1 Macro: 0.7572\nEpoch 7/10, Train Loss: 0.0422, Accuracy: 0.9045, F1 Micro: 0.7708, F1 Macro: 0.7639\nEpoch 8/10, Train Loss: 0.0338, Accuracy: 0.9019, F1 Micro: 0.7755, F1 Macro: 0.7674\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0256, Accuracy: 0.8997, F1 Micro: 0.7815, F1 Macro: 0.7797\nEpoch 10/10, Train Loss: 0.0185, Accuracy: 0.9019, F1 Micro: 0.7675, F1 Macro: 0.7575\n\nAccuracy: 0.8997, F1 Micro: 0.7815, F1 Macro: 0.7797\n                      precision    recall  f1-score   support\n\n          pornografi       0.91      0.92      0.92       366\n                sara       0.63      0.72      0.67       240\n         radikalisme       0.73      0.86      0.79       237\npencemaran_nama_baik       0.67      0.83      0.74       496\n\n           micro avg       0.73      0.84      0.78      1339\n           macro avg       0.73      0.83      0.78      1339\n        weighted avg       0.74      0.84      0.78      1339\n         samples avg       0.48      0.48      0.47      1339\n\nDuration: 346.3033595085144\n=========================================================================================\nBatch size: 32, sequence length: 80\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3715, Accuracy: 0.8859, F1 Micro: 0.7148, F1 Macro: 0.7008\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2441, Accuracy: 0.898, F1 Micro: 0.743, F1 Macro: 0.7356\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1978, Accuracy: 0.9038, F1 Micro: 0.7727, F1 Macro: 0.7674\nEpoch 4/10, Train Loss: 0.1559, Accuracy: 0.9034, F1 Micro: 0.7704, F1 Macro: 0.7649\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1174, Accuracy: 0.9023, F1 Micro: 0.7749, F1 Macro: 0.7711\nEpoch 6/10, Train Loss: 0.0847, Accuracy: 0.8995, F1 Micro: 0.7735, F1 Macro: 0.7635\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0672, Accuracy: 0.9048, F1 Micro: 0.775, F1 Macro: 0.7687\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.048, Accuracy: 0.9008, F1 Micro: 0.779, F1 Macro: 0.7761\nEpoch 9/10, Train Loss: 0.0347, Accuracy: 0.9019, F1 Micro: 0.7789, F1 Macro: 0.7783\nEpoch 10/10, Train Loss: 0.032, Accuracy: 0.9028, F1 Micro: 0.7713, F1 Macro: 0.7648\n\nAccuracy: 0.9008, F1 Micro: 0.779, F1 Macro: 0.7761\n                      precision    recall  f1-score   support\n\n          pornografi       0.92      0.92      0.92       370\n                sara       0.62      0.72      0.67       248\n         radikalisme       0.73      0.85      0.78       243\npencemaran_nama_baik       0.70      0.78      0.74       504\n\n           micro avg       0.74      0.82      0.78      1365\n           macro avg       0.74      0.82      0.78      1365\n        weighted avg       0.75      0.82      0.78      1365\n         samples avg       0.46      0.46      0.45      1365\n\nDuration: 207.24409580230713\n=========================================================================================\nBatch size: 32, sequence length: 96\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3705, Accuracy: 0.8867, F1 Micro: 0.721, F1 Macro: 0.7107\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.242, Accuracy: 0.9013, F1 Micro: 0.758, F1 Macro: 0.7516\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1938, Accuracy: 0.9042, F1 Micro: 0.7689, F1 Macro: 0.7581\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1526, Accuracy: 0.9069, F1 Micro: 0.7758, F1 Macro: 0.7729\nEpoch 5/10, Train Loss: 0.1136, Accuracy: 0.9028, F1 Micro: 0.7693, F1 Macro: 0.763\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.086, Accuracy: 0.9002, F1 Micro: 0.7803, F1 Macro: 0.7779\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0676, Accuracy: 0.9059, F1 Micro: 0.7814, F1 Macro: 0.7763\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0457, Accuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7802\nEpoch 9/10, Train Loss: 0.0357, Accuracy: 0.9, F1 Micro: 0.7831, F1 Macro: 0.7822\nEpoch 10/10, Train Loss: 0.0297, Accuracy: 0.9031, F1 Micro: 0.784, F1 Macro: 0.7819\n\nAccuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7802\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       370\n                sara       0.66      0.69      0.67       248\n         radikalisme       0.72      0.86      0.79       243\npencemaran_nama_baik       0.72      0.76      0.74       504\n\n           micro avg       0.76      0.81      0.78      1365\n           macro avg       0.76      0.80      0.78      1365\n        weighted avg       0.77      0.81      0.79      1365\n         samples avg       0.46      0.46      0.45      1365\n\nDuration: 223.0717215538025\n=========================================================================================\nBatch size: 32, sequence length: 128\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3729, Accuracy: 0.8863, F1 Micro: 0.7158, F1 Macro: 0.7045\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2413, Accuracy: 0.9017, F1 Micro: 0.7553, F1 Macro: 0.7472\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1939, Accuracy: 0.9038, F1 Micro: 0.7627, F1 Macro: 0.7554\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1532, Accuracy: 0.9067, F1 Micro: 0.7738, F1 Macro: 0.7689\nEpoch 5/10, Train Loss: 0.1186, Accuracy: 0.9022, F1 Micro: 0.7596, F1 Macro: 0.7491\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0857, Accuracy: 0.8969, F1 Micro: 0.7767, F1 Macro: 0.7735\nEpoch 7/10, Train Loss: 0.0659, Accuracy: 0.9061, F1 Micro: 0.7762, F1 Macro: 0.7677\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0503, Accuracy: 0.9033, F1 Micro: 0.7785, F1 Macro: 0.7751\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0352, Accuracy: 0.9044, F1 Micro: 0.78, F1 Macro: 0.7764\nEpoch 10/10, Train Loss: 0.0312, Accuracy: 0.9062, F1 Micro: 0.7734, F1 Macro: 0.7671\n\nAccuracy: 0.9044, F1 Micro: 0.78, F1 Macro: 0.7764\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.89      0.91       370\n                sara       0.67      0.67      0.67       248\n         radikalisme       0.75      0.84      0.79       243\npencemaran_nama_baik       0.71      0.77      0.74       504\n\n           micro avg       0.77      0.79      0.78      1365\n           macro avg       0.76      0.79      0.78      1365\n        weighted avg       0.77      0.79      0.78      1365\n         samples avg       0.46      0.45      0.45      1365\n\nDuration: 260.7377619743347\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Batch Size': used_sizes,\n    'Sequence Length': used_lengths,\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'hyperparameters_tuning.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:53:27.360867Z","iopub.execute_input":"2025-02-04T15:53:27.361114Z","iopub.status.idle":"2025-02-04T15:53:27.382920Z","shell.execute_reply.started":"2025-02-04T15:53:27.361091Z","shell.execute_reply":"2025-02-04T15:53:27.382110Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"seeds = [50, 81, 14, 3, 94]\n\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()\n\nfor seed in seeds:\n    print(\"=====================\")\n    print(\"SEED:\", seed)\n    set_seed(seed)\n    \n    LEARNING_RATE = 2e-5\n    BATCH_SIZE = 32\n    args = (96, 'indobenchmark/indobert-base-p1', (accuracies, f1_micros, f1_macros), seed, 6)\n    \n    notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T15:56:35.100830Z","iopub.execute_input":"2025-02-04T15:56:35.101189Z","iopub.status.idle":"2025-02-04T16:15:33.399978Z","shell.execute_reply.started":"2025-02-04T15:56:35.101159Z","shell.execute_reply":"2025-02-04T16:15:33.398891Z"}},"outputs":[{"name":"stdout","text":"=====================\nSEED: 50\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.366, Accuracy: 0.8914, F1 Micro: 0.7374, F1 Macro: 0.7284\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2436, Accuracy: 0.8992, F1 Micro: 0.7491, F1 Macro: 0.7421\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1964, Accuracy: 0.903, F1 Micro: 0.7625, F1 Macro: 0.7521\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1566, Accuracy: 0.9041, F1 Micro: 0.7678, F1 Macro: 0.7653\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1214, Accuracy: 0.9022, F1 Micro: 0.7743, F1 Macro: 0.7701\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0899, Accuracy: 0.8984, F1 Micro: 0.7791, F1 Macro: 0.776\nEpoch 7/10, Train Loss: 0.0706, Accuracy: 0.9031, F1 Micro: 0.7775, F1 Macro: 0.7735\nEpoch 8/10, Train Loss: 0.0507, Accuracy: 0.9036, F1 Micro: 0.7754, F1 Macro: 0.7728\nEpoch 9/10, Train Loss: 0.0398, Accuracy: 0.9016, F1 Micro: 0.778, F1 Macro: 0.7759\nEpoch 10/10, Train Loss: 0.0335, Accuracy: 0.9036, F1 Micro: 0.7769, F1 Macro: 0.7744\n\nAccuracy: 0.8984, F1 Micro: 0.7791, F1 Macro: 0.776\n                      precision    recall  f1-score   support\n\n          pornografi       0.86      0.94      0.90       370\n                sara       0.65      0.69      0.67       248\n         radikalisme       0.74      0.87      0.80       243\npencemaran_nama_baik       0.67      0.82      0.74       504\n\n           micro avg       0.73      0.84      0.78      1365\n           macro avg       0.73      0.83      0.78      1365\n        weighted avg       0.73      0.84      0.78      1365\n         samples avg       0.47      0.48      0.46      1365\n\nDuration: 221.00687074661255\n=====================\nSEED: 81\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3574, Accuracy: 0.8914, F1 Micro: 0.736, F1 Macro: 0.7236\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2367, Accuracy: 0.9033, F1 Micro: 0.76, F1 Macro: 0.7526\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1918, Accuracy: 0.9053, F1 Micro: 0.772, F1 Macro: 0.7622\nEpoch 4/10, Train Loss: 0.1483, Accuracy: 0.9028, F1 Micro: 0.764, F1 Macro: 0.7592\nEpoch 5/10, Train Loss: 0.1132, Accuracy: 0.9038, F1 Micro: 0.7672, F1 Macro: 0.7601\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0845, Accuracy: 0.9027, F1 Micro: 0.7784, F1 Macro: 0.774\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0633, Accuracy: 0.9053, F1 Micro: 0.784, F1 Macro: 0.7829\nEpoch 8/10, Train Loss: 0.0492, Accuracy: 0.9058, F1 Micro: 0.7795, F1 Macro: 0.7766\nEpoch 9/10, Train Loss: 0.0371, Accuracy: 0.9016, F1 Micro: 0.7793, F1 Macro: 0.7794\nEpoch 10/10, Train Loss: 0.0322, Accuracy: 0.9038, F1 Micro: 0.7734, F1 Macro: 0.7714\n\nAccuracy: 0.9053, F1 Micro: 0.784, F1 Macro: 0.7829\n                      precision    recall  f1-score   support\n\n          pornografi       0.92      0.91      0.91       370\n                sara       0.69      0.67      0.68       248\n         radikalisme       0.77      0.86      0.81       243\npencemaran_nama_baik       0.69      0.77      0.73       504\n\n           micro avg       0.76      0.81      0.78      1365\n           macro avg       0.77      0.80      0.78      1365\n        weighted avg       0.77      0.81      0.78      1365\n         samples avg       0.47      0.46      0.45      1365\n\nDuration: 220.8093011379242\n=====================\nSEED: 14\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3558, Accuracy: 0.8911, F1 Micro: 0.7379, F1 Macro: 0.7332\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2375, Accuracy: 0.9017, F1 Micro: 0.7593, F1 Macro: 0.7554\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1909, Accuracy: 0.907, F1 Micro: 0.782, F1 Macro: 0.7753\nEpoch 4/10, Train Loss: 0.149, Accuracy: 0.9066, F1 Micro: 0.7777, F1 Macro: 0.7728\nEpoch 5/10, Train Loss: 0.1173, Accuracy: 0.9003, F1 Micro: 0.7728, F1 Macro: 0.7715\nEpoch 6/10, Train Loss: 0.0843, Accuracy: 0.903, F1 Micro: 0.7738, F1 Macro: 0.7662\nEpoch 7/10, Train Loss: 0.0632, Accuracy: 0.9064, F1 Micro: 0.7772, F1 Macro: 0.7693\nEpoch 8/10, Train Loss: 0.0487, Accuracy: 0.9066, F1 Micro: 0.777, F1 Macro: 0.7697\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0372, Accuracy: 0.9069, F1 Micro: 0.7856, F1 Macro: 0.7836\nEpoch 10/10, Train Loss: 0.0334, Accuracy: 0.903, F1 Micro: 0.7814, F1 Macro: 0.7783\n\nAccuracy: 0.9069, F1 Micro: 0.7856, F1 Macro: 0.7836\n                      precision    recall  f1-score   support\n\n          pornografi       0.92      0.91      0.91       370\n                sara       0.68      0.69      0.68       248\n         radikalisme       0.80      0.80      0.80       243\npencemaran_nama_baik       0.71      0.77      0.74       504\n\n           micro avg       0.77      0.80      0.79      1365\n           macro avg       0.77      0.79      0.78      1365\n        weighted avg       0.77      0.80      0.79      1365\n         samples avg       0.46      0.45      0.45      1365\n\nDuration: 218.96688961982727\n=====================\nSEED: 3\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3637, Accuracy: 0.8922, F1 Micro: 0.7412, F1 Macro: 0.738\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2377, Accuracy: 0.9027, F1 Micro: 0.7573, F1 Macro: 0.7479\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1929, Accuracy: 0.9044, F1 Micro: 0.7652, F1 Macro: 0.7542\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1506, Accuracy: 0.9048, F1 Micro: 0.7772, F1 Macro: 0.7757\nEpoch 5/10, Train Loss: 0.1146, Accuracy: 0.9045, F1 Micro: 0.7679, F1 Macro: 0.7626\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0869, Accuracy: 0.9016, F1 Micro: 0.7816, F1 Macro: 0.7796\nEpoch 7/10, Train Loss: 0.0649, Accuracy: 0.9013, F1 Micro: 0.7733, F1 Macro: 0.7676\nEpoch 8/10, Train Loss: 0.0469, Accuracy: 0.9045, F1 Micro: 0.7769, F1 Macro: 0.7737\nEpoch 9/10, Train Loss: 0.0365, Accuracy: 0.9045, F1 Micro: 0.7714, F1 Macro: 0.7705\nEpoch 10/10, Train Loss: 0.0339, Accuracy: 0.8991, F1 Micro: 0.7632, F1 Macro: 0.7617\n\nAccuracy: 0.9016, F1 Micro: 0.7816, F1 Macro: 0.7796\n                      precision    recall  f1-score   support\n\n          pornografi       0.91      0.93      0.92       370\n                sara       0.63      0.71      0.67       248\n         radikalisme       0.75      0.86      0.80       243\npencemaran_nama_baik       0.69      0.79      0.74       504\n\n           micro avg       0.74      0.83      0.78      1365\n           macro avg       0.74      0.82      0.78      1365\n        weighted avg       0.75      0.83      0.78      1365\n         samples avg       0.47      0.47      0.46      1365\n\nDuration: 220.14860010147095\n=====================\nSEED: 94\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3548, Accuracy: 0.8878, F1 Micro: 0.7219, F1 Macro: 0.7142\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2397, Accuracy: 0.8992, F1 Micro: 0.7491, F1 Macro: 0.7401\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1948, Accuracy: 0.9045, F1 Micro: 0.7736, F1 Macro: 0.7644\nEpoch 4/10, Train Loss: 0.1531, Accuracy: 0.9031, F1 Micro: 0.7688, F1 Macro: 0.7667\nEpoch 5/10, Train Loss: 0.1152, Accuracy: 0.9014, F1 Micro: 0.7676, F1 Macro: 0.7653\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0875, Accuracy: 0.9023, F1 Micro: 0.7748, F1 Macro: 0.7707\nEpoch 7/10, Train Loss: 0.065, Accuracy: 0.9033, F1 Micro: 0.7691, F1 Macro: 0.7601\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.05, Accuracy: 0.9062, F1 Micro: 0.7813, F1 Macro: 0.7767\nEpoch 9/10, Train Loss: 0.0362, Accuracy: 0.9014, F1 Micro: 0.7802, F1 Macro: 0.7781\nEpoch 10/10, Train Loss: 0.0318, Accuracy: 0.9023, F1 Micro: 0.7635, F1 Macro: 0.7558\n\nAccuracy: 0.9062, F1 Micro: 0.7813, F1 Macro: 0.7767\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       370\n                sara       0.68      0.68      0.68       248\n         radikalisme       0.73      0.81      0.77       243\npencemaran_nama_baik       0.74      0.73      0.74       504\n\n           micro avg       0.78      0.79      0.78      1365\n           macro avg       0.77      0.78      0.78      1365\n        weighted avg       0.78      0.79      0.78      1365\n         samples avg       0.45      0.45      0.44      1365\n\nDuration: 222.14941143989563\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# NO ACCELERATE","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\ndef train_model_no_accel(sequence_length, model_name, seed=42, multi_classifier=False, layers_freezed=6):\n    model = BertForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(train_labels),\n        problem_type=\"multi_label_classification\"\n    )\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Wrap the model with DataParallel for multi-GPU training\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        model = torch.nn.DataParallel(model)\n\n    # Freeze specified layers\n    for name, param in model.named_parameters():\n        if \"encoder.layer\" in name:\n            layer_num = name.split(\".\")[3]\n            try:\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                continue\n\n    train_loader, val_loader = get_dataloaders(sequence_length, 4)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    scaler = GradScaler()\n\n    best_result = None\n    model.train()\n    start_time = time.time()\n\n    for epoch in range(EPOCHS):\n        epoch_loss = 0\n        model.train()  # Ensure model is in training mode\n        for step, batch in enumerate(train_loader):\n            inputs = {key: val.to(device, non_blocking=True) for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels'].to(device, non_blocking=True)\n\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(**inputs)\n                loss = loss_fn(outputs.logits, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            epoch_loss += loss.item()\n\n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {key: val.to(device, non_blocking=True) for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels'].to(device, non_blocking=True)\n\n                with autocast():\n                    outputs = model(**inputs)\n                    preds = torch.sigmoid(outputs.logits).round()\n\n                all_preds.append(preds)\n                all_labels.append(labels)\n\n        # Convert predictions and labels to tensors\n        all_preds = torch.cat(all_preds).cpu().numpy().astype(int)\n        all_labels = torch.cat(all_labels).cpu().numpy().astype(int)\n\n        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n            print(\"Higher f1 achieved, saving model\")\n            torch.save(model.state_dict(), f'no-accelerate.pth')\n            model.module.config.to_json_file(f'no-accelerate-config.json')\n            best_result = result\n\n        print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {epoch_loss / len(train_loader):.4f}, \"\n              f\"Accuracy: {result['accuracy']:.4f}, F1 Micro: {result['f1_micro']:.4f}, F1 Macro: {result['f1_macro']:.4f}\")\n\n    duration = time.time() - start_time\n    print(f\"Accuracy: {best_result['accuracy']:.4f}, F1 Micro: {best_result['f1_micro']:.4f}, F1 Macro: {best_result['f1_macro']:.4f}\")\n    print(best_result['report'])\n    print(f\"Training completed in {duration:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:23:55.120035Z","iopub.execute_input":"2025-01-05T15:23:55.120373Z","iopub.status.idle":"2025-01-05T15:23:55.131587Z","shell.execute_reply.started":"2025-01-05T15:23:55.120343Z","shell.execute_reply":"2025-01-05T15:23:55.130729Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"set_seed(42)\nLEARNING_RATE = 2e-5\ntrain_model_no_accel(96, 'indobenchmark/indobert-base-p1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:23:57.563744Z","iopub.execute_input":"2025-01-05T15:23:57.564073Z","iopub.status.idle":"2025-01-05T15:32:03.097465Z","shell.execute_reply.started":"2025-01-05T15:23:57.564046Z","shell.execute_reply":"2025-01-05T15:32:03.096369Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs\nHigher f1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3057, Accuracy: 0.9011, F1 Micro: 0.7632, F1 Macro: 0.7578\nHigher f1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2020, Accuracy: 0.9061, F1 Micro: 0.7964, F1 Macro: 0.7940\nEpoch 3/10, Train Loss: 0.1376, Accuracy: 0.8939, F1 Micro: 0.7218, F1 Macro: 0.7135\nEpoch 4/10, Train Loss: 0.0891, Accuracy: 0.9053, F1 Micro: 0.7723, F1 Macro: 0.7685\nEpoch 5/10, Train Loss: 0.0550, Accuracy: 0.9014, F1 Micro: 0.7755, F1 Macro: 0.7718\nEpoch 6/10, Train Loss: 0.0340, Accuracy: 0.9016, F1 Micro: 0.7752, F1 Macro: 0.7770\nEpoch 7/10, Train Loss: 0.0267, Accuracy: 0.9024, F1 Micro: 0.7741, F1 Macro: 0.7724\nEpoch 8/10, Train Loss: 0.0220, Accuracy: 0.9014, F1 Micro: 0.7654, F1 Macro: 0.7600\nEpoch 9/10, Train Loss: 0.0183, Accuracy: 0.9080, F1 Micro: 0.7864, F1 Macro: 0.7853\nEpoch 10/10, Train Loss: 0.0145, Accuracy: 0.9019, F1 Micro: 0.7821, F1 Macro: 0.7837\nAccuracy: 0.9061, F1 Micro: 0.7964, F1 Macro: 0.7940\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       362\n                sara       0.63      0.76      0.68       237\n         radikalisme       0.73      0.90      0.81       235\npencemaran_nama_baik       0.69      0.86      0.76       492\n\n           micro avg       0.74      0.86      0.80      1326\n           macro avg       0.74      0.86      0.79      1326\n        weighted avg       0.75      0.86      0.80      1326\n         samples avg       0.47      0.49      0.47      1326\n\nTraining completed in 483.48 seconds.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# P100 GPU","metadata":{}},{"cell_type":"code","source":"train_model(96, 'indobenchmark/indobert-base-p1', 42, 6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:37:04.613860Z","iopub.execute_input":"2025-01-05T15:37:04.614172Z","iopub.status.idle":"2025-01-05T15:45:53.446174Z","shell.execute_reply.started":"2025-01-05T15:37:04.614146Z","shell.execute_reply":"2025-01-05T15:45:53.445269Z"}},"outputs":[{"name":"stdout","text":"DistributedType.NO\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee8873346604f8f92139e3db09cb5be"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3272, Accuracy: 0.895, F1 Micro: 0.7418, F1 Macro: 0.7288\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.2188, Accuracy: 0.8957, F1 Micro: 0.7836, F1 Macro: 0.7825\nEpoch 3/10, Train Loss: 0.1692, Accuracy: 0.9048, F1 Micro: 0.7656, F1 Macro: 0.7517\nEpoch 4/10, Train Loss: 0.122, Accuracy: 0.9051, F1 Micro: 0.7813, F1 Macro: 0.7773\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.085, Accuracy: 0.9018, F1 Micro: 0.7885, F1 Macro: 0.7852\nEpoch 6/10, Train Loss: 0.0563, Accuracy: 0.9059, F1 Micro: 0.7803, F1 Macro: 0.7767\nEpoch 7/10, Train Loss: 0.0422, Accuracy: 0.9053, F1 Micro: 0.7827, F1 Macro: 0.7785\nEpoch 8/10, Train Loss: 0.0325, Accuracy: 0.9064, F1 Micro: 0.7797, F1 Macro: 0.7716\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0268, Accuracy: 0.9037, F1 Micro: 0.7886, F1 Macro: 0.7866\nEpoch 10/10, Train Loss: 0.0217, Accuracy: 0.9034, F1 Micro: 0.7783, F1 Macro: 0.7755\n\nAccuracy: 0.9037, F1 Micro: 0.7886, F1 Macro: 0.7866\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.92      0.93       362\n                sara       0.62      0.78      0.69       237\n         radikalisme       0.73      0.84      0.78       235\npencemaran_nama_baik       0.69      0.82      0.75       492\n\n           micro avg       0.74      0.84      0.79      1326\n           macro avg       0.74      0.84      0.79      1326\n        weighted avg       0.75      0.84      0.79      1326\n         samples avg       0.47      0.48      0.47      1326\n\nDuration: 524.3400189876556\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# T4 GPU","metadata":{}},{"cell_type":"code","source":"import os\nos._exit(0)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-05T15:49:57.153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"notebook_launcher(train_model, (96, 'indobenchmark/indobert-base-p1', 42, 6), num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:50:16.059853Z","iopub.execute_input":"2025-01-05T15:50:16.060206Z","iopub.status.idle":"2025-01-05T15:54:10.063508Z","shell.execute_reply.started":"2025-01-05T15:50:16.060174Z","shell.execute_reply":"2025-01-05T15:54:10.062369Z"}},"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\nDistributedType.MULTI_GPU\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.3705, Accuracy: 0.8867, F1 Micro: 0.721, F1 Macro: 0.7107\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.242, Accuracy: 0.9013, F1 Micro: 0.758, F1 Macro: 0.7516\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1938, Accuracy: 0.9042, F1 Micro: 0.7689, F1 Macro: 0.7581\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1526, Accuracy: 0.9069, F1 Micro: 0.7758, F1 Macro: 0.7729\nEpoch 5/10, Train Loss: 0.1136, Accuracy: 0.9028, F1 Micro: 0.7693, F1 Macro: 0.763\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.086, Accuracy: 0.9002, F1 Micro: 0.7803, F1 Macro: 0.7779\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0676, Accuracy: 0.9059, F1 Micro: 0.7814, F1 Macro: 0.7763\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0457, Accuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7802\nEpoch 9/10, Train Loss: 0.0357, Accuracy: 0.9, F1 Micro: 0.7831, F1 Macro: 0.7822\nEpoch 10/10, Train Loss: 0.0297, Accuracy: 0.9031, F1 Micro: 0.784, F1 Macro: 0.7819\n\nAccuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7802\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       370\n                sara       0.66      0.69      0.67       248\n         radikalisme       0.72      0.86      0.79       243\npencemaran_nama_baik       0.72      0.76      0.74       504\n\n           micro avg       0.76      0.81      0.78      1365\n           macro avg       0.76      0.80      0.78      1365\n        weighted avg       0.77      0.81      0.79      1365\n         samples avg       0.46      0.46      0.45      1365\n\nDuration: 230.90501928329468\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# INFERENCE","metadata":{}},{"cell_type":"markdown","source":"## MULTI GPU","metadata":{}},{"cell_type":"code","source":"def run_inference():\n    # Initialize the accelerator\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    # device = torch.cuda\n    device = accelerator.device\n\n    # with accelerator.main_process_first():\n    with accelerator.main_process_first():\n        model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/model-96-6-42\")\n\n    train_loader, val_loader = get_dataloaders(96, 4)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\n    model, optimizer, train_loader, val_loader = accelerator.prepare(\n        model, optimizer, train_loader, val_loader\n    )\n\n    start_time = time.time()\n    # Evaluation\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    accelerator.wait_for_everyone()\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {key: val.to(device, non_blocking=True) for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels'].to(device, non_blocking=True)\n\n            outputs = model(**inputs)\n            preds = torch.sigmoid(outputs.logits).round()\n\n            all_preds.append(accelerator.gather(preds))\n            all_labels.append(accelerator.gather(labels))\n\n    # Convert predictions and labels to tensors\n    all_preds = torch.cat(all_preds).cpu().numpy()\n    all_labels = torch.cat(all_labels).cpu().numpy()\n\n    accelerator.wait_for_everyone()\n\n    result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    # Print results\n    accelerator.print(f\"\\nAccuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n    accelerator.print(result['report'])\n    accelerator.print(f\"Duration: {duration} s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:54:14.827907Z","iopub.execute_input":"2025-01-05T15:54:14.828249Z","iopub.status.idle":"2025-01-05T15:54:14.837111Z","shell.execute_reply.started":"2025-01-05T15:54:14.828215Z","shell.execute_reply":"2025-01-05T15:54:14.836205Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"notebook_launcher(run_inference, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:54:17.096885Z","iopub.execute_input":"2025-01-05T15:54:17.097200Z","iopub.status.idle":"2025-01-05T15:54:23.594638Z","shell.execute_reply.started":"2025-01-05T15:54:17.097175Z","shell.execute_reply":"2025-01-05T15:54:23.593587Z"}},"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n\nAccuracy: 0.9055, F1 Micro: 0.7842, F1 Macro: 0.7802\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.91      0.92       370\n                sara       0.66      0.69      0.67       248\n         radikalisme       0.72      0.86      0.79       243\npencemaran_nama_baik       0.72      0.76      0.74       504\n\n           micro avg       0.76      0.81      0.78      1365\n           macro avg       0.76      0.80      0.78      1365\n        weighted avg       0.77      0.81      0.79      1365\n         samples avg       0.46      0.46      0.45      1365\n\nDuration: 4.6305317878723145 s\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## SINGLE GPU","metadata":{}},{"cell_type":"code","source":"run_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:46:28.585518Z","iopub.execute_input":"2025-01-05T15:46:28.585834Z","iopub.status.idle":"2025-01-05T15:46:33.482634Z","shell.execute_reply.started":"2025-01-05T15:46:28.585811Z","shell.execute_reply":"2025-01-05T15:46:33.481508Z"}},"outputs":[{"name":"stdout","text":"\nAccuracy: 0.9037, F1 Micro: 0.7886, F1 Macro: 0.7866\n                      precision    recall  f1-score   support\n\n          pornografi       0.93      0.92      0.93       362\n                sara       0.62      0.78      0.69       237\n         radikalisme       0.73      0.84      0.78       235\npencemaran_nama_baik       0.69      0.82      0.75       492\n\n           micro avg       0.74      0.84      0.79      1326\n           macro avg       0.74      0.84      0.79      1326\n        weighted avg       0.75      0.84      0.79      1326\n         samples avg       0.47      0.48      0.47      1326\n\nDuration: 4.6490256786346436 s\n","output_type":"stream"}],"execution_count":16}]}