{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e401a4",
   "metadata": {
    "_cell_guid": "8f2d6396-efc6-4b53-84db-3aa958184594",
    "_uuid": "1b6e9116-3bce-4741-b4b7-2312472cd198",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01447,
     "end_time": "2025-06-26T13:41:59.889359",
     "exception": false,
     "start_time": "2025-06-26T13:41:59.874889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dbb42c",
   "metadata": {
    "_cell_guid": "ae281f38-64a8-4638-b8e4-3a741900f078",
    "_uuid": "0cb0ec13-f16b-4d64-ad45-a739469d7ffa",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:41:59.914288Z",
     "iopub.status.busy": "2025-06-26T13:41:59.914038Z",
     "iopub.status.idle": "2025-06-26T13:42:15.032940Z",
     "shell.execute_reply": "2025-06-26T13:42:15.032270Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.133676,
     "end_time": "2025-06-26T13:42:15.034976",
     "exception": false,
     "start_time": "2025-06-26T13:41:59.901300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289401c1",
   "metadata": {
    "_cell_guid": "aa3b47c7-7d0a-4d2a-8267-db943bd61598",
    "_uuid": "f6c329ce-eae6-415f-9d08-e6ba4e461481",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011845,
     "end_time": "2025-06-26T13:42:15.059224",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.047379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9017d566",
   "metadata": {
    "_cell_guid": "2f504e88-e769-4bed-b543-507399330fa9",
    "_uuid": "baca7141-29a2-4cc0-a7b1-68464fb8ae0a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.086209Z",
     "iopub.status.busy": "2025-06-26T13:42:15.085295Z",
     "iopub.status.idle": "2025-06-26T13:42:15.089118Z",
     "shell.execute_reply": "2025-06-26T13:42:15.088414Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019143,
     "end_time": "2025-06-26T13:42:15.090787",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.071644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f581370c",
   "metadata": {
    "_cell_guid": "1d3f092c-89a0-4dca-9ae8-f5bc3b9e2bbd",
    "_uuid": "0e119db5-eedf-452a-8524-e2506cf4bb25",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.119222Z",
     "iopub.status.busy": "2025-06-26T13:42:15.118970Z",
     "iopub.status.idle": "2025-06-26T13:42:15.123771Z",
     "shell.execute_reply": "2025-06-26T13:42:15.122979Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021353,
     "end_time": "2025-06-26T13:42:15.125487",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.104134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509b1522",
   "metadata": {
    "_cell_guid": "21cae7f8-5784-4260-b607-544348a5957b",
    "_uuid": "f2a4d7e6-8bd1-4686-a0d2-741358d0ac30",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.151621Z",
     "iopub.status.busy": "2025-06-26T13:42:15.151106Z",
     "iopub.status.idle": "2025-06-26T13:42:15.162918Z",
     "shell.execute_reply": "2025-06-26T13:42:15.162121Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026755,
     "end_time": "2025-06-26T13:42:15.164597",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.137842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0414c",
   "metadata": {
    "_cell_guid": "378107af-fe84-4ac7-ab5e-9c58d58c1e78",
    "_uuid": "07ec7d8c-df7a-406c-8b7a-d99bc80ecb82",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012562,
     "end_time": "2025-06-26T13:42:15.189299",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.176737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a6fd93",
   "metadata": {
    "_cell_guid": "72722c74-8739-418e-8a07-94cc90074c24",
    "_uuid": "3d2207ba-9c36-4527-b655-1d1de816b75d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.215434Z",
     "iopub.status.busy": "2025-06-26T13:42:15.215142Z",
     "iopub.status.idle": "2025-06-26T13:42:15.276857Z",
     "shell.execute_reply": "2025-06-26T13:42:15.275070Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.078012,
     "end_time": "2025-06-26T13:42:15.280048",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.202036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-random-kfold'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d7240",
   "metadata": {
    "_cell_guid": "3412aec9-7062-4443-a997-c929f5aba1d6",
    "_uuid": "d9151052-079f-4af5-ac11-83766fd46fa8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017905,
     "end_time": "2025-06-26T13:42:15.313805",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.295900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f37745",
   "metadata": {
    "_cell_guid": "7cc1eac8-021d-472a-abe9-1a637583f03f",
    "_uuid": "bc271888-e771-4b27-91b1-87b22b016412",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.342317Z",
     "iopub.status.busy": "2025-06-26T13:42:15.341397Z",
     "iopub.status.idle": "2025-06-26T13:42:15.565244Z",
     "shell.execute_reply": "2025-06-26T13:42:15.564222Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.239701,
     "end_time": "2025-06-26T13:42:15.567122",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.327421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7773, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-3/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-3/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d887a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.594101Z",
     "iopub.status.busy": "2025-06-26T13:42:15.593803Z",
     "iopub.status.idle": "2025-06-26T13:42:15.615920Z",
     "shell.execute_reply": "2025-06-26T13:42:15.615188Z"
    },
    "papermill": {
     "duration": 0.037984,
     "end_time": "2025-06-26T13:42:15.617477",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.579493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59024a11",
   "metadata": {
    "_cell_guid": "6b8b9df4-8c4d-4965-9958-08bdb4e4d6c0",
    "_uuid": "54508b8d-e337-4316-84b3-bccf3654e678",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012633,
     "end_time": "2025-06-26T13:42:15.642635",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.630002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63ee24c",
   "metadata": {
    "_cell_guid": "82b6127b-1fda-4a9c-9d0c-2ceaa5c8753e",
    "_uuid": "fdc6f4f9-faad-4635-9257-4e5b6aedef67",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:15.668447Z",
     "iopub.status.busy": "2025-06-26T13:42:15.667967Z",
     "iopub.status.idle": "2025-06-26T13:42:16.304301Z",
     "shell.execute_reply": "2025-06-26T13:42:16.303674Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.651066,
     "end_time": "2025-06-26T13:42:16.305962",
     "exception": false,
     "start_time": "2025-06-26T13:42:15.654896",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d850678be32413baada246625ad9dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b66af42be242ea93c434f718eaabd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b21ff463b34be5b0877f12a7a379d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0b156c00184325be57a061e33250bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        # print(labels)\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        # print(item['labels'])\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff215469",
   "metadata": {
    "_cell_guid": "13b6b0cb-a10f-4625-a715-279f0d65403d",
    "_uuid": "8ad8ea35-915b-41ad-8dcc-7c1239a18be5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.334017Z",
     "iopub.status.busy": "2025-06-26T13:42:16.333743Z",
     "iopub.status.idle": "2025-06-26T13:42:16.338551Z",
     "shell.execute_reply": "2025-06-26T13:42:16.337809Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020295,
     "end_time": "2025-06-26T13:42:16.340203",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.319908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=96, num_workers=4):\n",
    "    train_dataset = NetifierDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc2dce",
   "metadata": {
    "_cell_guid": "3e15620a-7436-4931-9c40-3b18f927e14f",
    "_uuid": "f47480e0-4673-4956-96f2-ec5afee75608",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012551,
     "end_time": "2025-06-26T13:42:16.365407",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.352856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f751ef0c",
   "metadata": {
    "_cell_guid": "553c5bec-42aa-42b6-a127-d63080798201",
    "_uuid": "92648d98-9283-4d8e-b826-ea881a56b954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.391737Z",
     "iopub.status.busy": "2025-06-26T13:42:16.391516Z",
     "iopub.status.idle": "2025-06-26T13:42:16.396770Z",
     "shell.execute_reply": "2025-06-26T13:42:16.396123Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020253,
     "end_time": "2025-06-26T13:42:16.398194",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.377941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff8c3b0",
   "metadata": {
    "_cell_guid": "aa87bbfb-1bb9-44ec-aa17-2fb6bb1fc9ce",
    "_uuid": "4fe5abe5-7905-44d0-8e91-d8e22cd1a576",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.424352Z",
     "iopub.status.busy": "2025-06-26T13:42:16.424111Z",
     "iopub.status.idle": "2025-06-26T13:42:16.437234Z",
     "shell.execute_reply": "2025-06-26T13:42:16.436563Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027885,
     "end_time": "2025-06-26T13:42:16.438638",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.410753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, seed, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    accelerator.print(f\"Fold {trials + 1} - Training with {current_train_size} samples...\")\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders using the fold's data\n",
    "    current_X_train = [X_train_fold[i] for i in train_indices]\n",
    "    current_y_train = [y_train_fold[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val_fold, y_val_fold)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-fold-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Best result for {current_train_size} samples: F1 Micro: {round(best_result['f1_micro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(best_result['accuracy'])\n",
    "        metrics[2].append(best_result['f1_micro'])\n",
    "        metrics[3].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7774ee",
   "metadata": {
    "_cell_guid": "83167820-d8f9-4cbe-9d4e-41da47b49098",
    "_uuid": "49d74d1d-8911-454c-a3f3-ddc71ec8b7e1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012355,
     "end_time": "2025-06-26T13:42:16.463471",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.451116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877f2175",
   "metadata": {
    "_cell_guid": "c5d9b68b-b6ec-40c4-b61b-ba8899b9eaba",
    "_uuid": "ed4d258c-5ed4-4007-b569-a3aa85b85566",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.489594Z",
     "iopub.status.busy": "2025-06-26T13:42:16.489334Z",
     "iopub.status.idle": "2025-06-26T13:42:16.495699Z",
     "shell.execute_reply": "2025-06-26T13:42:16.494912Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021452,
     "end_time": "2025-06-26T13:42:16.497279",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.475827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ebd2a",
   "metadata": {
    "_cell_guid": "b8eed57f-f919-42cc-be90-ba5465a3897e",
    "_uuid": "abfd888c-20ce-4c43-8439-9fed0779dc77",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012572,
     "end_time": "2025-06-26T13:42:16.522493",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.509921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ee1665",
   "metadata": {
    "_cell_guid": "53882ced-6011-45de-bcaf-8770e74e80b7",
    "_uuid": "610f5ed2-f0fa-424a-9871-dd3f5f397bef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.548666Z",
     "iopub.status.busy": "2025-06-26T13:42:16.548452Z",
     "iopub.status.idle": "2025-06-26T13:42:16.555950Z",
     "shell.execute_reply": "2025-06-26T13:42:16.555287Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022388,
     "end_time": "2025-06-26T13:42:16.557396",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.535008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_sampling(current_train_size, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, X_train_fold, y_train_fold, n_samples=min_increment):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    nearest_cp = 0\n",
    "    arrived_at_cp = False\n",
    "    for cp in checkpoints:\n",
    "        if cp > current_train_size:\n",
    "            nearest_cp = cp\n",
    "            break\n",
    "\n",
    "    num_of_candidates = math.ceil(0.1 * len(remaining_indices))\n",
    "\n",
    "    if num_of_candidates <= n_samples and n_samples < nearest_cp - current_train_size:\n",
    "        num_of_candidates = n_samples\n",
    "    elif num_of_candidates > n_samples and num_of_candidates < nearest_cp - current_train_size:\n",
    "        num_of_candidates = max(n_samples, num_of_candidates)\n",
    "    else:\n",
    "        num_of_candidates = nearest_cp - current_train_size\n",
    "        arrived_at_cp = True\n",
    "\n",
    "    if len(remaining_indices) < n_samples:\n",
    "        random_indices = range(len(X_pool))\n",
    "    else:\n",
    "        random_indices = random.sample(range(len(X_pool)), num_of_candidates)\n",
    "\n",
    "    if arrived_at_cp:\n",
    "        temp = train_indices.copy()\n",
    "        temp.extend([remaining_indices[i] for i in random_indices])\n",
    "            \n",
    "        # Save acquired data up to checkpoint\n",
    "        acquired_data = pd.DataFrame({\n",
    "            'processed_text': [X_train_fold[i] for i in temp],\n",
    "            'pornografi': [y_train_fold[i][0] for i in temp],\n",
    "            'sara': [y_train_fold[i][1] for i in temp],\n",
    "            'radikalisme': [y_train_fold[i][2] for i in temp],\n",
    "            'pencemaran_nama_baik': [y_train_fold[i][3] for i in temp],\n",
    "        })\n",
    "\n",
    "        acquired_data.to_csv(f'acquired_data/{filename}-fold-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "    end_time = time.time() \n",
    "    duration = end_time - start_time\n",
    "\n",
    "    sampling_dur.append(duration)\n",
    "    for i in random_indices:\n",
    "        new_samples.append(remaining_indices[i])\n",
    "        \n",
    "    print(\"Nearest checkpoint:\", nearest_cp)\n",
    "    print(\"Acquired samples:\", len(random_indices))\n",
    "    print(f\"Sampling duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad752d3a",
   "metadata": {
    "_cell_guid": "779cdeb4-7cca-4a0a-b341-32491af70c9b",
    "_uuid": "3054b2af-d422-4eec-b77b-55a3a0427e77",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01242,
     "end_time": "2025-06-26T13:42:16.582355",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.569935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65947ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T13:42:16.608826Z",
     "iopub.status.busy": "2025-06-26T13:42:16.608616Z",
     "iopub.status.idle": "2025-06-26T18:42:11.299161Z",
     "shell.execute_reply": "2025-06-26T18:42:11.298082Z"
    },
    "papermill": {
     "duration": 17994.7061,
     "end_time": "2025-06-26T18:42:11.301079",
     "exception": false,
     "start_time": "2025-06-26T13:42:16.594979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "STARTING FOLD 1/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 388 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99918b649c4d4eda81b33521796d5bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5233, Accuracy: 0.7902, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4851, Accuracy: 0.8166, F1 Micro: 0.2367, F1 Macro: 0.1613\n",
      "Epoch 4/10, Train Loss: 0.4274, Accuracy: 0.8252, F1 Micro: 0.3327, F1 Macro: 0.227\n",
      "Epoch 5/10, Train Loss: 0.3978, Accuracy: 0.8386, F1 Micro: 0.4661, F1 Macro: 0.3619\n",
      "Epoch 6/10, Train Loss: 0.349, Accuracy: 0.8506, F1 Micro: 0.5631, F1 Macro: 0.5253\n",
      "Epoch 7/10, Train Loss: 0.3106, Accuracy: 0.862, F1 Micro: 0.652, F1 Macro: 0.6434\n",
      "Epoch 8/10, Train Loss: 0.2486, Accuracy: 0.8617, F1 Micro: 0.6641, F1 Macro: 0.6497\n",
      "Epoch 9/10, Train Loss: 0.2302, Accuracy: 0.8678, F1 Micro: 0.6874, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.8681, F1 Micro: 0.6544, F1 Macro: 0.6387\n",
      "Best result for 388 samples: F1 Micro: 0.6874\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.79      0.85       369\n",
      "                sara       0.55      0.57      0.56       262\n",
      "         radikalisme       0.65      0.72      0.68       234\n",
      "pencemaran_nama_baik       0.63      0.67      0.65       478\n",
      "\n",
      "           micro avg       0.68      0.69      0.69      1343\n",
      "           macro avg       0.68      0.69      0.68      1343\n",
      "        weighted avg       0.69      0.69      0.69      1343\n",
      "         samples avg       0.37      0.39      0.37      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 0.00048661231994628906 seconds\n",
      "\n",
      "Fold 1 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5462, Accuracy: 0.8044, F1 Micro: 0.133, F1 Macro: 0.1021\n",
      "Epoch 2/10, Train Loss: 0.4524, Accuracy: 0.8366, F1 Micro: 0.4137, F1 Macro: 0.3359\n",
      "Epoch 3/10, Train Loss: 0.3485, Accuracy: 0.8619, F1 Micro: 0.6078, F1 Macro: 0.5889\n",
      "Epoch 4/10, Train Loss: 0.2735, Accuracy: 0.8761, F1 Micro: 0.6993, F1 Macro: 0.6935\n",
      "Epoch 5/10, Train Loss: 0.2214, Accuracy: 0.8769, F1 Micro: 0.6919, F1 Macro: 0.6784\n",
      "Epoch 6/10, Train Loss: 0.1797, Accuracy: 0.8798, F1 Micro: 0.7205, F1 Macro: 0.7162\n",
      "Epoch 7/10, Train Loss: 0.1599, Accuracy: 0.8752, F1 Micro: 0.7192, F1 Macro: 0.7189\n",
      "Epoch 8/10, Train Loss: 0.1224, Accuracy: 0.8836, F1 Micro: 0.6876, F1 Macro: 0.6761\n",
      "Epoch 9/10, Train Loss: 0.0917, Accuracy: 0.882, F1 Micro: 0.7148, F1 Macro: 0.713\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.882, F1 Micro: 0.7008, F1 Macro: 0.6912\n",
      "Best result for 971 samples: F1 Micro: 0.7205\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.85      0.87       369\n",
      "                sara       0.55      0.61      0.58       262\n",
      "         radikalisme       0.66      0.84      0.74       234\n",
      "pencemaran_nama_baik       0.69      0.67      0.68       478\n",
      "\n",
      "           micro avg       0.70      0.74      0.72      1343\n",
      "           macro avg       0.70      0.74      0.72      1343\n",
      "        weighted avg       0.71      0.74      0.72      1343\n",
      "         samples avg       0.40      0.41      0.39      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.00037407875061035156 seconds\n",
      "\n",
      "Fold 1 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5106, Accuracy: 0.8258, F1 Micro: 0.307, F1 Macro: 0.21\n",
      "Epoch 2/10, Train Loss: 0.3856, Accuracy: 0.8584, F1 Micro: 0.6614, F1 Macro: 0.6551\n",
      "Epoch 3/10, Train Loss: 0.3087, Accuracy: 0.8716, F1 Micro: 0.6558, F1 Macro: 0.651\n",
      "Epoch 4/10, Train Loss: 0.2469, Accuracy: 0.8778, F1 Micro: 0.657, F1 Macro: 0.6367\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.8833, F1 Micro: 0.7011, F1 Macro: 0.6931\n",
      "Epoch 6/10, Train Loss: 0.1635, Accuracy: 0.8848, F1 Micro: 0.7136, F1 Macro: 0.7086\n",
      "Epoch 7/10, Train Loss: 0.131, Accuracy: 0.8831, F1 Micro: 0.6969, F1 Macro: 0.681\n",
      "Epoch 8/10, Train Loss: 0.0958, Accuracy: 0.8825, F1 Micro: 0.7162, F1 Macro: 0.7125\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.8867, F1 Micro: 0.7106, F1 Macro: 0.7011\n",
      "Epoch 10/10, Train Loss: 0.0505, Accuracy: 0.8809, F1 Micro: 0.6971, F1 Macro: 0.6856\n",
      "Best result for 1496 samples: F1 Micro: 0.7162\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       369\n",
      "                sara       0.58      0.59      0.58       262\n",
      "         radikalisme       0.73      0.73      0.73       234\n",
      "pencemaran_nama_baik       0.68      0.65      0.67       478\n",
      "\n",
      "           micro avg       0.73      0.71      0.72      1343\n",
      "           macro avg       0.72      0.70      0.71      1343\n",
      "        weighted avg       0.73      0.71      0.72      1343\n",
      "         samples avg       0.40      0.40      0.39      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0003383159637451172 seconds\n",
      "\n",
      "Fold 1 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4734, Accuracy: 0.832, F1 Micro: 0.3597, F1 Macro: 0.2561\n",
      "Epoch 2/10, Train Loss: 0.3362, Accuracy: 0.8711, F1 Micro: 0.6526, F1 Macro: 0.6432\n",
      "Epoch 3/10, Train Loss: 0.2601, Accuracy: 0.8794, F1 Micro: 0.7235, F1 Macro: 0.7179\n",
      "Epoch 4/10, Train Loss: 0.2123, Accuracy: 0.8898, F1 Micro: 0.7197, F1 Macro: 0.7117\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.8891, F1 Micro: 0.7061, F1 Macro: 0.6988\n",
      "Epoch 6/10, Train Loss: 0.1363, Accuracy: 0.89, F1 Micro: 0.7211, F1 Macro: 0.7147\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.8881, F1 Micro: 0.7244, F1 Macro: 0.7173\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.8905, F1 Micro: 0.7026, F1 Macro: 0.6907\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.8891, F1 Micro: 0.7337, F1 Macro: 0.7278\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.8872, F1 Micro: 0.7271, F1 Macro: 0.7238\n",
      "Best result for 1969 samples: F1 Micro: 0.7337\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.85      0.87       369\n",
      "                sara       0.64      0.57      0.60       262\n",
      "         radikalisme       0.76      0.72      0.74       234\n",
      "pencemaran_nama_baik       0.67      0.73      0.70       478\n",
      "\n",
      "           micro avg       0.74      0.73      0.73      1343\n",
      "           macro avg       0.74      0.72      0.73      1343\n",
      "        weighted avg       0.74      0.73      0.73      1343\n",
      "         samples avg       0.42      0.42      0.41      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 0.0003032684326171875 seconds\n",
      "\n",
      "Fold 1 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4706, Accuracy: 0.8363, F1 Micro: 0.4712, F1 Macro: 0.4161\n",
      "Epoch 2/10, Train Loss: 0.3103, Accuracy: 0.8809, F1 Micro: 0.6859, F1 Macro: 0.6729\n",
      "Epoch 3/10, Train Loss: 0.2406, Accuracy: 0.887, F1 Micro: 0.693, F1 Macro: 0.6728\n",
      "Epoch 4/10, Train Loss: 0.2044, Accuracy: 0.8902, F1 Micro: 0.7368, F1 Macro: 0.7298\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.887, F1 Micro: 0.7347, F1 Macro: 0.7305\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.8864, F1 Micro: 0.7218, F1 Macro: 0.7153\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.8902, F1 Micro: 0.7233, F1 Macro: 0.7154\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.8883, F1 Micro: 0.7341, F1 Macro: 0.7299\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.8905, F1 Micro: 0.7297, F1 Macro: 0.7248\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.8944, F1 Micro: 0.7313, F1 Macro: 0.7276\n",
      "Best result for 2394 samples: F1 Micro: 0.7368\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.82      0.87       369\n",
      "                sara       0.63      0.56      0.59       262\n",
      "         radikalisme       0.73      0.76      0.75       234\n",
      "pencemaran_nama_baik       0.68      0.75      0.71       478\n",
      "\n",
      "           micro avg       0.74      0.73      0.74      1343\n",
      "           macro avg       0.74      0.72      0.73      1343\n",
      "        weighted avg       0.75      0.73      0.74      1343\n",
      "         samples avg       0.42      0.41      0.41      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 0.0007584095001220703 seconds\n",
      "\n",
      "Fold 1 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4449, Accuracy: 0.8495, F1 Micro: 0.5337, F1 Macro: 0.4861\n",
      "Epoch 2/10, Train Loss: 0.2865, Accuracy: 0.8836, F1 Micro: 0.7033, F1 Macro: 0.687\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.8942, F1 Micro: 0.7369, F1 Macro: 0.7311\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.8881, F1 Micro: 0.7354, F1 Macro: 0.7321\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.8742, F1 Micro: 0.7285, F1 Macro: 0.7279\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.8919, F1 Micro: 0.7316, F1 Macro: 0.7267\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.8923, F1 Micro: 0.7128, F1 Macro: 0.7003\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.8847, F1 Micro: 0.7429, F1 Macro: 0.7445\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.8894, F1 Micro: 0.736, F1 Macro: 0.7322\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.8936, F1 Micro: 0.7439, F1 Macro: 0.7417\n",
      "Best result for 2777 samples: F1 Micro: 0.7439\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.85      0.89       369\n",
      "                sara       0.66      0.62      0.64       262\n",
      "         radikalisme       0.73      0.77      0.75       234\n",
      "pencemaran_nama_baik       0.69      0.69      0.69       478\n",
      "\n",
      "           micro avg       0.75      0.74      0.74      1343\n",
      "           macro avg       0.75      0.73      0.74      1343\n",
      "        weighted avg       0.75      0.74      0.74      1343\n",
      "         samples avg       0.43      0.42      0.41      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 0.0004124641418457031 seconds\n",
      "\n",
      "Fold 1 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4362, Accuracy: 0.857, F1 Micro: 0.5869, F1 Macro: 0.5658\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8878, F1 Micro: 0.706, F1 Macro: 0.6953\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.8945, F1 Micro: 0.7251, F1 Macro: 0.7154\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.8927, F1 Micro: 0.7038, F1 Macro: 0.6886\n",
      "Epoch 5/10, Train Loss: 0.1511, Accuracy: 0.8934, F1 Micro: 0.7212, F1 Macro: 0.7118\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.8936, F1 Micro: 0.7499, F1 Macro: 0.7513\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.8891, F1 Micro: 0.7374, F1 Macro: 0.7369\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.8947, F1 Micro: 0.742, F1 Macro: 0.7364\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.8927, F1 Micro: 0.7434, F1 Macro: 0.7383\n",
      "Epoch 10/10, Train Loss: 0.0366, Accuracy: 0.8942, F1 Micro: 0.7419, F1 Macro: 0.7381\n",
      "Best result for 3122 samples: F1 Micro: 0.7499\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.84      0.88       369\n",
      "                sara       0.65      0.68      0.66       262\n",
      "         radikalisme       0.74      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.67      0.73      0.70       478\n",
      "\n",
      "           micro avg       0.74      0.76      0.75      1343\n",
      "           macro avg       0.75      0.76      0.75      1343\n",
      "        weighted avg       0.75      0.76      0.75      1343\n",
      "         samples avg       0.42      0.43      0.42      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 0.00023484230041503906 seconds\n",
      "\n",
      "Fold 1 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4345, Accuracy: 0.865, F1 Micro: 0.6201, F1 Macro: 0.6\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.892, F1 Micro: 0.7194, F1 Macro: 0.7143\n",
      "Epoch 3/10, Train Loss: 0.2142, Accuracy: 0.8947, F1 Micro: 0.7373, F1 Macro: 0.7278\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.8958, F1 Micro: 0.7482, F1 Macro: 0.7439\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.8945, F1 Micro: 0.7432, F1 Macro: 0.7374\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.8961, F1 Micro: 0.7614, F1 Macro: 0.7584\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.8944, F1 Micro: 0.7544, F1 Macro: 0.7553\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.8961, F1 Micro: 0.7451, F1 Macro: 0.7421\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.8978, F1 Micro: 0.7492, F1 Macro: 0.7468\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.895, F1 Micro: 0.715, F1 Macro: 0.7022\n",
      "Best result for 3432 samples: F1 Micro: 0.7614\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.89       369\n",
      "                sara       0.64      0.66      0.65       262\n",
      "         radikalisme       0.73      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.67      0.79      0.73       478\n",
      "\n",
      "           micro avg       0.73      0.79      0.76      1343\n",
      "           macro avg       0.74      0.78      0.76      1343\n",
      "        weighted avg       0.74      0.79      0.76      1343\n",
      "         samples avg       0.43      0.44      0.43      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 0.00023865699768066406 seconds\n",
      "\n",
      "Fold 1 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4112, Accuracy: 0.8684, F1 Micro: 0.6333, F1 Macro: 0.6106\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.8917, F1 Micro: 0.7135, F1 Macro: 0.6965\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.895, F1 Micro: 0.7429, F1 Macro: 0.7394\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.8967, F1 Micro: 0.7495, F1 Macro: 0.7477\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.8973, F1 Micro: 0.7354, F1 Macro: 0.716\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.8973, F1 Micro: 0.7333, F1 Macro: 0.7203\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.8977, F1 Micro: 0.7497, F1 Macro: 0.7463\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.8995, F1 Micro: 0.7605, F1 Macro: 0.7575\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.9, F1 Micro: 0.7458, F1 Macro: 0.739\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.8981, F1 Micro: 0.7519, F1 Macro: 0.7517\n",
      "Best result for 3711 samples: F1 Micro: 0.7605\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.90       369\n",
      "                sara       0.67      0.66      0.67       262\n",
      "         radikalisme       0.76      0.74      0.75       234\n",
      "pencemaran_nama_baik       0.69      0.74      0.72       478\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1343\n",
      "           macro avg       0.76      0.75      0.76      1343\n",
      "        weighted avg       0.76      0.76      0.76      1343\n",
      "         samples avg       0.44      0.43      0.43      1343\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 0.09333634376525879 seconds\n",
      "\n",
      "Fold 1 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4179, Accuracy: 0.8717, F1 Micro: 0.6772, F1 Macro: 0.6737\n",
      "Epoch 2/10, Train Loss: 0.2669, Accuracy: 0.8905, F1 Micro: 0.7184, F1 Macro: 0.7129\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.8991, F1 Micro: 0.7368, F1 Macro: 0.726\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.8983, F1 Micro: 0.7541, F1 Macro: 0.7471\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9002, F1 Micro: 0.7545, F1 Macro: 0.7519\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.8931, F1 Micro: 0.754, F1 Macro: 0.7526\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.8994, F1 Micro: 0.7584, F1 Macro: 0.7566\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.9009, F1 Micro: 0.7608, F1 Macro: 0.76\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.8963, F1 Micro: 0.7546, F1 Macro: 0.7538\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8945, F1 Micro: 0.7575, F1 Macro: 0.7568\n",
      "Best result for 3886 samples: F1 Micro: 0.7608\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.85      0.89       369\n",
      "                sara       0.70      0.64      0.67       262\n",
      "         radikalisme       0.76      0.78      0.77       234\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       478\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1343\n",
      "           macro avg       0.77      0.75      0.76      1343\n",
      "        weighted avg       0.78      0.75      0.76      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 0.000213623046875 seconds\n",
      "\n",
      "Fold 1 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4149, Accuracy: 0.8745, F1 Micro: 0.6908, F1 Macro: 0.6874\n",
      "Epoch 2/10, Train Loss: 0.255, Accuracy: 0.8966, F1 Micro: 0.7402, F1 Macro: 0.7316\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.9013, F1 Micro: 0.7474, F1 Macro: 0.7352\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.9013, F1 Micro: 0.7406, F1 Macro: 0.7278\n",
      "Epoch 5/10, Train Loss: 0.1369, Accuracy: 0.8988, F1 Micro: 0.7289, F1 Macro: 0.7101\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9056, F1 Micro: 0.7622, F1 Macro: 0.7542\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9022, F1 Micro: 0.7609, F1 Macro: 0.7525\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.8997, F1 Micro: 0.7478, F1 Macro: 0.7333\n",
      "Epoch 9/10, Train Loss: 0.0442, Accuracy: 0.9009, F1 Micro: 0.7609, F1 Macro: 0.7553\n",
      "Epoch 10/10, Train Loss: 0.0345, Accuracy: 0.8969, F1 Micro: 0.7622, F1 Macro: 0.7616\n",
      "Best result for 4120 samples: F1 Micro: 0.7622\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       369\n",
      "                sara       0.62      0.72      0.67       262\n",
      "         radikalisme       0.71      0.82      0.76       234\n",
      "pencemaran_nama_baik       0.69      0.74      0.72       478\n",
      "\n",
      "           micro avg       0.74      0.79      0.76      1343\n",
      "           macro avg       0.74      0.79      0.76      1343\n",
      "        weighted avg       0.75      0.79      0.77      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 0.0002148151397705078 seconds\n",
      "\n",
      "Fold 1 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8719, F1 Micro: 0.6875, F1 Macro: 0.6752\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.8917, F1 Micro: 0.739, F1 Macro: 0.7291\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.8992, F1 Micro: 0.7491, F1 Macro: 0.7417\n",
      "Epoch 4/10, Train Loss: 0.1667, Accuracy: 0.9005, F1 Micro: 0.753, F1 Macro: 0.7453\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9009, F1 Micro: 0.7537, F1 Macro: 0.7505\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.8991, F1 Micro: 0.7711, F1 Macro: 0.7718\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.9013, F1 Micro: 0.7537, F1 Macro: 0.7475\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.9023, F1 Micro: 0.7563, F1 Macro: 0.7505\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9039, F1 Micro: 0.7585, F1 Macro: 0.7545\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.8994, F1 Micro: 0.7677, F1 Macro: 0.7681\n",
      "Best result for 4330 samples: F1 Micro: 0.7711\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       369\n",
      "                sara       0.61      0.77      0.68       262\n",
      "         radikalisme       0.74      0.80      0.77       234\n",
      "pencemaran_nama_baik       0.68      0.78      0.73       478\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1343\n",
      "           macro avg       0.74      0.81      0.77      1343\n",
      "        weighted avg       0.75      0.81      0.78      1343\n",
      "         samples avg       0.44      0.46      0.44      1343\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002014636993408203 seconds\n",
      "\n",
      "Fold 1 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4033, Accuracy: 0.8773, F1 Micro: 0.6879, F1 Macro: 0.6813\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.8959, F1 Micro: 0.7388, F1 Macro: 0.7178\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.8997, F1 Micro: 0.7574, F1 Macro: 0.7541\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.902, F1 Micro: 0.7692, F1 Macro: 0.7675\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.8967, F1 Micro: 0.7627, F1 Macro: 0.7614\n",
      "Epoch 6/10, Train Loss: 0.1034, Accuracy: 0.9031, F1 Micro: 0.7506, F1 Macro: 0.7451\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9027, F1 Micro: 0.769, F1 Macro: 0.7652\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.8992, F1 Micro: 0.7567, F1 Macro: 0.7508\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9006, F1 Micro: 0.7634, F1 Macro: 0.7604\n",
      "Epoch 10/10, Train Loss: 0.0287, Accuracy: 0.8973, F1 Micro: 0.7632, F1 Macro: 0.7624\n",
      "Best result for 4530 samples: F1 Micro: 0.7692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.87      0.90       369\n",
      "                sara       0.67      0.66      0.67       262\n",
      "         radikalisme       0.77      0.78      0.78       234\n",
      "pencemaran_nama_baik       0.69      0.77      0.73       478\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1343\n",
      "           macro avg       0.77      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.78      0.77      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 0.07250285148620605 seconds\n",
      "\n",
      "Fold 1 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3939, Accuracy: 0.8756, F1 Micro: 0.6563, F1 Macro: 0.6431\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.892, F1 Micro: 0.7146, F1 Macro: 0.6785\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.9061, F1 Micro: 0.768, F1 Macro: 0.7618\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9023, F1 Micro: 0.7614, F1 Macro: 0.7544\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9056, F1 Micro: 0.771, F1 Macro: 0.7677\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9044, F1 Micro: 0.7699, F1 Macro: 0.7684\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9058, F1 Micro: 0.7666, F1 Macro: 0.7604\n",
      "Epoch 8/10, Train Loss: 0.0566, Accuracy: 0.9025, F1 Micro: 0.768, F1 Macro: 0.7656\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.9059, F1 Micro: 0.769, F1 Macro: 0.7651\n",
      "Epoch 10/10, Train Loss: 0.0307, Accuracy: 0.9052, F1 Micro: 0.7652, F1 Macro: 0.7632\n",
      "Best result for 4663 samples: F1 Micro: 0.771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.88      0.91       369\n",
      "                sara       0.70      0.63      0.66       262\n",
      "         radikalisme       0.76      0.79      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       478\n",
      "\n",
      "           micro avg       0.79      0.76      0.77      1343\n",
      "           macro avg       0.78      0.75      0.77      1343\n",
      "        weighted avg       0.79      0.76      0.77      1343\n",
      "         samples avg       0.43      0.43      0.42      1343\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001728534698486328 seconds\n",
      "\n",
      "Fold 1 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3948, Accuracy: 0.8763, F1 Micro: 0.7197, F1 Macro: 0.7146\n",
      "Epoch 2/10, Train Loss: 0.2421, Accuracy: 0.8925, F1 Micro: 0.7562, F1 Macro: 0.7557\n",
      "Epoch 3/10, Train Loss: 0.2017, Accuracy: 0.903, F1 Micro: 0.7684, F1 Macro: 0.7634\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9023, F1 Micro: 0.7481, F1 Macro: 0.7405\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.9028, F1 Micro: 0.7703, F1 Macro: 0.7662\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9031, F1 Micro: 0.7717, F1 Macro: 0.7712\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9033, F1 Micro: 0.7732, F1 Macro: 0.7714\n",
      "Epoch 8/10, Train Loss: 0.0489, Accuracy: 0.9002, F1 Micro: 0.7639, F1 Macro: 0.7652\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.9028, F1 Micro: 0.7595, F1 Macro: 0.7537\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9009, F1 Micro: 0.7673, F1 Macro: 0.7648\n",
      "Best result for 4863 samples: F1 Micro: 0.7732\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       369\n",
      "                sara       0.67      0.65      0.66       262\n",
      "         radikalisme       0.76      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       478\n",
      "\n",
      "           micro avg       0.76      0.79      0.77      1343\n",
      "           macro avg       0.76      0.78      0.77      1343\n",
      "        weighted avg       0.77      0.79      0.77      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00032258033752441406 seconds\n",
      "\n",
      "Fold 1 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.387, Accuracy: 0.8842, F1 Micro: 0.703, F1 Macro: 0.6899\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.8964, F1 Micro: 0.7461, F1 Macro: 0.7423\n",
      "Epoch 3/10, Train Loss: 0.1967, Accuracy: 0.903, F1 Micro: 0.7539, F1 Macro: 0.7465\n",
      "Epoch 4/10, Train Loss: 0.1632, Accuracy: 0.9011, F1 Micro: 0.7716, F1 Macro: 0.7688\n",
      "Epoch 5/10, Train Loss: 0.1276, Accuracy: 0.9044, F1 Micro: 0.7703, F1 Macro: 0.7659\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.8989, F1 Micro: 0.7626, F1 Macro: 0.7609\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.8995, F1 Micro: 0.7711, F1 Macro: 0.773\n",
      "Epoch 8/10, Train Loss: 0.0549, Accuracy: 0.9038, F1 Micro: 0.7691, F1 Macro: 0.7666\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9061, F1 Micro: 0.7714, F1 Macro: 0.7659\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9025, F1 Micro: 0.7638, F1 Macro: 0.7624\n",
      "Best result for 5063 samples: F1 Micro: 0.7716\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       369\n",
      "                sara       0.68      0.65      0.66       262\n",
      "         radikalisme       0.72      0.82      0.77       234\n",
      "pencemaran_nama_baik       0.67      0.80      0.73       478\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1343\n",
      "           macro avg       0.76      0.79      0.77      1343\n",
      "        weighted avg       0.76      0.80      0.77      1343\n",
      "         samples avg       0.45      0.45      0.44      1343\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001926422119140625 seconds\n",
      "\n",
      "Fold 1 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3868, Accuracy: 0.8827, F1 Micro: 0.7165, F1 Macro: 0.7044\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.898, F1 Micro: 0.741, F1 Macro: 0.7323\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.8959, F1 Micro: 0.765, F1 Macro: 0.7635\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.902, F1 Micro: 0.7485, F1 Macro: 0.7436\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9017, F1 Micro: 0.7598, F1 Macro: 0.7555\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9081, F1 Micro: 0.7703, F1 Macro: 0.764\n",
      "Epoch 7/10, Train Loss: 0.0665, Accuracy: 0.9052, F1 Micro: 0.7714, F1 Macro: 0.7676\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9034, F1 Micro: 0.761, F1 Macro: 0.7556\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.9039, F1 Micro: 0.7718, F1 Macro: 0.7696\n",
      "Epoch 10/10, Train Loss: 0.0302, Accuracy: 0.9052, F1 Micro: 0.7579, F1 Macro: 0.7493\n",
      "Best result for 5263 samples: F1 Micro: 0.7718\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       369\n",
      "                sara       0.67      0.66      0.67       262\n",
      "         radikalisme       0.76      0.79      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.75      0.73       478\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1343\n",
      "           macro avg       0.77      0.77      0.77      1343\n",
      "        weighted avg       0.77      0.77      0.77      1343\n",
      "         samples avg       0.44      0.44      0.43      1343\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.08148860931396484 seconds\n",
      "\n",
      "Fold 1 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.393, Accuracy: 0.8744, F1 Micro: 0.6309, F1 Macro: 0.581\n",
      "Epoch 2/10, Train Loss: 0.241, Accuracy: 0.9017, F1 Micro: 0.7611, F1 Macro: 0.7557\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9017, F1 Micro: 0.7618, F1 Macro: 0.7607\n",
      "Epoch 4/10, Train Loss: 0.1596, Accuracy: 0.9039, F1 Micro: 0.7472, F1 Macro: 0.7385\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.8995, F1 Micro: 0.7668, F1 Macro: 0.7627\n",
      "Epoch 6/10, Train Loss: 0.0881, Accuracy: 0.9022, F1 Micro: 0.7692, F1 Macro: 0.7645\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9031, F1 Micro: 0.7555, F1 Macro: 0.7421\n",
      "Epoch 8/10, Train Loss: 0.0503, Accuracy: 0.9042, F1 Micro: 0.7668, F1 Macro: 0.7603\n",
      "Epoch 9/10, Train Loss: 0.0369, Accuracy: 0.9006, F1 Micro: 0.7727, F1 Macro: 0.7741\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9, F1 Micro: 0.7735, F1 Macro: 0.7768\n",
      "Best result for 5441 samples: F1 Micro: 0.7735\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       369\n",
      "                sara       0.68      0.67      0.68       262\n",
      "         radikalisme       0.79      0.80      0.80       234\n",
      "pencemaran_nama_baik       0.63      0.83      0.72       478\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1343\n",
      "           macro avg       0.76      0.80      0.78      1343\n",
      "        weighted avg       0.75      0.81      0.78      1343\n",
      "         samples avg       0.47      0.47      0.46      1343\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002276897430419922 seconds\n",
      "\n",
      "Fold 1 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3727, Accuracy: 0.8856, F1 Micro: 0.7206, F1 Macro: 0.7093\n",
      "Epoch 2/10, Train Loss: 0.2431, Accuracy: 0.8955, F1 Micro: 0.7262, F1 Macro: 0.7182\n",
      "Epoch 3/10, Train Loss: 0.1936, Accuracy: 0.9045, F1 Micro: 0.7719, F1 Macro: 0.7679\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.908, F1 Micro: 0.7763, F1 Macro: 0.7705\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9077, F1 Micro: 0.7754, F1 Macro: 0.7709\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.8938, F1 Micro: 0.7655, F1 Macro: 0.7646\n",
      "Epoch 7/10, Train Loss: 0.0623, Accuracy: 0.8995, F1 Micro: 0.7681, F1 Macro: 0.764\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9009, F1 Micro: 0.769, F1 Macro: 0.7691\n",
      "Epoch 9/10, Train Loss: 0.036, Accuracy: 0.9072, F1 Micro: 0.774, F1 Macro: 0.7683\n",
      "Epoch 10/10, Train Loss: 0.0301, Accuracy: 0.9038, F1 Micro: 0.7747, F1 Macro: 0.775\n",
      "Best result for 5641 samples: F1 Micro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       369\n",
      "                sara       0.71      0.60      0.65       262\n",
      "         radikalisme       0.77      0.82      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.72      0.73       478\n",
      "\n",
      "           micro avg       0.79      0.76      0.78      1343\n",
      "           macro avg       0.79      0.76      0.77      1343\n",
      "        weighted avg       0.79      0.76      0.77      1343\n",
      "         samples avg       0.44      0.43      0.42      1343\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00021386146545410156 seconds\n",
      "\n",
      "Fold 1 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3724, Accuracy: 0.882, F1 Micro: 0.6834, F1 Macro: 0.6476\n",
      "Epoch 2/10, Train Loss: 0.2286, Accuracy: 0.8975, F1 Micro: 0.7467, F1 Macro: 0.7308\n",
      "Epoch 3/10, Train Loss: 0.1924, Accuracy: 0.8988, F1 Micro: 0.7705, F1 Macro: 0.7701\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9047, F1 Micro: 0.7548, F1 Macro: 0.7391\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9048, F1 Micro: 0.7734, F1 Macro: 0.7742\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.9016, F1 Micro: 0.7663, F1 Macro: 0.7677\n",
      "Epoch 7/10, Train Loss: 0.0627, Accuracy: 0.9036, F1 Micro: 0.7558, F1 Macro: 0.7492\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.9033, F1 Micro: 0.773, F1 Macro: 0.7667\n",
      "Epoch 9/10, Train Loss: 0.0354, Accuracy: 0.9025, F1 Micro: 0.7765, F1 Macro: 0.7742\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.9048, F1 Micro: 0.7698, F1 Macro: 0.7643\n",
      "Best result for 5841 samples: F1 Micro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.91      0.92       369\n",
      "                sara       0.62      0.72      0.66       262\n",
      "         radikalisme       0.75      0.80      0.78       234\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       478\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1343\n",
      "           macro avg       0.75      0.80      0.77      1343\n",
      "        weighted avg       0.76      0.81      0.78      1343\n",
      "         samples avg       0.45      0.46      0.45      1343\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00020575523376464844 seconds\n",
      "\n",
      "Fold 1 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3712, Accuracy: 0.8838, F1 Micro: 0.7279, F1 Macro: 0.7228\n",
      "Epoch 2/10, Train Loss: 0.2281, Accuracy: 0.8981, F1 Micro: 0.7515, F1 Macro: 0.7481\n",
      "Epoch 3/10, Train Loss: 0.1973, Accuracy: 0.9014, F1 Micro: 0.7389, F1 Macro: 0.7333\n",
      "Epoch 4/10, Train Loss: 0.1584, Accuracy: 0.9091, F1 Micro: 0.7718, F1 Macro: 0.7601\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9019, F1 Micro: 0.7776, F1 Macro: 0.7766\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9059, F1 Micro: 0.777, F1 Macro: 0.7734\n",
      "Epoch 7/10, Train Loss: 0.0625, Accuracy: 0.9013, F1 Micro: 0.7416, F1 Macro: 0.7336\n",
      "Epoch 8/10, Train Loss: 0.0509, Accuracy: 0.9044, F1 Micro: 0.7727, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0362, Accuracy: 0.9031, F1 Micro: 0.7669, F1 Macro: 0.7646\n",
      "Epoch 10/10, Train Loss: 0.0283, Accuracy: 0.9034, F1 Micro: 0.7663, F1 Macro: 0.7621\n",
      "Best result for 6041 samples: F1 Micro: 0.7776\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       369\n",
      "                sara       0.64      0.74      0.69       262\n",
      "         radikalisme       0.78      0.76      0.77       234\n",
      "pencemaran_nama_baik       0.67      0.81      0.73       478\n",
      "\n",
      "           micro avg       0.74      0.82      0.78      1343\n",
      "           macro avg       0.75      0.81      0.78      1343\n",
      "        weighted avg       0.75      0.82      0.78      1343\n",
      "         samples avg       0.46      0.46      0.45      1343\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 0.0933382511138916 seconds\n",
      "\n",
      "Fold 1 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 1 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3657, Accuracy: 0.8855, F1 Micro: 0.7218, F1 Macro: 0.711\n",
      "Epoch 2/10, Train Loss: 0.2354, Accuracy: 0.8984, F1 Micro: 0.7713, F1 Macro: 0.7691\n",
      "Epoch 3/10, Train Loss: 0.1906, Accuracy: 0.9058, F1 Micro: 0.78, F1 Macro: 0.7772\n",
      "Epoch 4/10, Train Loss: 0.1575, Accuracy: 0.9016, F1 Micro: 0.7581, F1 Macro: 0.7517\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9047, F1 Micro: 0.7619, F1 Macro: 0.7608\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.9045, F1 Micro: 0.7683, F1 Macro: 0.7615\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9064, F1 Micro: 0.7674, F1 Macro: 0.7592\n",
      "Epoch 8/10, Train Loss: 0.0478, Accuracy: 0.9044, F1 Micro: 0.7673, F1 Macro: 0.7636\n",
      "Epoch 9/10, Train Loss: 0.0353, Accuracy: 0.9038, F1 Micro: 0.7719, F1 Macro: 0.7685\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9053, F1 Micro: 0.7664, F1 Macro: 0.76\n",
      "Best result for 6218 samples: F1 Micro: 0.78\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       369\n",
      "                sara       0.67      0.68      0.68       262\n",
      "         radikalisme       0.74      0.84      0.78       234\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       478\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1343\n",
      "           macro avg       0.76      0.79      0.78      1343\n",
      "        weighted avg       0.77      0.80      0.78      1343\n",
      "         samples avg       0.44      0.45      0.43      1343\n",
      "\n",
      "\n",
      "FOLD 1 COMPLETED in 3616.18 seconds\n",
      "===============================================\n",
      "STARTING FOLD 2/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.7841, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4954, Accuracy: 0.7953, F1 Micro: 0.099, F1 Macro: 0.08\n",
      "Epoch 3/10, Train Loss: 0.448, Accuracy: 0.8238, F1 Micro: 0.3229, F1 Macro: 0.2108\n",
      "Epoch 4/10, Train Loss: 0.3886, Accuracy: 0.8323, F1 Micro: 0.3975, F1 Macro: 0.3047\n",
      "Epoch 5/10, Train Loss: 0.3358, Accuracy: 0.8384, F1 Micro: 0.4615, F1 Macro: 0.3692\n",
      "Epoch 6/10, Train Loss: 0.2851, Accuracy: 0.8464, F1 Micro: 0.5131, F1 Macro: 0.4513\n",
      "Epoch 7/10, Train Loss: 0.2509, Accuracy: 0.8661, F1 Micro: 0.6355, F1 Macro: 0.5946\n",
      "Epoch 8/10, Train Loss: 0.2376, Accuracy: 0.872, F1 Micro: 0.6667, F1 Macro: 0.6441\n",
      "Epoch 9/10, Train Loss: 0.188, Accuracy: 0.8628, F1 Micro: 0.6132, F1 Macro: 0.5838\n",
      "Epoch 10/10, Train Loss: 0.1613, Accuracy: 0.8709, F1 Micro: 0.6541, F1 Macro: 0.628\n",
      "Best result for 388 samples: F1 Micro: 0.6667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.80      0.86       378\n",
      "                sara       0.62      0.35      0.45       253\n",
      "         radikalisme       0.67      0.62      0.65       234\n",
      "pencemaran_nama_baik       0.72      0.54      0.62       517\n",
      "\n",
      "           micro avg       0.76      0.59      0.67      1382\n",
      "           macro avg       0.74      0.58      0.64      1382\n",
      "        weighted avg       0.75      0.59      0.66      1382\n",
      "         samples avg       0.38      0.35      0.35      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 0.0003695487976074219 seconds\n",
      "\n",
      "Fold 2 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5189, Accuracy: 0.8003, F1 Micro: 0.1434, F1 Macro: 0.1106\n",
      "Epoch 2/10, Train Loss: 0.4055, Accuracy: 0.8308, F1 Micro: 0.3933, F1 Macro: 0.2768\n",
      "Epoch 3/10, Train Loss: 0.3361, Accuracy: 0.8658, F1 Micro: 0.6108, F1 Macro: 0.5349\n",
      "Epoch 4/10, Train Loss: 0.2707, Accuracy: 0.8838, F1 Micro: 0.7085, F1 Macro: 0.6911\n",
      "Epoch 5/10, Train Loss: 0.2257, Accuracy: 0.8895, F1 Micro: 0.725, F1 Macro: 0.7076\n",
      "Epoch 6/10, Train Loss: 0.1835, Accuracy: 0.8883, F1 Micro: 0.7331, F1 Macro: 0.7218\n",
      "Epoch 7/10, Train Loss: 0.13, Accuracy: 0.8886, F1 Micro: 0.7467, F1 Macro: 0.7377\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.887, F1 Micro: 0.7439, F1 Macro: 0.7312\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.888, F1 Micro: 0.7539, F1 Macro: 0.7436\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.8858, F1 Micro: 0.7288, F1 Macro: 0.7195\n",
      "Best result for 971 samples: F1 Micro: 0.7539\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       378\n",
      "                sara       0.59      0.68      0.64       253\n",
      "         radikalisme       0.68      0.74      0.71       234\n",
      "pencemaran_nama_baik       0.68      0.81      0.74       517\n",
      "\n",
      "           micro avg       0.72      0.79      0.75      1382\n",
      "           macro avg       0.71      0.78      0.74      1382\n",
      "        weighted avg       0.73      0.79      0.76      1382\n",
      "         samples avg       0.45      0.45      0.44      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0004146099090576172 seconds\n",
      "\n",
      "Fold 2 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4823, Accuracy: 0.833, F1 Micro: 0.411, F1 Macro: 0.2879\n",
      "Epoch 2/10, Train Loss: 0.3448, Accuracy: 0.8653, F1 Micro: 0.6049, F1 Macro: 0.5506\n",
      "Epoch 3/10, Train Loss: 0.2698, Accuracy: 0.8861, F1 Micro: 0.7436, F1 Macro: 0.7391\n",
      "Epoch 4/10, Train Loss: 0.2225, Accuracy: 0.8933, F1 Micro: 0.7416, F1 Macro: 0.7294\n",
      "Epoch 5/10, Train Loss: 0.1792, Accuracy: 0.8903, F1 Micro: 0.7518, F1 Macro: 0.7467\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.8902, F1 Micro: 0.7494, F1 Macro: 0.7446\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.8873, F1 Micro: 0.7576, F1 Macro: 0.75\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.8986, F1 Micro: 0.7651, F1 Macro: 0.7495\n",
      "Epoch 9/10, Train Loss: 0.0621, Accuracy: 0.8963, F1 Micro: 0.7605, F1 Macro: 0.7475\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.8959, F1 Micro: 0.7635, F1 Macro: 0.7557\n",
      "Best result for 1496 samples: F1 Micro: 0.7651\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       378\n",
      "                sara       0.64      0.57      0.60       253\n",
      "         radikalisme       0.69      0.79      0.74       234\n",
      "pencemaran_nama_baik       0.75      0.77      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.76      0.77      1382\n",
      "           macro avg       0.75      0.75      0.75      1382\n",
      "        weighted avg       0.77      0.76      0.76      1382\n",
      "         samples avg       0.45      0.44      0.43      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0004279613494873047 seconds\n",
      "\n",
      "Fold 2 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4645, Accuracy: 0.8291, F1 Micro: 0.372, F1 Macro: 0.2428\n",
      "Epoch 2/10, Train Loss: 0.3226, Accuracy: 0.8792, F1 Micro: 0.6692, F1 Macro: 0.6409\n",
      "Epoch 3/10, Train Loss: 0.2445, Accuracy: 0.8941, F1 Micro: 0.7301, F1 Macro: 0.7032\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.8975, F1 Micro: 0.7714, F1 Macro: 0.7644\n",
      "Epoch 5/10, Train Loss: 0.1655, Accuracy: 0.8966, F1 Micro: 0.7479, F1 Macro: 0.7387\n",
      "Epoch 6/10, Train Loss: 0.1166, Accuracy: 0.8959, F1 Micro: 0.7557, F1 Macro: 0.7401\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.8936, F1 Micro: 0.7353, F1 Macro: 0.7113\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.8994, F1 Micro: 0.7742, F1 Macro: 0.7668\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.8988, F1 Micro: 0.7607, F1 Macro: 0.745\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.897, F1 Micro: 0.7542, F1 Macro: 0.7474\n",
      "Best result for 1969 samples: F1 Micro: 0.7742\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       378\n",
      "                sara       0.64      0.70      0.67       253\n",
      "         radikalisme       0.70      0.81      0.75       234\n",
      "pencemaran_nama_baik       0.73      0.78      0.75       517\n",
      "\n",
      "           micro avg       0.75      0.80      0.77      1382\n",
      "           macro avg       0.74      0.79      0.77      1382\n",
      "        weighted avg       0.76      0.80      0.78      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 0.0003299713134765625 seconds\n",
      "\n",
      "Fold 2 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4553, Accuracy: 0.8502, F1 Micro: 0.5347, F1 Macro: 0.4381\n",
      "Epoch 2/10, Train Loss: 0.2977, Accuracy: 0.8894, F1 Micro: 0.7134, F1 Macro: 0.6829\n",
      "Epoch 3/10, Train Loss: 0.2307, Accuracy: 0.8889, F1 Micro: 0.7006, F1 Macro: 0.6802\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.8994, F1 Micro: 0.7708, F1 Macro: 0.7616\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.8927, F1 Micro: 0.7305, F1 Macro: 0.7202\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.8981, F1 Micro: 0.7665, F1 Macro: 0.7549\n",
      "Epoch 7/10, Train Loss: 0.0895, Accuracy: 0.9025, F1 Micro: 0.7839, F1 Macro: 0.7762\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.8994, F1 Micro: 0.7651, F1 Macro: 0.7568\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.8963, F1 Micro: 0.7481, F1 Macro: 0.7316\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9011, F1 Micro: 0.774, F1 Macro: 0.7681\n",
      "Best result for 2394 samples: F1 Micro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.91       378\n",
      "                sara       0.63      0.73      0.67       253\n",
      "         radikalisme       0.70      0.82      0.76       234\n",
      "pencemaran_nama_baik       0.73      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1382\n",
      "           macro avg       0.75      0.81      0.78      1382\n",
      "        weighted avg       0.76      0.82      0.79      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 0.00038743019104003906 seconds\n",
      "\n",
      "Fold 2 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.432, Accuracy: 0.8566, F1 Micro: 0.5637, F1 Macro: 0.4871\n",
      "Epoch 2/10, Train Loss: 0.2795, Accuracy: 0.8964, F1 Micro: 0.7482, F1 Macro: 0.7276\n",
      "Epoch 3/10, Train Loss: 0.2223, Accuracy: 0.9022, F1 Micro: 0.7742, F1 Macro: 0.7615\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.8981, F1 Micro: 0.7485, F1 Macro: 0.7273\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.9038, F1 Micro: 0.777, F1 Macro: 0.7622\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9045, F1 Micro: 0.7728, F1 Macro: 0.7534\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9075, F1 Micro: 0.788, F1 Macro: 0.7839\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.905, F1 Micro: 0.7858, F1 Macro: 0.7797\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.8988, F1 Micro: 0.7486, F1 Macro: 0.7336\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.8963, F1 Micro: 0.7342, F1 Macro: 0.7156\n",
      "Best result for 2777 samples: F1 Micro: 0.788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.87      0.91       378\n",
      "                sara       0.67      0.73      0.70       253\n",
      "         radikalisme       0.71      0.83      0.77       234\n",
      "pencemaran_nama_baik       0.76      0.75      0.76       517\n",
      "\n",
      "           micro avg       0.78      0.80      0.79      1382\n",
      "           macro avg       0.77      0.80      0.78      1382\n",
      "        weighted avg       0.79      0.80      0.79      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 0.00033855438232421875 seconds\n",
      "\n",
      "Fold 2 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4251, Accuracy: 0.8522, F1 Micro: 0.5159, F1 Macro: 0.4093\n",
      "Epoch 2/10, Train Loss: 0.2691, Accuracy: 0.8969, F1 Micro: 0.7483, F1 Macro: 0.7356\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9017, F1 Micro: 0.7671, F1 Macro: 0.7589\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9042, F1 Micro: 0.767, F1 Macro: 0.7585\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.8953, F1 Micro: 0.727, F1 Macro: 0.7125\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9017, F1 Micro: 0.7769, F1 Macro: 0.7635\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9031, F1 Micro: 0.7742, F1 Macro: 0.7629\n",
      "Epoch 8/10, Train Loss: 0.0578, Accuracy: 0.9048, F1 Micro: 0.7797, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9052, F1 Micro: 0.7863, F1 Macro: 0.7778\n",
      "Epoch 10/10, Train Loss: 0.0336, Accuracy: 0.9056, F1 Micro: 0.7893, F1 Macro: 0.7805\n",
      "Best result for 3122 samples: F1 Micro: 0.7893\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       378\n",
      "                sara       0.64      0.68      0.66       253\n",
      "         radikalisme       0.72      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.73      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.82      0.79      1382\n",
      "           macro avg       0.76      0.81      0.78      1382\n",
      "        weighted avg       0.77      0.82      0.79      1382\n",
      "         samples avg       0.48      0.47      0.46      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 0.00026988983154296875 seconds\n",
      "\n",
      "Fold 2 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4152, Accuracy: 0.867, F1 Micro: 0.6105, F1 Macro: 0.5817\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.8984, F1 Micro: 0.7621, F1 Macro: 0.7498\n",
      "Epoch 3/10, Train Loss: 0.2161, Accuracy: 0.8989, F1 Micro: 0.744, F1 Macro: 0.7158\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9084, F1 Micro: 0.7934, F1 Macro: 0.7876\n",
      "Epoch 5/10, Train Loss: 0.1266, Accuracy: 0.9042, F1 Micro: 0.7684, F1 Macro: 0.7576\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.9095, F1 Micro: 0.7901, F1 Macro: 0.782\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9013, F1 Micro: 0.7591, F1 Macro: 0.7519\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9055, F1 Micro: 0.789, F1 Macro: 0.7838\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9073, F1 Micro: 0.7937, F1 Macro: 0.7866\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.9061, F1 Micro: 0.7747, F1 Macro: 0.7614\n",
      "Best result for 3432 samples: F1 Micro: 0.7937\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       378\n",
      "                sara       0.65      0.72      0.68       253\n",
      "         radikalisme       0.70      0.86      0.77       234\n",
      "pencemaran_nama_baik       0.74      0.81      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1382\n",
      "           macro avg       0.76      0.82      0.79      1382\n",
      "        weighted avg       0.77      0.83      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 0.00020956993103027344 seconds\n",
      "\n",
      "Fold 2 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4046, Accuracy: 0.8816, F1 Micro: 0.7135, F1 Macro: 0.7059\n",
      "Epoch 2/10, Train Loss: 0.2573, Accuracy: 0.8931, F1 Micro: 0.7305, F1 Macro: 0.7176\n",
      "Epoch 3/10, Train Loss: 0.2125, Accuracy: 0.8959, F1 Micro: 0.7334, F1 Macro: 0.7117\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.9059, F1 Micro: 0.7923, F1 Macro: 0.7824\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9027, F1 Micro: 0.7836, F1 Macro: 0.7777\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9027, F1 Micro: 0.7592, F1 Macro: 0.7544\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9059, F1 Micro: 0.7889, F1 Macro: 0.7802\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9053, F1 Micro: 0.774, F1 Macro: 0.7623\n",
      "Epoch 9/10, Train Loss: 0.0408, Accuracy: 0.9033, F1 Micro: 0.7723, F1 Macro: 0.7613\n",
      "Epoch 10/10, Train Loss: 0.0296, Accuracy: 0.9062, F1 Micro: 0.7815, F1 Macro: 0.7721\n",
      "Best result for 3711 samples: F1 Micro: 0.7923\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       378\n",
      "                sara       0.67      0.70      0.69       253\n",
      "         radikalisme       0.72      0.79      0.75       234\n",
      "pencemaran_nama_baik       0.71      0.86      0.78       517\n",
      "\n",
      "           micro avg       0.76      0.83      0.79      1382\n",
      "           macro avg       0.76      0.81      0.78      1382\n",
      "        weighted avg       0.76      0.83      0.79      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 0.05903220176696777 seconds\n",
      "\n",
      "Fold 2 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4107, Accuracy: 0.8841, F1 Micro: 0.7003, F1 Macro: 0.6749\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.8984, F1 Micro: 0.7449, F1 Macro: 0.7305\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9005, F1 Micro: 0.7495, F1 Macro: 0.7492\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9056, F1 Micro: 0.7728, F1 Macro: 0.7564\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.8991, F1 Micro: 0.7492, F1 Macro: 0.7352\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9042, F1 Micro: 0.7707, F1 Macro: 0.7602\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9014, F1 Micro: 0.7616, F1 Macro: 0.7421\n",
      "Epoch 8/10, Train Loss: 0.0469, Accuracy: 0.9059, F1 Micro: 0.7841, F1 Macro: 0.7777\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9062, F1 Micro: 0.7818, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9036, F1 Micro: 0.7732, F1 Macro: 0.7623\n",
      "Best result for 3886 samples: F1 Micro: 0.7841\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       378\n",
      "                sara       0.68      0.69      0.68       253\n",
      "         radikalisme       0.70      0.85      0.77       234\n",
      "pencemaran_nama_baik       0.77      0.75      0.76       517\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1382\n",
      "           macro avg       0.77      0.79      0.78      1382\n",
      "        weighted avg       0.78      0.79      0.79      1382\n",
      "         samples avg       0.46      0.45      0.45      1382\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 0.00018024444580078125 seconds\n",
      "\n",
      "Fold 2 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3878, Accuracy: 0.8863, F1 Micro: 0.7372, F1 Macro: 0.727\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.893, F1 Micro: 0.7104, F1 Macro: 0.6796\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9017, F1 Micro: 0.7544, F1 Macro: 0.7427\n",
      "Epoch 4/10, Train Loss: 0.1653, Accuracy: 0.9102, F1 Micro: 0.7952, F1 Macro: 0.79\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9058, F1 Micro: 0.7869, F1 Macro: 0.7832\n",
      "Epoch 6/10, Train Loss: 0.0887, Accuracy: 0.9072, F1 Micro: 0.7758, F1 Macro: 0.7631\n",
      "Epoch 7/10, Train Loss: 0.065, Accuracy: 0.9067, F1 Micro: 0.7978, F1 Macro: 0.7922\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9087, F1 Micro: 0.7856, F1 Macro: 0.7735\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.9069, F1 Micro: 0.7818, F1 Macro: 0.7743\n",
      "Epoch 10/10, Train Loss: 0.0335, Accuracy: 0.9055, F1 Micro: 0.7747, F1 Macro: 0.7663\n",
      "Best result for 4120 samples: F1 Micro: 0.7978\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.91      0.92       378\n",
      "                sara       0.64      0.75      0.69       253\n",
      "         radikalisme       0.69      0.89      0.78       234\n",
      "pencemaran_nama_baik       0.72      0.84      0.78       517\n",
      "\n",
      "           micro avg       0.75      0.85      0.80      1382\n",
      "           macro avg       0.75      0.85      0.79      1382\n",
      "        weighted avg       0.76      0.85      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 0.00019979476928710938 seconds\n",
      "\n",
      "Fold 2 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3905, Accuracy: 0.8792, F1 Micro: 0.6723, F1 Macro: 0.6616\n",
      "Epoch 2/10, Train Loss: 0.2505, Accuracy: 0.9011, F1 Micro: 0.7787, F1 Macro: 0.7772\n",
      "Epoch 3/10, Train Loss: 0.2056, Accuracy: 0.9053, F1 Micro: 0.7627, F1 Macro: 0.7504\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.9025, F1 Micro: 0.776, F1 Macro: 0.7731\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.9055, F1 Micro: 0.7669, F1 Macro: 0.7504\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9092, F1 Micro: 0.7961, F1 Macro: 0.7845\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.9041, F1 Micro: 0.7699, F1 Macro: 0.7649\n",
      "Epoch 8/10, Train Loss: 0.0525, Accuracy: 0.9059, F1 Micro: 0.7861, F1 Macro: 0.781\n",
      "Epoch 9/10, Train Loss: 0.0411, Accuracy: 0.908, F1 Micro: 0.7853, F1 Macro: 0.7735\n",
      "Epoch 10/10, Train Loss: 0.0361, Accuracy: 0.9052, F1 Micro: 0.7802, F1 Macro: 0.7754\n",
      "Best result for 4330 samples: F1 Micro: 0.7961\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.92      0.92       378\n",
      "                sara       0.69      0.65      0.67       253\n",
      "         radikalisme       0.72      0.82      0.77       234\n",
      "pencemaran_nama_baik       0.73      0.83      0.78       517\n",
      "\n",
      "           micro avg       0.77      0.82      0.80      1382\n",
      "           macro avg       0.77      0.81      0.78      1382\n",
      "        weighted avg       0.78      0.82      0.80      1382\n",
      "         samples avg       0.48      0.47      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002014636993408203 seconds\n",
      "\n",
      "Fold 2 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3885, Accuracy: 0.8872, F1 Micro: 0.7229, F1 Macro: 0.7112\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.9031, F1 Micro: 0.7591, F1 Macro: 0.7426\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9038, F1 Micro: 0.7658, F1 Macro: 0.7613\n",
      "Epoch 4/10, Train Loss: 0.1684, Accuracy: 0.903, F1 Micro: 0.7596, F1 Macro: 0.7432\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9083, F1 Micro: 0.7932, F1 Macro: 0.7877\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9069, F1 Micro: 0.7893, F1 Macro: 0.7831\n",
      "Epoch 7/10, Train Loss: 0.0691, Accuracy: 0.9062, F1 Micro: 0.7722, F1 Macro: 0.7641\n",
      "Epoch 8/10, Train Loss: 0.054, Accuracy: 0.9092, F1 Micro: 0.786, F1 Macro: 0.7795\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9097, F1 Micro: 0.7843, F1 Macro: 0.7781\n",
      "Epoch 10/10, Train Loss: 0.03, Accuracy: 0.91, F1 Micro: 0.7927, F1 Macro: 0.7846\n",
      "Best result for 4530 samples: F1 Micro: 0.7932\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.91       378\n",
      "                sara       0.69      0.69      0.69       253\n",
      "         radikalisme       0.74      0.84      0.78       234\n",
      "pencemaran_nama_baik       0.73      0.80      0.76       517\n",
      "\n",
      "           micro avg       0.77      0.81      0.79      1382\n",
      "           macro avg       0.77      0.81      0.79      1382\n",
      "        weighted avg       0.78      0.81      0.79      1382\n",
      "         samples avg       0.47      0.46      0.46      1382\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 0.0717470645904541 seconds\n",
      "\n",
      "Fold 2 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.8852, F1 Micro: 0.6929, F1 Macro: 0.6676\n",
      "Epoch 2/10, Train Loss: 0.2529, Accuracy: 0.8988, F1 Micro: 0.7797, F1 Macro: 0.7776\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.9025, F1 Micro: 0.7615, F1 Macro: 0.7446\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9108, F1 Micro: 0.7915, F1 Macro: 0.7851\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9103, F1 Micro: 0.7931, F1 Macro: 0.7848\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9106, F1 Micro: 0.7878, F1 Macro: 0.7806\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9092, F1 Micro: 0.7903, F1 Macro: 0.7812\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9069, F1 Micro: 0.795, F1 Macro: 0.7883\n",
      "Epoch 9/10, Train Loss: 0.0381, Accuracy: 0.9086, F1 Micro: 0.7853, F1 Macro: 0.7743\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9089, F1 Micro: 0.789, F1 Macro: 0.7815\n",
      "Best result for 4663 samples: F1 Micro: 0.795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.93      0.93       378\n",
      "                sara       0.65      0.74      0.69       253\n",
      "         radikalisme       0.70      0.85      0.77       234\n",
      "pencemaran_nama_baik       0.73      0.80      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.75      0.83      0.79      1382\n",
      "        weighted avg       0.76      0.84      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001571178436279297 seconds\n",
      "\n",
      "Fold 2 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3754, Accuracy: 0.883, F1 Micro: 0.6988, F1 Macro: 0.6672\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.9028, F1 Micro: 0.763, F1 Macro: 0.7411\n",
      "Epoch 3/10, Train Loss: 0.2047, Accuracy: 0.9097, F1 Micro: 0.7963, F1 Macro: 0.7891\n",
      "Epoch 4/10, Train Loss: 0.161, Accuracy: 0.905, F1 Micro: 0.7946, F1 Macro: 0.7884\n",
      "Epoch 5/10, Train Loss: 0.12, Accuracy: 0.9052, F1 Micro: 0.7663, F1 Macro: 0.7524\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.908, F1 Micro: 0.7986, F1 Macro: 0.793\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9091, F1 Micro: 0.7975, F1 Macro: 0.7942\n",
      "Epoch 8/10, Train Loss: 0.0518, Accuracy: 0.9052, F1 Micro: 0.7606, F1 Macro: 0.7451\n",
      "Epoch 9/10, Train Loss: 0.041, Accuracy: 0.9095, F1 Micro: 0.7877, F1 Macro: 0.7808\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.9077, F1 Micro: 0.7815, F1 Macro: 0.7696\n",
      "Best result for 4863 samples: F1 Micro: 0.7986\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.94      0.93       378\n",
      "                sara       0.65      0.77      0.71       253\n",
      "         radikalisme       0.73      0.81      0.77       234\n",
      "pencemaran_nama_baik       0.71      0.83      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.85      0.80      1382\n",
      "           macro avg       0.75      0.84      0.79      1382\n",
      "        weighted avg       0.76      0.85      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017189979553222656 seconds\n",
      "\n",
      "Fold 2 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.378, Accuracy: 0.8898, F1 Micro: 0.7239, F1 Macro: 0.7146\n",
      "Epoch 2/10, Train Loss: 0.2463, Accuracy: 0.9048, F1 Micro: 0.7727, F1 Macro: 0.7669\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.9086, F1 Micro: 0.7792, F1 Macro: 0.7681\n",
      "Epoch 4/10, Train Loss: 0.1584, Accuracy: 0.9059, F1 Micro: 0.774, F1 Macro: 0.7596\n",
      "Epoch 5/10, Train Loss: 0.1175, Accuracy: 0.9109, F1 Micro: 0.7939, F1 Macro: 0.787\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9075, F1 Micro: 0.7811, F1 Macro: 0.7766\n",
      "Epoch 7/10, Train Loss: 0.0611, Accuracy: 0.9084, F1 Micro: 0.7996, F1 Macro: 0.7941\n",
      "Epoch 8/10, Train Loss: 0.0483, Accuracy: 0.9083, F1 Micro: 0.7915, F1 Macro: 0.781\n",
      "Epoch 9/10, Train Loss: 0.0368, Accuracy: 0.9089, F1 Micro: 0.8, F1 Macro: 0.7967\n",
      "Epoch 10/10, Train Loss: 0.0319, Accuracy: 0.9061, F1 Micro: 0.7842, F1 Macro: 0.7782\n",
      "Best result for 5063 samples: F1 Micro: 0.8\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.93       378\n",
      "                sara       0.68      0.75      0.71       253\n",
      "         radikalisme       0.73      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.71      0.83      0.77       517\n",
      "\n",
      "           micro avg       0.76      0.84      0.80      1382\n",
      "           macro avg       0.76      0.84      0.80      1382\n",
      "        weighted avg       0.77      0.84      0.80      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001876354217529297 seconds\n",
      "\n",
      "Fold 2 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3798, Accuracy: 0.8922, F1 Micro: 0.7609, F1 Macro: 0.7576\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9042, F1 Micro: 0.7818, F1 Macro: 0.7802\n",
      "Epoch 3/10, Train Loss: 0.1961, Accuracy: 0.9116, F1 Micro: 0.7921, F1 Macro: 0.7862\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.9117, F1 Micro: 0.7981, F1 Macro: 0.7966\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9094, F1 Micro: 0.7841, F1 Macro: 0.7752\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.9098, F1 Micro: 0.7833, F1 Macro: 0.7736\n",
      "Epoch 7/10, Train Loss: 0.0622, Accuracy: 0.9069, F1 Micro: 0.7751, F1 Macro: 0.7638\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.9089, F1 Micro: 0.7967, F1 Macro: 0.7931\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9078, F1 Micro: 0.7789, F1 Macro: 0.7725\n",
      "Epoch 10/10, Train Loss: 0.0283, Accuracy: 0.908, F1 Micro: 0.7854, F1 Macro: 0.7784\n",
      "Best result for 5263 samples: F1 Micro: 0.7981\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.96      0.89      0.92       378\n",
      "                sara       0.69      0.77      0.73       253\n",
      "         radikalisme       0.72      0.85      0.78       234\n",
      "pencemaran_nama_baik       0.76      0.75      0.76       517\n",
      "\n",
      "           micro avg       0.79      0.81      0.80      1382\n",
      "           macro avg       0.78      0.82      0.80      1382\n",
      "        weighted avg       0.80      0.81      0.80      1382\n",
      "         samples avg       0.47      0.46      0.45      1382\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.09009909629821777 seconds\n",
      "\n",
      "Fold 2 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3691, Accuracy: 0.8942, F1 Micro: 0.7592, F1 Macro: 0.7514\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.9058, F1 Micro: 0.7813, F1 Macro: 0.7731\n",
      "Epoch 3/10, Train Loss: 0.2024, Accuracy: 0.9111, F1 Micro: 0.8076, F1 Macro: 0.8042\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.9078, F1 Micro: 0.7795, F1 Macro: 0.7565\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.9103, F1 Micro: 0.7887, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.0845, Accuracy: 0.9073, F1 Micro: 0.7731, F1 Macro: 0.7612\n",
      "Epoch 7/10, Train Loss: 0.063, Accuracy: 0.9109, F1 Micro: 0.7856, F1 Macro: 0.7773\n",
      "Epoch 8/10, Train Loss: 0.0461, Accuracy: 0.908, F1 Micro: 0.7798, F1 Macro: 0.7686\n",
      "Epoch 9/10, Train Loss: 0.0392, Accuracy: 0.9103, F1 Micro: 0.7992, F1 Macro: 0.7937\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9094, F1 Micro: 0.7875, F1 Macro: 0.7815\n",
      "Best result for 5441 samples: F1 Micro: 0.8076\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.90      0.92       378\n",
      "                sara       0.65      0.81      0.72       253\n",
      "         radikalisme       0.72      0.87      0.79       234\n",
      "pencemaran_nama_baik       0.73      0.85      0.79       517\n",
      "\n",
      "           micro avg       0.76      0.86      0.81      1382\n",
      "           macro avg       0.76      0.86      0.80      1382\n",
      "        weighted avg       0.77      0.86      0.81      1382\n",
      "         samples avg       0.49      0.49      0.48      1382\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001952648162841797 seconds\n",
      "\n",
      "Fold 2 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3523, Accuracy: 0.8927, F1 Micro: 0.7373, F1 Macro: 0.716\n",
      "Epoch 2/10, Train Loss: 0.228, Accuracy: 0.9052, F1 Micro: 0.7749, F1 Macro: 0.7659\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9108, F1 Micro: 0.7948, F1 Macro: 0.7915\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.9102, F1 Micro: 0.801, F1 Macro: 0.7957\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9139, F1 Micro: 0.8004, F1 Macro: 0.7929\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9075, F1 Micro: 0.7944, F1 Macro: 0.7865\n",
      "Epoch 7/10, Train Loss: 0.0616, Accuracy: 0.9087, F1 Micro: 0.7873, F1 Macro: 0.7841\n",
      "Epoch 8/10, Train Loss: 0.0469, Accuracy: 0.9127, F1 Micro: 0.8046, F1 Macro: 0.8016\n",
      "Epoch 9/10, Train Loss: 0.0377, Accuracy: 0.9106, F1 Micro: 0.7984, F1 Macro: 0.7923\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9091, F1 Micro: 0.7902, F1 Macro: 0.7824\n",
      "Best result for 5641 samples: F1 Micro: 0.8046\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       378\n",
      "                sara       0.66      0.80      0.72       253\n",
      "         radikalisme       0.74      0.83      0.78       234\n",
      "pencemaran_nama_baik       0.75      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.78      0.83      0.80      1382\n",
      "           macro avg       0.77      0.83      0.80      1382\n",
      "        weighted avg       0.79      0.83      0.81      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019693374633789062 seconds\n",
      "\n",
      "Fold 2 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3616, Accuracy: 0.8864, F1 Micro: 0.6913, F1 Macro: 0.6741\n",
      "Epoch 2/10, Train Loss: 0.2398, Accuracy: 0.9083, F1 Micro: 0.7812, F1 Macro: 0.7752\n",
      "Epoch 3/10, Train Loss: 0.1972, Accuracy: 0.9095, F1 Micro: 0.7814, F1 Macro: 0.7741\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9092, F1 Micro: 0.7973, F1 Macro: 0.7939\n",
      "Epoch 5/10, Train Loss: 0.1186, Accuracy: 0.9075, F1 Micro: 0.7704, F1 Macro: 0.7483\n",
      "Epoch 6/10, Train Loss: 0.0829, Accuracy: 0.9067, F1 Micro: 0.7819, F1 Macro: 0.7787\n",
      "Epoch 7/10, Train Loss: 0.0633, Accuracy: 0.9114, F1 Micro: 0.804, F1 Macro: 0.799\n",
      "Epoch 8/10, Train Loss: 0.0469, Accuracy: 0.9112, F1 Micro: 0.7987, F1 Macro: 0.7928\n",
      "Epoch 9/10, Train Loss: 0.0373, Accuracy: 0.9075, F1 Micro: 0.7815, F1 Macro: 0.7793\n",
      "Epoch 10/10, Train Loss: 0.0284, Accuracy: 0.9102, F1 Micro: 0.7934, F1 Macro: 0.7886\n",
      "Best result for 5841 samples: F1 Micro: 0.804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.91      0.93       378\n",
      "                sara       0.69      0.73      0.71       253\n",
      "         radikalisme       0.72      0.86      0.79       234\n",
      "pencemaran_nama_baik       0.72      0.84      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.84      0.80      1382\n",
      "           macro avg       0.77      0.83      0.80      1382\n",
      "        weighted avg       0.78      0.84      0.81      1382\n",
      "         samples avg       0.48      0.48      0.47      1382\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001678466796875 seconds\n",
      "\n",
      "Fold 2 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3597, Accuracy: 0.8945, F1 Micro: 0.7266, F1 Macro: 0.7135\n",
      "Epoch 2/10, Train Loss: 0.2425, Accuracy: 0.9072, F1 Micro: 0.79, F1 Macro: 0.7859\n",
      "Epoch 3/10, Train Loss: 0.2096, Accuracy: 0.9056, F1 Micro: 0.7599, F1 Macro: 0.7351\n",
      "Epoch 4/10, Train Loss: 0.1578, Accuracy: 0.9112, F1 Micro: 0.7944, F1 Macro: 0.786\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9103, F1 Micro: 0.7976, F1 Macro: 0.7925\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9061, F1 Micro: 0.7799, F1 Macro: 0.7765\n",
      "Epoch 7/10, Train Loss: 0.065, Accuracy: 0.9112, F1 Micro: 0.7916, F1 Macro: 0.7844\n",
      "Epoch 8/10, Train Loss: 0.0467, Accuracy: 0.9106, F1 Micro: 0.8008, F1 Macro: 0.7971\n",
      "Epoch 9/10, Train Loss: 0.0366, Accuracy: 0.9086, F1 Micro: 0.7863, F1 Macro: 0.7812\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.9095, F1 Micro: 0.7931, F1 Macro: 0.7889\n",
      "Best result for 6041 samples: F1 Micro: 0.8008\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.93      0.93       378\n",
      "                sara       0.64      0.77      0.70       253\n",
      "         radikalisme       0.74      0.85      0.79       234\n",
      "pencemaran_nama_baik       0.74      0.79      0.77       517\n",
      "\n",
      "           micro avg       0.77      0.83      0.80      1382\n",
      "           macro avg       0.77      0.83      0.80      1382\n",
      "        weighted avg       0.78      0.83      0.80      1382\n",
      "         samples avg       0.47      0.47      0.46      1382\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 0.10838961601257324 seconds\n",
      "\n",
      "Fold 2 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 2 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3603, Accuracy: 0.8961, F1 Micro: 0.7461, F1 Macro: 0.7351\n",
      "Epoch 2/10, Train Loss: 0.2362, Accuracy: 0.908, F1 Micro: 0.7972, F1 Macro: 0.7899\n",
      "Epoch 3/10, Train Loss: 0.1926, Accuracy: 0.9073, F1 Micro: 0.7984, F1 Macro: 0.793\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.91, F1 Micro: 0.7868, F1 Macro: 0.7764\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9122, F1 Micro: 0.7965, F1 Macro: 0.7867\n",
      "Epoch 6/10, Train Loss: 0.0859, Accuracy: 0.9102, F1 Micro: 0.7902, F1 Macro: 0.7835\n",
      "Epoch 7/10, Train Loss: 0.0615, Accuracy: 0.9103, F1 Micro: 0.7975, F1 Macro: 0.7953\n",
      "Epoch 8/10, Train Loss: 0.0438, Accuracy: 0.9089, F1 Micro: 0.7955, F1 Macro: 0.7905\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9086, F1 Micro: 0.7867, F1 Macro: 0.7811\n",
      "Epoch 10/10, Train Loss: 0.0298, Accuracy: 0.9103, F1 Micro: 0.7917, F1 Macro: 0.782\n",
      "Best result for 6218 samples: F1 Micro: 0.7984\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.93      0.93       378\n",
      "                sara       0.65      0.76      0.70       253\n",
      "         radikalisme       0.69      0.88      0.77       234\n",
      "pencemaran_nama_baik       0.73      0.82      0.77       517\n",
      "\n",
      "           micro avg       0.75      0.85      0.80      1382\n",
      "           macro avg       0.75      0.85      0.79      1382\n",
      "        weighted avg       0.76      0.85      0.80      1382\n",
      "         samples avg       0.47      0.48      0.47      1382\n",
      "\n",
      "\n",
      "FOLD 2 COMPLETED in 3537.71 seconds\n",
      "===============================================\n",
      "STARTING FOLD 3/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6513, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7766, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4812, Accuracy: 0.7903, F1 Micro: 0.1183, F1 Macro: 0.098\n",
      "Epoch 4/10, Train Loss: 0.4323, Accuracy: 0.8141, F1 Micro: 0.2942, F1 Macro: 0.2249\n",
      "Epoch 5/10, Train Loss: 0.3936, Accuracy: 0.8339, F1 Micro: 0.4907, F1 Macro: 0.409\n",
      "Epoch 6/10, Train Loss: 0.3469, Accuracy: 0.8489, F1 Micro: 0.565, F1 Macro: 0.4941\n",
      "Epoch 7/10, Train Loss: 0.2903, Accuracy: 0.8502, F1 Micro: 0.5889, F1 Macro: 0.5297\n",
      "Epoch 8/10, Train Loss: 0.227, Accuracy: 0.8589, F1 Micro: 0.6596, F1 Macro: 0.6437\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.8583, F1 Micro: 0.6931, F1 Macro: 0.6907\n",
      "Epoch 10/10, Train Loss: 0.1624, Accuracy: 0.8569, F1 Micro: 0.6041, F1 Macro: 0.5588\n",
      "Best result for 388 samples: F1 Micro: 0.6931\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.85      0.85       355\n",
      "                sara       0.58      0.58      0.58       273\n",
      "         radikalisme       0.64      0.70      0.67       281\n",
      "pencemaran_nama_baik       0.62      0.71      0.66       521\n",
      "\n",
      "           micro avg       0.67      0.72      0.69      1430\n",
      "           macro avg       0.67      0.71      0.69      1430\n",
      "        weighted avg       0.68      0.72      0.69      1430\n",
      "         samples avg       0.40      0.40      0.39      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 583\n",
      "Sampling duration: 0.0005049705505371094 seconds\n",
      "\n",
      "Fold 3 - New train size: 971\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 971 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5712, Accuracy: 0.7778, F1 Micro: 0.0111, F1 Macro: 0.011\n",
      "Epoch 2/10, Train Loss: 0.4403, Accuracy: 0.8161, F1 Micro: 0.327, F1 Macro: 0.2422\n",
      "Epoch 3/10, Train Loss: 0.3525, Accuracy: 0.8353, F1 Micro: 0.4918, F1 Macro: 0.4158\n",
      "Epoch 4/10, Train Loss: 0.2867, Accuracy: 0.8672, F1 Micro: 0.6918, F1 Macro: 0.6791\n",
      "Epoch 5/10, Train Loss: 0.2167, Accuracy: 0.8694, F1 Micro: 0.7044, F1 Macro: 0.6948\n",
      "Epoch 6/10, Train Loss: 0.1873, Accuracy: 0.8677, F1 Micro: 0.6569, F1 Macro: 0.6477\n",
      "Epoch 7/10, Train Loss: 0.1517, Accuracy: 0.8753, F1 Micro: 0.7156, F1 Macro: 0.7016\n",
      "Epoch 8/10, Train Loss: 0.1105, Accuracy: 0.8769, F1 Micro: 0.7122, F1 Macro: 0.6965\n",
      "Epoch 9/10, Train Loss: 0.0901, Accuracy: 0.8758, F1 Micro: 0.6978, F1 Macro: 0.6861\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.8788, F1 Micro: 0.719, F1 Macro: 0.7076\n",
      "Best result for 971 samples: F1 Micro: 0.719\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.89      0.88       355\n",
      "                sara       0.66      0.47      0.55       273\n",
      "         radikalisme       0.74      0.69      0.71       281\n",
      "pencemaran_nama_baik       0.69      0.68      0.69       521\n",
      "\n",
      "           micro avg       0.75      0.69      0.72      1430\n",
      "           macro avg       0.74      0.68      0.71      1430\n",
      "        weighted avg       0.74      0.69      0.71      1430\n",
      "         samples avg       0.43      0.40      0.40      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.00035119056701660156 seconds\n",
      "\n",
      "Fold 3 - New train size: 1496\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1496 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5202, Accuracy: 0.8039, F1 Micro: 0.2181, F1 Macro: 0.1657\n",
      "Epoch 2/10, Train Loss: 0.372, Accuracy: 0.8484, F1 Micro: 0.5719, F1 Macro: 0.4945\n",
      "Epoch 3/10, Train Loss: 0.2815, Accuracy: 0.8775, F1 Micro: 0.6973, F1 Macro: 0.6841\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.8805, F1 Micro: 0.7358, F1 Macro: 0.7297\n",
      "Epoch 5/10, Train Loss: 0.1757, Accuracy: 0.883, F1 Micro: 0.7127, F1 Macro: 0.7004\n",
      "Epoch 6/10, Train Loss: 0.1309, Accuracy: 0.8755, F1 Micro: 0.6795, F1 Macro: 0.6477\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.8773, F1 Micro: 0.7235, F1 Macro: 0.7144\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.8794, F1 Micro: 0.7033, F1 Macro: 0.6907\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.8773, F1 Micro: 0.7163, F1 Macro: 0.7149\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.8783, F1 Micro: 0.7019, F1 Macro: 0.6881\n",
      "Best result for 1496 samples: F1 Micro: 0.7358\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.86      0.88       355\n",
      "                sara       0.64      0.57      0.60       273\n",
      "         radikalisme       0.69      0.78      0.73       281\n",
      "pencemaran_nama_baik       0.69      0.74      0.71       521\n",
      "\n",
      "           micro avg       0.73      0.74      0.74      1430\n",
      "           macro avg       0.73      0.74      0.73      1430\n",
      "        weighted avg       0.73      0.74      0.74      1430\n",
      "         samples avg       0.44      0.43      0.42      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.0006415843963623047 seconds\n",
      "\n",
      "Fold 3 - New train size: 1969\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 1969 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5078, Accuracy: 0.8283, F1 Micro: 0.4285, F1 Macro: 0.3482\n",
      "Epoch 2/10, Train Loss: 0.3286, Accuracy: 0.8697, F1 Micro: 0.6987, F1 Macro: 0.6994\n",
      "Epoch 3/10, Train Loss: 0.2543, Accuracy: 0.8834, F1 Micro: 0.7322, F1 Macro: 0.7237\n",
      "Epoch 4/10, Train Loss: 0.2049, Accuracy: 0.8848, F1 Micro: 0.7209, F1 Macro: 0.7097\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.8825, F1 Micro: 0.7287, F1 Macro: 0.725\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.8825, F1 Micro: 0.7251, F1 Macro: 0.7149\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.8778, F1 Micro: 0.7333, F1 Macro: 0.7294\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.8816, F1 Micro: 0.708, F1 Macro: 0.7037\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.8819, F1 Micro: 0.7106, F1 Macro: 0.7025\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.8853, F1 Micro: 0.7296, F1 Macro: 0.7245\n",
      "Best result for 1969 samples: F1 Micro: 0.7333\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.82      0.92      0.87       355\n",
      "                sara       0.61      0.60      0.61       273\n",
      "         radikalisme       0.73      0.77      0.75       281\n",
      "pencemaran_nama_baik       0.68      0.71      0.70       521\n",
      "\n",
      "           micro avg       0.72      0.75      0.73      1430\n",
      "           macro avg       0.71      0.75      0.73      1430\n",
      "        weighted avg       0.71      0.75      0.73      1430\n",
      "         samples avg       0.44      0.44      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 0.0003590583801269531 seconds\n",
      "\n",
      "Fold 3 - New train size: 2394\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2394 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4785, Accuracy: 0.8294, F1 Micro: 0.44, F1 Macro: 0.369\n",
      "Epoch 2/10, Train Loss: 0.3167, Accuracy: 0.8772, F1 Micro: 0.7368, F1 Macro: 0.7362\n",
      "Epoch 3/10, Train Loss: 0.2496, Accuracy: 0.8894, F1 Micro: 0.7433, F1 Macro: 0.7371\n",
      "Epoch 4/10, Train Loss: 0.1923, Accuracy: 0.8836, F1 Micro: 0.7517, F1 Macro: 0.752\n",
      "Epoch 5/10, Train Loss: 0.1742, Accuracy: 0.8881, F1 Micro: 0.7538, F1 Macro: 0.7488\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.8884, F1 Micro: 0.7304, F1 Macro: 0.7216\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.8881, F1 Micro: 0.7296, F1 Macro: 0.7246\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.8895, F1 Micro: 0.7525, F1 Macro: 0.7497\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.8873, F1 Micro: 0.7499, F1 Macro: 0.7528\n",
      "Epoch 10/10, Train Loss: 0.0393, Accuracy: 0.8872, F1 Micro: 0.7338, F1 Macro: 0.7283\n",
      "Best result for 2394 samples: F1 Micro: 0.7538\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.89       355\n",
      "                sara       0.68      0.57      0.62       273\n",
      "         radikalisme       0.71      0.82      0.76       281\n",
      "pencemaran_nama_baik       0.69      0.76      0.72       521\n",
      "\n",
      "           micro avg       0.74      0.77      0.75      1430\n",
      "           macro avg       0.74      0.76      0.75      1430\n",
      "        weighted avg       0.74      0.77      0.75      1430\n",
      "         samples avg       0.46      0.44      0.44      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 0.00040912628173828125 seconds\n",
      "\n",
      "Fold 3 - New train size: 2777\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 2777 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.469, Accuracy: 0.8406, F1 Micro: 0.5058, F1 Macro: 0.4416\n",
      "Epoch 2/10, Train Loss: 0.2976, Accuracy: 0.8856, F1 Micro: 0.7307, F1 Macro: 0.7231\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.8906, F1 Micro: 0.7482, F1 Macro: 0.7469\n",
      "Epoch 4/10, Train Loss: 0.1764, Accuracy: 0.8919, F1 Micro: 0.7482, F1 Macro: 0.7425\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.8916, F1 Micro: 0.7293, F1 Macro: 0.7223\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.8905, F1 Micro: 0.7515, F1 Macro: 0.7514\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.8903, F1 Micro: 0.7365, F1 Macro: 0.7318\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.8891, F1 Micro: 0.7317, F1 Macro: 0.7202\n",
      "Epoch 9/10, Train Loss: 0.0465, Accuracy: 0.8917, F1 Micro: 0.747, F1 Macro: 0.7398\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.8898, F1 Micro: 0.7546, F1 Macro: 0.7492\n",
      "Best result for 2777 samples: F1 Micro: 0.7546\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.88      0.89       355\n",
      "                sara       0.72      0.54      0.62       273\n",
      "         radikalisme       0.77      0.76      0.76       281\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       521\n",
      "\n",
      "           micro avg       0.75      0.76      0.75      1430\n",
      "           macro avg       0.77      0.74      0.75      1430\n",
      "        weighted avg       0.76      0.76      0.75      1430\n",
      "         samples avg       0.46      0.44      0.44      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 0.0003299713134765625 seconds\n",
      "\n",
      "Fold 3 - New train size: 3122\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3122 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4452, Accuracy: 0.8305, F1 Micro: 0.4316, F1 Macro: 0.35\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.8842, F1 Micro: 0.7179, F1 Macro: 0.719\n",
      "Epoch 3/10, Train Loss: 0.217, Accuracy: 0.8842, F1 Micro: 0.7042, F1 Macro: 0.6879\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.8922, F1 Micro: 0.7504, F1 Macro: 0.7449\n",
      "Epoch 5/10, Train Loss: 0.143, Accuracy: 0.8886, F1 Micro: 0.76, F1 Macro: 0.7585\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.89, F1 Micro: 0.7486, F1 Macro: 0.7474\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.892, F1 Micro: 0.7462, F1 Macro: 0.7409\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.8902, F1 Micro: 0.7358, F1 Macro: 0.7275\n",
      "Epoch 9/10, Train Loss: 0.0428, Accuracy: 0.8934, F1 Micro: 0.7605, F1 Macro: 0.7621\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8888, F1 Micro: 0.7523, F1 Macro: 0.7497\n",
      "Best result for 3122 samples: F1 Micro: 0.7605\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.87      0.90       355\n",
      "                sara       0.66      0.67      0.66       273\n",
      "         radikalisme       0.76      0.79      0.77       281\n",
      "pencemaran_nama_baik       0.72      0.71      0.71       521\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1430\n",
      "           macro avg       0.77      0.76      0.76      1430\n",
      "        weighted avg       0.77      0.76      0.76      1430\n",
      "         samples avg       0.45      0.44      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 0.00023698806762695312 seconds\n",
      "\n",
      "Fold 3 - New train size: 3432\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3432 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4397, Accuracy: 0.8448, F1 Micro: 0.5144, F1 Macro: 0.4704\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.8788, F1 Micro: 0.6791, F1 Macro: 0.6757\n",
      "Epoch 3/10, Train Loss: 0.2292, Accuracy: 0.8952, F1 Micro: 0.7601, F1 Macro: 0.7562\n",
      "Epoch 4/10, Train Loss: 0.1812, Accuracy: 0.895, F1 Micro: 0.7535, F1 Macro: 0.7503\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.8908, F1 Micro: 0.7476, F1 Macro: 0.7444\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.8953, F1 Micro: 0.7529, F1 Macro: 0.7544\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.892, F1 Micro: 0.7578, F1 Macro: 0.7592\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.8914, F1 Micro: 0.7488, F1 Macro: 0.75\n",
      "Epoch 9/10, Train Loss: 0.0427, Accuracy: 0.89, F1 Micro: 0.7431, F1 Macro: 0.7415\n",
      "Epoch 10/10, Train Loss: 0.0317, Accuracy: 0.8891, F1 Micro: 0.748, F1 Macro: 0.752\n",
      "Best result for 3432 samples: F1 Micro: 0.7601\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.86      0.88       355\n",
      "                sara       0.69      0.61      0.64       273\n",
      "         radikalisme       0.73      0.81      0.77       281\n",
      "pencemaran_nama_baik       0.77      0.69      0.73       521\n",
      "\n",
      "           micro avg       0.78      0.74      0.76      1430\n",
      "           macro avg       0.77      0.74      0.76      1430\n",
      "        weighted avg       0.78      0.74      0.76      1430\n",
      "         samples avg       0.44      0.43      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 0.0004405975341796875 seconds\n",
      "\n",
      "Fold 3 - New train size: 3711\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3711 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.44, Accuracy: 0.8591, F1 Micro: 0.5998, F1 Macro: 0.5743\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.8888, F1 Micro: 0.7488, F1 Macro: 0.7491\n",
      "Epoch 3/10, Train Loss: 0.2142, Accuracy: 0.8923, F1 Micro: 0.7533, F1 Macro: 0.7534\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.8944, F1 Micro: 0.7388, F1 Macro: 0.7303\n",
      "Epoch 5/10, Train Loss: 0.1369, Accuracy: 0.8867, F1 Micro: 0.721, F1 Macro: 0.714\n",
      "Epoch 6/10, Train Loss: 0.1034, Accuracy: 0.8884, F1 Micro: 0.7379, F1 Macro: 0.7299\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.8897, F1 Micro: 0.7364, F1 Macro: 0.7314\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.893, F1 Micro: 0.7596, F1 Macro: 0.7626\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.89, F1 Micro: 0.7489, F1 Macro: 0.7463\n",
      "Epoch 10/10, Train Loss: 0.0303, Accuracy: 0.8884, F1 Micro: 0.7477, F1 Macro: 0.748\n",
      "Best result for 3711 samples: F1 Micro: 0.7596\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.68      0.66      0.67       273\n",
      "         radikalisme       0.76      0.80      0.78       281\n",
      "pencemaran_nama_baik       0.70      0.70      0.70       521\n",
      "\n",
      "           micro avg       0.76      0.76      0.76      1430\n",
      "           macro avg       0.77      0.76      0.76      1430\n",
      "        weighted avg       0.76      0.76      0.76      1430\n",
      "         samples avg       0.45      0.44      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 175\n",
      "Sampling duration: 0.06979703903198242 seconds\n",
      "\n",
      "Fold 3 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4248, Accuracy: 0.8636, F1 Micro: 0.6388, F1 Macro: 0.5938\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.8923, F1 Micro: 0.7542, F1 Macro: 0.7512\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.8902, F1 Micro: 0.724, F1 Macro: 0.7101\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.8845, F1 Micro: 0.7132, F1 Macro: 0.6959\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.8934, F1 Micro: 0.754, F1 Macro: 0.7503\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.8877, F1 Micro: 0.7298, F1 Macro: 0.721\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.888, F1 Micro: 0.7628, F1 Macro: 0.7645\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.8864, F1 Micro: 0.7367, F1 Macro: 0.7395\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.8877, F1 Micro: 0.7451, F1 Macro: 0.7443\n",
      "Epoch 10/10, Train Loss: 0.0299, Accuracy: 0.8883, F1 Micro: 0.7458, F1 Macro: 0.7477\n",
      "Best result for 3886 samples: F1 Micro: 0.7628\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.92      0.89       355\n",
      "                sara       0.63      0.75      0.69       273\n",
      "         radikalisme       0.74      0.78      0.76       281\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       521\n",
      "\n",
      "           micro avg       0.72      0.81      0.76      1430\n",
      "           macro avg       0.73      0.81      0.76      1430\n",
      "        weighted avg       0.73      0.81      0.76      1430\n",
      "         samples avg       0.46      0.47      0.46      1430\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 0.000225067138671875 seconds\n",
      "\n",
      "Fold 3 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4196, Accuracy: 0.8645, F1 Micro: 0.7233, F1 Macro: 0.7275\n",
      "Epoch 2/10, Train Loss: 0.2635, Accuracy: 0.8939, F1 Micro: 0.7557, F1 Macro: 0.7522\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.892, F1 Micro: 0.7682, F1 Macro: 0.7685\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.8864, F1 Micro: 0.7611, F1 Macro: 0.7646\n",
      "Epoch 5/10, Train Loss: 0.1298, Accuracy: 0.892, F1 Micro: 0.7494, F1 Macro: 0.7485\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.8886, F1 Micro: 0.7376, F1 Macro: 0.7327\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.8919, F1 Micro: 0.7604, F1 Macro: 0.7602\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.8906, F1 Micro: 0.7426, F1 Macro: 0.7392\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.8872, F1 Micro: 0.7543, F1 Macro: 0.7548\n",
      "Epoch 10/10, Train Loss: 0.032, Accuracy: 0.8898, F1 Micro: 0.7501, F1 Macro: 0.7511\n",
      "Best result for 4120 samples: F1 Micro: 0.7682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       355\n",
      "                sara       0.64      0.73      0.68       273\n",
      "         radikalisme       0.69      0.86      0.77       281\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       521\n",
      "\n",
      "           micro avg       0.74      0.80      0.77      1430\n",
      "           macro avg       0.74      0.81      0.77      1430\n",
      "        weighted avg       0.74      0.80      0.77      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 0.00018835067749023438 seconds\n",
      "\n",
      "Fold 3 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.41, Accuracy: 0.8614, F1 Micro: 0.6168, F1 Macro: 0.5881\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.8866, F1 Micro: 0.7424, F1 Macro: 0.7387\n",
      "Epoch 3/10, Train Loss: 0.1989, Accuracy: 0.89, F1 Micro: 0.7566, F1 Macro: 0.7533\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.8948, F1 Micro: 0.7426, F1 Macro: 0.7377\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.8917, F1 Micro: 0.7488, F1 Macro: 0.7492\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.8917, F1 Micro: 0.7449, F1 Macro: 0.7396\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.8895, F1 Micro: 0.7284, F1 Macro: 0.7175\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.8888, F1 Micro: 0.7481, F1 Macro: 0.7494\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.8911, F1 Micro: 0.74, F1 Macro: 0.7376\n",
      "Epoch 10/10, Train Loss: 0.0289, Accuracy: 0.8884, F1 Micro: 0.75, F1 Macro: 0.7511\n",
      "Best result for 4330 samples: F1 Micro: 0.7566\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.86      0.91      0.88       355\n",
      "                sara       0.67      0.61      0.64       273\n",
      "         radikalisme       0.75      0.80      0.77       281\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       521\n",
      "\n",
      "           micro avg       0.75      0.77      0.76      1430\n",
      "           macro avg       0.75      0.76      0.75      1430\n",
      "        weighted avg       0.75      0.77      0.75      1430\n",
      "         samples avg       0.46      0.45      0.44      1430\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018644332885742188 seconds\n",
      "\n",
      "Fold 3 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4089, Accuracy: 0.8692, F1 Micro: 0.6491, F1 Macro: 0.6341\n",
      "Epoch 2/10, Train Loss: 0.2551, Accuracy: 0.8845, F1 Micro: 0.7617, F1 Macro: 0.7636\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.8969, F1 Micro: 0.7648, F1 Macro: 0.7641\n",
      "Epoch 4/10, Train Loss: 0.1569, Accuracy: 0.8922, F1 Micro: 0.7617, F1 Macro: 0.7607\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.8913, F1 Micro: 0.7623, F1 Macro: 0.7618\n",
      "Epoch 6/10, Train Loss: 0.089, Accuracy: 0.8892, F1 Micro: 0.7398, F1 Macro: 0.7408\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.8867, F1 Micro: 0.748, F1 Macro: 0.7473\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.8892, F1 Micro: 0.7656, F1 Macro: 0.7687\n",
      "Epoch 9/10, Train Loss: 0.0396, Accuracy: 0.89, F1 Micro: 0.7349, F1 Macro: 0.7271\n",
      "Epoch 10/10, Train Loss: 0.0293, Accuracy: 0.8939, F1 Micro: 0.7546, F1 Macro: 0.7562\n",
      "Best result for 4530 samples: F1 Micro: 0.7656\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       355\n",
      "                sara       0.63      0.76      0.69       273\n",
      "         radikalisme       0.73      0.79      0.76       281\n",
      "pencemaran_nama_baik       0.67      0.79      0.72       521\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1430\n",
      "           macro avg       0.74      0.81      0.77      1430\n",
      "        weighted avg       0.73      0.81      0.77      1430\n",
      "         samples avg       0.47      0.47      0.46      1430\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 0.07938575744628906 seconds\n",
      "\n",
      "Fold 3 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.875, F1 Micro: 0.7004, F1 Macro: 0.6877\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.8898, F1 Micro: 0.7527, F1 Macro: 0.7541\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.8972, F1 Micro: 0.7623, F1 Macro: 0.7559\n",
      "Epoch 4/10, Train Loss: 0.1534, Accuracy: 0.8944, F1 Micro: 0.7611, F1 Macro: 0.7583\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.897, F1 Micro: 0.7705, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.8925, F1 Micro: 0.7522, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.892, F1 Micro: 0.7505, F1 Macro: 0.7512\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.8884, F1 Micro: 0.7215, F1 Macro: 0.7134\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.8945, F1 Micro: 0.7564, F1 Macro: 0.7573\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.8916, F1 Micro: 0.761, F1 Macro: 0.7634\n",
      "Best result for 4663 samples: F1 Micro: 0.7705\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       355\n",
      "                sara       0.69      0.64      0.67       273\n",
      "         radikalisme       0.74      0.81      0.78       281\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       521\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1430\n",
      "           macro avg       0.77      0.77      0.77      1430\n",
      "        weighted avg       0.77      0.77      0.77      1430\n",
      "         samples avg       0.46      0.45      0.45      1430\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019073486328125 seconds\n",
      "\n",
      "Fold 3 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3987, Accuracy: 0.8822, F1 Micro: 0.7313, F1 Macro: 0.7224\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.8884, F1 Micro: 0.7498, F1 Macro: 0.7411\n",
      "Epoch 3/10, Train Loss: 0.1994, Accuracy: 0.8986, F1 Micro: 0.7636, F1 Macro: 0.7588\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.8917, F1 Micro: 0.7294, F1 Macro: 0.7183\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.8884, F1 Micro: 0.7502, F1 Macro: 0.7517\n",
      "Epoch 6/10, Train Loss: 0.0856, Accuracy: 0.8908, F1 Micro: 0.7361, F1 Macro: 0.7297\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.8922, F1 Micro: 0.733, F1 Macro: 0.7301\n",
      "Epoch 8/10, Train Loss: 0.0467, Accuracy: 0.8908, F1 Micro: 0.744, F1 Macro: 0.7474\n",
      "Epoch 9/10, Train Loss: 0.0353, Accuracy: 0.8936, F1 Micro: 0.7586, F1 Macro: 0.7596\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.8925, F1 Micro: 0.7564, F1 Macro: 0.7587\n",
      "Best result for 4863 samples: F1 Micro: 0.7636\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.86      0.89       355\n",
      "                sara       0.72      0.56      0.63       273\n",
      "         radikalisme       0.80      0.77      0.78       281\n",
      "pencemaran_nama_baik       0.75      0.72      0.73       521\n",
      "\n",
      "           micro avg       0.80      0.73      0.76      1430\n",
      "           macro avg       0.80      0.73      0.76      1430\n",
      "        weighted avg       0.80      0.73      0.76      1430\n",
      "         samples avg       0.45      0.42      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017189979553222656 seconds\n",
      "\n",
      "Fold 3 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4032, Accuracy: 0.878, F1 Micro: 0.7083, F1 Macro: 0.6761\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.8947, F1 Micro: 0.7535, F1 Macro: 0.7458\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.8931, F1 Micro: 0.7672, F1 Macro: 0.77\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.895, F1 Micro: 0.7652, F1 Macro: 0.7676\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.8923, F1 Micro: 0.7654, F1 Macro: 0.7706\n",
      "Epoch 6/10, Train Loss: 0.0854, Accuracy: 0.8888, F1 Micro: 0.7601, F1 Macro: 0.7611\n",
      "Epoch 7/10, Train Loss: 0.0626, Accuracy: 0.893, F1 Micro: 0.7635, F1 Macro: 0.7652\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.892, F1 Micro: 0.7538, F1 Macro: 0.7542\n",
      "Epoch 9/10, Train Loss: 0.0359, Accuracy: 0.8898, F1 Micro: 0.7525, F1 Macro: 0.7535\n",
      "Epoch 10/10, Train Loss: 0.0326, Accuracy: 0.8891, F1 Micro: 0.745, F1 Macro: 0.7454\n",
      "Best result for 5063 samples: F1 Micro: 0.7672\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.89      0.89       355\n",
      "                sara       0.67      0.74      0.70       273\n",
      "         radikalisme       0.68      0.88      0.77       281\n",
      "pencemaran_nama_baik       0.73      0.70      0.71       521\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1430\n",
      "           macro avg       0.75      0.80      0.77      1430\n",
      "        weighted avg       0.75      0.79      0.77      1430\n",
      "         samples avg       0.45      0.45      0.44      1430\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00021457672119140625 seconds\n",
      "\n",
      "Fold 3 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3808, Accuracy: 0.8867, F1 Micro: 0.7232, F1 Macro: 0.7185\n",
      "Epoch 2/10, Train Loss: 0.2404, Accuracy: 0.8953, F1 Micro: 0.7585, F1 Macro: 0.7528\n",
      "Epoch 3/10, Train Loss: 0.1935, Accuracy: 0.8902, F1 Micro: 0.7189, F1 Macro: 0.7025\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.8941, F1 Micro: 0.7563, F1 Macro: 0.7563\n",
      "Epoch 5/10, Train Loss: 0.1177, Accuracy: 0.8905, F1 Micro: 0.7519, F1 Macro: 0.7514\n",
      "Epoch 6/10, Train Loss: 0.0817, Accuracy: 0.8894, F1 Micro: 0.7526, F1 Macro: 0.7536\n",
      "Epoch 7/10, Train Loss: 0.0619, Accuracy: 0.8947, F1 Micro: 0.7584, F1 Macro: 0.7561\n",
      "Epoch 8/10, Train Loss: 0.0474, Accuracy: 0.8903, F1 Micro: 0.7528, F1 Macro: 0.7547\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.8898, F1 Micro: 0.7573, F1 Macro: 0.7571\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.8894, F1 Micro: 0.7555, F1 Macro: 0.756\n",
      "Best result for 5263 samples: F1 Micro: 0.7585\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.88       355\n",
      "                sara       0.72      0.55      0.62       273\n",
      "         radikalisme       0.76      0.78      0.77       281\n",
      "pencemaran_nama_baik       0.73      0.74      0.73       521\n",
      "\n",
      "           micro avg       0.78      0.74      0.76      1430\n",
      "           macro avg       0.79      0.73      0.75      1430\n",
      "        weighted avg       0.78      0.74      0.76      1430\n",
      "         samples avg       0.46      0.43      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.09504842758178711 seconds\n",
      "\n",
      "Fold 3 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3896, Accuracy: 0.8845, F1 Micro: 0.7335, F1 Macro: 0.7294\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.8945, F1 Micro: 0.7552, F1 Macro: 0.7511\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.8984, F1 Micro: 0.7573, F1 Macro: 0.7471\n",
      "Epoch 4/10, Train Loss: 0.1637, Accuracy: 0.8933, F1 Micro: 0.7655, F1 Macro: 0.7656\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.8888, F1 Micro: 0.7664, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.0836, Accuracy: 0.897, F1 Micro: 0.7672, F1 Macro: 0.7694\n",
      "Epoch 7/10, Train Loss: 0.0683, Accuracy: 0.898, F1 Micro: 0.7637, F1 Macro: 0.7643\n",
      "Epoch 8/10, Train Loss: 0.0453, Accuracy: 0.8922, F1 Micro: 0.7532, F1 Macro: 0.7518\n",
      "Epoch 9/10, Train Loss: 0.0382, Accuracy: 0.8897, F1 Micro: 0.7608, F1 Macro: 0.763\n",
      "Epoch 10/10, Train Loss: 0.0334, Accuracy: 0.8905, F1 Micro: 0.7497, F1 Macro: 0.7517\n",
      "Best result for 5441 samples: F1 Micro: 0.7672\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.87      0.91       355\n",
      "                sara       0.66      0.66      0.66       273\n",
      "         radikalisme       0.78      0.80      0.79       281\n",
      "pencemaran_nama_baik       0.72      0.71      0.72       521\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1430\n",
      "           macro avg       0.78      0.76      0.77      1430\n",
      "        weighted avg       0.78      0.76      0.77      1430\n",
      "         samples avg       0.44      0.43      0.43      1430\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00021767616271972656 seconds\n",
      "\n",
      "Fold 3 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3844, Accuracy: 0.8827, F1 Micro: 0.7406, F1 Macro: 0.7386\n",
      "Epoch 2/10, Train Loss: 0.2392, Accuracy: 0.8952, F1 Micro: 0.7606, F1 Macro: 0.7595\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.8981, F1 Micro: 0.7683, F1 Macro: 0.7685\n",
      "Epoch 4/10, Train Loss: 0.1509, Accuracy: 0.9, F1 Micro: 0.7793, F1 Macro: 0.7795\n",
      "Epoch 5/10, Train Loss: 0.112, Accuracy: 0.8917, F1 Micro: 0.7645, F1 Macro: 0.7687\n",
      "Epoch 6/10, Train Loss: 0.083, Accuracy: 0.8933, F1 Micro: 0.7565, F1 Macro: 0.7583\n",
      "Epoch 7/10, Train Loss: 0.0621, Accuracy: 0.8917, F1 Micro: 0.7448, F1 Macro: 0.7424\n",
      "Epoch 8/10, Train Loss: 0.0461, Accuracy: 0.8895, F1 Micro: 0.7506, F1 Macro: 0.7487\n",
      "Epoch 9/10, Train Loss: 0.033, Accuracy: 0.8911, F1 Micro: 0.7602, F1 Macro: 0.7626\n",
      "Epoch 10/10, Train Loss: 0.0304, Accuracy: 0.8905, F1 Micro: 0.7555, F1 Macro: 0.756\n",
      "Best result for 5641 samples: F1 Micro: 0.7793\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       355\n",
      "                sara       0.73      0.66      0.69       273\n",
      "         radikalisme       0.79      0.78      0.78       281\n",
      "pencemaran_nama_baik       0.69      0.80      0.74       521\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1430\n",
      "           macro avg       0.78      0.78      0.78      1430\n",
      "        weighted avg       0.77      0.79      0.78      1430\n",
      "         samples avg       0.47      0.46      0.45      1430\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002243518829345703 seconds\n",
      "\n",
      "Fold 3 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3797, Accuracy: 0.8808, F1 Micro: 0.7473, F1 Macro: 0.7486\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.895, F1 Micro: 0.7746, F1 Macro: 0.778\n",
      "Epoch 3/10, Train Loss: 0.1854, Accuracy: 0.8956, F1 Micro: 0.7626, F1 Macro: 0.7589\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.8969, F1 Micro: 0.7643, F1 Macro: 0.7646\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.8909, F1 Micro: 0.7409, F1 Macro: 0.7405\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.897, F1 Micro: 0.7639, F1 Macro: 0.7663\n",
      "Epoch 7/10, Train Loss: 0.059, Accuracy: 0.8941, F1 Micro: 0.7613, F1 Macro: 0.7631\n",
      "Epoch 8/10, Train Loss: 0.0469, Accuracy: 0.8861, F1 Micro: 0.7643, F1 Macro: 0.7696\n",
      "Epoch 9/10, Train Loss: 0.0352, Accuracy: 0.8914, F1 Micro: 0.7508, F1 Macro: 0.7432\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.8934, F1 Micro: 0.7648, F1 Macro: 0.7686\n",
      "Best result for 5841 samples: F1 Micro: 0.7746\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.85      0.89       355\n",
      "                sara       0.64      0.75      0.69       273\n",
      "         radikalisme       0.75      0.85      0.80       281\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       521\n",
      "\n",
      "           micro avg       0.74      0.81      0.77      1430\n",
      "           macro avg       0.75      0.81      0.78      1430\n",
      "        weighted avg       0.75      0.81      0.78      1430\n",
      "         samples avg       0.46      0.46      0.45      1430\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019693374633789062 seconds\n",
      "\n",
      "Fold 3 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8839, F1 Micro: 0.7253, F1 Macro: 0.7282\n",
      "Epoch 2/10, Train Loss: 0.2278, Accuracy: 0.8977, F1 Micro: 0.7736, F1 Macro: 0.7748\n",
      "Epoch 3/10, Train Loss: 0.1921, Accuracy: 0.8967, F1 Micro: 0.7578, F1 Macro: 0.7495\n",
      "Epoch 4/10, Train Loss: 0.1531, Accuracy: 0.8952, F1 Micro: 0.7631, F1 Macro: 0.764\n",
      "Epoch 5/10, Train Loss: 0.12, Accuracy: 0.8989, F1 Micro: 0.7633, F1 Macro: 0.7633\n",
      "Epoch 6/10, Train Loss: 0.0861, Accuracy: 0.893, F1 Micro: 0.7584, F1 Macro: 0.7584\n",
      "Epoch 7/10, Train Loss: 0.0585, Accuracy: 0.8952, F1 Micro: 0.7621, F1 Macro: 0.7624\n",
      "Epoch 8/10, Train Loss: 0.0449, Accuracy: 0.8928, F1 Micro: 0.7591, F1 Macro: 0.7615\n",
      "Epoch 9/10, Train Loss: 0.0373, Accuracy: 0.8938, F1 Micro: 0.7642, F1 Macro: 0.768\n",
      "Epoch 10/10, Train Loss: 0.0277, Accuracy: 0.8894, F1 Micro: 0.7598, F1 Macro: 0.7635\n",
      "Best result for 6041 samples: F1 Micro: 0.7736\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.86      0.90       355\n",
      "                sara       0.64      0.71      0.68       273\n",
      "         radikalisme       0.76      0.81      0.78       281\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       521\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1430\n",
      "           macro avg       0.77      0.78      0.77      1430\n",
      "        weighted avg       0.77      0.78      0.78      1430\n",
      "         samples avg       0.46      0.45      0.45      1430\n",
      "\n",
      "Nearest checkpoint: 6218\n",
      "Acquired samples: 177\n",
      "Sampling duration: 0.10277318954467773 seconds\n",
      "\n",
      "Fold 3 - New train size: 6218\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 3 - Training with 6218 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.377, Accuracy: 0.8833, F1 Micro: 0.7469, F1 Macro: 0.75\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.895, F1 Micro: 0.7476, F1 Macro: 0.7466\n",
      "Epoch 3/10, Train Loss: 0.1833, Accuracy: 0.8984, F1 Micro: 0.7771, F1 Macro: 0.7769\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.8966, F1 Micro: 0.7714, F1 Macro: 0.7715\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.8961, F1 Micro: 0.7628, F1 Macro: 0.7628\n",
      "Epoch 6/10, Train Loss: 0.0811, Accuracy: 0.8948, F1 Micro: 0.7609, F1 Macro: 0.7586\n",
      "Epoch 7/10, Train Loss: 0.0637, Accuracy: 0.8917, F1 Micro: 0.7501, F1 Macro: 0.7499\n",
      "Epoch 8/10, Train Loss: 0.0464, Accuracy: 0.8911, F1 Micro: 0.744, F1 Macro: 0.7375\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.8941, F1 Micro: 0.7566, F1 Macro: 0.7611\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.8939, F1 Micro: 0.7623, F1 Macro: 0.7609\n",
      "Best result for 6218 samples: F1 Micro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       355\n",
      "                sara       0.63      0.69      0.66       273\n",
      "         radikalisme       0.75      0.86      0.80       281\n",
      "pencemaran_nama_baik       0.74      0.74      0.74       521\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1430\n",
      "           macro avg       0.76      0.80      0.78      1430\n",
      "        weighted avg       0.77      0.79      0.78      1430\n",
      "         samples avg       0.45      0.45      0.44      1430\n",
      "\n",
      "\n",
      "FOLD 3 COMPLETED in 3599.40 seconds\n",
      "===============================================\n",
      "STARTING FOLD 4/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5795, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7811, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 3/10, Train Loss: 0.4106, Accuracy: 0.8267, F1 Micro: 0.3666, F1 Macro: 0.3071\n",
      "Epoch 4/10, Train Loss: 0.3659, Accuracy: 0.8505, F1 Micro: 0.5812, F1 Macro: 0.5497\n",
      "Epoch 5/10, Train Loss: 0.3165, Accuracy: 0.8614, F1 Micro: 0.6272, F1 Macro: 0.608\n",
      "Epoch 6/10, Train Loss: 0.2503, Accuracy: 0.8612, F1 Micro: 0.655, F1 Macro: 0.6302\n",
      "Epoch 7/10, Train Loss: 0.2107, Accuracy: 0.8628, F1 Micro: 0.6613, F1 Macro: 0.6477\n",
      "Epoch 8/10, Train Loss: 0.1713, Accuracy: 0.8617, F1 Micro: 0.6757, F1 Macro: 0.6675\n",
      "Epoch 9/10, Train Loss: 0.1484, Accuracy: 0.8644, F1 Micro: 0.672, F1 Macro: 0.662\n",
      "Epoch 10/10, Train Loss: 0.1116, Accuracy: 0.8662, F1 Micro: 0.686, F1 Macro: 0.6766\n",
      "Best result for 388 samples: F1 Micro: 0.686\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.81      0.86       342\n",
      "                sara       0.54      0.47      0.51       249\n",
      "         radikalisme       0.72      0.63      0.67       302\n",
      "pencemaran_nama_baik       0.64      0.69      0.66       508\n",
      "\n",
      "           micro avg       0.71      0.67      0.69      1401\n",
      "           macro avg       0.71      0.65      0.68      1401\n",
      "        weighted avg       0.71      0.67      0.69      1401\n",
      "         samples avg       0.38      0.39      0.37      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.00036907196044921875 seconds\n",
      "\n",
      "Fold 4 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5264, Accuracy: 0.7819, F1 Micro: 0.0085, F1 Macro: 0.0086\n",
      "Epoch 2/10, Train Loss: 0.3883, Accuracy: 0.8375, F1 Micro: 0.4846, F1 Macro: 0.4037\n",
      "Epoch 3/10, Train Loss: 0.3114, Accuracy: 0.867, F1 Micro: 0.6807, F1 Macro: 0.6584\n",
      "Epoch 4/10, Train Loss: 0.2632, Accuracy: 0.8712, F1 Micro: 0.6993, F1 Macro: 0.6834\n",
      "Epoch 5/10, Train Loss: 0.2149, Accuracy: 0.8766, F1 Micro: 0.724, F1 Macro: 0.7192\n",
      "Epoch 6/10, Train Loss: 0.1807, Accuracy: 0.8827, F1 Micro: 0.7023, F1 Macro: 0.6959\n",
      "Epoch 7/10, Train Loss: 0.1373, Accuracy: 0.8781, F1 Micro: 0.7252, F1 Macro: 0.7232\n",
      "Epoch 8/10, Train Loss: 0.103, Accuracy: 0.8777, F1 Micro: 0.7314, F1 Macro: 0.7316\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.8842, F1 Micro: 0.7285, F1 Macro: 0.7265\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.8756, F1 Micro: 0.7329, F1 Macro: 0.7339\n",
      "Best result for 972 samples: F1 Micro: 0.7329\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.88      0.88       342\n",
      "                sara       0.55      0.70      0.61       249\n",
      "         radikalisme       0.73      0.75      0.74       302\n",
      "pencemaran_nama_baik       0.64      0.77      0.70       508\n",
      "\n",
      "           micro avg       0.69      0.78      0.73      1401\n",
      "           macro avg       0.70      0.77      0.73      1401\n",
      "        weighted avg       0.70      0.78      0.74      1401\n",
      "         samples avg       0.43      0.45      0.43      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0003681182861328125 seconds\n",
      "\n",
      "Fold 4 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5028, Accuracy: 0.8183, F1 Micro: 0.3274, F1 Macro: 0.238\n",
      "Epoch 2/10, Train Loss: 0.3345, Accuracy: 0.8595, F1 Micro: 0.6221, F1 Macro: 0.6109\n",
      "Epoch 3/10, Train Loss: 0.2757, Accuracy: 0.8802, F1 Micro: 0.6832, F1 Macro: 0.6656\n",
      "Epoch 4/10, Train Loss: 0.222, Accuracy: 0.8841, F1 Micro: 0.7137, F1 Macro: 0.7074\n",
      "Epoch 5/10, Train Loss: 0.1793, Accuracy: 0.8875, F1 Micro: 0.7399, F1 Macro: 0.7345\n",
      "Epoch 6/10, Train Loss: 0.1377, Accuracy: 0.8875, F1 Micro: 0.7479, F1 Macro: 0.7448\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.888, F1 Micro: 0.7442, F1 Macro: 0.7406\n",
      "Epoch 8/10, Train Loss: 0.0838, Accuracy: 0.8805, F1 Micro: 0.7484, F1 Macro: 0.7518\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.8709, F1 Micro: 0.7376, F1 Macro: 0.745\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.8828, F1 Micro: 0.747, F1 Macro: 0.7484\n",
      "Best result for 1497 samples: F1 Micro: 0.7484\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.88      0.90       342\n",
      "                sara       0.48      0.81      0.61       249\n",
      "         radikalisme       0.76      0.78      0.77       302\n",
      "pencemaran_nama_baik       0.68      0.79      0.73       508\n",
      "\n",
      "           micro avg       0.69      0.81      0.75      1401\n",
      "           macro avg       0.71      0.81      0.75      1401\n",
      "        weighted avg       0.72      0.81      0.76      1401\n",
      "         samples avg       0.43      0.46      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.00036978721618652344 seconds\n",
      "\n",
      "Fold 4 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4583, Accuracy: 0.8369, F1 Micro: 0.4842, F1 Macro: 0.4226\n",
      "Epoch 2/10, Train Loss: 0.3094, Accuracy: 0.8753, F1 Micro: 0.7144, F1 Macro: 0.7113\n",
      "Epoch 3/10, Train Loss: 0.2521, Accuracy: 0.8916, F1 Micro: 0.7432, F1 Macro: 0.7385\n",
      "Epoch 4/10, Train Loss: 0.2064, Accuracy: 0.8906, F1 Micro: 0.7514, F1 Macro: 0.749\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.8942, F1 Micro: 0.7492, F1 Macro: 0.7418\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.8956, F1 Micro: 0.7567, F1 Macro: 0.7505\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.8888, F1 Micro: 0.7567, F1 Macro: 0.7566\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.8891, F1 Micro: 0.7526, F1 Macro: 0.7532\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.89, F1 Micro: 0.7618, F1 Macro: 0.7597\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.8906, F1 Micro: 0.7583, F1 Macro: 0.7562\n",
      "Best result for 1970 samples: F1 Micro: 0.7618\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.91      0.90       342\n",
      "                sara       0.58      0.71      0.64       249\n",
      "         radikalisme       0.76      0.78      0.77       302\n",
      "pencemaran_nama_baik       0.68      0.80      0.74       508\n",
      "\n",
      "           micro avg       0.72      0.80      0.76      1401\n",
      "           macro avg       0.73      0.80      0.76      1401\n",
      "        weighted avg       0.73      0.80      0.76      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 0.0006916522979736328 seconds\n",
      "\n",
      "Fold 4 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4427, Accuracy: 0.855, F1 Micro: 0.5556, F1 Macro: 0.4923\n",
      "Epoch 2/10, Train Loss: 0.2889, Accuracy: 0.8825, F1 Micro: 0.7247, F1 Macro: 0.7108\n",
      "Epoch 3/10, Train Loss: 0.218, Accuracy: 0.8884, F1 Micro: 0.7514, F1 Macro: 0.7478\n",
      "Epoch 4/10, Train Loss: 0.1914, Accuracy: 0.8927, F1 Micro: 0.7667, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.8923, F1 Micro: 0.7649, F1 Macro: 0.7651\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.8944, F1 Micro: 0.7656, F1 Macro: 0.7631\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.8798, F1 Micro: 0.7573, F1 Macro: 0.764\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.8944, F1 Micro: 0.7591, F1 Macro: 0.7569\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.8956, F1 Micro: 0.7663, F1 Macro: 0.7645\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.8902, F1 Micro: 0.7568, F1 Macro: 0.7557\n",
      "Best result for 2395 samples: F1 Micro: 0.7667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.57      0.73      0.64       249\n",
      "         radikalisme       0.76      0.80      0.78       302\n",
      "pencemaran_nama_baik       0.70      0.79      0.74       508\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1401\n",
      "           macro avg       0.74      0.80      0.77      1401\n",
      "        weighted avg       0.74      0.81      0.77      1401\n",
      "         samples avg       0.45      0.46      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 0.0004661083221435547 seconds\n",
      "\n",
      "Fold 4 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4326, Accuracy: 0.8659, F1 Micro: 0.6512, F1 Macro: 0.572\n",
      "Epoch 2/10, Train Loss: 0.2904, Accuracy: 0.8881, F1 Micro: 0.7426, F1 Macro: 0.7348\n",
      "Epoch 3/10, Train Loss: 0.2322, Accuracy: 0.8967, F1 Micro: 0.7425, F1 Macro: 0.7304\n",
      "Epoch 4/10, Train Loss: 0.184, Accuracy: 0.8978, F1 Micro: 0.7663, F1 Macro: 0.7621\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.8948, F1 Micro: 0.7677, F1 Macro: 0.765\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.8894, F1 Micro: 0.7557, F1 Macro: 0.751\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.8986, F1 Micro: 0.7698, F1 Macro: 0.7647\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.8939, F1 Micro: 0.7653, F1 Macro: 0.7637\n",
      "Epoch 9/10, Train Loss: 0.0451, Accuracy: 0.8956, F1 Micro: 0.7597, F1 Macro: 0.7544\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.8963, F1 Micro: 0.7608, F1 Macro: 0.7576\n",
      "Best result for 2778 samples: F1 Micro: 0.7698\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       342\n",
      "                sara       0.65      0.63      0.64       249\n",
      "         radikalisme       0.74      0.80      0.77       302\n",
      "pencemaran_nama_baik       0.73      0.74      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1401\n",
      "           macro avg       0.76      0.77      0.76      1401\n",
      "        weighted avg       0.77      0.77      0.77      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 0.0003521442413330078 seconds\n",
      "\n",
      "Fold 4 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4155, Accuracy: 0.87, F1 Micro: 0.6429, F1 Macro: 0.6263\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.8922, F1 Micro: 0.7617, F1 Macro: 0.7589\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.8955, F1 Micro: 0.761, F1 Macro: 0.754\n",
      "Epoch 4/10, Train Loss: 0.1685, Accuracy: 0.8978, F1 Micro: 0.7599, F1 Macro: 0.7548\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.8988, F1 Micro: 0.7611, F1 Macro: 0.7561\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.8966, F1 Micro: 0.7642, F1 Macro: 0.7605\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.8973, F1 Micro: 0.761, F1 Macro: 0.7575\n",
      "Epoch 8/10, Train Loss: 0.0548, Accuracy: 0.8892, F1 Micro: 0.761, F1 Macro: 0.7631\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.8938, F1 Micro: 0.767, F1 Macro: 0.7661\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.8998, F1 Micro: 0.7702, F1 Macro: 0.7685\n",
      "Best result for 3123 samples: F1 Micro: 0.7702\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       342\n",
      "                sara       0.65      0.68      0.67       249\n",
      "         radikalisme       0.79      0.76      0.77       302\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.77      0.77      1401\n",
      "           macro avg       0.77      0.76      0.77      1401\n",
      "        weighted avg       0.78      0.77      0.77      1401\n",
      "         samples avg       0.45      0.44      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 0.0003273487091064453 seconds\n",
      "\n",
      "Fold 4 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3993, Accuracy: 0.8745, F1 Micro: 0.706, F1 Macro: 0.7025\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.8923, F1 Micro: 0.7333, F1 Macro: 0.727\n",
      "Epoch 3/10, Train Loss: 0.2188, Accuracy: 0.8956, F1 Micro: 0.7666, F1 Macro: 0.7627\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.895, F1 Micro: 0.7622, F1 Macro: 0.7625\n",
      "Epoch 5/10, Train Loss: 0.1299, Accuracy: 0.9, F1 Micro: 0.7563, F1 Macro: 0.7524\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9, F1 Micro: 0.7598, F1 Macro: 0.757\n",
      "Epoch 7/10, Train Loss: 0.0722, Accuracy: 0.8969, F1 Micro: 0.7633, F1 Macro: 0.7615\n",
      "Epoch 8/10, Train Loss: 0.0501, Accuracy: 0.898, F1 Micro: 0.7692, F1 Macro: 0.7675\n",
      "Epoch 9/10, Train Loss: 0.0393, Accuracy: 0.9002, F1 Micro: 0.7562, F1 Macro: 0.7534\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.8923, F1 Micro: 0.7644, F1 Macro: 0.7658\n",
      "Best result for 3433 samples: F1 Micro: 0.7692\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.89      0.90       342\n",
      "                sara       0.64      0.69      0.67       249\n",
      "         radikalisme       0.78      0.75      0.77       302\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       508\n",
      "\n",
      "           micro avg       0.76      0.78      0.77      1401\n",
      "           macro avg       0.76      0.77      0.77      1401\n",
      "        weighted avg       0.77      0.78      0.77      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 0.00022649765014648438 seconds\n",
      "\n",
      "Fold 4 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4035, Accuracy: 0.8745, F1 Micro: 0.6898, F1 Macro: 0.6842\n",
      "Epoch 2/10, Train Loss: 0.2642, Accuracy: 0.8905, F1 Micro: 0.759, F1 Macro: 0.7576\n",
      "Epoch 3/10, Train Loss: 0.2153, Accuracy: 0.8958, F1 Micro: 0.7702, F1 Macro: 0.7684\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9003, F1 Micro: 0.7734, F1 Macro: 0.7659\n",
      "Epoch 5/10, Train Loss: 0.124, Accuracy: 0.8939, F1 Micro: 0.7363, F1 Macro: 0.7244\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.8945, F1 Micro: 0.7675, F1 Macro: 0.7623\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.8889, F1 Micro: 0.7612, F1 Macro: 0.7596\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.8922, F1 Micro: 0.7637, F1 Macro: 0.7625\n",
      "Epoch 9/10, Train Loss: 0.0385, Accuracy: 0.8983, F1 Micro: 0.7664, F1 Macro: 0.7651\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9, F1 Micro: 0.7739, F1 Macro: 0.7728\n",
      "Best result for 3712 samples: F1 Micro: 0.7739\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       342\n",
      "                sara       0.62      0.69      0.66       249\n",
      "         radikalisme       0.81      0.77      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.74      0.73       508\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1401\n",
      "           macro avg       0.77      0.78      0.77      1401\n",
      "        weighted avg       0.77      0.78      0.78      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 174\n",
      "Sampling duration: 0.0634610652923584 seconds\n",
      "\n",
      "Fold 4 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8773, F1 Micro: 0.7181, F1 Macro: 0.7098\n",
      "Epoch 2/10, Train Loss: 0.2506, Accuracy: 0.8956, F1 Micro: 0.759, F1 Macro: 0.7562\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.903, F1 Micro: 0.7754, F1 Macro: 0.7746\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.8997, F1 Micro: 0.7762, F1 Macro: 0.7709\n",
      "Epoch 5/10, Train Loss: 0.1184, Accuracy: 0.8922, F1 Micro: 0.7661, F1 Macro: 0.7664\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.8913, F1 Micro: 0.756, F1 Macro: 0.753\n",
      "Epoch 7/10, Train Loss: 0.0642, Accuracy: 0.8967, F1 Micro: 0.7694, F1 Macro: 0.7686\n",
      "Epoch 8/10, Train Loss: 0.0495, Accuracy: 0.8944, F1 Micro: 0.7716, F1 Macro: 0.7701\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.8958, F1 Micro: 0.7726, F1 Macro: 0.7728\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.8917, F1 Micro: 0.7671, F1 Macro: 0.7647\n",
      "Best result for 3886 samples: F1 Micro: 0.7762\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.89      0.91       342\n",
      "                sara       0.66      0.65      0.65       249\n",
      "         radikalisme       0.80      0.74      0.77       302\n",
      "pencemaran_nama_baik       0.69      0.84      0.76       508\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1401\n",
      "           macro avg       0.77      0.78      0.77      1401\n",
      "        weighted avg       0.77      0.79      0.78      1401\n",
      "         samples avg       0.45      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 0.00019311904907226562 seconds\n",
      "\n",
      "Fold 4 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8777, F1 Micro: 0.7152, F1 Macro: 0.7072\n",
      "Epoch 2/10, Train Loss: 0.2467, Accuracy: 0.8792, F1 Micro: 0.7473, F1 Macro: 0.7484\n",
      "Epoch 3/10, Train Loss: 0.2024, Accuracy: 0.8988, F1 Micro: 0.7562, F1 Macro: 0.7516\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9019, F1 Micro: 0.7726, F1 Macro: 0.7664\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.8986, F1 Micro: 0.7763, F1 Macro: 0.7741\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.898, F1 Micro: 0.7741, F1 Macro: 0.7717\n",
      "Epoch 7/10, Train Loss: 0.0625, Accuracy: 0.8958, F1 Micro: 0.766, F1 Macro: 0.7635\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8988, F1 Micro: 0.7616, F1 Macro: 0.755\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.8997, F1 Micro: 0.7725, F1 Macro: 0.7742\n",
      "Epoch 10/10, Train Loss: 0.0257, Accuracy: 0.9005, F1 Micro: 0.7683, F1 Macro: 0.7642\n",
      "Best result for 4120 samples: F1 Micro: 0.7763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.58      0.74      0.65       249\n",
      "         radikalisme       0.77      0.80      0.78       302\n",
      "pencemaran_nama_baik       0.74      0.78      0.76       508\n",
      "\n",
      "           micro avg       0.75      0.80      0.78      1401\n",
      "           macro avg       0.75      0.80      0.77      1401\n",
      "        weighted avg       0.76      0.80      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 0.0002033710479736328 seconds\n",
      "\n",
      "Fold 4 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3844, Accuracy: 0.8791, F1 Micro: 0.696, F1 Macro: 0.687\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.898, F1 Micro: 0.7474, F1 Macro: 0.7403\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.8972, F1 Micro: 0.774, F1 Macro: 0.7743\n",
      "Epoch 4/10, Train Loss: 0.1627, Accuracy: 0.8969, F1 Micro: 0.7729, F1 Macro: 0.7739\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.8991, F1 Micro: 0.762, F1 Macro: 0.7614\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.8959, F1 Micro: 0.7707, F1 Macro: 0.7724\n",
      "Epoch 7/10, Train Loss: 0.0606, Accuracy: 0.9025, F1 Micro: 0.7797, F1 Macro: 0.7774\n",
      "Epoch 8/10, Train Loss: 0.049, Accuracy: 0.9, F1 Micro: 0.7745, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0383, Accuracy: 0.9011, F1 Micro: 0.7743, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.0323, Accuracy: 0.9014, F1 Micro: 0.7741, F1 Macro: 0.772\n",
      "Best result for 4330 samples: F1 Micro: 0.7797\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       342\n",
      "                sara       0.66      0.68      0.67       249\n",
      "         radikalisme       0.80      0.77      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       508\n",
      "\n",
      "           micro avg       0.77      0.79      0.78      1401\n",
      "           macro avg       0.77      0.78      0.78      1401\n",
      "        weighted avg       0.77      0.79      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001952648162841797 seconds\n",
      "\n",
      "Fold 4 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3778, Accuracy: 0.8789, F1 Micro: 0.6974, F1 Macro: 0.6938\n",
      "Epoch 2/10, Train Loss: 0.2426, Accuracy: 0.8928, F1 Micro: 0.7684, F1 Macro: 0.7662\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.9028, F1 Micro: 0.7701, F1 Macro: 0.7671\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9025, F1 Micro: 0.7562, F1 Macro: 0.7444\n",
      "Epoch 5/10, Train Loss: 0.1177, Accuracy: 0.8981, F1 Micro: 0.7541, F1 Macro: 0.7479\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9017, F1 Micro: 0.7728, F1 Macro: 0.772\n",
      "Epoch 7/10, Train Loss: 0.0636, Accuracy: 0.9013, F1 Micro: 0.774, F1 Macro: 0.7684\n",
      "Epoch 8/10, Train Loss: 0.0467, Accuracy: 0.9019, F1 Micro: 0.7731, F1 Macro: 0.7733\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.9006, F1 Micro: 0.7709, F1 Macro: 0.7692\n",
      "Epoch 10/10, Train Loss: 0.0288, Accuracy: 0.8969, F1 Micro: 0.7716, F1 Macro: 0.7712\n",
      "Best result for 4530 samples: F1 Micro: 0.774\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.91      0.91       342\n",
      "                sara       0.67      0.61      0.64       249\n",
      "         radikalisme       0.79      0.78      0.79       302\n",
      "pencemaran_nama_baik       0.73      0.75      0.74       508\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1401\n",
      "           macro avg       0.77      0.76      0.77      1401\n",
      "        weighted avg       0.77      0.77      0.77      1401\n",
      "         samples avg       0.45      0.45      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 0.08175063133239746 seconds\n",
      "\n",
      "Fold 4 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.8788, F1 Micro: 0.7054, F1 Macro: 0.6936\n",
      "Epoch 2/10, Train Loss: 0.2392, Accuracy: 0.8997, F1 Micro: 0.7555, F1 Macro: 0.7479\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.8983, F1 Micro: 0.7646, F1 Macro: 0.7616\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.8994, F1 Micro: 0.7651, F1 Macro: 0.7641\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.8986, F1 Micro: 0.7732, F1 Macro: 0.7696\n",
      "Epoch 6/10, Train Loss: 0.083, Accuracy: 0.8961, F1 Micro: 0.7661, F1 Macro: 0.7637\n",
      "Epoch 7/10, Train Loss: 0.0645, Accuracy: 0.8975, F1 Micro: 0.7733, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.045, Accuracy: 0.9028, F1 Micro: 0.7771, F1 Macro: 0.7728\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9003, F1 Micro: 0.77, F1 Macro: 0.7676\n",
      "Epoch 10/10, Train Loss: 0.0273, Accuracy: 0.8973, F1 Micro: 0.7687, F1 Macro: 0.7657\n",
      "Best result for 4663 samples: F1 Micro: 0.7771\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.70      0.61      0.65       249\n",
      "         radikalisme       0.79      0.80      0.80       302\n",
      "pencemaran_nama_baik       0.72      0.76      0.74       508\n",
      "\n",
      "           micro avg       0.78      0.77      0.78      1401\n",
      "           macro avg       0.78      0.77      0.77      1401\n",
      "        weighted avg       0.78      0.77      0.78      1401\n",
      "         samples avg       0.46      0.45      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00018453598022460938 seconds\n",
      "\n",
      "Fold 4 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3815, Accuracy: 0.8856, F1 Micro: 0.7129, F1 Macro: 0.6918\n",
      "Epoch 2/10, Train Loss: 0.2423, Accuracy: 0.9017, F1 Micro: 0.7683, F1 Macro: 0.7646\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9016, F1 Micro: 0.7686, F1 Macro: 0.7618\n",
      "Epoch 4/10, Train Loss: 0.1615, Accuracy: 0.9019, F1 Micro: 0.7607, F1 Macro: 0.7537\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.8898, F1 Micro: 0.7654, F1 Macro: 0.7652\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9009, F1 Micro: 0.7678, F1 Macro: 0.761\n",
      "Epoch 7/10, Train Loss: 0.066, Accuracy: 0.8981, F1 Micro: 0.7719, F1 Macro: 0.7693\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.897, F1 Micro: 0.769, F1 Macro: 0.7685\n",
      "Epoch 9/10, Train Loss: 0.04, Accuracy: 0.8997, F1 Micro: 0.7757, F1 Macro: 0.7753\n",
      "Epoch 10/10, Train Loss: 0.0283, Accuracy: 0.8998, F1 Micro: 0.7678, F1 Macro: 0.7686\n",
      "Best result for 4863 samples: F1 Micro: 0.7757\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       342\n",
      "                sara       0.66      0.70      0.68       249\n",
      "         radikalisme       0.79      0.78      0.79       302\n",
      "pencemaran_nama_baik       0.71      0.77      0.74       508\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1401\n",
      "           macro avg       0.76      0.79      0.78      1401\n",
      "        weighted avg       0.76      0.79      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00020003318786621094 seconds\n",
      "\n",
      "Fold 4 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3616, Accuracy: 0.8786, F1 Micro: 0.7329, F1 Macro: 0.7318\n",
      "Epoch 2/10, Train Loss: 0.245, Accuracy: 0.8991, F1 Micro: 0.7635, F1 Macro: 0.759\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.8916, F1 Micro: 0.7685, F1 Macro: 0.7677\n",
      "Epoch 4/10, Train Loss: 0.159, Accuracy: 0.9011, F1 Micro: 0.7758, F1 Macro: 0.7732\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.8997, F1 Micro: 0.7786, F1 Macro: 0.7765\n",
      "Epoch 6/10, Train Loss: 0.0835, Accuracy: 0.8917, F1 Micro: 0.7703, F1 Macro: 0.7733\n",
      "Epoch 7/10, Train Loss: 0.0608, Accuracy: 0.8998, F1 Micro: 0.7777, F1 Macro: 0.777\n",
      "Epoch 8/10, Train Loss: 0.0438, Accuracy: 0.897, F1 Micro: 0.7708, F1 Macro: 0.7711\n",
      "Epoch 9/10, Train Loss: 0.0346, Accuracy: 0.8978, F1 Micro: 0.7759, F1 Macro: 0.7748\n",
      "Epoch 10/10, Train Loss: 0.0309, Accuracy: 0.9011, F1 Micro: 0.7635, F1 Macro: 0.7607\n",
      "Best result for 5063 samples: F1 Micro: 0.7786\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.90      0.91       342\n",
      "                sara       0.63      0.69      0.66       249\n",
      "         radikalisme       0.78      0.80      0.79       302\n",
      "pencemaran_nama_baik       0.70      0.81      0.75       508\n",
      "\n",
      "           micro avg       0.75      0.81      0.78      1401\n",
      "           macro avg       0.76      0.80      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0003628730773925781 seconds\n",
      "\n",
      "Fold 4 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3656, Accuracy: 0.8855, F1 Micro: 0.7396, F1 Macro: 0.7332\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.903, F1 Micro: 0.7607, F1 Macro: 0.7541\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9002, F1 Micro: 0.7827, F1 Macro: 0.7816\n",
      "Epoch 4/10, Train Loss: 0.1551, Accuracy: 0.9039, F1 Micro: 0.7699, F1 Macro: 0.7618\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9016, F1 Micro: 0.7811, F1 Macro: 0.7775\n",
      "Epoch 6/10, Train Loss: 0.085, Accuracy: 0.8981, F1 Micro: 0.773, F1 Macro: 0.7729\n",
      "Epoch 7/10, Train Loss: 0.0622, Accuracy: 0.9025, F1 Micro: 0.7757, F1 Macro: 0.7704\n",
      "Epoch 8/10, Train Loss: 0.0485, Accuracy: 0.8988, F1 Micro: 0.7667, F1 Macro: 0.7606\n",
      "Epoch 9/10, Train Loss: 0.0329, Accuracy: 0.8969, F1 Micro: 0.7734, F1 Macro: 0.7731\n",
      "Epoch 10/10, Train Loss: 0.029, Accuracy: 0.9017, F1 Micro: 0.7704, F1 Macro: 0.7662\n",
      "Best result for 5263 samples: F1 Micro: 0.7827\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.93      0.90       342\n",
      "                sara       0.64      0.73      0.69       249\n",
      "         radikalisme       0.73      0.86      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.77      0.74       508\n",
      "\n",
      "           micro avg       0.75      0.82      0.78      1401\n",
      "           macro avg       0.74      0.82      0.78      1401\n",
      "        weighted avg       0.75      0.82      0.78      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.09181547164916992 seconds\n",
      "\n",
      "Fold 4 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3665, Accuracy: 0.8852, F1 Micro: 0.7332, F1 Macro: 0.728\n",
      "Epoch 2/10, Train Loss: 0.229, Accuracy: 0.8913, F1 Micro: 0.7669, F1 Macro: 0.7684\n",
      "Epoch 3/10, Train Loss: 0.1903, Accuracy: 0.9019, F1 Micro: 0.7827, F1 Macro: 0.7798\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.8995, F1 Micro: 0.7699, F1 Macro: 0.7648\n",
      "Epoch 5/10, Train Loss: 0.1119, Accuracy: 0.8927, F1 Micro: 0.7724, F1 Macro: 0.7727\n",
      "Epoch 6/10, Train Loss: 0.0827, Accuracy: 0.8938, F1 Micro: 0.7649, F1 Macro: 0.7595\n",
      "Epoch 7/10, Train Loss: 0.0565, Accuracy: 0.8978, F1 Micro: 0.7779, F1 Macro: 0.7789\n",
      "Epoch 8/10, Train Loss: 0.0431, Accuracy: 0.9003, F1 Micro: 0.7697, F1 Macro: 0.7653\n",
      "Epoch 9/10, Train Loss: 0.0312, Accuracy: 0.8966, F1 Micro: 0.7676, F1 Macro: 0.7641\n",
      "Epoch 10/10, Train Loss: 0.0241, Accuracy: 0.9014, F1 Micro: 0.7765, F1 Macro: 0.7746\n",
      "Best result for 5441 samples: F1 Micro: 0.7827\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.88      0.90       342\n",
      "                sara       0.66      0.67      0.67       249\n",
      "         radikalisme       0.76      0.84      0.80       302\n",
      "pencemaran_nama_baik       0.71      0.80      0.76       508\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1401\n",
      "           macro avg       0.76      0.80      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.78      1401\n",
      "         samples avg       0.46      0.46      0.45      1401\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002148151397705078 seconds\n",
      "\n",
      "Fold 4 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3529, Accuracy: 0.8903, F1 Micro: 0.7185, F1 Macro: 0.7032\n",
      "Epoch 2/10, Train Loss: 0.2345, Accuracy: 0.9013, F1 Micro: 0.7676, F1 Macro: 0.7631\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9045, F1 Micro: 0.7797, F1 Macro: 0.7696\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.9038, F1 Micro: 0.7708, F1 Macro: 0.7679\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9006, F1 Micro: 0.7808, F1 Macro: 0.7814\n",
      "Epoch 6/10, Train Loss: 0.0851, Accuracy: 0.9011, F1 Micro: 0.7699, F1 Macro: 0.7677\n",
      "Epoch 7/10, Train Loss: 0.0632, Accuracy: 0.8991, F1 Micro: 0.7754, F1 Macro: 0.7736\n",
      "Epoch 8/10, Train Loss: 0.0448, Accuracy: 0.9016, F1 Micro: 0.7789, F1 Macro: 0.7768\n",
      "Epoch 9/10, Train Loss: 0.0327, Accuracy: 0.8989, F1 Micro: 0.7815, F1 Macro: 0.781\n",
      "Epoch 10/10, Train Loss: 0.0266, Accuracy: 0.9033, F1 Micro: 0.7725, F1 Macro: 0.77\n",
      "Best result for 5641 samples: F1 Micro: 0.7815\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       342\n",
      "                sara       0.63      0.71      0.67       249\n",
      "         radikalisme       0.81      0.79      0.80       302\n",
      "pencemaran_nama_baik       0.67      0.84      0.75       508\n",
      "\n",
      "           micro avg       0.74      0.83      0.78      1401\n",
      "           macro avg       0.75      0.81      0.78      1401\n",
      "        weighted avg       0.75      0.83      0.78      1401\n",
      "         samples avg       0.47      0.48      0.47      1401\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017976760864257812 seconds\n",
      "\n",
      "Fold 4 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3594, Accuracy: 0.8964, F1 Micro: 0.7465, F1 Macro: 0.7336\n",
      "Epoch 2/10, Train Loss: 0.2292, Accuracy: 0.9016, F1 Micro: 0.7644, F1 Macro: 0.7613\n",
      "Epoch 3/10, Train Loss: 0.1934, Accuracy: 0.8961, F1 Micro: 0.7719, F1 Macro: 0.7694\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.9025, F1 Micro: 0.7714, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.8991, F1 Micro: 0.7627, F1 Macro: 0.7595\n",
      "Epoch 6/10, Train Loss: 0.0813, Accuracy: 0.895, F1 Micro: 0.771, F1 Macro: 0.767\n",
      "Epoch 7/10, Train Loss: 0.0585, Accuracy: 0.8969, F1 Micro: 0.7721, F1 Macro: 0.7683\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9, F1 Micro: 0.7795, F1 Macro: 0.7772\n",
      "Epoch 9/10, Train Loss: 0.0326, Accuracy: 0.9017, F1 Micro: 0.7839, F1 Macro: 0.7843\n",
      "Epoch 10/10, Train Loss: 0.0263, Accuracy: 0.9031, F1 Micro: 0.7771, F1 Macro: 0.7784\n",
      "Best result for 5841 samples: F1 Micro: 0.7839\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.91       342\n",
      "                sara       0.62      0.73      0.67       249\n",
      "         radikalisme       0.78      0.86      0.81       302\n",
      "pencemaran_nama_baik       0.72      0.77      0.75       508\n",
      "\n",
      "           micro avg       0.76      0.81      0.78      1401\n",
      "           macro avg       0.76      0.82      0.78      1401\n",
      "        weighted avg       0.76      0.81      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017952919006347656 seconds\n",
      "\n",
      "Fold 4 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3531, Accuracy: 0.8911, F1 Micro: 0.7481, F1 Macro: 0.741\n",
      "Epoch 2/10, Train Loss: 0.2372, Accuracy: 0.9031, F1 Micro: 0.7752, F1 Macro: 0.769\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.9019, F1 Micro: 0.7688, F1 Macro: 0.7713\n",
      "Epoch 4/10, Train Loss: 0.152, Accuracy: 0.9025, F1 Micro: 0.7739, F1 Macro: 0.7645\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9002, F1 Micro: 0.7768, F1 Macro: 0.776\n",
      "Epoch 6/10, Train Loss: 0.0835, Accuracy: 0.8989, F1 Micro: 0.7706, F1 Macro: 0.7666\n",
      "Epoch 7/10, Train Loss: 0.0588, Accuracy: 0.9042, F1 Micro: 0.7777, F1 Macro: 0.774\n",
      "Epoch 8/10, Train Loss: 0.0416, Accuracy: 0.9014, F1 Micro: 0.7721, F1 Macro: 0.7695\n",
      "Epoch 9/10, Train Loss: 0.0337, Accuracy: 0.8992, F1 Micro: 0.7699, F1 Macro: 0.7681\n",
      "Epoch 10/10, Train Loss: 0.027, Accuracy: 0.9017, F1 Micro: 0.7766, F1 Macro: 0.7751\n",
      "Best result for 6041 samples: F1 Micro: 0.7777\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.89      0.90       342\n",
      "                sara       0.70      0.61      0.65       249\n",
      "         radikalisme       0.80      0.81      0.81       302\n",
      "pencemaran_nama_baik       0.74      0.73      0.74       508\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1401\n",
      "           macro avg       0.79      0.76      0.77      1401\n",
      "        weighted avg       0.79      0.77      0.78      1401\n",
      "         samples avg       0.46      0.45      0.44      1401\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.10803484916687012 seconds\n",
      "\n",
      "Fold 4 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 4 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3465, Accuracy: 0.8892, F1 Micro: 0.751, F1 Macro: 0.7441\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9019, F1 Micro: 0.7855, F1 Macro: 0.7851\n",
      "Epoch 3/10, Train Loss: 0.1874, Accuracy: 0.9056, F1 Micro: 0.7738, F1 Macro: 0.773\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.903, F1 Micro: 0.7544, F1 Macro: 0.7442\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.8977, F1 Micro: 0.7761, F1 Macro: 0.7735\n",
      "Epoch 6/10, Train Loss: 0.0806, Accuracy: 0.8994, F1 Micro: 0.7739, F1 Macro: 0.7721\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.8997, F1 Micro: 0.7735, F1 Macro: 0.7702\n",
      "Epoch 8/10, Train Loss: 0.0438, Accuracy: 0.9023, F1 Micro: 0.7781, F1 Macro: 0.7746\n",
      "Epoch 9/10, Train Loss: 0.0325, Accuracy: 0.8948, F1 Micro: 0.7723, F1 Macro: 0.7706\n",
      "Epoch 10/10, Train Loss: 0.0257, Accuracy: 0.9008, F1 Micro: 0.7791, F1 Macro: 0.7758\n",
      "Best result for 6219 samples: F1 Micro: 0.7855\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.88      0.92      0.90       342\n",
      "                sara       0.65      0.74      0.70       249\n",
      "         radikalisme       0.75      0.84      0.79       302\n",
      "pencemaran_nama_baik       0.72      0.78      0.75       508\n",
      "\n",
      "           micro avg       0.75      0.82      0.79      1401\n",
      "           macro avg       0.75      0.82      0.79      1401\n",
      "        weighted avg       0.76      0.82      0.79      1401\n",
      "         samples avg       0.46      0.47      0.46      1401\n",
      "\n",
      "\n",
      "FOLD 4 COMPLETED in 3648.02 seconds\n",
      "===============================================\n",
      "STARTING FOLD 5/5\n",
      "===============================================\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 388 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.7891, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Epoch 2/10, Train Loss: 0.4673, Accuracy: 0.7939, F1 Micro: 0.0449, F1 Macro: 0.0404\n",
      "Epoch 3/10, Train Loss: 0.421, Accuracy: 0.8194, F1 Micro: 0.2561, F1 Macro: 0.1786\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8313, F1 Micro: 0.3933, F1 Macro: 0.3032\n",
      "Epoch 5/10, Train Loss: 0.3375, Accuracy: 0.8416, F1 Micro: 0.4483, F1 Macro: 0.3709\n",
      "Epoch 6/10, Train Loss: 0.2901, Accuracy: 0.8577, F1 Micro: 0.5902, F1 Macro: 0.5609\n",
      "Epoch 7/10, Train Loss: 0.2547, Accuracy: 0.8578, F1 Micro: 0.5852, F1 Macro: 0.5546\n",
      "Epoch 8/10, Train Loss: 0.2265, Accuracy: 0.867, F1 Micro: 0.6411, F1 Macro: 0.6284\n",
      "Epoch 9/10, Train Loss: 0.174, Accuracy: 0.8655, F1 Micro: 0.6682, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.1497, Accuracy: 0.8669, F1 Micro: 0.6611, F1 Macro: 0.6529\n",
      "Best result for 388 samples: F1 Micro: 0.6682\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.87      0.82      0.84       353\n",
      "                sara       0.54      0.56      0.55       239\n",
      "         radikalisme       0.69      0.58      0.63       273\n",
      "pencemaran_nama_baik       0.66      0.59      0.62       485\n",
      "\n",
      "           micro avg       0.70      0.64      0.67      1350\n",
      "           macro avg       0.69      0.64      0.66      1350\n",
      "        weighted avg       0.70      0.64      0.67      1350\n",
      "         samples avg       0.36      0.36      0.35      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 584\n",
      "Sampling duration: 0.00040340423583984375 seconds\n",
      "\n",
      "Fold 5 - New train size: 972\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 972 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5008, Accuracy: 0.7997, F1 Micro: 0.0959, F1 Macro: 0.0808\n",
      "Epoch 2/10, Train Loss: 0.377, Accuracy: 0.8453, F1 Micro: 0.4795, F1 Macro: 0.4082\n",
      "Epoch 3/10, Train Loss: 0.3034, Accuracy: 0.8675, F1 Micro: 0.6391, F1 Macro: 0.6284\n",
      "Epoch 4/10, Train Loss: 0.2527, Accuracy: 0.8753, F1 Micro: 0.7062, F1 Macro: 0.7034\n",
      "Epoch 5/10, Train Loss: 0.205, Accuracy: 0.8777, F1 Micro: 0.7203, F1 Macro: 0.7168\n",
      "Epoch 6/10, Train Loss: 0.1669, Accuracy: 0.8866, F1 Micro: 0.7094, F1 Macro: 0.7018\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.8834, F1 Micro: 0.709, F1 Macro: 0.7016\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.8811, F1 Micro: 0.6841, F1 Macro: 0.6758\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.8819, F1 Micro: 0.7049, F1 Macro: 0.7001\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.882, F1 Micro: 0.6889, F1 Macro: 0.6775\n",
      "Best result for 972 samples: F1 Micro: 0.7203\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.84      0.86       353\n",
      "                sara       0.54      0.66      0.59       239\n",
      "         radikalisme       0.69      0.77      0.72       273\n",
      "pencemaran_nama_baik       0.67      0.71      0.69       485\n",
      "\n",
      "           micro avg       0.70      0.75      0.72      1350\n",
      "           macro avg       0.69      0.74      0.72      1350\n",
      "        weighted avg       0.71      0.75      0.72      1350\n",
      "         samples avg       0.40      0.42      0.40      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 525\n",
      "Sampling duration: 0.0006225109100341797 seconds\n",
      "\n",
      "Fold 5 - New train size: 1497\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1497 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4726, Accuracy: 0.8281, F1 Micro: 0.3373, F1 Macro: 0.2703\n",
      "Epoch 2/10, Train Loss: 0.3398, Accuracy: 0.8702, F1 Micro: 0.6664, F1 Macro: 0.6505\n",
      "Epoch 3/10, Train Loss: 0.2623, Accuracy: 0.8823, F1 Micro: 0.7185, F1 Macro: 0.7124\n",
      "Epoch 4/10, Train Loss: 0.2206, Accuracy: 0.883, F1 Micro: 0.7393, F1 Macro: 0.7383\n",
      "Epoch 5/10, Train Loss: 0.1776, Accuracy: 0.8855, F1 Micro: 0.7304, F1 Macro: 0.7282\n",
      "Epoch 6/10, Train Loss: 0.126, Accuracy: 0.89, F1 Micro: 0.722, F1 Macro: 0.7171\n",
      "Epoch 7/10, Train Loss: 0.114, Accuracy: 0.8858, F1 Micro: 0.7407, F1 Macro: 0.7413\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.887, F1 Micro: 0.7421, F1 Macro: 0.7442\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.8878, F1 Micro: 0.7303, F1 Macro: 0.7284\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.8848, F1 Micro: 0.7287, F1 Macro: 0.7284\n",
      "Best result for 1497 samples: F1 Micro: 0.7421\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.85      0.88       353\n",
      "                sara       0.59      0.72      0.65       239\n",
      "         radikalisme       0.72      0.81      0.77       273\n",
      "pencemaran_nama_baik       0.66      0.72      0.69       485\n",
      "\n",
      "           micro avg       0.72      0.77      0.74      1350\n",
      "           macro avg       0.72      0.77      0.74      1350\n",
      "        weighted avg       0.73      0.77      0.75      1350\n",
      "         samples avg       0.42      0.43      0.41      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 473\n",
      "Sampling duration: 0.00040721893310546875 seconds\n",
      "\n",
      "Fold 5 - New train size: 1970\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 1970 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4655, Accuracy: 0.8541, F1 Micro: 0.5565, F1 Macro: 0.486\n",
      "Epoch 2/10, Train Loss: 0.3195, Accuracy: 0.8794, F1 Micro: 0.6897, F1 Macro: 0.684\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.8925, F1 Micro: 0.738, F1 Macro: 0.7368\n",
      "Epoch 4/10, Train Loss: 0.2068, Accuracy: 0.8828, F1 Micro: 0.7473, F1 Macro: 0.7499\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.8969, F1 Micro: 0.7535, F1 Macro: 0.7525\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.8964, F1 Micro: 0.7495, F1 Macro: 0.7471\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.8961, F1 Micro: 0.7529, F1 Macro: 0.7507\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.895, F1 Micro: 0.7453, F1 Macro: 0.7401\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.8959, F1 Micro: 0.7462, F1 Macro: 0.7431\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.8978, F1 Micro: 0.7562, F1 Macro: 0.756\n",
      "Best result for 1970 samples: F1 Micro: 0.7562\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.88      0.90       353\n",
      "                sara       0.65      0.67      0.66       239\n",
      "         radikalisme       0.74      0.81      0.78       273\n",
      "pencemaran_nama_baik       0.72      0.66      0.69       485\n",
      "\n",
      "           micro avg       0.76      0.75      0.76      1350\n",
      "           macro avg       0.76      0.76      0.76      1350\n",
      "        weighted avg       0.76      0.75      0.76      1350\n",
      "         samples avg       0.43      0.42      0.41      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 425\n",
      "Sampling duration: 0.0003609657287597656 seconds\n",
      "\n",
      "Fold 5 - New train size: 2395\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2395 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4564, Accuracy: 0.8587, F1 Micro: 0.595, F1 Macro: 0.5187\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.8825, F1 Micro: 0.688, F1 Macro: 0.6826\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.8948, F1 Micro: 0.7461, F1 Macro: 0.7412\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.8959, F1 Micro: 0.753, F1 Macro: 0.748\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.8975, F1 Micro: 0.7508, F1 Macro: 0.7424\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.8998, F1 Micro: 0.76, F1 Macro: 0.7581\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.8945, F1 Micro: 0.7454, F1 Macro: 0.7402\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.8884, F1 Micro: 0.7394, F1 Macro: 0.7315\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.8938, F1 Micro: 0.7507, F1 Macro: 0.7454\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.8942, F1 Micro: 0.7516, F1 Macro: 0.7498\n",
      "Best result for 2395 samples: F1 Micro: 0.76\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       353\n",
      "                sara       0.65      0.64      0.64       239\n",
      "         radikalisme       0.79      0.78      0.79       273\n",
      "pencemaran_nama_baik       0.71      0.68      0.70       485\n",
      "\n",
      "           micro avg       0.77      0.75      0.76      1350\n",
      "           macro avg       0.77      0.75      0.76      1350\n",
      "        weighted avg       0.77      0.75      0.76      1350\n",
      "         samples avg       0.42      0.42      0.41      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 383\n",
      "Sampling duration: 0.0003757476806640625 seconds\n",
      "\n",
      "Fold 5 - New train size: 2778\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 2778 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4329, Accuracy: 0.8677, F1 Micro: 0.621, F1 Macro: 0.5948\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.8939, F1 Micro: 0.7369, F1 Macro: 0.726\n",
      "Epoch 3/10, Train Loss: 0.2195, Accuracy: 0.8972, F1 Micro: 0.763, F1 Macro: 0.762\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.8964, F1 Micro: 0.7435, F1 Macro: 0.7331\n",
      "Epoch 5/10, Train Loss: 0.1335, Accuracy: 0.8981, F1 Micro: 0.7585, F1 Macro: 0.7586\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.9006, F1 Micro: 0.7614, F1 Macro: 0.7558\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.8927, F1 Micro: 0.7607, F1 Macro: 0.7598\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9006, F1 Micro: 0.7607, F1 Macro: 0.7549\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9011, F1 Micro: 0.7559, F1 Macro: 0.747\n",
      "Epoch 10/10, Train Loss: 0.0381, Accuracy: 0.8963, F1 Micro: 0.7557, F1 Macro: 0.7523\n",
      "Best result for 2778 samples: F1 Micro: 0.763\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.90      0.90       353\n",
      "                sara       0.60      0.70      0.65       239\n",
      "         radikalisme       0.77      0.81      0.79       273\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       485\n",
      "\n",
      "           micro avg       0.74      0.78      0.76      1350\n",
      "           macro avg       0.74      0.78      0.76      1350\n",
      "        weighted avg       0.75      0.78      0.76      1350\n",
      "         samples avg       0.43      0.43      0.42      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 345\n",
      "Sampling duration: 0.00035309791564941406 seconds\n",
      "\n",
      "Fold 5 - New train size: 3123\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3123 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4208, Accuracy: 0.8736, F1 Micro: 0.6622, F1 Macro: 0.6562\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.8931, F1 Micro: 0.7246, F1 Macro: 0.6996\n",
      "Epoch 3/10, Train Loss: 0.2136, Accuracy: 0.9009, F1 Micro: 0.7556, F1 Macro: 0.7526\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9028, F1 Micro: 0.7665, F1 Macro: 0.7617\n",
      "Epoch 5/10, Train Loss: 0.1299, Accuracy: 0.9006, F1 Micro: 0.7598, F1 Macro: 0.7561\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9025, F1 Micro: 0.7484, F1 Macro: 0.7376\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.8986, F1 Micro: 0.7415, F1 Macro: 0.7304\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.8998, F1 Micro: 0.7602, F1 Macro: 0.7544\n",
      "Epoch 9/10, Train Loss: 0.0454, Accuracy: 0.9011, F1 Micro: 0.7663, F1 Macro: 0.7596\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.8998, F1 Micro: 0.7614, F1 Macro: 0.7563\n",
      "Best result for 3123 samples: F1 Micro: 0.7665\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.87      0.90       353\n",
      "                sara       0.67      0.59      0.63       239\n",
      "         radikalisme       0.78      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.72      0.72      0.72       485\n",
      "\n",
      "           micro avg       0.78      0.76      0.77      1350\n",
      "           macro avg       0.77      0.75      0.76      1350\n",
      "        weighted avg       0.78      0.76      0.77      1350\n",
      "         samples avg       0.43      0.43      0.42      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 310\n",
      "Sampling duration: 0.0002880096435546875 seconds\n",
      "\n",
      "Fold 5 - New train size: 3433\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3433 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4105, Accuracy: 0.8703, F1 Micro: 0.6337, F1 Macro: 0.6209\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.8991, F1 Micro: 0.7519, F1 Macro: 0.7493\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.8966, F1 Micro: 0.7354, F1 Macro: 0.7272\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.8988, F1 Micro: 0.7667, F1 Macro: 0.7659\n",
      "Epoch 5/10, Train Loss: 0.1316, Accuracy: 0.8964, F1 Micro: 0.7565, F1 Macro: 0.7481\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9017, F1 Micro: 0.7666, F1 Macro: 0.7606\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.8989, F1 Micro: 0.7596, F1 Macro: 0.7514\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9031, F1 Micro: 0.7644, F1 Macro: 0.7576\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.8977, F1 Micro: 0.7582, F1 Macro: 0.7519\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.8989, F1 Micro: 0.7585, F1 Macro: 0.7527\n",
      "Best result for 3433 samples: F1 Micro: 0.7667\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       353\n",
      "                sara       0.59      0.71      0.64       239\n",
      "         radikalisme       0.79      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.72      0.71       485\n",
      "\n",
      "           micro avg       0.75      0.79      0.77      1350\n",
      "           macro avg       0.74      0.79      0.77      1350\n",
      "        weighted avg       0.75      0.79      0.77      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 279\n",
      "Sampling duration: 0.0002110004425048828 seconds\n",
      "\n",
      "Fold 5 - New train size: 3712\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3712 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8811, F1 Micro: 0.6947, F1 Macro: 0.6866\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.8917, F1 Micro: 0.7271, F1 Macro: 0.7186\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9039, F1 Micro: 0.7585, F1 Macro: 0.7435\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.8978, F1 Micro: 0.7674, F1 Macro: 0.7666\n",
      "Epoch 5/10, Train Loss: 0.1333, Accuracy: 0.8986, F1 Micro: 0.772, F1 Macro: 0.7716\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.8995, F1 Micro: 0.7535, F1 Macro: 0.7477\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.8984, F1 Micro: 0.7612, F1 Macro: 0.7546\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.8955, F1 Micro: 0.7645, F1 Macro: 0.7629\n",
      "Epoch 9/10, Train Loss: 0.0413, Accuracy: 0.903, F1 Micro: 0.7583, F1 Macro: 0.7512\n",
      "Epoch 10/10, Train Loss: 0.0371, Accuracy: 0.9017, F1 Micro: 0.7656, F1 Macro: 0.7612\n",
      "Best result for 3712 samples: F1 Micro: 0.772\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.92      0.91       353\n",
      "                sara       0.64      0.68      0.66       239\n",
      "         radikalisme       0.76      0.85      0.80       273\n",
      "pencemaran_nama_baik       0.66      0.79      0.72       485\n",
      "\n",
      "           micro avg       0.73      0.81      0.77      1350\n",
      "           macro avg       0.74      0.81      0.77      1350\n",
      "        weighted avg       0.74      0.81      0.77      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 3886\n",
      "Acquired samples: 174\n",
      "Sampling duration: 0.06733846664428711 seconds\n",
      "\n",
      "Fold 5 - New train size: 3886\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 3886 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3971, Accuracy: 0.8789, F1 Micro: 0.6906, F1 Macro: 0.6787\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9013, F1 Micro: 0.7645, F1 Macro: 0.7575\n",
      "Epoch 3/10, Train Loss: 0.2142, Accuracy: 0.8994, F1 Micro: 0.7765, F1 Macro: 0.7768\n",
      "Epoch 4/10, Train Loss: 0.1637, Accuracy: 0.8939, F1 Micro: 0.7675, F1 Macro: 0.7681\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.8992, F1 Micro: 0.7702, F1 Macro: 0.7683\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9053, F1 Micro: 0.7706, F1 Macro: 0.7672\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9031, F1 Micro: 0.7749, F1 Macro: 0.7731\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.9034, F1 Micro: 0.7692, F1 Macro: 0.7662\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9003, F1 Micro: 0.7705, F1 Macro: 0.7689\n",
      "Epoch 10/10, Train Loss: 0.0362, Accuracy: 0.9025, F1 Micro: 0.7627, F1 Macro: 0.7538\n",
      "Best result for 3886 samples: F1 Micro: 0.7765\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.90       353\n",
      "                sara       0.58      0.76      0.66       239\n",
      "         radikalisme       0.77      0.85      0.81       273\n",
      "pencemaran_nama_baik       0.68      0.80      0.73       485\n",
      "\n",
      "           micro avg       0.73      0.83      0.78      1350\n",
      "           macro avg       0.74      0.83      0.78      1350\n",
      "        weighted avg       0.74      0.83      0.78      1350\n",
      "         samples avg       0.45      0.46      0.45      1350\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 234\n",
      "Sampling duration: 0.00020647048950195312 seconds\n",
      "\n",
      "Fold 5 - New train size: 4120\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.878, F1 Micro: 0.7165, F1 Macro: 0.7118\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.8942, F1 Micro: 0.7562, F1 Macro: 0.7537\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.8995, F1 Micro: 0.7593, F1 Macro: 0.7496\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.8923, F1 Micro: 0.7667, F1 Macro: 0.7659\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9045, F1 Micro: 0.7679, F1 Macro: 0.7598\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9041, F1 Micro: 0.7764, F1 Macro: 0.7745\n",
      "Epoch 7/10, Train Loss: 0.0638, Accuracy: 0.9022, F1 Micro: 0.7661, F1 Macro: 0.7627\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.8966, F1 Micro: 0.7667, F1 Macro: 0.7645\n",
      "Epoch 9/10, Train Loss: 0.046, Accuracy: 0.902, F1 Micro: 0.7684, F1 Macro: 0.7618\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9008, F1 Micro: 0.769, F1 Macro: 0.7664\n",
      "Best result for 4120 samples: F1 Micro: 0.7764\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.91       353\n",
      "                sara       0.65      0.69      0.67       239\n",
      "         radikalisme       0.78      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.71      0.74      0.72       485\n",
      "\n",
      "           micro avg       0.76      0.79      0.78      1350\n",
      "           macro avg       0.76      0.79      0.77      1350\n",
      "        weighted avg       0.76      0.79      0.78      1350\n",
      "         samples avg       0.44      0.44      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 210\n",
      "Sampling duration: 0.0001888275146484375 seconds\n",
      "\n",
      "Fold 5 - New train size: 4330\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4330 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3856, Accuracy: 0.8817, F1 Micro: 0.6951, F1 Macro: 0.6853\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.8953, F1 Micro: 0.7227, F1 Macro: 0.7018\n",
      "Epoch 3/10, Train Loss: 0.2021, Accuracy: 0.9058, F1 Micro: 0.7701, F1 Macro: 0.7677\n",
      "Epoch 4/10, Train Loss: 0.1652, Accuracy: 0.9059, F1 Micro: 0.7648, F1 Macro: 0.762\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9047, F1 Micro: 0.7661, F1 Macro: 0.7574\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9056, F1 Micro: 0.7557, F1 Macro: 0.7422\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9034, F1 Micro: 0.7735, F1 Macro: 0.7734\n",
      "Epoch 8/10, Train Loss: 0.0512, Accuracy: 0.9041, F1 Micro: 0.7665, F1 Macro: 0.7611\n",
      "Epoch 9/10, Train Loss: 0.0422, Accuracy: 0.9055, F1 Micro: 0.7747, F1 Macro: 0.7722\n",
      "Epoch 10/10, Train Loss: 0.0385, Accuracy: 0.8994, F1 Micro: 0.7728, F1 Macro: 0.7713\n",
      "Best result for 4330 samples: F1 Micro: 0.7747\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.92      0.91      0.91       353\n",
      "                sara       0.68      0.67      0.68       239\n",
      "         radikalisme       0.76      0.81      0.78       273\n",
      "pencemaran_nama_baik       0.74      0.69      0.72       485\n",
      "\n",
      "           micro avg       0.78      0.77      0.77      1350\n",
      "           macro avg       0.77      0.77      0.77      1350\n",
      "        weighted avg       0.78      0.77      0.77      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00019884109497070312 seconds\n",
      "\n",
      "Fold 5 - New train size: 4530\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4530 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.8811, F1 Micro: 0.6979, F1 Macro: 0.697\n",
      "Epoch 2/10, Train Loss: 0.2457, Accuracy: 0.8988, F1 Micro: 0.7316, F1 Macro: 0.7186\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.9002, F1 Micro: 0.7676, F1 Macro: 0.7653\n",
      "Epoch 4/10, Train Loss: 0.1534, Accuracy: 0.9038, F1 Micro: 0.751, F1 Macro: 0.7412\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9016, F1 Micro: 0.7682, F1 Macro: 0.7644\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9009, F1 Micro: 0.7624, F1 Macro: 0.7613\n",
      "Epoch 7/10, Train Loss: 0.0662, Accuracy: 0.9067, F1 Micro: 0.771, F1 Macro: 0.7665\n",
      "Epoch 8/10, Train Loss: 0.0538, Accuracy: 0.9056, F1 Micro: 0.7646, F1 Macro: 0.7599\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.902, F1 Micro: 0.7606, F1 Macro: 0.7537\n",
      "Epoch 10/10, Train Loss: 0.0325, Accuracy: 0.9075, F1 Micro: 0.7788, F1 Macro: 0.7775\n",
      "Best result for 4530 samples: F1 Micro: 0.7788\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.88      0.91       353\n",
      "                sara       0.67      0.70      0.69       239\n",
      "         radikalisme       0.79      0.76      0.78       273\n",
      "pencemaran_nama_baik       0.73      0.73      0.73       485\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      1350\n",
      "           macro avg       0.79      0.77      0.78      1350\n",
      "        weighted avg       0.79      0.77      0.78      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 4663\n",
      "Acquired samples: 133\n",
      "Sampling duration: 0.08512067794799805 seconds\n",
      "\n",
      "Fold 5 - New train size: 4663\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4663 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3745, Accuracy: 0.8792, F1 Micro: 0.7343, F1 Macro: 0.736\n",
      "Epoch 2/10, Train Loss: 0.2425, Accuracy: 0.9044, F1 Micro: 0.7706, F1 Macro: 0.7624\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9077, F1 Micro: 0.781, F1 Macro: 0.778\n",
      "Epoch 4/10, Train Loss: 0.1616, Accuracy: 0.9023, F1 Micro: 0.773, F1 Macro: 0.7663\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9019, F1 Micro: 0.77, F1 Macro: 0.764\n",
      "Epoch 6/10, Train Loss: 0.0959, Accuracy: 0.9045, F1 Micro: 0.7627, F1 Macro: 0.7533\n",
      "Epoch 7/10, Train Loss: 0.0695, Accuracy: 0.9039, F1 Micro: 0.7787, F1 Macro: 0.7765\n",
      "Epoch 8/10, Train Loss: 0.054, Accuracy: 0.9059, F1 Micro: 0.763, F1 Macro: 0.7546\n",
      "Epoch 9/10, Train Loss: 0.0437, Accuracy: 0.9059, F1 Micro: 0.763, F1 Macro: 0.7564\n",
      "Epoch 10/10, Train Loss: 0.0343, Accuracy: 0.9047, F1 Micro: 0.7689, F1 Macro: 0.7618\n",
      "Best result for 4663 samples: F1 Micro: 0.781\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.89      0.91       353\n",
      "                sara       0.67      0.64      0.66       239\n",
      "         radikalisme       0.80      0.81      0.81       273\n",
      "pencemaran_nama_baik       0.71      0.75      0.73       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.45      0.44      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0001842975616455078 seconds\n",
      "\n",
      "Fold 5 - New train size: 4863\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 4863 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3825, Accuracy: 0.8852, F1 Micro: 0.6916, F1 Macro: 0.6754\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.9, F1 Micro: 0.7403, F1 Macro: 0.7284\n",
      "Epoch 3/10, Train Loss: 0.1958, Accuracy: 0.9047, F1 Micro: 0.753, F1 Macro: 0.7492\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.8936, F1 Micro: 0.7717, F1 Macro: 0.7729\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9052, F1 Micro: 0.7769, F1 Macro: 0.7751\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.9052, F1 Micro: 0.7738, F1 Macro: 0.7713\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.9041, F1 Micro: 0.7795, F1 Macro: 0.7805\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9052, F1 Micro: 0.77, F1 Macro: 0.7652\n",
      "Epoch 9/10, Train Loss: 0.0441, Accuracy: 0.9048, F1 Micro: 0.777, F1 Macro: 0.7742\n",
      "Epoch 10/10, Train Loss: 0.0307, Accuracy: 0.9027, F1 Micro: 0.7748, F1 Macro: 0.7731\n",
      "Best result for 4863 samples: F1 Micro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.90      0.91       353\n",
      "                sara       0.65      0.72      0.69       239\n",
      "         radikalisme       0.80      0.81      0.80       273\n",
      "pencemaran_nama_baik       0.68      0.77      0.72       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.76      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017642974853515625 seconds\n",
      "\n",
      "Fold 5 - New train size: 5063\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5063 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3668, Accuracy: 0.8892, F1 Micro: 0.7317, F1 Macro: 0.7233\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.8984, F1 Micro: 0.7734, F1 Macro: 0.776\n",
      "Epoch 3/10, Train Loss: 0.1938, Accuracy: 0.9038, F1 Micro: 0.7544, F1 Macro: 0.7399\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9, F1 Micro: 0.7756, F1 Macro: 0.7758\n",
      "Epoch 5/10, Train Loss: 0.1187, Accuracy: 0.9052, F1 Micro: 0.7722, F1 Macro: 0.7709\n",
      "Epoch 6/10, Train Loss: 0.0892, Accuracy: 0.9019, F1 Micro: 0.765, F1 Macro: 0.7593\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9066, F1 Micro: 0.7795, F1 Macro: 0.7782\n",
      "Epoch 8/10, Train Loss: 0.047, Accuracy: 0.9038, F1 Micro: 0.7771, F1 Macro: 0.7749\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.9022, F1 Micro: 0.775, F1 Macro: 0.7695\n",
      "Epoch 10/10, Train Loss: 0.0295, Accuracy: 0.9055, F1 Micro: 0.774, F1 Macro: 0.7709\n",
      "Best result for 5063 samples: F1 Micro: 0.7795\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.94      0.90      0.92       353\n",
      "                sara       0.69      0.67      0.68       239\n",
      "         radikalisme       0.77      0.84      0.80       273\n",
      "pencemaran_nama_baik       0.71      0.73      0.72       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.78      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.45      0.44      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00017952919006347656 seconds\n",
      "\n",
      "Fold 5 - New train size: 5263\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5263 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3689, Accuracy: 0.8897, F1 Micro: 0.7453, F1 Macro: 0.7434\n",
      "Epoch 2/10, Train Loss: 0.2385, Accuracy: 0.9045, F1 Micro: 0.7748, F1 Macro: 0.7726\n",
      "Epoch 3/10, Train Loss: 0.199, Accuracy: 0.9059, F1 Micro: 0.7827, F1 Macro: 0.7811\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9097, F1 Micro: 0.7758, F1 Macro: 0.7683\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9061, F1 Micro: 0.7823, F1 Macro: 0.7807\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9056, F1 Micro: 0.7775, F1 Macro: 0.7734\n",
      "Epoch 7/10, Train Loss: 0.0645, Accuracy: 0.9075, F1 Micro: 0.7751, F1 Macro: 0.7681\n",
      "Epoch 8/10, Train Loss: 0.0486, Accuracy: 0.907, F1 Micro: 0.7782, F1 Macro: 0.7751\n",
      "Epoch 9/10, Train Loss: 0.0379, Accuracy: 0.8983, F1 Micro: 0.7732, F1 Macro: 0.7721\n",
      "Epoch 10/10, Train Loss: 0.0297, Accuracy: 0.9075, F1 Micro: 0.7739, F1 Macro: 0.7671\n",
      "Best result for 5263 samples: F1 Micro: 0.7827\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.93      0.91       353\n",
      "                sara       0.68      0.68      0.68       239\n",
      "         radikalisme       0.79      0.83      0.81       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.80      0.78      1350\n",
      "        weighted avg       0.76      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 5441\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.08308053016662598 seconds\n",
      "\n",
      "Fold 5 - New train size: 5441\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5441 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3695, Accuracy: 0.8886, F1 Micro: 0.7138, F1 Macro: 0.7045\n",
      "Epoch 2/10, Train Loss: 0.2371, Accuracy: 0.9025, F1 Micro: 0.7598, F1 Macro: 0.7552\n",
      "Epoch 3/10, Train Loss: 0.1958, Accuracy: 0.9061, F1 Micro: 0.7673, F1 Macro: 0.7565\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9073, F1 Micro: 0.7627, F1 Macro: 0.7571\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9034, F1 Micro: 0.7694, F1 Macro: 0.7649\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9067, F1 Micro: 0.776, F1 Macro: 0.7727\n",
      "Epoch 7/10, Train Loss: 0.0621, Accuracy: 0.9053, F1 Micro: 0.7808, F1 Macro: 0.7779\n",
      "Epoch 8/10, Train Loss: 0.0505, Accuracy: 0.9053, F1 Micro: 0.7735, F1 Macro: 0.7707\n",
      "Epoch 9/10, Train Loss: 0.0403, Accuracy: 0.9066, F1 Micro: 0.7772, F1 Macro: 0.7702\n",
      "Epoch 10/10, Train Loss: 0.031, Accuracy: 0.9053, F1 Micro: 0.7727, F1 Macro: 0.7703\n",
      "Best result for 5441 samples: F1 Micro: 0.7808\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.92      0.91       353\n",
      "                sara       0.67      0.67      0.67       239\n",
      "         radikalisme       0.77      0.82      0.80       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.76      0.80      0.78      1350\n",
      "           macro avg       0.76      0.79      0.78      1350\n",
      "        weighted avg       0.76      0.80      0.78      1350\n",
      "         samples avg       0.45      0.45      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.0002009868621826172 seconds\n",
      "\n",
      "Fold 5 - New train size: 5641\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5641 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3626, Accuracy: 0.8866, F1 Micro: 0.735, F1 Macro: 0.7367\n",
      "Epoch 2/10, Train Loss: 0.2398, Accuracy: 0.9019, F1 Micro: 0.7549, F1 Macro: 0.743\n",
      "Epoch 3/10, Train Loss: 0.2007, Accuracy: 0.9055, F1 Micro: 0.7693, F1 Macro: 0.7619\n",
      "Epoch 4/10, Train Loss: 0.1524, Accuracy: 0.8894, F1 Micro: 0.7666, F1 Macro: 0.77\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9066, F1 Micro: 0.7774, F1 Macro: 0.7724\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9069, F1 Micro: 0.7737, F1 Macro: 0.771\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9066, F1 Micro: 0.7813, F1 Macro: 0.7787\n",
      "Epoch 8/10, Train Loss: 0.0451, Accuracy: 0.9036, F1 Micro: 0.777, F1 Macro: 0.7733\n",
      "Epoch 9/10, Train Loss: 0.0378, Accuracy: 0.9081, F1 Micro: 0.7824, F1 Macro: 0.7757\n",
      "Epoch 10/10, Train Loss: 0.03, Accuracy: 0.9075, F1 Micro: 0.7742, F1 Macro: 0.767\n",
      "Best result for 5641 samples: F1 Micro: 0.7824\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.91      0.94      0.92       353\n",
      "                sara       0.69      0.63      0.66       239\n",
      "         radikalisme       0.80      0.77      0.78       273\n",
      "pencemaran_nama_baik       0.72      0.75      0.74       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.78      0.77      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.45      0.44      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00021648406982421875 seconds\n",
      "\n",
      "Fold 5 - New train size: 5841\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 5841 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3603, Accuracy: 0.8956, F1 Micro: 0.7353, F1 Macro: 0.7284\n",
      "Epoch 2/10, Train Loss: 0.2378, Accuracy: 0.9036, F1 Micro: 0.7802, F1 Macro: 0.7774\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9081, F1 Micro: 0.7849, F1 Macro: 0.7807\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9052, F1 Micro: 0.7789, F1 Macro: 0.7749\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.9081, F1 Micro: 0.7846, F1 Macro: 0.7825\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9058, F1 Micro: 0.7685, F1 Macro: 0.7622\n",
      "Epoch 7/10, Train Loss: 0.0604, Accuracy: 0.908, F1 Micro: 0.7737, F1 Macro: 0.7696\n",
      "Epoch 8/10, Train Loss: 0.0504, Accuracy: 0.9047, F1 Micro: 0.7679, F1 Macro: 0.7593\n",
      "Epoch 9/10, Train Loss: 0.0388, Accuracy: 0.9013, F1 Micro: 0.7756, F1 Macro: 0.775\n",
      "Epoch 10/10, Train Loss: 0.0318, Accuracy: 0.9036, F1 Micro: 0.7702, F1 Macro: 0.7684\n",
      "Best result for 5841 samples: F1 Micro: 0.7849\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.95      0.90      0.92       353\n",
      "                sara       0.68      0.63      0.66       239\n",
      "         radikalisme       0.78      0.83      0.81       273\n",
      "pencemaran_nama_baik       0.70      0.78      0.74       485\n",
      "\n",
      "           micro avg       0.78      0.79      0.78      1350\n",
      "           macro avg       0.78      0.78      0.78      1350\n",
      "        weighted avg       0.78      0.79      0.79      1350\n",
      "         samples avg       0.45      0.44      0.44      1350\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 200\n",
      "Sampling duration: 0.00016236305236816406 seconds\n",
      "\n",
      "Fold 5 - New train size: 6041\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6041 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3485, Accuracy: 0.8905, F1 Micro: 0.7315, F1 Macro: 0.7344\n",
      "Epoch 2/10, Train Loss: 0.2323, Accuracy: 0.9034, F1 Micro: 0.7634, F1 Macro: 0.7584\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9097, F1 Micro: 0.7779, F1 Macro: 0.774\n",
      "Epoch 4/10, Train Loss: 0.1524, Accuracy: 0.9083, F1 Micro: 0.7706, F1 Macro: 0.7608\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.907, F1 Micro: 0.7791, F1 Macro: 0.7761\n",
      "Epoch 6/10, Train Loss: 0.082, Accuracy: 0.9055, F1 Micro: 0.7775, F1 Macro: 0.7718\n",
      "Epoch 7/10, Train Loss: 0.0644, Accuracy: 0.9023, F1 Micro: 0.7681, F1 Macro: 0.7671\n",
      "Epoch 8/10, Train Loss: 0.0496, Accuracy: 0.903, F1 Micro: 0.7672, F1 Macro: 0.7535\n",
      "Epoch 9/10, Train Loss: 0.0387, Accuracy: 0.9034, F1 Micro: 0.7721, F1 Macro: 0.7684\n",
      "Epoch 10/10, Train Loss: 0.028, Accuracy: 0.9038, F1 Micro: 0.776, F1 Macro: 0.7732\n",
      "Best result for 6041 samples: F1 Micro: 0.7791\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.93      0.92      0.92       353\n",
      "                sara       0.69      0.67      0.68       239\n",
      "         radikalisme       0.82      0.73      0.77       273\n",
      "pencemaran_nama_baik       0.70      0.76      0.73       485\n",
      "\n",
      "           micro avg       0.78      0.78      0.78      1350\n",
      "           macro avg       0.79      0.77      0.78      1350\n",
      "        weighted avg       0.78      0.78      0.78      1350\n",
      "         samples avg       0.44      0.43      0.43      1350\n",
      "\n",
      "Nearest checkpoint: 6219\n",
      "Acquired samples: 178\n",
      "Sampling duration: 0.09588146209716797 seconds\n",
      "\n",
      "Fold 5 - New train size: 6219\n",
      "\n",
      "Launching training on 2 GPUs.\n",
      "Fold 5 - Training with 6219 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3507, Accuracy: 0.8898, F1 Micro: 0.7102, F1 Macro: 0.6954\n",
      "Epoch 2/10, Train Loss: 0.2275, Accuracy: 0.9048, F1 Micro: 0.7624, F1 Macro: 0.7545\n",
      "Epoch 3/10, Train Loss: 0.1891, Accuracy: 0.9059, F1 Micro: 0.7732, F1 Macro: 0.7691\n",
      "Epoch 4/10, Train Loss: 0.1546, Accuracy: 0.9055, F1 Micro: 0.7772, F1 Macro: 0.7708\n",
      "Epoch 5/10, Train Loss: 0.1135, Accuracy: 0.9042, F1 Micro: 0.7658, F1 Macro: 0.756\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.9044, F1 Micro: 0.7783, F1 Macro: 0.7753\n",
      "Epoch 7/10, Train Loss: 0.0634, Accuracy: 0.9078, F1 Micro: 0.7777, F1 Macro: 0.7751\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9062, F1 Micro: 0.7705, F1 Macro: 0.7637\n",
      "Epoch 9/10, Train Loss: 0.0386, Accuracy: 0.9016, F1 Micro: 0.7766, F1 Macro: 0.7722\n",
      "Epoch 10/10, Train Loss: 0.0279, Accuracy: 0.9002, F1 Micro: 0.7796, F1 Macro: 0.7794\n",
      "Best result for 6219 samples: F1 Micro: 0.7796\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.89      0.94      0.92       353\n",
      "                sara       0.60      0.73      0.66       239\n",
      "         radikalisme       0.78      0.84      0.81       273\n",
      "pencemaran_nama_baik       0.66      0.81      0.73       485\n",
      "\n",
      "           micro avg       0.73      0.84      0.78      1350\n",
      "           macro avg       0.74      0.83      0.78      1350\n",
      "        weighted avg       0.74      0.84      0.78      1350\n",
      "         samples avg       0.46      0.47      0.46      1350\n",
      "\n",
      "\n",
      "FOLD 5 COMPLETED in 3593.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "label_columns = data.columns[2:6]\n",
    "X = data['processed_text'].values\n",
    "y = data[label_columns].values\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Each element in these lists will be a list of metrics for one fold's learning curve\n",
    "all_fold_accuracies = []\n",
    "all_fold_f1_micros = []\n",
    "all_fold_f1_macros = []\n",
    "all_fold_data_used = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"===============================================\")\n",
    "    print(f\"STARTING FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Shared resources for this fold's processes\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    \n",
    "    # Set seed for reproducibility within the fold\n",
    "    set_seed(RANDOM_SEED + fold)\n",
    "    \n",
    "    # Define the initial labeled pool from the current fold's training data\n",
    "    total_train_fold_size = len(X_train_fold) + len(X_val_fold)\n",
    "    initial_train_size = int(0.05 * total_train_fold_size)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train_fold)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train_fold))) - set(train_indices))\n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    # Adjust checkpoints based on the current fold's training size\n",
    "    checkpoints = [\n",
    "        int(0.5 * total_train_fold_size), \n",
    "        int(0.6 * total_train_fold_size),\n",
    "        int(0.7 * total_train_fold_size),\n",
    "        len(X_train_fold)\n",
    "    ]\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    while current_train_size < total_train_fold_size:\n",
    "        # 1. Train the model on the current labeled set\n",
    "        train_args = (\n",
    "            current_train_size, train_indices, (data_used, accuracies, f1_micros, f1_macros),\n",
    "            fold, RANDOM_SEED + fold, X_train_fold, y_train_fold, X_val_fold, y_val_fold, label_columns\n",
    "        )\n",
    "        notebook_launcher(train_model, train_args, num_processes=2)\n",
    "        \n",
    "        # Stop if we've reached the last checkpoint\n",
    "        if current_train_size >= checkpoints[-1]:\n",
    "            break\n",
    "        \n",
    "        # 3. Perform query strategy to select new samples\n",
    "        new_samples_shared = manager.list()\n",
    "        X_pool = [X_train_fold[i] for i in remaining_indices]\n",
    "        random_sampling(current_train_size, X_pool, train_indices, remaining_indices, sampling_dur, new_samples_shared, fold, X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 4. Update the pools\n",
    "        newly_acquired_indices = list(new_samples_shared)\n",
    "        train_indices.extend(newly_acquired_indices)\n",
    "        remaining_indices = list(set(remaining_indices) - set(newly_acquired_indices))\n",
    "    \n",
    "        current_train_size = len(train_indices)\n",
    "        print(f\"\\nFold {fold + 1} - New train size: {current_train_size}\\n\")\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    print(f\"\\nFOLD {fold + 1} COMPLETED in {fold_end_time - fold_start_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    all_fold_data_used.append(list(data_used))\n",
    "    all_fold_accuracies.append(list(accuracies))\n",
    "    all_fold_f1_micros.append(list(f1_micros))\n",
    "    all_fold_f1_macros.append(list(f1_macros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a504f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T18:42:11.601422Z",
     "iopub.status.busy": "2025-06-26T18:42:11.601029Z",
     "iopub.status.idle": "2025-06-26T18:42:12.448673Z",
     "shell.execute_reply": "2025-06-26T18:42:12.447779Z"
    },
    "papermill": {
     "duration": 0.999938,
     "end_time": "2025-06-26T18:42:12.451034",
     "exception": false,
     "start_time": "2025-06-26T18:42:11.451096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "common_data_points = sorted(list(set(point for fold_points in all_fold_data_used for point in fold_points)))\n",
    "\n",
    "# Interpolate metrics for each fold to the common data points\n",
    "avg_accuracies = []\n",
    "avg_f1_micros = []\n",
    "avg_f1_macros = []\n",
    "std_accuracies = []\n",
    "std_f1_micros = []\n",
    "std_f1_macros = []\n",
    "\n",
    "for point in common_data_points:\n",
    "    point_accuracies = []\n",
    "    point_f1_micros = []\n",
    "    point_f1_macros = []\n",
    "    for i in range(N_SPLITS):\n",
    "        # np.interp requires x-coordinates to be increasing\n",
    "        sorted_indices = np.argsort(all_fold_data_used[i])\n",
    "        sorted_data = np.array(all_fold_data_used[i])[sorted_indices]\n",
    "        \n",
    "        sorted_acc = np.array(all_fold_accuracies[i])[sorted_indices]\n",
    "        sorted_f1m = np.array(all_fold_f1_micros[i])[sorted_indices]\n",
    "        sorted_f1ma = np.array(all_fold_f1_macros[i])[sorted_indices]\n",
    "        \n",
    "        # Use interpolation to estimate the metric value at the common 'point'\n",
    "        point_accuracies.append(np.interp(point, sorted_data, sorted_acc))\n",
    "        point_f1_micros.append(np.interp(point, sorted_data, sorted_f1m))\n",
    "        point_f1_macros.append(np.interp(point, sorted_data, sorted_f1ma))\n",
    "    \n",
    "    avg_accuracies.append(np.mean(point_accuracies))\n",
    "    avg_f1_micros.append(np.mean(point_f1_micros))\n",
    "    avg_f1_macros.append(np.mean(point_f1_macros))\n",
    "    \n",
    "    std_accuracies.append(np.std(point_accuracies))\n",
    "    std_f1_micros.append(np.std(point_f1_micros))\n",
    "    std_f1_macros.append(np.std(point_f1_macros))\n",
    "\n",
    "# Convert to numpy arrays for easier plotting\n",
    "avg_accuracies = np.array(avg_accuracies)\n",
    "avg_f1_micros = np.array(avg_f1_micros)\n",
    "avg_f1_macros = np.array(avg_f1_macros)\n",
    "std_accuracies = np.array(std_accuracies)\n",
    "std_f1_micros = np.array(std_f1_micros)\n",
    "std_f1_macros = np.array(std_f1_macros)\n",
    "\n",
    "# --- Plotting the Averaged Learning Curve ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "data_used_percent = [round(data / len(X) * 100, 1) for data in common_data_points]\n",
    "\n",
    "# Plot for Accuracy\n",
    "axs[0].plot(data_used_percent, avg_accuracies, label=\"Avg Accuracy\", color=\"blue\")\n",
    "axs[0].fill_between(data_used_percent, avg_accuracies - std_accuracies, avg_accuracies + std_accuracies, color='blue', alpha=0.2)\n",
    "axs[0].set_xlabel(\"Percentage of data used\")\n",
    "axs[0].set_title(\"Average Accuracy Across Folds\")\n",
    "\n",
    "# Plot for F1 Micro\n",
    "axs[1].plot(data_used_percent, avg_f1_micros, label=\"Avg F1 Micro\", color=\"orange\")\n",
    "axs[1].fill_between(data_used_percent, avg_f1_micros - std_f1_micros, avg_f1_micros + std_f1_micros, color='orange', alpha=0.2)\n",
    "axs[1].set_xlabel(\"Percentage of data used\")\n",
    "axs[1].set_title(\"Average F1 Micro Across Folds\")\n",
    "\n",
    "# Plot for F1 Macro\n",
    "axs[2].plot(data_used_percent, avg_f1_macros, label=\"Avg F1 Macro\", color=\"green\")\n",
    "axs[2].fill_between(data_used_percent, avg_f1_macros - std_f1_macros, avg_f1_macros + std_f1_macros, color='green', alpha=0.2)\n",
    "axs[2].set_xlabel(\"Percentage of data used\")\n",
    "axs[2].set_title(\"Average F1 Macro Across Folds\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    result = pd.DataFrame({\n",
    "        'Data Used': all_fold_data_used[i],\n",
    "        'Accuracy': all_fold_accuracies[i],\n",
    "        'F1 Micro': all_fold_f1_micros[i],\n",
    "        'F1 Macro': all_fold_f1_macros[i],\n",
    "    })\n",
    "\n",
    "    result.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6854798,
     "sourceId": 11010126,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18022.536173,
   "end_time": "2025-06-26T18:42:18.355608",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-26T13:41:55.819435",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0416b2d606734361b573521cabd03a59": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b1f5d10a57748b5ad305d98bcd111a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1058754de87f47e5b7c4d43f1705654d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19191ab01bd74d06ac08bea525021c38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bdf599500ba4cbc91021b9cb56960c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1058754de87f47e5b7c4d43f1705654d",
       "placeholder": "",
       "style": "IPY_MODEL_d32d997d99b64157bad27d023596d25b",
       "value": "special_tokens_map.json:100%"
      }
     },
     "1c0b156c00184325be57a061e33250bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9a67b6c930804ac688666d87992b8057",
        "IPY_MODEL_1cd8b20048d64764b8eae6c1acc19d9b",
        "IPY_MODEL_7c9edba61af04048add1594ade2e797f"
       ],
       "layout": "IPY_MODEL_0416b2d606734361b573521cabd03a59"
      }
     },
     "1cd8b20048d64764b8eae6c1acc19d9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e264b8a75d8c45d480c8c565853dcc10",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5420272a9b3e45f58bd2067ffc4f63dd",
       "value": 1.0
      }
     },
     "23b72dd43db14660ac4403ca56a15ede": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31b66af42be242ea93c434f718eaabd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_979185bdf41c4f43a6a64f3a47a5846d",
        "IPY_MODEL_bb1f91d866e849e9981b931107282606",
        "IPY_MODEL_bad3c1015de340ab8cc2a2bec3e44b1e"
       ],
       "layout": "IPY_MODEL_6dc2cea11ad8452f8ddce589750ec786"
      }
     },
     "345f918b052044928df8011c1c829ba0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45c08bf3bdc240dd9de0e98d51d70ec2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "460b8a539a984cc993cd0592be8084a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5132e9a2a7f44a2780b5cb2e9101a6e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "53bb91a5d8c74b2a9b2d2feb11a25f5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5b65c85d2e1544f0bf04920bb3f562fd",
       "placeholder": "",
       "style": "IPY_MODEL_a778f0a8e7aa4bb2b23d0490a6954d20",
       "value": "498M/498M[00:02&lt;00:00,237MB/s]"
      }
     },
     "5420272a9b3e45f58bd2067ffc4f63dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b65c85d2e1544f0bf04920bb3f562fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d850678be32413baada246625ad9dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e26ae7d249fa433f9efcfc3aaf67de03",
        "IPY_MODEL_b4701ab2c6184af6947761c66692617d",
        "IPY_MODEL_849e2c60879e4b03926fd439010e09da"
       ],
       "layout": "IPY_MODEL_636bb28c31d5406e850943d8ab619d2e"
      }
     },
     "636bb28c31d5406e850943d8ab619d2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dc2cea11ad8452f8ddce589750ec786": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75be9b1eb5a348ea924ab9cad2f24480": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c9edba61af04048add1594ade2e797f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9bacb6a9237b45f2a4bb3eb8fd9337d1",
       "placeholder": "",
       "style": "IPY_MODEL_e429468a5a9b4a2293102b8b15ef40ae",
       "value": "1.53k/?[00:00&lt;00:00,165kB/s]"
      }
     },
     "7d55e06fb6434d10bdd44075a76cc2e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "849e2c60879e4b03926fd439010e09da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23b72dd43db14660ac4403ca56a15ede",
       "placeholder": "",
       "style": "IPY_MODEL_a08de04768964e65aeb29be2fafc65ca",
       "value": "2.00/2.00[00:00&lt;00:00,161B/s]"
      }
     },
     "89b21ff463b34be5b0877f12a7a379d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1bdf599500ba4cbc91021b9cb56960c3",
        "IPY_MODEL_b8672a99d6d745f8b3dd8353e988deaf",
        "IPY_MODEL_8cd277e278954f4689230e2a93cac34f"
       ],
       "layout": "IPY_MODEL_d9eb73c6956f46f5899a54aa29711200"
      }
     },
     "8afb3a025df64a9fb0c096c299f4f301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_96865167ed7f45b3b5f61f1fcf8e46c6",
       "max": 497810400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7d55e06fb6434d10bdd44075a76cc2e9",
       "value": 497810400.0
      }
     },
     "8cd277e278954f4689230e2a93cac34f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b813ea91124a452c9fcbe515dc1bf584",
       "placeholder": "",
       "style": "IPY_MODEL_e09d4fe60a8e4065907a3ab9461dd875",
       "value": "112/112[00:00&lt;00:00,12.2kB/s]"
      }
     },
     "92f9e5f4f6364ae6801aa74018e7c83a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96865167ed7f45b3b5f61f1fcf8e46c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "979185bdf41c4f43a6a64f3a47a5846d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c75c3b3bd5af41509a2ee3ae6ce4d4ec",
       "placeholder": "",
       "style": "IPY_MODEL_75be9b1eb5a348ea924ab9cad2f24480",
       "value": "vocab.txt:"
      }
     },
     "99918b649c4d4eda81b33521796d5bf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a36417399e034fc997395552a0be8ea8",
        "IPY_MODEL_8afb3a025df64a9fb0c096c299f4f301",
        "IPY_MODEL_53bb91a5d8c74b2a9b2d2feb11a25f5a"
       ],
       "layout": "IPY_MODEL_0b1f5d10a57748b5ad305d98bcd111a7"
      }
     },
     "9a67b6c930804ac688666d87992b8057": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19191ab01bd74d06ac08bea525021c38",
       "placeholder": "",
       "style": "IPY_MODEL_f9b8387c92354c9f93ec3905b8231d87",
       "value": "config.json:"
      }
     },
     "9bacb6a9237b45f2a4bb3eb8fd9337d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a08de04768964e65aeb29be2fafc65ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a36417399e034fc997395552a0be8ea8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0653dbe90c14d62aef5fc6cddba7199",
       "placeholder": "",
       "style": "IPY_MODEL_5132e9a2a7f44a2780b5cb2e9101a6e4",
       "value": "pytorch_model.bin:100%"
      }
     },
     "a778f0a8e7aa4bb2b23d0490a6954d20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b0653dbe90c14d62aef5fc6cddba7199": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1d363a036cb4448801e0f0da153c3da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b4701ab2c6184af6947761c66692617d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_345f918b052044928df8011c1c829ba0",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f7c3ff7d420a4fbd87fd47f2e06c107b",
       "value": 2.0
      }
     },
     "b58c20f53b4c47c8a1699c40a5885a43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b813ea91124a452c9fcbe515dc1bf584": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8672a99d6d745f8b3dd8353e988deaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45c08bf3bdc240dd9de0e98d51d70ec2",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e3cf390d685f4ffaae6f560b0c9ced4f",
       "value": 112.0
      }
     },
     "bad3c1015de340ab8cc2a2bec3e44b1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_460b8a539a984cc993cd0592be8084a6",
       "placeholder": "",
       "style": "IPY_MODEL_b58c20f53b4c47c8a1699c40a5885a43",
       "value": "229k/?[00:00&lt;00:00,6.34MB/s]"
      }
     },
     "bb1f91d866e849e9981b931107282606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cc4c76cad97745c49cb665aa5fd27dbf",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1d363a036cb4448801e0f0da153c3da",
       "value": 1.0
      }
     },
     "c75c3b3bd5af41509a2ee3ae6ce4d4ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc4c76cad97745c49cb665aa5fd27dbf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "d32d997d99b64157bad27d023596d25b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d9eb73c6956f46f5899a54aa29711200": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e09d4fe60a8e4065907a3ab9461dd875": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e264b8a75d8c45d480c8c565853dcc10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "e26ae7d249fa433f9efcfc3aaf67de03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_92f9e5f4f6364ae6801aa74018e7c83a",
       "placeholder": "",
       "style": "IPY_MODEL_fee03fefb01244ed88ab756a9cf3e888",
       "value": "tokenizer_config.json:100%"
      }
     },
     "e3cf390d685f4ffaae6f560b0c9ced4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e429468a5a9b4a2293102b8b15ef40ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7c3ff7d420a4fbd87fd47f2e06c107b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f9b8387c92354c9f93ec3905b8231d87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fee03fefb01244ed88ab756a9cf3e888": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
