{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T07:25:49.167426Z",
     "iopub.status.busy": "2025-06-21T07:25:49.167002Z",
     "iopub.status.idle": "2025-06-21T07:26:10.550064Z",
     "shell.execute_reply": "2025-06-21T07:26:10.549352Z",
     "shell.execute_reply.started": "2025-06-21T07:25:49.167385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.551711Z",
     "iopub.status.busy": "2025-06-21T07:26:10.551106Z",
     "iopub.status.idle": "2025-06-21T07:26:10.555599Z",
     "shell.execute_reply": "2025-06-21T07:26:10.554649Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.551685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.557731Z",
     "iopub.status.busy": "2025-06-21T07:26:10.557494Z",
     "iopub.status.idle": "2025-06-21T07:26:10.581362Z",
     "shell.execute_reply": "2025-06-21T07:26:10.580550Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.557711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.583070Z",
     "iopub.status.busy": "2025-06-21T07:26:10.582771Z",
     "iopub.status.idle": "2025-06-21T07:26:10.636845Z",
     "shell.execute_reply": "2025-06-21T07:26:10.635345Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.583039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'netifier-lc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.638973Z",
     "iopub.status.busy": "2025-06-21T07:26:10.638681Z",
     "iopub.status.idle": "2025-06-21T07:26:10.810747Z",
     "shell.execute_reply": "2025-06-21T07:26:10.809943Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.638947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/netifier-3/processed_train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/netifier-3/processed_test.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.812109Z",
     "iopub.status.busy": "2025-06-21T07:26:10.811817Z",
     "iopub.status.idle": "2025-06-21T07:26:10.852576Z",
     "shell.execute_reply": "2025-06-21T07:26:10.851678Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.812071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6218,) (6218, 4)\n",
      "(1555,) (1555, 4)\n",
      "['yos di rt aku yang pacar tidak pernah di sedih rt rt nyata saja papua mau pisah tidak guna ikut negara kesatuan republik indonesia ruginya pasti'\n",
      " 'yu rt siap di rt heh ketek acem jembut lebat pada mau ngaco tidak neh malam'\n",
      " 'yups setuju kakak laki laki orang sekarang punya bnyak istri alsanya agama tapi knytanya tidak sesuai dengan printah agama krena yang dinikahin itu prempuan yang dari segi ekonominya trgolong mampu bukan dari janda miskin yang mang butuh untuk ditolong poligami hnyalah topeng untuk mnutupi nafsu belaka bukan brdasarkan apa yang nabi muhamad lakukakn'\n",
      " 'zaman sekrng poligami sudah salah tidak ngikutin niatan nabi menyantunin anak yatim dn menikahi para janda yang ditingal wafat para suami yang gugur di brjuang'\n",
      " 'zaman sudah berkembang kok di paksa pakai budaya abad pertengahan bang mamad boleh juga ibarat zaman sudah buat pesawat ke mars masi di paksa pakai busana ninja dan naik onta boleh juga sunguh terlalu boleh juga ancam neraka bre boleh juga']\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "train_data = train_data.sort_values(by=['processed_text', 'pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'])\n",
    "\n",
    "train_labels = train_data.columns[2:6]\n",
    "val_labels = val_data.columns[2:6]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['processed_text'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['processed_text'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_train[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK ACQUIRED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.853903Z",
     "iopub.status.busy": "2025-06-21T07:26:10.853630Z",
     "iopub.status.idle": "2025-06-21T07:26:10.921251Z",
     "shell.execute_reply": "2025-06-21T07:26:10.920485Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.853881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6218,) (6218, 4)\n",
      "['yos di rt aku yang pacar tidak pernah di sedih rt rt nyata saja papua mau pisah tidak guna ikut negara kesatuan republik indonesia ruginya pasti'\n",
      " 'yu rt siap di rt heh ketek acem jembut lebat pada mau ngaco tidak neh malam'\n",
      " 'yups setuju kakak laki laki orang sekarang punya bnyak istri alsanya agama tapi knytanya tidak sesuai dengan printah agama krena yang dinikahin itu prempuan yang dari segi ekonominya trgolong mampu bukan dari janda miskin yang mang butuh untuk ditolong poligami hnyalah topeng untuk mnutupi nafsu belaka bukan brdasarkan apa yang nabi muhamad lakukakn'\n",
      " 'zaman sekrng poligami sudah salah tidak ngikutin niatan nabi menyantunin anak yatim dn menikahi para janda yang ditingal wafat para suami yang gugur di brjuang'\n",
      " 'zaman sudah berkembang kok di paksa pakai budaya abad pertengahan bang mamad boleh juga ibarat zaman sudah buat pesawat ke mars masi di paksa pakai busana ninja dan naik onta boleh juga sunguh terlalu boleh juga ancam neraka bre boleh juga']\n"
     ]
    }
   ],
   "source": [
    "acquired_data = pd.read_csv('/kaggle/input/netifier-acquired-6218/netifier-lc-1-data-6218.csv')\n",
    "acquired_data = acquired_data.sort_values(by=['processed_text', 'pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'])\n",
    "\n",
    "acq_X_train = acquired_data['processed_text'].values\n",
    "acq_y_train = acquired_data[acquired_data.columns[1:]].values\n",
    "\n",
    "print(acq_X_train.shape, acq_y_train.shape)\n",
    "print(acq_X_train[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.924232Z",
     "iopub.status.busy": "2025-06-21T07:26:10.923963Z",
     "iopub.status.idle": "2025-06-21T07:26:10.936166Z",
     "shell.execute_reply": "2025-06-21T07:26:10.935283Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.924207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train and acq_X_train contain the same elements (ignoring order).\n",
      "Elements in X_train but not in acq_X_train: 0\n",
      "Elements in acq_X_train but not in X_train: 0\n",
      "Mismatched indices: []\n",
      "X_train mismatches: []\n",
      "acq_X_train mismatches: []\n"
     ]
    }
   ],
   "source": [
    "if np.array_equal(X_train, acq_X_train):\n",
    "    print(\"X_train and acq_X_train contain the same elements (ignoring order).\")\n",
    "else:\n",
    "    print(\"X_train and acq_X_train have different elements.\")\n",
    "\n",
    "set_X_train = set(X_train)\n",
    "set_acq_X_train = set(acq_X_train)\n",
    "\n",
    "print(\"Elements in X_train but not in acq_X_train:\", len(set_X_train - set_acq_X_train))\n",
    "print(\"Elements in acq_X_train but not in X_train:\", len(set_acq_X_train - set_X_train))\n",
    "\n",
    "diff_indices = np.where(X_train != acq_X_train)[0]  # Get indices where elements differ\n",
    "print(\"Mismatched indices:\", diff_indices)\n",
    "print(\"X_train mismatches:\", X_train[diff_indices])\n",
    "print(\"acq_X_train mismatches:\", acq_X_train[diff_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.937863Z",
     "iopub.status.busy": "2025-06-21T07:26:10.937502Z",
     "iopub.status.idle": "2025-06-21T07:26:10.953547Z",
     "shell.execute_reply": "2025-06-21T07:26:10.952647Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.937831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different label values: 0 out of 24872\n",
      "Number of rows with different labels: 0 out of 6218\n"
     ]
    }
   ],
   "source": [
    "# Count how many labels differ in total\n",
    "diff_count = np.sum(y_train != acq_y_train)\n",
    "print(f\"Number of different label values: {diff_count} out of {y_train.size}\")\n",
    "\n",
    "# Identify which rows have different labels\n",
    "diff_rows = np.where(np.any(y_train != acq_y_train, axis=1))[0]\n",
    "print(f\"Number of rows with different labels: {len(diff_rows)} out of {len(y_train)}\")\n",
    "\n",
    "# Display a sample of the differences\n",
    "if len(diff_rows) > 0:\n",
    "    sample_size = min(10, len(diff_rows))\n",
    "    sample_indices = diff_rows[:sample_size]\n",
    "    \n",
    "    print(\"\\nSample of differences:\")\n",
    "    for idx in sample_indices:\n",
    "        print(f\"Text: {X_train[idx]}\")\n",
    "        print(f\"Original labels: {y_train[idx]}\")\n",
    "        print(f\"Acquired labels: {acq_y_train[idx]}\")\n",
    "        print(f\"Difference: {y_train[idx] != acq_y_train[idx]}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    # Calculate label-wise differences\n",
    "    label_diffs = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        label_diff = np.sum(y_train[:, i] != acq_y_train[:, i])\n",
    "        label_diffs.append((train_labels[i], label_diff))\n",
    "        \n",
    "    print(\"\\nDifferences by label:\")\n",
    "    for label, diff in label_diffs:\n",
    "        print(f\"{label}: {diff} differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TO VERIFY RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:10.954770Z",
     "iopub.status.busy": "2025-06-21T07:26:10.954504Z",
     "iopub.status.idle": "2025-06-21T07:26:10.971809Z",
     "shell.execute_reply": "2025-06-21T07:26:10.970930Z",
     "shell.execute_reply.started": "2025-06-21T07:26:10.954749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:18.743328Z",
     "iopub.status.busy": "2025-06-21T07:26:18.742944Z",
     "iopub.status.idle": "2025-06-21T07:26:20.046019Z",
     "shell.execute_reply": "2025-06-21T07:26:20.045012Z",
     "shell.execute_reply.started": "2025-06-21T07:26:18.743284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a20a53a26e42f5bb946424a8c4fbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcac749ed7d4bcd8724021aa3938427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6117e6452f42f19540a0a6d9c5ec63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fae81738d241099b5c13fefc2cc300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define custom Dataset class\n",
    "class NetifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:21.182148Z",
     "iopub.status.busy": "2025-06-21T07:26:21.181827Z",
     "iopub.status.idle": "2025-06-21T07:26:21.187847Z",
     "shell.execute_reply": "2025-06-21T07:26:21.186975Z",
     "shell.execute_reply.started": "2025-06-21T07:26:21.182123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik'],\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:27.039167Z",
     "iopub.status.busy": "2025-06-21T07:26:27.038730Z",
     "iopub.status.idle": "2025-06-21T07:26:27.045580Z",
     "shell.execute_reply": "2025-06-21T07:26:27.044534Z",
     "shell.execute_reply.started": "2025-06-21T07:26:27.039129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(train_x, train_y, val_x, val_y, sequence_length, num_workers=4):\n",
    "    train_dataset = NetifierDataset(train_x, train_y, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = NetifierDataset(val_x, val_y, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:29.593591Z",
     "iopub.status.busy": "2025-06-21T07:26:29.593138Z",
     "iopub.status.idle": "2025-06-21T07:26:29.606775Z",
     "shell.execute_reply": "2025-06-21T07:26:29.605586Z",
     "shell.execute_reply.started": "2025-06-21T07:26:29.593548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(sequence_length, model_name, train_x, train_y, val_x, val_y, seed=42):\n",
    "    set_seed(seed)\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(train_labels),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define DataLoaders\n",
    "    train_loader, val_loader = get_dataloaders(train_x, train_y, val_x, val_y, sequence_length)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "        \n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            \n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "        \n",
    "    accelerator.print(f\"\\nAccuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "    accelerator.print(f\"Duration: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:26:51.375808Z",
     "iopub.status.busy": "2025-06-21T07:26:51.375424Z",
     "iopub.status.idle": "2025-06-21T07:26:51.379826Z",
     "shell.execute_reply": "2025-06-21T07:26:51.378730Z",
     "shell.execute_reply.started": "2025-06-21T07:26:51.375781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on sorted original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:27:17.525591Z",
     "iopub.status.busy": "2025-06-21T07:27:17.525202Z",
     "iopub.status.idle": "2025-06-21T07:28:24.847602Z",
     "shell.execute_reply": "2025-06-21T07:28:24.846370Z",
     "shell.execute_reply.started": "2025-06-21T07:27:17.525559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/3, Train Loss: 0.3657, Accuracy: 0.8884, F1 Micro: 0.731, F1 Macro: 0.7274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/3, Train Loss: 0.2359, Accuracy: 0.898, F1 Micro: 0.7718, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/3, Train Loss: 0.2019, Accuracy: 0.9022, F1 Micro: 0.772, F1 Macro: 0.7641\n",
      "\n",
      "Accuracy: 0.9022, F1 Micro: 0.772, F1 Macro: 0.7641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Duration: 61.84658408164978\n"
     ]
    }
   ],
   "source": [
    "args = (96, 'indobenchmark/indobert-base-p1', X_train, y_train, X_val, y_val, seed)\n",
    "notebook_launcher(train_model, args, num_processes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on sorted acquired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:28:31.750990Z",
     "iopub.status.busy": "2025-06-21T07:28:31.750622Z",
     "iopub.status.idle": "2025-06-21T07:29:38.489398Z",
     "shell.execute_reply": "2025-06-21T07:29:38.488531Z",
     "shell.execute_reply.started": "2025-06-21T07:28:31.750960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/3, Train Loss: 0.3657, Accuracy: 0.8884, F1 Micro: 0.731, F1 Macro: 0.7274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/3, Train Loss: 0.2359, Accuracy: 0.898, F1 Micro: 0.7718, F1 Macro: 0.7702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/3, Train Loss: 0.2019, Accuracy: 0.9022, F1 Micro: 0.772, F1 Macro: 0.7641\n",
      "\n",
      "Accuracy: 0.9022, F1 Micro: 0.772, F1 Macro: 0.7641\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          pornografi       0.90      0.91      0.90       370\n",
      "                sara       0.68      0.62      0.65       248\n",
      "         radikalisme       0.75      0.78      0.77       243\n",
      "pencemaran_nama_baik       0.72      0.75      0.73       504\n",
      "\n",
      "           micro avg       0.77      0.78      0.77      1365\n",
      "           macro avg       0.76      0.77      0.76      1365\n",
      "        weighted avg       0.77      0.78      0.77      1365\n",
      "         samples avg       0.45      0.44      0.44      1365\n",
      "\n",
      "Duration: 63.85293936729431\n"
     ]
    }
   ],
   "source": [
    "args = (96, 'indobenchmark/indobert-base-p1', acq_X_train, acq_y_train, X_val, y_val, seed)\n",
    "notebook_launcher(train_model, args, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6053344,
     "sourceId": 9862714,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6852473,
     "sourceId": 11390320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
