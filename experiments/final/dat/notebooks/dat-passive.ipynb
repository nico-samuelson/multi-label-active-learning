{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10027613,"sourceType":"datasetVersion","datasetId":6175435}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.multiprocessing import Manager\nfrom torch.utils.data import DataLoader, Dataset\nfrom accelerate import Accelerator, notebook_launcher\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\nfrom transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:04.814668Z","iopub.execute_input":"2025-04-12T14:15:04.814948Z","iopub.status.idle":"2025-04-12T14:15:15.725195Z","shell.execute_reply.started":"2025-04-12T14:15:04.814927Z","shell.execute_reply":"2025-04-12T14:15:15.724265Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.726572Z","iopub.execute_input":"2025-04-12T14:15:15.727077Z","iopub.status.idle":"2025-04-12T14:15:15.730263Z","shell.execute_reply.started":"2025-04-12T14:15:15.727041Z","shell.execute_reply":"2025-04-12T14:15:15.729649Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.732161Z","iopub.execute_input":"2025-04-12T14:15:15.732449Z","iopub.status.idle":"2025-04-12T14:15:15.760820Z","shell.execute_reply.started":"2025-04-12T14:15:15.732406Z","shell.execute_reply":"2025-04-12T14:15:15.760135Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/doctors-answer-dataset/Indo-Online Health Consultation-Medical Interview-Clean.csv', encoding='latin-1')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.762003Z","iopub.execute_input":"2025-04-12T14:15:15.762269Z","iopub.status.idle":"2025-04-12T14:15:15.901629Z","shell.execute_reply.started":"2025-04-12T14:15:15.762249Z","shell.execute_reply":"2025-04-12T14:15:15.900935Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   No                                             answer  1-FR  2-GI  3-PI  \\\n0   1  Halo Rizal,Radang tenggorokan umunya disebabka...     1     0     1   \n1   2  Halo Hellas,Cacar air merupakan suatu penyakit...     1     0     1   \n2   3  Halo Rory.......Terimakasih atas pertanyaan An...     1     0     1   \n3   4  Alo AfriYani, Terimakasih atas pertanyaannya. ...     1     0     1   \n4   5  Halo,Telinga berdenging atau  tinitus  merupak...     1     0     1   \n\n   4-DM  5-EDTRB  6-RE                                         Text_Clean  \\\n0     1        1     0  halo rizal radang tenggorokan umunya disebabka...   \n1     1        1     0  halo hellas cacar air merupakan suatu penyakit...   \n2     1        1     0  halo rory terimakasih atas pertanyaan anda per...   \n3     1        1     0  alo afriyani terimakasih atas pertanyaannya ku...   \n4     1        1     0  halo telinga berdenging atau tinitus merupakan...   \n\n                                       filtered_text  \\\n0  halo rizal radang tenggorokan umunya disebabka...   \n1  halo hellas cacar air penyakit disebabkan viru...   \n2  halo rory terimakasih ketahui gangguan kulit s...   \n3  alo afriyani terimakasih pertanyaannya kuku ja...   \n4  halo telinga berdenging tinitus sensasi penden...   \n\n                                               token  \\\n0  ['halo', 'rizal', 'radang', 'tenggorokan', 'um...   \n1  ['halo', 'hellas', 'cacar', 'air', 'penyakit',...   \n2  ['halo', 'rory', 'terimakasih', 'ketahui', 'ga...   \n3  ['alo', 'afriyani', 'terimakasih', 'pertanyaan...   \n4  ['halo', 'telinga', 'berdenging', 'tinitus', '...   \n\n                                      tokens_stemmed  \\\n0  ['halo', 'rizal', 'radang', 'tenggorok', 'umu'...   \n1  ['halo', 'hellas', 'cacar', 'air', 'sakit', 's...   \n2  ['halo', 'rory', 'terimakasih', 'tahu', 'gangg...   \n3  ['alo', 'afriyani', 'terimakasih', 'tanya', 'k...   \n4  ['halo', 'telinga', 'denging', 'tinitus', 'sen...   \n\n                                        Process_Data  \n0  halo rizal radang tenggorok umu sebab infeksi ...  \n1  halo hellas cacar air sakit sebab virus varise...  \n2  halo rory terimakasih tahu ganggu kulit rangka...  \n3  alo afriyani terimakasih tanya kuku jari kaki ...  \n4  halo telinga denging tinitus sensasi dengar de...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>answer</th>\n      <th>1-FR</th>\n      <th>2-GI</th>\n      <th>3-PI</th>\n      <th>4-DM</th>\n      <th>5-EDTRB</th>\n      <th>6-RE</th>\n      <th>Text_Clean</th>\n      <th>filtered_text</th>\n      <th>token</th>\n      <th>tokens_stemmed</th>\n      <th>Process_Data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Halo Rizal,Radang tenggorokan umunya disebabka...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo rizal radang tenggorokan umunya disebabka...</td>\n      <td>halo rizal radang tenggorokan umunya disebabka...</td>\n      <td>['halo', 'rizal', 'radang', 'tenggorokan', 'um...</td>\n      <td>['halo', 'rizal', 'radang', 'tenggorok', 'umu'...</td>\n      <td>halo rizal radang tenggorok umu sebab infeksi ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Halo Hellas,Cacar air merupakan suatu penyakit...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo hellas cacar air merupakan suatu penyakit...</td>\n      <td>halo hellas cacar air penyakit disebabkan viru...</td>\n      <td>['halo', 'hellas', 'cacar', 'air', 'penyakit',...</td>\n      <td>['halo', 'hellas', 'cacar', 'air', 'sakit', 's...</td>\n      <td>halo hellas cacar air sakit sebab virus varise...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Halo Rory.......Terimakasih atas pertanyaan An...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo rory terimakasih atas pertanyaan anda per...</td>\n      <td>halo rory terimakasih ketahui gangguan kulit s...</td>\n      <td>['halo', 'rory', 'terimakasih', 'ketahui', 'ga...</td>\n      <td>['halo', 'rory', 'terimakasih', 'tahu', 'gangg...</td>\n      <td>halo rory terimakasih tahu ganggu kulit rangka...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Alo AfriYani, Terimakasih atas pertanyaannya. ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>alo afriyani terimakasih atas pertanyaannya ku...</td>\n      <td>alo afriyani terimakasih pertanyaannya kuku ja...</td>\n      <td>['alo', 'afriyani', 'terimakasih', 'pertanyaan...</td>\n      <td>['alo', 'afriyani', 'terimakasih', 'tanya', 'k...</td>\n      <td>alo afriyani terimakasih tanya kuku jari kaki ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Halo,Telinga berdenging atau  tinitus  merupak...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo telinga berdenging atau tinitus merupakan...</td>\n      <td>halo telinga berdenging tinitus sensasi penden...</td>\n      <td>['halo', 'telinga', 'berdenging', 'tinitus', '...</td>\n      <td>['halo', 'telinga', 'denging', 'tinitus', 'sen...</td>\n      <td>halo telinga denging tinitus sensasi dengar de...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n\nlabels = data.columns[2:8]\n# Extract features and labels for training and validation\nX_train = train_data['Text_Clean'].values\ny_train = train_data[labels].values\nX_val = val_data['Text_Clean'].values\ny_val = val_data[labels].values\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.902356Z","iopub.execute_input":"2025-04-12T14:15:15.902644Z","iopub.status.idle":"2025-04-12T14:15:15.923942Z","shell.execute_reply.started":"2025-04-12T14:15:15.902608Z","shell.execute_reply":"2025-04-12T14:15:15.923071Z"}},"outputs":[{"name":"stdout","text":"(400,) (400, 6)\n(100,) (100, 6)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 8\nLEARNING_RATE = 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.924654Z","iopub.execute_input":"2025-04-12T14:15:15.924850Z","iopub.status.idle":"2025-04-12T14:15:15.939887Z","shell.execute_reply.started":"2025-04-12T14:15:15.924832Z","shell.execute_reply":"2025-04-12T14:15:15.939048Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DoctorAnswerDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.use_float = use_float\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        labels = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n        return item\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:15.940696Z","iopub.execute_input":"2025-04-12T14:15:15.940918Z","iopub.status.idle":"2025-04-12T14:15:16.776518Z","shell.execute_reply.started":"2025-04-12T14:15:15.940890Z","shell.execute_reply":"2025-04-12T14:15:16.775906Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2161d5fe9b4149ff801941060c2c3aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9bc608268944be19a3dcb3a4dfb15ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c552d619893a42a18e0bafa3bf75f9e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e2dc2035564d58ade7318b56600441"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class BertForMultiLabelClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = [2,2,2,2,2,2]\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, i) for i in self.num_labels])\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        subword_to_word_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        sequence_output = self.dropout(outputs[1])\n        logits = []\n        for classifier in self.classifiers:\n            logit = classifier(sequence_output)\n            logits.append(logit)\n\n        \n        logits = [torch.sigmoid(logit) for logit in logits]\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        \n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            total_loss = 0\n            for i, (logit, num_label) in enumerate(zip(logits, self.num_labels)):\n                label = labels[:, i]\n                loss = loss_fct(logit.view(-1, num_label), label.view(-1))\n                total_loss += loss\n\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n# Define compute metrics for evaluation\ndef compute_metrics_multi(p):\n    logits = p.predictions # logits list<tensor(bs, num_label)> ~ list of batch prediction per class \n    label_batch = p.label_ids\n\n    # print(p.predictions)\n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = [torch.topk(torch.tensor(logit, dtype=torch.float), 1)[1] for logit in logits] # list<tensor(bs)>\n    batch_size = label_batch.shape[0]\n    num_label = len(hyp)\n    for i in range(batch_size):\n        hyps = []\n        labels = torch.tensor(label_batch[i,:], dtype=torch.float)\n        for j in range(num_label):\n            hyps.append(hyp[j][i].item())\n\n        hyps = torch.tensor(hyps, dtype=torch.float)\n        list_hyp.append(hyps)\n        list_label.append(labels)\n    \n    accuracy = accuracy_score(list_label, list_hyp)\n    # print(accuracy)\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(list_label, list_hyp, average='micro', zero_division=0)\n    f1_macro = f1_score(list_label, list_hyp, average='macro', zero_division=0)\n\n    # print(classification_report(list_label, list_hyp, zero_division=0, target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik']))\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.778701Z","iopub.execute_input":"2025-04-12T14:15:16.778938Z","iopub.status.idle":"2025-04-12T14:15:16.789069Z","shell.execute_reply.started":"2025-04-12T14:15:16.778917Z","shell.execute_reply":"2025-04-12T14:15:16.788459Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n    labels = torch.tensor(p.label_ids)\n\n    # Hamming accuracy: proportion of correctly predicted labels over total labels\n    accuracy = (preds == labels).float().mean().item()\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n\n    report = classification_report(\n        labels, \n        preds, \n        target_names=['1-FR', '2-GI', '3-PI', '4-DM', '5-EDTRB', '6-RE'],\n        zero_division=0\n    )   \n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'report': report\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.790195Z","iopub.execute_input":"2025-04-12T14:15:16.790394Z","iopub.status.idle":"2025-04-12T14:15:16.819935Z","shell.execute_reply.started":"2025-04-12T14:15:16.790375Z","shell.execute_reply":"2025-04-12T14:15:16.819324Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_dataloaders(sequence_length, num_workers=4):\n    train_dataset = DoctorAnswerDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n    val_dataset = DoctorAnswerDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=num_workers\n    )\n\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.820604Z","iopub.execute_input":"2025-04-12T14:15:16.820788Z","iopub.status.idle":"2025-04-12T14:15:16.847136Z","shell.execute_reply.started":"2025-04-12T14:15:16.820772Z","shell.execute_reply":"2025-04-12T14:15:16.846561Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"manager = Manager()\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.847746Z","iopub.execute_input":"2025-04-12T14:15:16.847985Z","iopub.status.idle":"2025-04-12T14:15:16.903067Z","shell.execute_reply.started":"2025-04-12T14:15:16.847960Z","shell.execute_reply":"2025-04-12T14:15:16.901786Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_model(sequence_length, model_name, metrics, seed=42, layers_freezed=6, trial=1, num_workers=4):\n    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n    device = accelerator.device\n\n    with accelerator.main_process_first():\n        model = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=6,\n            problem_type=\"multi_label_classification\"\n        )\n\n    # Freeze the first few layers of the encoder\n    for name, param in model.named_parameters():\n        if \"encoder.layer\" in name:\n            layer_num = name.split(\".\")[3]\n            try:\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                continue\n\n    # Define DataLoaders\n    train_loader, val_loader = get_dataloaders(sequence_length, num_workers)\n\n    # Define optimizer and loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    # Prepare everything with Accelerator\n    model, optimizer, train_loader, val_loader = accelerator.prepare(\n        model, optimizer, train_loader, val_loader\n    )\n\n    best_result = None\n    start_time = time.time()\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for batch in train_loader:\n            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels']\n\n            optimizer.zero_grad()\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels)\n            accelerator.backward(loss)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n                labels = batch['labels']\n                \n                outputs = model(**inputs)\n                preds = torch.sigmoid(outputs.logits).round()\n\n                # Gather predictions and labels from all devices\n                all_preds.append(accelerator.gather(preds))\n                all_labels.append(accelerator.gather(labels))\n\n        all_preds = torch.cat(all_preds).cpu().numpy()\n        all_labels = torch.cat(all_labels).cpu().numpy()\n        \n        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n\n        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n            accelerator.print(\"Higher F1 achieved, saving model\")\n            best_result = result\n            \n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                f'model-{BATCH_SIZE}-{sequence_length}-{layers_freezed}-{trial}',\n                is_main_process=accelerator.is_main_process,\n                save_function=accelerator.save,\n            )\n\n        accelerator.print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    if accelerator.is_main_process:\n        metrics[0].append(best_result['accuracy'])\n        metrics[1].append(best_result['f1_micro'])\n        metrics[2].append(best_result['f1_macro'])\n        \n    accelerator.print(f\"\\nAccuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n    accelerator.print(best_result['report'])\n    accelerator.print(f\"Duration: {duration}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.904507Z","iopub.execute_input":"2025-04-12T14:15:16.904787Z","iopub.status.idle":"2025-04-12T14:15:16.917753Z","shell.execute_reply.started":"2025-04-12T14:15:16.904766Z","shell.execute_reply":"2025-04-12T14:15:16.916955Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Tokenize each text and calculate their lengths\ntoken_lengths = [len(tokenizer.tokenize(text)) for text in X_train]\n\n# Calculate the average length\naverage_length = sum(token_lengths) / len(token_lengths)\nmax_length = max(token_lengths)\n\nprint(\"Average length of tokenized text:\", average_length)\nprint(\"Max token length:\", max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:16.918730Z","iopub.execute_input":"2025-04-12T14:15:16.919010Z","iopub.status.idle":"2025-04-12T14:15:18.751584Z","shell.execute_reply.started":"2025-04-12T14:15:16.918979Z","shell.execute_reply":"2025-04-12T14:15:18.750653Z"}},"outputs":[{"name":"stdout","text":"Average length of tokenized text: 222.2575\nMax token length: 868\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"counts, bins = np.histogram(token_lengths, range=(0, 500))\nplt.stairs(counts, bins)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:18.752576Z","iopub.execute_input":"2025-04-12T14:15:18.752890Z","iopub.status.idle":"2025-04-12T14:15:19.013230Z","shell.execute_reply.started":"2025-04-12T14:15:18.752865Z","shell.execute_reply":"2025-04-12T14:15:19.012380Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# HYPERPARAMETER TUNING","metadata":{}},{"cell_type":"code","source":"sizes = [4, 8]\nlengths = [192, 256, 300, 364]\n\nused_sizes = []\nused_lengths = []\n\nfor size in sizes:\n    BATCH_SIZE = size\n    for length in lengths:\n        print(\"=========================================================================================\")\n        print(f\"Batch size: {BATCH_SIZE}, sequence length: {length}\")\n        used_sizes.append(BATCH_SIZE)\n        used_lengths.append(length)\n        \n        args = (length, 'indobenchmark/indobert-base-p2', (accuracies, f1_micros, f1_macros), 42, 6, 1)\n        notebook_launcher(train_model, args, num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:15:19.014090Z","iopub.execute_input":"2025-04-12T14:15:19.014350Z","iopub.status.idle":"2025-04-12T14:27:55.311127Z","shell.execute_reply.started":"2025-04-12T14:15:19.014326Z","shell.execute_reply":"2025-04-12T14:27:55.310250Z"}},"outputs":[{"name":"stdout","text":"=========================================================================================\nBatch size: 4, sequence length: 192\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2332, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1577, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1359, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1207, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0843, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.734\nEpoch 6/10, Train Loss: 0.0555, Accuracy: 0.9567, F1 Micro: 0.9672, F1 Macro: 0.7291\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0458, Accuracy: 0.9599, F1 Micro: 0.9695, F1 Macro: 0.7347\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0383, Accuracy: 0.9631, F1 Micro: 0.9719, F1 Macro: 0.8283\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0271, Accuracy: 0.9631, F1 Micro: 0.972, F1 Macro: 0.8106\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.022, Accuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8317\n\nAccuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8317\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       0.60      0.75      0.67         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.97      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.97      0.98      0.98       406\n   macro avg       0.91      0.83      0.83       406\nweighted avg       0.97      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 117.73663091659546\n=========================================================================================\nBatch size: 4, sequence length: 256\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2324, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1583, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1383, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.128, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0953, Accuracy: 0.9679, F1 Micro: 0.9754, F1 Macro: 0.7215\nEpoch 6/10, Train Loss: 0.0618, Accuracy: 0.9647, F1 Micro: 0.9732, F1 Macro: 0.754\nEpoch 7/10, Train Loss: 0.0483, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.7372\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.04, Accuracy: 0.9712, F1 Micro: 0.978, F1 Macro: 0.8328\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0283, Accuracy: 0.9728, F1 Micro: 0.9792, F1 Macro: 0.8644\nEpoch 10/10, Train Loss: 0.0195, Accuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8317\n\nAccuracy: 0.9728, F1 Micro: 0.9792, F1 Macro: 0.8644\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       1.00      0.75      0.86         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.98      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.97      0.99      0.98       406\n   macro avg       0.98      0.83      0.86       406\nweighted avg       0.98      0.99      0.98       406\n samples avg       0.97      0.99      0.98       406\n\nDuration: 101.93196439743042\n=========================================================================================\nBatch size: 4, sequence length: 300\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2322, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1568, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1368, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1305, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0967, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.7198\nEpoch 6/10, Train Loss: 0.0602, Accuracy: 0.9615, F1 Micro: 0.9709, F1 Macro: 0.7444\nEpoch 7/10, Train Loss: 0.0503, Accuracy: 0.9615, F1 Micro: 0.9708, F1 Macro: 0.7523\nEpoch 8/10, Train Loss: 0.0401, Accuracy: 0.9615, F1 Micro: 0.9707, F1 Macro: 0.7617\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0283, Accuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8617\nEpoch 10/10, Train Loss: 0.0201, Accuracy: 0.9631, F1 Micro: 0.972, F1 Macro: 0.8189\n\nAccuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8617\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       1.00      0.75      0.86         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.88      0.97      0.92        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.97      0.98      0.98       406\n   macro avg       0.98      0.83      0.86       406\nweighted avg       0.97      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 101.75135707855225\n=========================================================================================\nBatch size: 4, sequence length: 364\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2358, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1566, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1392, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1327, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0981, Accuracy: 0.9663, F1 Micro: 0.9742, F1 Macro: 0.7206\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0665, Accuracy: 0.9663, F1 Micro: 0.9743, F1 Macro: 0.7382\nEpoch 7/10, Train Loss: 0.053, Accuracy: 0.9599, F1 Micro: 0.9695, F1 Macro: 0.7264\nEpoch 8/10, Train Loss: 0.0433, Accuracy: 0.9615, F1 Micro: 0.9707, F1 Macro: 0.7839\nEpoch 9/10, Train Loss: 0.0303, Accuracy: 0.9631, F1 Micro: 0.9718, F1 Macro: 0.7847\nEpoch 10/10, Train Loss: 0.0233, Accuracy: 0.9615, F1 Micro: 0.9706, F1 Macro: 0.718\n\nAccuracy: 0.9663, F1 Micro: 0.9743, F1 Macro: 0.7382\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       0.50      0.50      0.50         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.98      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.97      0.98      0.97       406\n   macro avg       0.73      0.75      0.74       406\nweighted avg       0.96      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 104.64737963676453\n=========================================================================================\nBatch size: 8, sequence length: 192\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2846, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1597, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1586, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1559, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nEpoch 5/10, Train Loss: 0.1274, Accuracy: 0.9598, F1 Micro: 0.9694, F1 Macro: 0.6512\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1035, Accuracy: 0.9628, F1 Micro: 0.9718, F1 Macro: 0.7376\nEpoch 7/10, Train Loss: 0.0803, Accuracy: 0.9613, F1 Micro: 0.9706, F1 Macro: 0.7003\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0603, Accuracy: 0.9643, F1 Micro: 0.9727, F1 Macro: 0.7447\nEpoch 9/10, Train Loss: 0.0475, Accuracy: 0.9598, F1 Micro: 0.9696, F1 Macro: 0.7427\nHigher F1 achieved, saving model\nEpoch 10/10, Train Loss: 0.0489, Accuracy: 0.9643, F1 Micro: 0.9728, F1 Macro: 0.7676\n\nAccuracy: 0.9643, F1 Micro: 0.9728, F1 Macro: 0.7676\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       112\n        2-GI       0.33      0.25      0.29         4\n        3-PI       1.00      1.00      1.00       112\n        4-DM       0.88      0.98      0.92        94\n     5-EDTRB       0.99      1.00      1.00       111\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.96      0.98      0.97       437\n   macro avg       0.87      0.75      0.77       437\nweighted avg       0.96      0.98      0.97       437\n samples avg       0.97      0.98      0.97       437\n\nDuration: 68.27851438522339\n=========================================================================================\nBatch size: 8, sequence length: 256\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2839, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1572, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1567, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1546, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1244, Accuracy: 0.9628, F1 Micro: 0.9717, F1 Macro: 0.6528\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0983, Accuracy: 0.9643, F1 Micro: 0.9729, F1 Macro: 0.7384\nEpoch 7/10, Train Loss: 0.0833, Accuracy: 0.9643, F1 Micro: 0.9727, F1 Macro: 0.7018\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0554, Accuracy: 0.9702, F1 Micro: 0.9772, F1 Macro: 0.7224\nEpoch 9/10, Train Loss: 0.0473, Accuracy: 0.9598, F1 Micro: 0.9694, F1 Macro: 0.6904\nEpoch 10/10, Train Loss: 0.0479, Accuracy: 0.9613, F1 Micro: 0.9707, F1 Macro: 0.7808\n\nAccuracy: 0.9702, F1 Micro: 0.9772, F1 Macro: 0.7224\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       112\n        2-GI       1.00      0.25      0.40         4\n        3-PI       1.00      1.00      1.00       112\n        4-DM       0.90      0.98      0.94        94\n     5-EDTRB       0.99      1.00      1.00       111\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.97      0.98      0.98       437\n   macro avg       0.82      0.70      0.72       437\nweighted avg       0.97      0.98      0.97       437\n samples avg       0.98      0.98      0.98       437\n\nDuration: 75.2936692237854\n=========================================================================================\nBatch size: 8, sequence length: 300\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2845, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1563, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1569, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1543, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1278, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1077, Accuracy: 0.9702, F1 Micro: 0.9772, F1 Macro: 0.7224\nEpoch 7/10, Train Loss: 0.0917, Accuracy: 0.9673, F1 Micro: 0.975, F1 Macro: 0.7209\nEpoch 8/10, Train Loss: 0.0627, Accuracy: 0.9658, F1 Micro: 0.9738, F1 Macro: 0.7024\nEpoch 9/10, Train Loss: 0.0486, Accuracy: 0.9628, F1 Micro: 0.9718, F1 Macro: 0.7376\nEpoch 10/10, Train Loss: 0.0494, Accuracy: 0.9658, F1 Micro: 0.974, F1 Macro: 0.8034\n\nAccuracy: 0.9702, F1 Micro: 0.9772, F1 Macro: 0.7224\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       112\n        2-GI       1.00      0.25      0.40         4\n        3-PI       1.00      1.00      1.00       112\n        4-DM       0.90      0.98      0.94        94\n     5-EDTRB       0.99      1.00      1.00       111\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.97      0.98      0.98       437\n   macro avg       0.82      0.70      0.72       437\nweighted avg       0.97      0.98      0.97       437\n samples avg       0.98      0.98      0.98       437\n\nDuration: 77.07796621322632\n=========================================================================================\nBatch size: 8, sequence length: 364\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2818, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.158, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1571, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1551, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1282, Accuracy: 0.9598, F1 Micro: 0.9695, F1 Macro: 0.6514\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.1032, Accuracy: 0.9643, F1 Micro: 0.9728, F1 Macro: 0.7019\nEpoch 7/10, Train Loss: 0.0883, Accuracy: 0.9613, F1 Micro: 0.9706, F1 Macro: 0.7003\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0608, Accuracy: 0.9717, F1 Micro: 0.9783, F1 Macro: 0.7232\nEpoch 9/10, Train Loss: 0.0499, Accuracy: 0.9673, F1 Micro: 0.9751, F1 Macro: 0.7384\nEpoch 10/10, Train Loss: 0.0502, Accuracy: 0.9613, F1 Micro: 0.9707, F1 Macro: 0.7201\n\nAccuracy: 0.9717, F1 Micro: 0.9783, F1 Macro: 0.7232\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       112\n        2-GI       1.00      0.25      0.40         4\n        3-PI       1.00      1.00      1.00       112\n        4-DM       0.91      0.98      0.94        94\n     5-EDTRB       0.99      1.00      1.00       111\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.98      0.98      0.98       437\n   macro avg       0.82      0.70      0.72       437\nweighted avg       0.97      0.98      0.97       437\n samples avg       0.98      0.98      0.98       437\n\nDuration: 83.3243157863617\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Batch Size': used_sizes,\n    'Sequence Length': used_lengths,\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'hyperparameters_tuning.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:27:55.312178Z","iopub.execute_input":"2025-04-12T14:27:55.312398Z","iopub.status.idle":"2025-04-12T14:27:55.348963Z","shell.execute_reply.started":"2025-04-12T14:27:55.312378Z","shell.execute_reply":"2025-04-12T14:27:55.348189Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"seeds = [50, 81, 14, 3, 94]\n\naccuracies = manager.list()\nf1_micros = manager.list()\nf1_macros = manager.list()\n\ntrial = 1\nfor seed in seeds:\n    print(\"=====================\")\n    print(\"SEED:\", seed)\n    set_seed(seed)\n    \n    LEARNING_RATE = 2e-5\n    BATCH_SIZE = 4\n    args = (256, 'indobenchmark/indobert-base-p2', (accuracies, f1_micros, f1_macros), seed, 6, trial)\n    \n    notebook_launcher(train_model, args, num_processes=2)\n    trial += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:27:55.349849Z","iopub.execute_input":"2025-04-12T14:27:55.350063Z","iopub.status.idle":"2025-04-12T14:36:44.644043Z","shell.execute_reply.started":"2025-04-12T14:27:55.350032Z","shell.execute_reply":"2025-04-12T14:36:44.643205Z"}},"outputs":[{"name":"stdout","text":"=====================\nSEED: 50\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2346, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1574, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1342, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1244, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0857, Accuracy: 0.9663, F1 Micro: 0.9742, F1 Macro: 0.7206\nEpoch 6/10, Train Loss: 0.0569, Accuracy: 0.9631, F1 Micro: 0.9719, F1 Macro: 0.7215\nEpoch 7/10, Train Loss: 0.0457, Accuracy: 0.9615, F1 Micro: 0.9707, F1 Macro: 0.7207\nEpoch 8/10, Train Loss: 0.0377, Accuracy: 0.9631, F1 Micro: 0.9719, F1 Macro: 0.7872\nEpoch 9/10, Train Loss: 0.0254, Accuracy: 0.9615, F1 Micro: 0.9708, F1 Macro: 0.7865\nEpoch 10/10, Train Loss: 0.0194, Accuracy: 0.9647, F1 Micro: 0.9731, F1 Macro: 0.7881\n\nAccuracy: 0.9663, F1 Micro: 0.9742, F1 Macro: 0.7206\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       1.00      0.25      0.40         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.97      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.97      0.98      0.97       406\n   macro avg       0.81      0.70      0.72       406\nweighted avg       0.96      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 99.32929062843323\n=====================\nSEED: 81\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2383, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1571, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1341, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1289, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0942, Accuracy: 0.9631, F1 Micro: 0.9718, F1 Macro: 0.7015\nHigher F1 achieved, saving model\nEpoch 6/10, Train Loss: 0.0619, Accuracy: 0.9647, F1 Micro: 0.9731, F1 Macro: 0.7539\nEpoch 7/10, Train Loss: 0.0438, Accuracy: 0.9631, F1 Micro: 0.9718, F1 Macro: 0.728\nEpoch 8/10, Train Loss: 0.0382, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.7372\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0264, Accuracy: 0.9663, F1 Micro: 0.9744, F1 Macro: 0.8206\nEpoch 10/10, Train Loss: 0.0202, Accuracy: 0.9631, F1 Micro: 0.9718, F1 Macro: 0.7364\n\nAccuracy: 0.9663, F1 Micro: 0.9744, F1 Macro: 0.8206\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       0.50      0.75      0.60         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.97      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.97      0.98      0.97       406\n   macro avg       0.90      0.83      0.82       406\nweighted avg       0.97      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 102.78597831726074\n=====================\nSEED: 14\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2313, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1559, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1393, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1355, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0994, Accuracy: 0.9631, F1 Micro: 0.9718, F1 Macro: 0.6533\nEpoch 6/10, Train Loss: 0.0608, Accuracy: 0.9567, F1 Micro: 0.9669, F1 Macro: 0.734\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0449, Accuracy: 0.9663, F1 Micro: 0.9744, F1 Macro: 0.755\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0381, Accuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.7502\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0228, Accuracy: 0.9679, F1 Micro: 0.9757, F1 Macro: 0.8135\nEpoch 10/10, Train Loss: 0.022, Accuracy: 0.9679, F1 Micro: 0.9756, F1 Macro: 0.8159\n\nAccuracy: 0.9679, F1 Micro: 0.9757, F1 Macro: 0.8135\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       0.43      0.75      0.55         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      1.00      0.94        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.96      0.99      0.98       406\n   macro avg       0.88      0.83      0.81       406\nweighted avg       0.97      0.99      0.98       406\n samples avg       0.96      0.99      0.97       406\n\nDuration: 105.182532787323\n=====================\nSEED: 3\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2202, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.157, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1391, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1314, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.0928, Accuracy: 0.9696, F1 Micro: 0.9767, F1 Macro: 0.766\nEpoch 6/10, Train Loss: 0.0578, Accuracy: 0.9615, F1 Micro: 0.9707, F1 Macro: 0.7317\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.043, Accuracy: 0.9696, F1 Micro: 0.9767, F1 Macro: 0.766\nHigher F1 achieved, saving model\nEpoch 8/10, Train Loss: 0.0363, Accuracy: 0.9696, F1 Micro: 0.9767, F1 Macro: 0.766\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0216, Accuracy: 0.9696, F1 Micro: 0.9767, F1 Macro: 0.8318\nEpoch 10/10, Train Loss: 0.0169, Accuracy: 0.9663, F1 Micro: 0.9744, F1 Macro: 0.8207\n\nAccuracy: 0.9696, F1 Micro: 0.9767, F1 Macro: 0.8318\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       1.00      0.50      0.67         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.98      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       1.00      0.25      0.40         4\n\n   micro avg       0.97      0.98      0.98       406\n   macro avg       0.98      0.79      0.83       406\nweighted avg       0.97      0.98      0.97       406\n samples avg       0.97      0.98      0.97       406\n\nDuration: 104.35146617889404\n=====================\nSEED: 94\nLaunching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Higher F1 achieved, saving model\nEpoch 1/10, Train Loss: 0.2248, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 2/10, Train Loss: 0.1557, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 3/10, Train Loss: 0.1375, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 4/10, Train Loss: 0.1342, Accuracy: 0.9583, F1 Micro: 0.9684, F1 Macro: 0.651\nHigher F1 achieved, saving model\nEpoch 5/10, Train Loss: 0.1096, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.7197\nEpoch 6/10, Train Loss: 0.0653, Accuracy: 0.9599, F1 Micro: 0.9695, F1 Macro: 0.7364\nHigher F1 achieved, saving model\nEpoch 7/10, Train Loss: 0.0426, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.6541\nEpoch 8/10, Train Loss: 0.0382, Accuracy: 0.9647, F1 Micro: 0.973, F1 Macro: 0.7632\nHigher F1 achieved, saving model\nEpoch 9/10, Train Loss: 0.0221, Accuracy: 0.9647, F1 Micro: 0.9732, F1 Macro: 0.7459\nEpoch 10/10, Train Loss: 0.0184, Accuracy: 0.9615, F1 Micro: 0.9706, F1 Macro: 0.7354\n\nAccuracy: 0.9647, F1 Micro: 0.9732, F1 Macro: 0.7459\n              precision    recall  f1-score   support\n\n        1-FR       1.00      1.00      1.00       104\n        2-GI       0.43      0.75      0.55         4\n        3-PI       1.00      1.00      1.00       104\n        4-DM       0.89      0.99      0.93        87\n     5-EDTRB       0.99      1.00      1.00       103\n        6-RE       0.00      0.00      0.00         4\n\n   micro avg       0.96      0.99      0.97       406\n   macro avg       0.72      0.79      0.75       406\nweighted avg       0.96      0.99      0.97       406\n samples avg       0.96      0.99      0.97       406\n\nDuration: 103.12986946105957\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Trial': [1,2,3,4,5],\n    'Accuracy': list(accuracies),\n    'F1 Micro': list(f1_micros),\n    'F1 Macro': list(f1_macros),\n})\n\nresults.to_csv(f'dat-passive-results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:36:44.645065Z","iopub.execute_input":"2025-04-12T14:36:44.645295Z","iopub.status.idle":"2025-04-12T14:36:44.654881Z","shell.execute_reply.started":"2025-04-12T14:36:44.645273Z","shell.execute_reply":"2025-04-12T14:36:44.654326Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}